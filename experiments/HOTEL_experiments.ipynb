{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8585402,
          "sourceType": "datasetVersion",
          "datasetId": 5134904
        }
      ],
      "dockerImageVersionId": 30716,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Defining the model\n",
        "\n",
        "model = social_stgcnn(n_stgcnn =args.n_stgcnn,n_txpcnn=args.n_txpcnn,\n",
        "output_feat=args.output_size,seq_len=args.obs_seq_len,\n",
        "kernel_size=args.kernel_size,pred_seq_len=args.pred_seq_len)\n",
        "\n",
        "\n",
        "#Training settings\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(),lr=args.lr)\n",
        "\n",
        "if args.use_lrschd:\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_sh_rate, gamma=0.1)\n",
        "\n",
        "\n",
        "\n",
        "checkpoint_dir = './checkpoint/'+args.tag+'/'\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "with open(checkpoint_dir+'args.pkl', 'wb') as fp:\n",
        "    pickle.dump(args, fp)\n",
        "\n",
        "\n",
        "\n",
        "print('Data and model loaded')\n",
        "print('Checkpoint dir:', checkpoint_dir)\n",
        "\n",
        "#Training\n",
        "metrics = {'train_loss':[],  'val_loss':[]}\n",
        "constant_metrics = {'min_val_epoch':-1, 'min_val_loss':9999999999999999}\n",
        "\n",
        "def train(epoch):\n",
        "    global metrics,loader_train\n",
        "    model.train()\n",
        "    loss_batch = 0\n",
        "    batch_count = 0\n",
        "    is_fst_loss = True\n",
        "    loader_len = len(loader_train)\n",
        "    turn_point =int(loader_len/args.batch_size)*args.batch_size+ loader_len%args.batch_size -1\n",
        "\n",
        "\n",
        "    for cnt,batch in enumerate(loader_train):\n",
        "        batch_count+=1\n",
        "\n",
        "        #Get data\n",
        "        batch = [tensor for tensor in batch]\n",
        "        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, non_linear_ped,\\\n",
        "         loss_mask,V_obs,A_obs,A_dir_obs, V_tr,A_tr,A_dir_tr = batch\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        #Forward\n",
        "        #V_obs = batch,seq,node,feat\n",
        "        #V_obs_tmp = batch,feat,seq,node\n",
        "        V_obs_tmp =V_obs.permute(0,3,1,2)\n",
        "\n",
        "        V_pred,_,_ = model(V_obs_tmp,A_obs.squeeze(), A_dir_obs.squeeze())\n",
        "\n",
        "        V_pred = V_pred.permute(0,2,3,1)\n",
        "\n",
        "\n",
        "\n",
        "        V_tr = V_tr.squeeze()\n",
        "        A_tr = A_tr.squeeze()\n",
        "        A_dir_tr = A_dir_tr.squeeze()\n",
        "        V_pred = V_pred.squeeze()\n",
        "\n",
        "        if batch_count%args.batch_size !=0 and cnt != turn_point :\n",
        "            l = graph_loss(V_pred,V_tr)\n",
        "            if is_fst_loss :\n",
        "                loss = l\n",
        "                is_fst_loss = False\n",
        "            else:\n",
        "                loss += l\n",
        "\n",
        "        else:\n",
        "            loss = loss/args.batch_size\n",
        "            is_fst_loss = True\n",
        "            loss.backward()\n",
        "\n",
        "            if args.clip_grad is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(),args.clip_grad)\n",
        "\n",
        "\n",
        "            optimizer.step()\n",
        "            #Metrics\n",
        "            loss_batch += loss.item()\n",
        "            print('TRAIN:','\\t Epoch:', epoch,'\\t Loss:',loss_batch/batch_count)\n",
        "\n",
        "    metrics['train_loss'].append(loss_batch/batch_count)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def vald(epoch):\n",
        "    global metrics,loader_val,constant_metrics\n",
        "    model.eval()\n",
        "    loss_batch = 0\n",
        "    batch_count = 0\n",
        "    is_fst_loss = True\n",
        "    loader_len = len(loader_val)\n",
        "    turn_point =int(loader_len/args.batch_size)*args.batch_size+ loader_len%args.batch_size -1\n",
        "\n",
        "    for cnt,batch in enumerate(loader_val):\n",
        "        batch_count+=1\n",
        "\n",
        "        #Get data\n",
        "        batch = [tensor for tensor in batch]\n",
        "        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, non_linear_ped,\\\n",
        "         loss_mask,V_obs,A_obs,A_dir_obs, V_tr,A_tr,A_dir_tr = batch\n",
        "\n",
        "\n",
        "        V_obs_tmp =V_obs.permute(0,3,1,2)\n",
        "\n",
        "        V_pred,_,_ = model(V_obs_tmp,A_obs.squeeze(), A_dir_obs.squeeze())\n",
        "\n",
        "        V_pred = V_pred.permute(0,2,3,1)\n",
        "\n",
        "        V_tr = V_tr.squeeze()\n",
        "        A_tr = A_tr.squeeze()\n",
        "        A_dir_tr = A_dir_tr.squeeze()\n",
        "        V_pred = V_pred.squeeze()\n",
        "\n",
        "        if batch_count%args.batch_size !=0 and cnt != turn_point :\n",
        "            l = graph_loss(V_pred,V_tr)\n",
        "            if is_fst_loss :\n",
        "                loss = l\n",
        "                is_fst_loss = False\n",
        "            else:\n",
        "                loss += l\n",
        "\n",
        "        else:\n",
        "            loss = loss/args.batch_size\n",
        "            is_fst_loss = True\n",
        "            #Metrics\n",
        "            loss_batch += loss.item()\n",
        "            print('VALD:','\\t Epoch:', epoch,'\\t Loss:',loss_batch/batch_count)\n",
        "\n",
        "    metrics['val_loss'].append(loss_batch/batch_count)\n",
        "\n",
        "    if  metrics['val_loss'][-1]< constant_metrics['min_val_loss']:\n",
        "        constant_metrics['min_val_loss'] =  metrics['val_loss'][-1]\n",
        "        constant_metrics['min_val_epoch'] = epoch\n",
        "        torch.save(model.state_dict(),checkpoint_dir+'val_best.pth')  # OK\n",
        "        check_test_performance()\n",
        "\n",
        "print('Training started ...')\n",
        "for epoch in range(args.num_epochs):\n",
        "    train(epoch)\n",
        "    vald(epoch)\n",
        "    if args.use_lrschd:\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "    print('*'*30)\n",
        "    print('Epoch:',args.tag,\":\", epoch)\n",
        "    for k,v in metrics.items():\n",
        "        if len(v)>0:\n",
        "            print(k,v[-1])\n",
        "\n",
        "\n",
        "    print(constant_metrics)\n",
        "    print('*'*30)\n",
        "\n",
        "    with open(checkpoint_dir+'metrics.pkl', 'wb') as fp:\n",
        "        pickle.dump(metrics, fp)\n",
        "\n",
        "    with open(checkpoint_dir+'constant_metrics.pkl', 'wb') as fp:\n",
        "        pickle.dump(constant_metrics, fp)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T16:25:13.331154Z",
          "iopub.execute_input": "2024-06-03T16:25:13.331506Z",
          "iopub.status.idle": "2024-06-03T20:06:46.889013Z",
          "shell.execute_reply.started": "2024-06-03T16:25:13.331480Z",
          "shell.execute_reply": "2024-06-03T20:06:46.887773Z"
        },
        "trusted": true,
        "id": "fDOUJ8BdbTbb",
        "outputId": "bdf1c3d1-1d29-4a52-e557-6d079d73e8c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Data and model loaded\nCheckpoint dir: ./checkpoint/social-tag/\nTraining started ...\nTRAIN: \t Epoch: 0 \t Loss: 0.01424446888267994\nTRAIN: \t Epoch: 0 \t Loss: 0.014114801306277514\nTRAIN: \t Epoch: 0 \t Loss: 0.013962577717999617\nTRAIN: \t Epoch: 0 \t Loss: 0.013809907250106335\nTRAIN: \t Epoch: 0 \t Loss: 0.01364209335297346\nTRAIN: \t Epoch: 0 \t Loss: 0.013494737756748995\nTRAIN: \t Epoch: 0 \t Loss: 0.013324068859219551\nTRAIN: \t Epoch: 0 \t Loss: 0.01315830263774842\nTRAIN: \t Epoch: 0 \t Loss: 0.012993430201378133\nTRAIN: \t Epoch: 0 \t Loss: 0.012832795176655054\nTRAIN: \t Epoch: 0 \t Loss: 0.012663833204318176\nTRAIN: \t Epoch: 0 \t Loss: 0.012502921046689153\nTRAIN: \t Epoch: 0 \t Loss: 0.012340806567898164\nTRAIN: \t Epoch: 0 \t Loss: 0.012168123785938536\nTRAIN: \t Epoch: 0 \t Loss: 0.01200572041173776\nTRAIN: \t Epoch: 0 \t Loss: 0.011846053239423782\nTRAIN: \t Epoch: 0 \t Loss: 0.011687614461954902\nTRAIN: \t Epoch: 0 \t Loss: 0.011515091121610668\nTRAIN: \t Epoch: 0 \t Loss: 0.01134265934754359\nTRAIN: \t Epoch: 0 \t Loss: 0.011178971827030182\nTRAIN: \t Epoch: 0 \t Loss: 0.011134542381202798\nVALD: \t Epoch: 0 \t Loss: 0.011265235021710396\nVALD: \t Epoch: 0 \t Loss: 0.012266531586647034\nVALD: \t Epoch: 0 \t Loss: 0.021288984765609104\nVALD: \t Epoch: 0 \t Loss: 0.029730469919741154\nVALD: \t Epoch: 0 \t Loss: 0.028863569004716122\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.48it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 1.381831180121023  FDE: 1.713589116786552\n**************************************************\n******************************\nEpoch: social-tag : 0\ntrain_loss 0.011134542381202798\nval_loss 0.028863569004716122\n{'min_val_epoch': 0, 'min_val_loss': 0.028863569004716122}\n******************************\nTRAIN: \t Epoch: 1 \t Loss: 0.007916895672678947\nTRAIN: \t Epoch: 1 \t Loss: 0.007647848920896649\nTRAIN: \t Epoch: 1 \t Loss: 0.0074487597060700255\nTRAIN: \t Epoch: 1 \t Loss: 0.007221706444397569\nTRAIN: \t Epoch: 1 \t Loss: 0.007074274029582739\nTRAIN: \t Epoch: 1 \t Loss: 0.007005832623690367\nTRAIN: \t Epoch: 1 \t Loss: 0.006859700860721725\nTRAIN: \t Epoch: 1 \t Loss: 0.006744254671502858\nTRAIN: \t Epoch: 1 \t Loss: 0.006622018292546272\nTRAIN: \t Epoch: 1 \t Loss: 0.006487485114485025\nTRAIN: \t Epoch: 1 \t Loss: 0.006371751554648985\nTRAIN: \t Epoch: 1 \t Loss: 0.006249602107952039\nTRAIN: \t Epoch: 1 \t Loss: 0.006155698810918973\nTRAIN: \t Epoch: 1 \t Loss: 0.006052422636587705\nTRAIN: \t Epoch: 1 \t Loss: 0.005968943238258362\nTRAIN: \t Epoch: 1 \t Loss: 0.00586632639169693\nTRAIN: \t Epoch: 1 \t Loss: 0.005753936328213005\nTRAIN: \t Epoch: 1 \t Loss: 0.005648983254407843\nTRAIN: \t Epoch: 1 \t Loss: 0.00555632240138948\nTRAIN: \t Epoch: 1 \t Loss: 0.005459208437241614\nTRAIN: \t Epoch: 1 \t Loss: 0.005442202183128598\nVALD: \t Epoch: 1 \t Loss: 0.0036773718893527985\nVALD: \t Epoch: 1 \t Loss: 0.004173816880211234\nVALD: \t Epoch: 1 \t Loss: 0.007112406349430482\nVALD: \t Epoch: 1 \t Loss: 0.009622044512070715\nVALD: \t Epoch: 1 \t Loss: 0.009833550779523866\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.22it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 1.1641308828307921  FDE: 1.4924957926160711\n**************************************************\n******************************\nEpoch: social-tag : 1\ntrain_loss 0.005442202183128598\nval_loss 0.009833550779523866\n{'min_val_epoch': 1, 'min_val_loss': 0.009833550779523866}\n******************************\nTRAIN: \t Epoch: 2 \t Loss: 0.0035637456458061934\nTRAIN: \t Epoch: 2 \t Loss: 0.003337160567753017\nTRAIN: \t Epoch: 2 \t Loss: 0.003440725772331158\nTRAIN: \t Epoch: 2 \t Loss: 0.0035766358487308025\nTRAIN: \t Epoch: 2 \t Loss: 0.003508110577240586\nTRAIN: \t Epoch: 2 \t Loss: 0.003302373535310229\nTRAIN: \t Epoch: 2 \t Loss: 0.0031825840872313294\nTRAIN: \t Epoch: 2 \t Loss: 0.0030686432728543878\nTRAIN: \t Epoch: 2 \t Loss: 0.003120854057164656\nTRAIN: \t Epoch: 2 \t Loss: 0.0032846756046637894\nTRAIN: \t Epoch: 2 \t Loss: 0.003250789257105101\nTRAIN: \t Epoch: 2 \t Loss: 0.0031721413639994958\nTRAIN: \t Epoch: 2 \t Loss: 0.0030960540036455942\nTRAIN: \t Epoch: 2 \t Loss: 0.0031300921026351197\nTRAIN: \t Epoch: 2 \t Loss: 0.0033234219532459973\nTRAIN: \t Epoch: 2 \t Loss: 0.00336972092918586\nTRAIN: \t Epoch: 2 \t Loss: 0.0033518016091821823\nTRAIN: \t Epoch: 2 \t Loss: 0.0032508463353022104\nTRAIN: \t Epoch: 2 \t Loss: 0.0031352834224602894\nTRAIN: \t Epoch: 2 \t Loss: 0.003041140822460875\nTRAIN: \t Epoch: 2 \t Loss: 0.003028253066075611\nVALD: \t Epoch: 2 \t Loss: 0.0009603957878425717\nVALD: \t Epoch: 2 \t Loss: 0.00200614653294906\nVALD: \t Epoch: 2 \t Loss: 0.0049277723689253134\nVALD: \t Epoch: 2 \t Loss: 0.00781866765464656\nVALD: \t Epoch: 2 \t Loss: 0.007786622633872592\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:23<00:00, 12.70it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 1.145014887523967  FDE: 1.5905012437169346\n**************************************************\n******************************\nEpoch: social-tag : 2\ntrain_loss 0.003028253066075611\nval_loss 0.007786622633872592\n{'min_val_epoch': 2, 'min_val_loss': 0.007786622633872592}\n******************************\nTRAIN: \t Epoch: 3 \t Loss: 0.0009723652619868517\nTRAIN: \t Epoch: 3 \t Loss: 0.00096656393725425\nTRAIN: \t Epoch: 3 \t Loss: 0.0009906327662368615\nTRAIN: \t Epoch: 3 \t Loss: 0.0012046998599544168\nTRAIN: \t Epoch: 3 \t Loss: 0.0020803003571927547\nTRAIN: \t Epoch: 3 \t Loss: 0.0023748271632939577\nTRAIN: \t Epoch: 3 \t Loss: 0.0023846159622605357\nTRAIN: \t Epoch: 3 \t Loss: 0.00229964817117434\nTRAIN: \t Epoch: 3 \t Loss: 0.0021142519997536307\nTRAIN: \t Epoch: 3 \t Loss: 0.001886298724275548\nTRAIN: \t Epoch: 3 \t Loss: 0.0017747043549950997\nTRAIN: \t Epoch: 3 \t Loss: 0.0016168685372880038\nTRAIN: \t Epoch: 3 \t Loss: 0.0015316526024924735\nTRAIN: \t Epoch: 3 \t Loss: 0.0017423581843364186\nTRAIN: \t Epoch: 3 \t Loss: 0.0019587996396391344\nTRAIN: \t Epoch: 3 \t Loss: 0.002076027083603549\nTRAIN: \t Epoch: 3 \t Loss: 0.002123276797219125\nTRAIN: \t Epoch: 3 \t Loss: 0.0021112414591091997\nTRAIN: \t Epoch: 3 \t Loss: 0.002031209766803505\nTRAIN: \t Epoch: 3 \t Loss: 0.0019155045185470954\nTRAIN: \t Epoch: 3 \t Loss: 0.0018925833696773766\nVALD: \t Epoch: 3 \t Loss: -0.002232413971796632\nVALD: \t Epoch: 3 \t Loss: -0.002263276488520205\nVALD: \t Epoch: 3 \t Loss: -0.0011306755089511473\nVALD: \t Epoch: 3 \t Loss: -0.000989906708127819\nVALD: \t Epoch: 3 \t Loss: -0.0007397370133998889\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:23<00:00, 12.73it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 1.1051843891728832  FDE: 1.6153221274132372\n**************************************************\n******************************\nEpoch: social-tag : 3\ntrain_loss 0.0018925833696773766\nval_loss -0.0007397370133998889\n{'min_val_epoch': 3, 'min_val_loss': -0.0007397370133998889}\n******************************\nTRAIN: \t Epoch: 4 \t Loss: -0.00015849637566134334\nTRAIN: \t Epoch: 4 \t Loss: -8.775253627391066e-05\nTRAIN: \t Epoch: 4 \t Loss: 4.923441277545256e-05\nTRAIN: \t Epoch: 4 \t Loss: 0.0015840628520891187\nTRAIN: \t Epoch: 4 \t Loss: 0.0020576724120473953\nTRAIN: \t Epoch: 4 \t Loss: 0.002171884002867349\nTRAIN: \t Epoch: 4 \t Loss: 0.0020509381039508817\nTRAIN: \t Epoch: 4 \t Loss: 0.0018487235961401893\nTRAIN: \t Epoch: 4 \t Loss: 0.0016331262648034478\nTRAIN: \t Epoch: 4 \t Loss: 0.0013884489617339568\nTRAIN: \t Epoch: 4 \t Loss: 0.0012733976410262667\nTRAIN: \t Epoch: 4 \t Loss: 0.0014791370961878176\nTRAIN: \t Epoch: 4 \t Loss: 0.0016362760118052112\nTRAIN: \t Epoch: 4 \t Loss: 0.0016549569546831272\nTRAIN: \t Epoch: 4 \t Loss: 0.0015932854994995674\nTRAIN: \t Epoch: 4 \t Loss: 0.0014862857317439193\nTRAIN: \t Epoch: 4 \t Loss: 0.001371642865920234\nTRAIN: \t Epoch: 4 \t Loss: 0.0012539818904365853\nTRAIN: \t Epoch: 4 \t Loss: 0.0011896369276081188\nTRAIN: \t Epoch: 4 \t Loss: 0.0012797532634067465\nTRAIN: \t Epoch: 4 \t Loss: 0.0012819660914043624\nVALD: \t Epoch: 4 \t Loss: -0.0008247854420915246\nVALD: \t Epoch: 4 \t Loss: -0.0007858074968680739\nVALD: \t Epoch: 4 \t Loss: 0.00010545431481053431\nVALD: \t Epoch: 4 \t Loss: 0.00024443090660497546\nVALD: \t Epoch: 4 \t Loss: 0.0003252535242581329\n******************************\nEpoch: social-tag : 4\ntrain_loss 0.0012819660914043624\nval_loss 0.0003252535242581329\n{'min_val_epoch': 3, 'min_val_loss': -0.0007397370133998889}\n******************************\nTRAIN: \t Epoch: 5 \t Loss: 0.0012353610945865512\nTRAIN: \t Epoch: 5 \t Loss: 0.00044731707021128386\nTRAIN: \t Epoch: 5 \t Loss: -6.0764073471849166e-05\nTRAIN: \t Epoch: 5 \t Loss: -0.0004165172649663873\nTRAIN: \t Epoch: 5 \t Loss: -0.0005604565201792866\nTRAIN: \t Epoch: 5 \t Loss: 7.148331011800717e-05\nTRAIN: \t Epoch: 5 \t Loss: 0.0007136634708980896\nTRAIN: \t Epoch: 5 \t Loss: 0.0010498186129552778\nTRAIN: \t Epoch: 5 \t Loss: 0.0012085434637912032\nTRAIN: \t Epoch: 5 \t Loss: 0.0012117231584852562\nTRAIN: \t Epoch: 5 \t Loss: 0.0010880978727734393\nTRAIN: \t Epoch: 5 \t Loss: 0.0009093253477961601\nTRAIN: \t Epoch: 5 \t Loss: 0.0007094995828363328\nTRAIN: \t Epoch: 5 \t Loss: 0.0005398911900036703\nTRAIN: \t Epoch: 5 \t Loss: 0.0004214779793983325\nTRAIN: \t Epoch: 5 \t Loss: 0.00037639944912370993\nTRAIN: \t Epoch: 5 \t Loss: 0.0005658891022903845\nTRAIN: \t Epoch: 5 \t Loss: 0.0006039603702245384\nTRAIN: \t Epoch: 5 \t Loss: 0.0005882851278568667\nTRAIN: \t Epoch: 5 \t Loss: 0.0005273766822938341\nTRAIN: \t Epoch: 5 \t Loss: 0.0004913654004610679\nVALD: \t Epoch: 5 \t Loss: -0.0038842917419970036\nVALD: \t Epoch: 5 \t Loss: -0.004284855676814914\nVALD: \t Epoch: 5 \t Loss: -0.0033155661076307297\nVALD: \t Epoch: 5 \t Loss: -0.0033926719333976507\nVALD: \t Epoch: 5 \t Loss: -0.003041775234271555\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:25<00:00, 11.61it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 1.0833316552155905  FDE: 1.597284087800337\n**************************************************\n******************************\nEpoch: social-tag : 5\ntrain_loss 0.0004913654004610679\nval_loss -0.003041775234271555\n{'min_val_epoch': 5, 'min_val_loss': -0.003041775234271555}\n******************************\nTRAIN: \t Epoch: 6 \t Loss: -0.002089714165776968\nTRAIN: \t Epoch: 6 \t Loss: -0.002285504247993231\nTRAIN: \t Epoch: 6 \t Loss: -0.0022800499573349953\nTRAIN: \t Epoch: 6 \t Loss: -0.0021316208294592798\nTRAIN: \t Epoch: 6 \t Loss: -0.001755381841212511\nTRAIN: \t Epoch: 6 \t Loss: -0.0010221051052212715\nTRAIN: \t Epoch: 6 \t Loss: -0.0007641627619575177\nTRAIN: \t Epoch: 6 \t Loss: -0.0007169455166149419\nTRAIN: \t Epoch: 6 \t Loss: -0.0008365149034135458\nTRAIN: \t Epoch: 6 \t Loss: -0.0009757804771652445\nTRAIN: \t Epoch: 6 \t Loss: -0.0010789597952018746\nTRAIN: \t Epoch: 6 \t Loss: -0.0011781938252776551\nTRAIN: \t Epoch: 6 \t Loss: -0.0011034025228582323\nTRAIN: \t Epoch: 6 \t Loss: -0.0008906919351180217\nTRAIN: \t Epoch: 6 \t Loss: -0.0008461942585806052\nTRAIN: \t Epoch: 6 \t Loss: -0.0008587016418459825\nTRAIN: \t Epoch: 6 \t Loss: -0.0009173559066017761\nTRAIN: \t Epoch: 6 \t Loss: -0.00100820241884018\nTRAIN: \t Epoch: 6 \t Loss: -0.0010656038679084496\nTRAIN: \t Epoch: 6 \t Loss: -0.0010217457020189613\nTRAIN: \t Epoch: 6 \t Loss: -0.0009925096750305355\nVALD: \t Epoch: 6 \t Loss: -0.001825986779294908\nVALD: \t Epoch: 6 \t Loss: -0.0020673799444921315\nVALD: \t Epoch: 6 \t Loss: -0.0011788018940327067\nVALD: \t Epoch: 6 \t Loss: -0.0009758176893228665\nVALD: \t Epoch: 6 \t Loss: -0.0009532461899682135\n******************************\nEpoch: social-tag : 6\ntrain_loss -0.0009925096750305355\nval_loss -0.0009532461899682135\n{'min_val_epoch': 5, 'min_val_loss': -0.003041775234271555}\n******************************\nTRAIN: \t Epoch: 7 \t Loss: -0.0004768539220094681\nTRAIN: \t Epoch: 7 \t Loss: -0.0010944046662189066\nTRAIN: \t Epoch: 7 \t Loss: -0.0015475035567457478\nTRAIN: \t Epoch: 7 \t Loss: -0.0018368459132034332\nTRAIN: \t Epoch: 7 \t Loss: -0.0017841001739725471\nTRAIN: \t Epoch: 7 \t Loss: -0.0010591132837968569\nTRAIN: \t Epoch: 7 \t Loss: -0.0006645395353968654\nTRAIN: \t Epoch: 7 \t Loss: -0.0005491555457410868\nTRAIN: \t Epoch: 7 \t Loss: -0.0005474196223076433\nTRAIN: \t Epoch: 7 \t Loss: -0.0006618009501835331\nTRAIN: \t Epoch: 7 \t Loss: -0.0008308989794882523\nTRAIN: \t Epoch: 7 \t Loss: -0.0009767067846648085\nTRAIN: \t Epoch: 7 \t Loss: -0.0010803025462127363\nTRAIN: \t Epoch: 7 \t Loss: -0.0008139487451574366\nTRAIN: \t Epoch: 7 \t Loss: -0.0006666609299524377\nTRAIN: \t Epoch: 7 \t Loss: -0.000648963832645677\nTRAIN: \t Epoch: 7 \t Loss: -0.0007162118563428521\nTRAIN: \t Epoch: 7 \t Loss: -0.0008140087916722728\nTRAIN: \t Epoch: 7 \t Loss: -0.0009306572978139707\nTRAIN: \t Epoch: 7 \t Loss: -0.0009597114520147442\nTRAIN: \t Epoch: 7 \t Loss: -0.0009219085621024979\nVALD: \t Epoch: 7 \t Loss: -0.0022223147097975016\nVALD: \t Epoch: 7 \t Loss: -0.0024001981364563107\nVALD: \t Epoch: 7 \t Loss: -0.0018130858079530299\nVALD: \t Epoch: 7 \t Loss: -0.0018699623615248129\nVALD: \t Epoch: 7 \t Loss: -0.0018000179562783663\n******************************\nEpoch: social-tag : 7\ntrain_loss -0.0009219085621024979\nval_loss -0.0018000179562783663\n{'min_val_epoch': 5, 'min_val_loss': -0.003041775234271555}\n******************************\nTRAIN: \t Epoch: 8 \t Loss: -0.000995576847344637\nTRAIN: \t Epoch: 8 \t Loss: -0.0015913823153823614\nTRAIN: \t Epoch: 8 \t Loss: -0.0021992297066996493\nTRAIN: \t Epoch: 8 \t Loss: -0.0024220815976150334\nTRAIN: \t Epoch: 8 \t Loss: -0.002416751300916076\nTRAIN: \t Epoch: 8 \t Loss: -0.0024019326083362103\nTRAIN: \t Epoch: 8 \t Loss: -0.0022667545958289076\nTRAIN: \t Epoch: 8 \t Loss: -0.0020617755144485272\nTRAIN: \t Epoch: 8 \t Loss: -0.0020092424949527616\nTRAIN: \t Epoch: 8 \t Loss: -0.002126282505923882\nTRAIN: \t Epoch: 8 \t Loss: -0.0022860110832632267\nTRAIN: \t Epoch: 8 \t Loss: -0.0024317957625801987\nTRAIN: \t Epoch: 8 \t Loss: -0.002339893643063708\nTRAIN: \t Epoch: 8 \t Loss: -0.0016249837145941065\nTRAIN: \t Epoch: 8 \t Loss: -0.0013183446717448533\nTRAIN: \t Epoch: 8 \t Loss: -0.0011164852585352492\nTRAIN: \t Epoch: 8 \t Loss: -0.0010280267520369414\nTRAIN: \t Epoch: 8 \t Loss: -0.0010137914375971174\nTRAIN: \t Epoch: 8 \t Loss: -0.0010684535893807677\nTRAIN: \t Epoch: 8 \t Loss: -0.001166926117730327\nTRAIN: \t Epoch: 8 \t Loss: -0.001198762153191298\nVALD: \t Epoch: 8 \t Loss: -0.0034014848060905933\nVALD: \t Epoch: 8 \t Loss: -0.003866081591695547\nVALD: \t Epoch: 8 \t Loss: -0.002789847746801873\nVALD: \t Epoch: 8 \t Loss: -0.003053431719308719\nVALD: \t Epoch: 8 \t Loss: -0.0025839938897633512\n******************************\nEpoch: social-tag : 8\ntrain_loss -0.001198762153191298\nval_loss -0.0025839938897633512\n{'min_val_epoch': 5, 'min_val_loss': -0.003041775234271555}\n******************************\nTRAIN: \t Epoch: 9 \t Loss: -0.0031202100217342377\nTRAIN: \t Epoch: 9 \t Loss: -0.003185701905749738\nTRAIN: \t Epoch: 9 \t Loss: -0.0033580755504469075\nTRAIN: \t Epoch: 9 \t Loss: -0.0030683435034006834\nTRAIN: \t Epoch: 9 \t Loss: -0.001815028442069888\nTRAIN: \t Epoch: 9 \t Loss: -0.0013643097772728652\nTRAIN: \t Epoch: 9 \t Loss: -0.0012620550114661455\nTRAIN: \t Epoch: 9 \t Loss: -0.0012681998632615432\nTRAIN: \t Epoch: 9 \t Loss: -0.0014387646353700096\nTRAIN: \t Epoch: 9 \t Loss: -0.0016461396240629255\nTRAIN: \t Epoch: 9 \t Loss: -0.0018256603718989275\nTRAIN: \t Epoch: 9 \t Loss: -0.001984937203815207\nTRAIN: \t Epoch: 9 \t Loss: -0.0020938773478309694\nTRAIN: \t Epoch: 9 \t Loss: -0.001973638456547633\nTRAIN: \t Epoch: 9 \t Loss: -0.0018270172954847415\nTRAIN: \t Epoch: 9 \t Loss: -0.001845008780946955\nTRAIN: \t Epoch: 9 \t Loss: -0.001925130278858192\nTRAIN: \t Epoch: 9 \t Loss: -0.0019957252208971316\nTRAIN: \t Epoch: 9 \t Loss: -0.002100869866186067\nTRAIN: \t Epoch: 9 \t Loss: -0.0022061993833631275\nTRAIN: \t Epoch: 9 \t Loss: -0.0022153303963502002\nVALD: \t Epoch: 9 \t Loss: -0.004275770857930183\nVALD: \t Epoch: 9 \t Loss: -0.004930534400045872\nVALD: \t Epoch: 9 \t Loss: -0.0035838164621964097\nVALD: \t Epoch: 9 \t Loss: -0.00364724182873033\nVALD: \t Epoch: 9 \t Loss: -0.003109062351466377\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.47it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 1.0755327069785858  FDE: 1.677143288114584\n**************************************************\n******************************\nEpoch: social-tag : 9\ntrain_loss -0.0022153303963502002\nval_loss -0.003109062351466377\n{'min_val_epoch': 9, 'min_val_loss': -0.003109062351466377}\n******************************\nTRAIN: \t Epoch: 10 \t Loss: -0.004371887072920799\nTRAIN: \t Epoch: 10 \t Loss: -0.004512206884101033\nTRAIN: \t Epoch: 10 \t Loss: -0.0034972637658938766\nTRAIN: \t Epoch: 10 \t Loss: -0.0018813009082805365\nTRAIN: \t Epoch: 10 \t Loss: -0.001487071267911233\nTRAIN: \t Epoch: 10 \t Loss: -0.0014114995647105388\nTRAIN: \t Epoch: 10 \t Loss: -0.0016012079951386632\nTRAIN: \t Epoch: 10 \t Loss: -0.0018568835166661302\nTRAIN: \t Epoch: 10 \t Loss: -0.002078536634346367\nTRAIN: \t Epoch: 10 \t Loss: -0.002250657598779071\nTRAIN: \t Epoch: 10 \t Loss: -0.0023252207492160696\nTRAIN: \t Epoch: 10 \t Loss: -0.0021199998239656757\nTRAIN: \t Epoch: 10 \t Loss: -0.0018767207366181538\nTRAIN: \t Epoch: 10 \t Loss: -0.0018655166651504779\nTRAIN: \t Epoch: 10 \t Loss: -0.0019476809684420004\nTRAIN: \t Epoch: 10 \t Loss: -0.002070542031106015\nTRAIN: \t Epoch: 10 \t Loss: -0.0022011213954743545\nTRAIN: \t Epoch: 10 \t Loss: -0.0023183810594168287\nTRAIN: \t Epoch: 10 \t Loss: -0.002344564337889958\nTRAIN: \t Epoch: 10 \t Loss: -0.002198106413561618\nTRAIN: \t Epoch: 10 \t Loss: -0.00215885561439241\nVALD: \t Epoch: 10 \t Loss: -0.0008690436370670795\nVALD: \t Epoch: 10 \t Loss: -0.0008608113566879183\nVALD: \t Epoch: 10 \t Loss: -0.0004125601165772726\nVALD: \t Epoch: 10 \t Loss: -0.000467599842522759\nVALD: \t Epoch: 10 \t Loss: -0.0004918882728294858\n******************************\nEpoch: social-tag : 10\ntrain_loss -0.00215885561439241\nval_loss -0.0004918882728294858\n{'min_val_epoch': 9, 'min_val_loss': -0.003109062351466377}\n******************************\nTRAIN: \t Epoch: 11 \t Loss: -0.000746145669836551\nTRAIN: \t Epoch: 11 \t Loss: -0.0014570991334039718\nTRAIN: \t Epoch: 11 \t Loss: -0.00212176883360371\nTRAIN: \t Epoch: 11 \t Loss: -0.0026089347229572013\nTRAIN: \t Epoch: 11 \t Loss: -0.002911172213498503\nTRAIN: \t Epoch: 11 \t Loss: -0.003092459994756306\nTRAIN: \t Epoch: 11 \t Loss: -0.0030755008587480654\nTRAIN: \t Epoch: 11 \t Loss: -0.0027198631541978102\nTRAIN: \t Epoch: 11 \t Loss: -0.002651431633340609\nTRAIN: \t Epoch: 11 \t Loss: -0.00273729887267109\nTRAIN: \t Epoch: 11 \t Loss: -0.0028733104673764583\nTRAIN: \t Epoch: 11 \t Loss: -0.0030447298461998193\nTRAIN: \t Epoch: 11 \t Loss: -0.003132411088489999\nTRAIN: \t Epoch: 11 \t Loss: -0.003212062465276436\nTRAIN: \t Epoch: 11 \t Loss: -0.003262613983436798\nTRAIN: \t Epoch: 11 \t Loss: -0.003133627235001768\nTRAIN: \t Epoch: 11 \t Loss: -0.003020693190217785\nTRAIN: \t Epoch: 11 \t Loss: -0.0030321066199879474\nTRAIN: \t Epoch: 11 \t Loss: -0.003091103493811955\nTRAIN: \t Epoch: 11 \t Loss: -0.0031666695125750267\nTRAIN: \t Epoch: 11 \t Loss: -0.0031857146144074113\nVALD: \t Epoch: 11 \t Loss: -0.0059109581634402275\nVALD: \t Epoch: 11 \t Loss: -0.006556186359375715\nVALD: \t Epoch: 11 \t Loss: -0.005835924142350753\nVALD: \t Epoch: 11 \t Loss: -0.006166381761431694\nVALD: \t Epoch: 11 \t Loss: -0.005623959448794428\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.44it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 1.0051598344108654  FDE: 1.5886973333636423\n**************************************************\n******************************\nEpoch: social-tag : 11\ntrain_loss -0.0031857146144074113\nval_loss -0.005623959448794428\n{'min_val_epoch': 11, 'min_val_loss': -0.005623959448794428}\n******************************\nTRAIN: \t Epoch: 12 \t Loss: -0.004304093774408102\nTRAIN: \t Epoch: 12 \t Loss: -0.004454587120562792\nTRAIN: \t Epoch: 12 \t Loss: -0.004231076687574387\nTRAIN: \t Epoch: 12 \t Loss: -0.0031750038444897655\nTRAIN: \t Epoch: 12 \t Loss: -0.0027823218473713497\nTRAIN: \t Epoch: 12 \t Loss: -0.0028967516150260053\nTRAIN: \t Epoch: 12 \t Loss: -0.0030779841458102703\nTRAIN: \t Epoch: 12 \t Loss: -0.0033105768003451885\nTRAIN: \t Epoch: 12 \t Loss: -0.0034932527977515645\nTRAIN: \t Epoch: 12 \t Loss: -0.003624090103130584\nTRAIN: \t Epoch: 12 \t Loss: -0.00340983015923319\nTRAIN: \t Epoch: 12 \t Loss: -0.0029283899388398518\nTRAIN: \t Epoch: 12 \t Loss: -0.002851797996859554\nTRAIN: \t Epoch: 12 \t Loss: -0.0028748572090080415\nTRAIN: \t Epoch: 12 \t Loss: -0.002965368605327967\nTRAIN: \t Epoch: 12 \t Loss: -0.0030370930865615264\nTRAIN: \t Epoch: 12 \t Loss: -0.0031725896810980083\nTRAIN: \t Epoch: 12 \t Loss: -0.0032441621970570444\nTRAIN: \t Epoch: 12 \t Loss: -0.0032871490994741755\nTRAIN: \t Epoch: 12 \t Loss: -0.0032319547923634674\nTRAIN: \t Epoch: 12 \t Loss: -0.003191876308154171\nVALD: \t Epoch: 12 \t Loss: -0.0009204047382809222\nVALD: \t Epoch: 12 \t Loss: -0.0017267153889406472\nVALD: \t Epoch: 12 \t Loss: -0.0012758734422580649\nVALD: \t Epoch: 12 \t Loss: -0.0015067356362123974\nVALD: \t Epoch: 12 \t Loss: -0.0015691608586461071\n******************************\nEpoch: social-tag : 12\ntrain_loss -0.003191876308154171\nval_loss -0.0015691608586461071\n{'min_val_epoch': 11, 'min_val_loss': -0.005623959448794428}\n******************************\nTRAIN: \t Epoch: 13 \t Loss: -0.00252234167419374\nTRAIN: \t Epoch: 13 \t Loss: -0.003354026353918016\nTRAIN: \t Epoch: 13 \t Loss: -0.0038449473213404417\nTRAIN: \t Epoch: 13 \t Loss: -0.004252468526829034\nTRAIN: \t Epoch: 13 \t Loss: -0.004489515302702784\nTRAIN: \t Epoch: 13 \t Loss: -0.0043622395023703575\nTRAIN: \t Epoch: 13 \t Loss: -0.0037833228083660026\nTRAIN: \t Epoch: 13 \t Loss: -0.0034948461870953906\nTRAIN: \t Epoch: 13 \t Loss: -0.0033788689211683553\nTRAIN: \t Epoch: 13 \t Loss: -0.003445019936771132\nTRAIN: \t Epoch: 13 \t Loss: -0.0035757001579358157\nTRAIN: \t Epoch: 13 \t Loss: -0.0037161924337851815\nTRAIN: \t Epoch: 13 \t Loss: -0.003863693208236677\nTRAIN: \t Epoch: 13 \t Loss: -0.003958452919115578\nTRAIN: \t Epoch: 13 \t Loss: -0.0038317731057759376\nTRAIN: \t Epoch: 13 \t Loss: -0.0034296896283194656\nTRAIN: \t Epoch: 13 \t Loss: -0.00334311618558679\nTRAIN: \t Epoch: 13 \t Loss: -0.0033416510252411375\nTRAIN: \t Epoch: 13 \t Loss: -0.003389089339179918\nTRAIN: \t Epoch: 13 \t Loss: -0.00348859468504088\nTRAIN: \t Epoch: 13 \t Loss: -0.0035125082724928028\nVALD: \t Epoch: 13 \t Loss: -0.0020407806150615215\nVALD: \t Epoch: 13 \t Loss: -0.0030227098613977432\nVALD: \t Epoch: 13 \t Loss: 0.0004827260660628478\nVALD: \t Epoch: 13 \t Loss: 6.282454705797136e-05\nVALD: \t Epoch: 13 \t Loss: 0.0002265679807478679\n******************************\nEpoch: social-tag : 13\ntrain_loss -0.0035125082724928028\nval_loss 0.0002265679807478679\n{'min_val_epoch': 11, 'min_val_loss': -0.005623959448794428}\n******************************\nTRAIN: \t Epoch: 14 \t Loss: -0.005274314898997545\nTRAIN: \t Epoch: 14 \t Loss: -0.005209426395595074\nTRAIN: \t Epoch: 14 \t Loss: -0.004824784118682146\nTRAIN: \t Epoch: 14 \t Loss: -0.004170478670857847\nTRAIN: \t Epoch: 14 \t Loss: -0.003909772494807839\nTRAIN: \t Epoch: 14 \t Loss: -0.004033216391690075\nTRAIN: \t Epoch: 14 \t Loss: -0.004253619915938803\nTRAIN: \t Epoch: 14 \t Loss: -0.004419135191710666\nTRAIN: \t Epoch: 14 \t Loss: -0.004582231424541937\nTRAIN: \t Epoch: 14 \t Loss: -0.0045555584831163285\nTRAIN: \t Epoch: 14 \t Loss: -0.004112568305572495\nTRAIN: \t Epoch: 14 \t Loss: -0.0038194402271377235\nTRAIN: \t Epoch: 14 \t Loss: -0.003744296485540242\nTRAIN: \t Epoch: 14 \t Loss: -0.0037750649727448554\nTRAIN: \t Epoch: 14 \t Loss: -0.003880218887934461\nTRAIN: \t Epoch: 14 \t Loss: -0.003983969339969917\nTRAIN: \t Epoch: 14 \t Loss: -0.004066808948717902\nTRAIN: \t Epoch: 14 \t Loss: -0.004127405718059486\nTRAIN: \t Epoch: 14 \t Loss: -0.004027331914250298\nTRAIN: \t Epoch: 14 \t Loss: -0.003890230406250339\nTRAIN: \t Epoch: 14 \t Loss: -0.003891689317988816\nVALD: \t Epoch: 14 \t Loss: -0.0047991895116865635\nVALD: \t Epoch: 14 \t Loss: -0.004935289267450571\nVALD: \t Epoch: 14 \t Loss: -0.004495628255729874\nVALD: \t Epoch: 14 \t Loss: -0.004743704164866358\nVALD: \t Epoch: 14 \t Loss: -0.004314405186740673\n******************************\nEpoch: social-tag : 14\ntrain_loss -0.003891689317988816\nval_loss -0.004314405186740673\n{'min_val_epoch': 11, 'min_val_loss': -0.005623959448794428}\n******************************\nTRAIN: \t Epoch: 15 \t Loss: -0.003995297942310572\nTRAIN: \t Epoch: 15 \t Loss: -0.0044407357927411795\nTRAIN: \t Epoch: 15 \t Loss: -0.004794147020826737\nTRAIN: \t Epoch: 15 \t Loss: -0.004980992409400642\nTRAIN: \t Epoch: 15 \t Loss: -0.005186125729233026\nTRAIN: \t Epoch: 15 \t Loss: -0.005292792494098346\nTRAIN: \t Epoch: 15 \t Loss: -0.005031353993607419\nTRAIN: \t Epoch: 15 \t Loss: -0.0041628401522757486\nTRAIN: \t Epoch: 15 \t Loss: -0.0038938334910199046\nTRAIN: \t Epoch: 15 \t Loss: -0.003773390024434775\nTRAIN: \t Epoch: 15 \t Loss: -0.0038259075425395913\nTRAIN: \t Epoch: 15 \t Loss: -0.0039893136417958885\nTRAIN: \t Epoch: 15 \t Loss: -0.004118822577696007\nTRAIN: \t Epoch: 15 \t Loss: -0.004205928408607308\nTRAIN: \t Epoch: 15 \t Loss: -0.004303445473002891\nTRAIN: \t Epoch: 15 \t Loss: -0.004240903224854264\nTRAIN: \t Epoch: 15 \t Loss: -0.003985650509575327\nTRAIN: \t Epoch: 15 \t Loss: -0.003912301696093184\nTRAIN: \t Epoch: 15 \t Loss: -0.003907447620325624\nTRAIN: \t Epoch: 15 \t Loss: -0.003935256752447458\nTRAIN: \t Epoch: 15 \t Loss: -0.00394918819072275\nVALD: \t Epoch: 15 \t Loss: -0.005893281660974026\nVALD: \t Epoch: 15 \t Loss: -0.006689870031550527\nVALD: \t Epoch: 15 \t Loss: -0.006245519345005353\nVALD: \t Epoch: 15 \t Loss: -0.006617270060814917\nVALD: \t Epoch: 15 \t Loss: -0.0062072026076908085\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.29it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.9609132428788073  FDE: 1.5064566208704575\n**************************************************\n******************************\nEpoch: social-tag : 15\ntrain_loss -0.00394918819072275\nval_loss -0.0062072026076908085\n{'min_val_epoch': 15, 'min_val_loss': -0.0062072026076908085}\n******************************\nTRAIN: \t Epoch: 16 \t Loss: -0.00566659402102232\nTRAIN: \t Epoch: 16 \t Loss: -0.005625391378998756\nTRAIN: \t Epoch: 16 \t Loss: -0.005689938707898061\nTRAIN: \t Epoch: 16 \t Loss: -0.005709924851544201\nTRAIN: \t Epoch: 16 \t Loss: -0.005712593998759985\nTRAIN: \t Epoch: 16 \t Loss: -0.005633231174821655\nTRAIN: \t Epoch: 16 \t Loss: -0.0051366946926074365\nTRAIN: \t Epoch: 16 \t Loss: -0.004911786440061405\nTRAIN: \t Epoch: 16 \t Loss: -0.0049363168525613016\nTRAIN: \t Epoch: 16 \t Loss: -0.0049849047092720864\nTRAIN: \t Epoch: 16 \t Loss: -0.005081070396541195\nTRAIN: \t Epoch: 16 \t Loss: -0.005173089661790679\nTRAIN: \t Epoch: 16 \t Loss: -0.005199741410951202\nTRAIN: \t Epoch: 16 \t Loss: -0.005156020484199482\nTRAIN: \t Epoch: 16 \t Loss: -0.005012176915382346\nTRAIN: \t Epoch: 16 \t Loss: -0.004945324049913324\nTRAIN: \t Epoch: 16 \t Loss: -0.004966905847301378\nTRAIN: \t Epoch: 16 \t Loss: -0.00503457975315137\nTRAIN: \t Epoch: 16 \t Loss: -0.005107384481418289\nTRAIN: \t Epoch: 16 \t Loss: -0.005066640058066696\nTRAIN: \t Epoch: 16 \t Loss: -0.005008402245324579\nVALD: \t Epoch: 16 \t Loss: -0.00570658640936017\nVALD: \t Epoch: 16 \t Loss: -0.005005071172490716\nVALD: \t Epoch: 16 \t Loss: -0.004089777745927374\nVALD: \t Epoch: 16 \t Loss: -0.0041417895699851215\nVALD: \t Epoch: 16 \t Loss: -0.00395696726590155\n******************************\nEpoch: social-tag : 16\ntrain_loss -0.005008402245324579\nval_loss -0.00395696726590155\n{'min_val_epoch': 15, 'min_val_loss': -0.0062072026076908085}\n******************************\nTRAIN: \t Epoch: 17 \t Loss: -0.004212376661598682\nTRAIN: \t Epoch: 17 \t Loss: -0.004631834803149104\nTRAIN: \t Epoch: 17 \t Loss: -0.005115410313010216\nTRAIN: \t Epoch: 17 \t Loss: -0.005310168373398483\nTRAIN: \t Epoch: 17 \t Loss: -0.005225079413503408\nTRAIN: \t Epoch: 17 \t Loss: -0.004716088335650663\nTRAIN: \t Epoch: 17 \t Loss: -0.004508618065821273\nTRAIN: \t Epoch: 17 \t Loss: -0.004579592554364353\nTRAIN: \t Epoch: 17 \t Loss: -0.004718945930815405\nTRAIN: \t Epoch: 17 \t Loss: -0.00489829396829009\nTRAIN: \t Epoch: 17 \t Loss: -0.00506295081736012\nTRAIN: \t Epoch: 17 \t Loss: -0.005030139698646963\nTRAIN: \t Epoch: 17 \t Loss: -0.004539232674198082\nTRAIN: \t Epoch: 17 \t Loss: -0.004243419453684639\nTRAIN: \t Epoch: 17 \t Loss: -0.004128487530397251\nTRAIN: \t Epoch: 17 \t Loss: -0.004104356075913529\nTRAIN: \t Epoch: 17 \t Loss: -0.004142813656739343\nTRAIN: \t Epoch: 17 \t Loss: -0.0042453653732081875\nTRAIN: \t Epoch: 17 \t Loss: -0.004343336936711383\nTRAIN: \t Epoch: 17 \t Loss: -0.0044251746250665745\nTRAIN: \t Epoch: 17 \t Loss: -0.00443742232815109\nVALD: \t Epoch: 17 \t Loss: -0.007455315440893173\nVALD: \t Epoch: 17 \t Loss: -0.007292347028851509\nVALD: \t Epoch: 17 \t Loss: -0.006477754563093185\nVALD: \t Epoch: 17 \t Loss: -0.00693158945068717\nVALD: \t Epoch: 17 \t Loss: -0.006459400801650952\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.22it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.9122043497462593  FDE: 1.4511400010256896\n**************************************************\n******************************\nEpoch: social-tag : 17\ntrain_loss -0.00443742232815109\nval_loss -0.006459400801650952\n{'min_val_epoch': 17, 'min_val_loss': -0.006459400801650952}\n******************************\nTRAIN: \t Epoch: 18 \t Loss: -0.00680188275873661\nTRAIN: \t Epoch: 18 \t Loss: -0.006433258531615138\nTRAIN: \t Epoch: 18 \t Loss: -0.006356845299402873\nTRAIN: \t Epoch: 18 \t Loss: -0.0061262238305062056\nTRAIN: \t Epoch: 18 \t Loss: -0.00513363212812692\nTRAIN: \t Epoch: 18 \t Loss: -0.004345333892464017\nTRAIN: \t Epoch: 18 \t Loss: -0.004180414161445307\nTRAIN: \t Epoch: 18 \t Loss: -0.004170614491158631\nTRAIN: \t Epoch: 18 \t Loss: -0.004325847367807809\nTRAIN: \t Epoch: 18 \t Loss: -0.0045320280536543574\nTRAIN: \t Epoch: 18 \t Loss: -0.004666857437272979\nTRAIN: \t Epoch: 18 \t Loss: -0.004862695714109577\nTRAIN: \t Epoch: 18 \t Loss: -0.004961277049285575\nTRAIN: \t Epoch: 18 \t Loss: -0.004880471699704815\nTRAIN: \t Epoch: 18 \t Loss: -0.0046835133999896545\nTRAIN: \t Epoch: 18 \t Loss: -0.004654988722904818\nTRAIN: \t Epoch: 18 \t Loss: -0.004716054224820041\nTRAIN: \t Epoch: 18 \t Loss: -0.0048121779594415175\nTRAIN: \t Epoch: 18 \t Loss: -0.0049175630280698995\nTRAIN: \t Epoch: 18 \t Loss: -0.00503565852122847\nTRAIN: \t Epoch: 18 \t Loss: -0.005051637931497812\nVALD: \t Epoch: 18 \t Loss: -0.007471037562936544\nVALD: \t Epoch: 18 \t Loss: -0.007404650328680873\nVALD: \t Epoch: 18 \t Loss: -0.006695251756658156\nVALD: \t Epoch: 18 \t Loss: -0.007124195224605501\nVALD: \t Epoch: 18 \t Loss: -0.006571566041157058\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:25<00:00, 11.99it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.9254664582831366  FDE: 1.488900077732216\n**************************************************\n******************************\nEpoch: social-tag : 18\ntrain_loss -0.005051637931497812\nval_loss -0.006571566041157058\n{'min_val_epoch': 18, 'min_val_loss': -0.006571566041157058}\n******************************\nTRAIN: \t Epoch: 19 \t Loss: -0.006648974027484655\nTRAIN: \t Epoch: 19 \t Loss: -0.006805567303672433\nTRAIN: \t Epoch: 19 \t Loss: -0.005961497159053882\nTRAIN: \t Epoch: 19 \t Loss: -0.004956136952387169\nTRAIN: \t Epoch: 19 \t Loss: -0.004754127864725888\nTRAIN: \t Epoch: 19 \t Loss: -0.0049427182918104036\nTRAIN: \t Epoch: 19 \t Loss: -0.005150874095436718\nTRAIN: \t Epoch: 19 \t Loss: -0.005360592287615873\nTRAIN: \t Epoch: 19 \t Loss: -0.00547110880466385\nTRAIN: \t Epoch: 19 \t Loss: -0.005555401288438589\nTRAIN: \t Epoch: 19 \t Loss: -0.005465112002143128\nTRAIN: \t Epoch: 19 \t Loss: -0.005360457124576594\nTRAIN: \t Epoch: 19 \t Loss: -0.005226169130764902\nTRAIN: \t Epoch: 19 \t Loss: -0.0052143694816290265\nTRAIN: \t Epoch: 19 \t Loss: -0.005280009866692126\nTRAIN: \t Epoch: 19 \t Loss: -0.005387721488659736\nTRAIN: \t Epoch: 19 \t Loss: -0.005427580210380256\nTRAIN: \t Epoch: 19 \t Loss: -0.005380449396195925\nTRAIN: \t Epoch: 19 \t Loss: -0.005215701236585646\nTRAIN: \t Epoch: 19 \t Loss: -0.005219637235859409\nTRAIN: \t Epoch: 19 \t Loss: -0.005223307146783415\nVALD: \t Epoch: 19 \t Loss: -0.007659300696104765\nVALD: \t Epoch: 19 \t Loss: -0.007490519667044282\nVALD: \t Epoch: 19 \t Loss: -0.0069351162140568095\nVALD: \t Epoch: 19 \t Loss: -0.007220644969493151\nVALD: \t Epoch: 19 \t Loss: -0.006807594675564728\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:25<00:00, 11.99it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.8928508404267608  FDE: 1.3749929203572735\n**************************************************\n******************************\nEpoch: social-tag : 19\ntrain_loss -0.005223307146783415\nval_loss -0.006807594675564728\n{'min_val_epoch': 19, 'min_val_loss': -0.006807594675564728}\n******************************\nTRAIN: \t Epoch: 20 \t Loss: -0.006051167380064726\nTRAIN: \t Epoch: 20 \t Loss: -0.00642776838503778\nTRAIN: \t Epoch: 20 \t Loss: -0.006644763673345248\nTRAIN: \t Epoch: 20 \t Loss: -0.0064704755786806345\nTRAIN: \t Epoch: 20 \t Loss: -0.006199514400213957\nTRAIN: \t Epoch: 20 \t Loss: -0.005981246164689462\nTRAIN: \t Epoch: 20 \t Loss: -0.00564290742788996\nTRAIN: \t Epoch: 20 \t Loss: -0.005558044707868248\nTRAIN: \t Epoch: 20 \t Loss: -0.005595811435745822\nTRAIN: \t Epoch: 20 \t Loss: -0.005777759477496147\nTRAIN: \t Epoch: 20 \t Loss: -0.00593769537623633\nTRAIN: \t Epoch: 20 \t Loss: -0.0060698753998925286\nTRAIN: \t Epoch: 20 \t Loss: -0.005983912600920751\nTRAIN: \t Epoch: 20 \t Loss: -0.005443580573358174\nTRAIN: \t Epoch: 20 \t Loss: -0.005189526228544613\nTRAIN: \t Epoch: 20 \t Loss: -0.005073420667031314\nTRAIN: \t Epoch: 20 \t Loss: -0.005050520981004571\nTRAIN: \t Epoch: 20 \t Loss: -0.00508085885990618\nTRAIN: \t Epoch: 20 \t Loss: -0.005146434548997173\nTRAIN: \t Epoch: 20 \t Loss: -0.00520973852253519\nTRAIN: \t Epoch: 20 \t Loss: -0.005241878583464332\nVALD: \t Epoch: 20 \t Loss: -0.008437124080955982\nVALD: \t Epoch: 20 \t Loss: -0.00846872664988041\nVALD: \t Epoch: 20 \t Loss: -0.007858326969047388\nVALD: \t Epoch: 20 \t Loss: -0.008185501210391521\nVALD: \t Epoch: 20 \t Loss: -0.007608320018135599\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.40it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.8660155526780599  FDE: 1.3774093275818393\n**************************************************\n******************************\nEpoch: social-tag : 20\ntrain_loss -0.005241878583464332\nval_loss -0.007608320018135599\n{'min_val_epoch': 20, 'min_val_loss': -0.007608320018135599}\n******************************\nTRAIN: \t Epoch: 21 \t Loss: -0.006958470679819584\nTRAIN: \t Epoch: 21 \t Loss: -0.006904043257236481\nTRAIN: \t Epoch: 21 \t Loss: -0.006786149150381486\nTRAIN: \t Epoch: 21 \t Loss: -0.006118023884482682\nTRAIN: \t Epoch: 21 \t Loss: -0.005150300590321422\nTRAIN: \t Epoch: 21 \t Loss: -0.005082890352544685\nTRAIN: \t Epoch: 21 \t Loss: -0.005225778657144734\nTRAIN: \t Epoch: 21 \t Loss: -0.005256358155747876\nTRAIN: \t Epoch: 21 \t Loss: -0.0053994211420002915\nTRAIN: \t Epoch: 21 \t Loss: -0.005531713855452836\nTRAIN: \t Epoch: 21 \t Loss: -0.005660910222848708\nTRAIN: \t Epoch: 21 \t Loss: -0.0056973932272133725\nTRAIN: \t Epoch: 21 \t Loss: -0.005621406016871333\nTRAIN: \t Epoch: 21 \t Loss: -0.005438565526024571\nTRAIN: \t Epoch: 21 \t Loss: -0.005417231610044837\nTRAIN: \t Epoch: 21 \t Loss: -0.005478327264427207\nTRAIN: \t Epoch: 21 \t Loss: -0.005544561687309076\nTRAIN: \t Epoch: 21 \t Loss: -0.005635171960521903\nTRAIN: \t Epoch: 21 \t Loss: -0.0056759486783688005\nTRAIN: \t Epoch: 21 \t Loss: -0.005717712792102247\nTRAIN: \t Epoch: 21 \t Loss: -0.005696890400342052\nVALD: \t Epoch: 21 \t Loss: -0.008700842037796974\nVALD: \t Epoch: 21 \t Loss: -0.0086184018291533\nVALD: \t Epoch: 21 \t Loss: -0.007962373085319996\nVALD: \t Epoch: 21 \t Loss: -0.008240497903898358\nVALD: \t Epoch: 21 \t Loss: -0.007741330710394179\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.38it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.8876882481878031  FDE: 1.4218819397142057\n**************************************************\n******************************\nEpoch: social-tag : 21\ntrain_loss -0.005696890400342052\nval_loss -0.007741330710394179\n{'min_val_epoch': 21, 'min_val_loss': -0.007741330710394179}\n******************************\nTRAIN: \t Epoch: 22 \t Loss: -0.006868482567369938\nTRAIN: \t Epoch: 22 \t Loss: -0.0067910070065408945\nTRAIN: \t Epoch: 22 \t Loss: -0.006901439279317856\nTRAIN: \t Epoch: 22 \t Loss: -0.006712038652040064\nTRAIN: \t Epoch: 22 \t Loss: -0.006351033039391041\nTRAIN: \t Epoch: 22 \t Loss: -0.0060858499103536206\nTRAIN: \t Epoch: 22 \t Loss: -0.00609883659386209\nTRAIN: \t Epoch: 22 \t Loss: -0.006256155495066196\nTRAIN: \t Epoch: 22 \t Loss: -0.0063303459125260515\nTRAIN: \t Epoch: 22 \t Loss: -0.006193533819168806\nTRAIN: \t Epoch: 22 \t Loss: -0.006066016895188527\nTRAIN: \t Epoch: 22 \t Loss: -0.005983946844935417\nTRAIN: \t Epoch: 22 \t Loss: -0.005998284806712315\nTRAIN: \t Epoch: 22 \t Loss: -0.006070167291909456\nTRAIN: \t Epoch: 22 \t Loss: -0.006154794401178758\nTRAIN: \t Epoch: 22 \t Loss: -0.006197748210979626\nTRAIN: \t Epoch: 22 \t Loss: -0.006263573818347033\nTRAIN: \t Epoch: 22 \t Loss: -0.006247914525576764\nTRAIN: \t Epoch: 22 \t Loss: -0.006148787656504857\nTRAIN: \t Epoch: 22 \t Loss: -0.006084132706746459\nTRAIN: \t Epoch: 22 \t Loss: -0.006076600474399517\nVALD: \t Epoch: 22 \t Loss: -0.008007972501218319\nVALD: \t Epoch: 22 \t Loss: -0.0075645120814442635\nVALD: \t Epoch: 22 \t Loss: -0.007021009145925443\nVALD: \t Epoch: 22 \t Loss: -0.0072795095620676875\nVALD: \t Epoch: 22 \t Loss: -0.006934268271097622\n******************************\nEpoch: social-tag : 22\ntrain_loss -0.006076600474399517\nval_loss -0.006934268271097622\n{'min_val_epoch': 21, 'min_val_loss': -0.007741330710394179}\n******************************\nTRAIN: \t Epoch: 23 \t Loss: -0.006409826688468456\nTRAIN: \t Epoch: 23 \t Loss: -0.007159717381000519\nTRAIN: \t Epoch: 23 \t Loss: -0.007493380146721999\nTRAIN: \t Epoch: 23 \t Loss: -0.007406148710288107\nTRAIN: \t Epoch: 23 \t Loss: -0.006867153290659189\nTRAIN: \t Epoch: 23 \t Loss: -0.006149724242277443\nTRAIN: \t Epoch: 23 \t Loss: -0.0060578015566404375\nTRAIN: \t Epoch: 23 \t Loss: -0.0060944049910176545\nTRAIN: \t Epoch: 23 \t Loss: -0.006198831187147234\nTRAIN: \t Epoch: 23 \t Loss: -0.006311714299954474\nTRAIN: \t Epoch: 23 \t Loss: -0.006454497850923376\nTRAIN: \t Epoch: 23 \t Loss: -0.006397694911962996\nTRAIN: \t Epoch: 23 \t Loss: -0.006204656361100765\nTRAIN: \t Epoch: 23 \t Loss: -0.0060273360660565755\nTRAIN: \t Epoch: 23 \t Loss: -0.00596964576592048\nTRAIN: \t Epoch: 23 \t Loss: -0.006031582364812493\nTRAIN: \t Epoch: 23 \t Loss: -0.006115970722235301\nTRAIN: \t Epoch: 23 \t Loss: -0.006156575426252352\nTRAIN: \t Epoch: 23 \t Loss: -0.006198908692519916\nTRAIN: \t Epoch: 23 \t Loss: -0.006217850791290403\nTRAIN: \t Epoch: 23 \t Loss: -0.006176955661892247\nVALD: \t Epoch: 23 \t Loss: -0.006802916526794434\nVALD: \t Epoch: 23 \t Loss: -0.006669282214716077\nVALD: \t Epoch: 23 \t Loss: -0.005523527817179759\nVALD: \t Epoch: 23 \t Loss: -0.006059473846107721\nVALD: \t Epoch: 23 \t Loss: -0.0059181117781118495\n******************************\nEpoch: social-tag : 23\ntrain_loss -0.006176955661892247\nval_loss -0.0059181117781118495\n{'min_val_epoch': 21, 'min_val_loss': -0.007741330710394179}\n******************************\nTRAIN: \t Epoch: 24 \t Loss: -0.007069086655974388\nTRAIN: \t Epoch: 24 \t Loss: -0.007299889577552676\nTRAIN: \t Epoch: 24 \t Loss: -0.007293747738003731\nTRAIN: \t Epoch: 24 \t Loss: -0.006537403562106192\nTRAIN: \t Epoch: 24 \t Loss: -0.005808775126934051\nTRAIN: \t Epoch: 24 \t Loss: -0.005632395623251796\nTRAIN: \t Epoch: 24 \t Loss: -0.005733257159590721\nTRAIN: \t Epoch: 24 \t Loss: -0.00599726103246212\nTRAIN: \t Epoch: 24 \t Loss: -0.006216329625911183\nTRAIN: \t Epoch: 24 \t Loss: -0.006287317257374525\nTRAIN: \t Epoch: 24 \t Loss: -0.006370899309827523\nTRAIN: \t Epoch: 24 \t Loss: -0.006361818872392178\nTRAIN: \t Epoch: 24 \t Loss: -0.006255658200153938\nTRAIN: \t Epoch: 24 \t Loss: -0.006117180555260607\nTRAIN: \t Epoch: 24 \t Loss: -0.006102775192509095\nTRAIN: \t Epoch: 24 \t Loss: -0.006159994576591998\nTRAIN: \t Epoch: 24 \t Loss: -0.006236924050266252\nTRAIN: \t Epoch: 24 \t Loss: -0.006324766798772746\nTRAIN: \t Epoch: 24 \t Loss: -0.006343293738992591\nTRAIN: \t Epoch: 24 \t Loss: -0.006355548906140029\nTRAIN: \t Epoch: 24 \t Loss: -0.00634398354460114\nVALD: \t Epoch: 24 \t Loss: -0.008454694412648678\nVALD: \t Epoch: 24 \t Loss: -0.008131519425660372\nVALD: \t Epoch: 24 \t Loss: -0.007603205585231383\nVALD: \t Epoch: 24 \t Loss: -0.008034297614358366\nVALD: \t Epoch: 24 \t Loss: -0.007380174697885191\n******************************\nEpoch: social-tag : 24\ntrain_loss -0.00634398354460114\nval_loss -0.007380174697885191\n{'min_val_epoch': 21, 'min_val_loss': -0.007741330710394179}\n******************************\nTRAIN: \t Epoch: 25 \t Loss: -0.00658314349129796\nTRAIN: \t Epoch: 25 \t Loss: -0.0065252394415438175\nTRAIN: \t Epoch: 25 \t Loss: -0.006532841362059116\nTRAIN: \t Epoch: 25 \t Loss: -0.006187843508087099\nTRAIN: \t Epoch: 25 \t Loss: -0.0059849082492291926\nTRAIN: \t Epoch: 25 \t Loss: -0.006104841673125823\nTRAIN: \t Epoch: 25 \t Loss: -0.006205282041004726\nTRAIN: \t Epoch: 25 \t Loss: -0.006305794580839574\nTRAIN: \t Epoch: 25 \t Loss: -0.006469615010751618\nTRAIN: \t Epoch: 25 \t Loss: -0.006562958750873804\nTRAIN: \t Epoch: 25 \t Loss: -0.006378300106999549\nTRAIN: \t Epoch: 25 \t Loss: -0.006122507019123684\nTRAIN: \t Epoch: 25 \t Loss: -0.006082607057088843\nTRAIN: \t Epoch: 25 \t Loss: -0.00614285736810416\nTRAIN: \t Epoch: 25 \t Loss: -0.006257719872519374\nTRAIN: \t Epoch: 25 \t Loss: -0.006365702967741527\nTRAIN: \t Epoch: 25 \t Loss: -0.006480128189329715\nTRAIN: \t Epoch: 25 \t Loss: -0.006479110659307076\nTRAIN: \t Epoch: 25 \t Loss: -0.006431190578855182\nTRAIN: \t Epoch: 25 \t Loss: -0.006295760348439217\nTRAIN: \t Epoch: 25 \t Loss: -0.006290126330547362\nVALD: \t Epoch: 25 \t Loss: -0.008326811715960503\nVALD: \t Epoch: 25 \t Loss: -0.007943779462948442\nVALD: \t Epoch: 25 \t Loss: -0.007546921726316214\nVALD: \t Epoch: 25 \t Loss: -0.00784569897223264\nVALD: \t Epoch: 25 \t Loss: -0.0072948732045921534\n******************************\nEpoch: social-tag : 25\ntrain_loss -0.006290126330547362\nval_loss -0.0072948732045921534\n{'min_val_epoch': 21, 'min_val_loss': -0.007741330710394179}\n******************************\nTRAIN: \t Epoch: 26 \t Loss: -0.0064582740887999535\nTRAIN: \t Epoch: 26 \t Loss: -0.0069135078229010105\nTRAIN: \t Epoch: 26 \t Loss: -0.007110864855349064\nTRAIN: \t Epoch: 26 \t Loss: -0.0073645010124892\nTRAIN: \t Epoch: 26 \t Loss: -0.007402928918600083\nTRAIN: \t Epoch: 26 \t Loss: -0.0071274225289622945\nTRAIN: \t Epoch: 26 \t Loss: -0.006270743424205908\nTRAIN: \t Epoch: 26 \t Loss: -0.006023707494023256\nTRAIN: \t Epoch: 26 \t Loss: -0.006071023371381064\nTRAIN: \t Epoch: 26 \t Loss: -0.006218800542410463\nTRAIN: \t Epoch: 26 \t Loss: -0.006331897965124385\nTRAIN: \t Epoch: 26 \t Loss: -0.006503997942976032\nTRAIN: \t Epoch: 26 \t Loss: -0.006586964340665593\nTRAIN: \t Epoch: 26 \t Loss: -0.006688572025658297\nTRAIN: \t Epoch: 26 \t Loss: -0.006611253965335588\nTRAIN: \t Epoch: 26 \t Loss: -0.006139668399555376\nTRAIN: \t Epoch: 26 \t Loss: -0.005995022732189254\nTRAIN: \t Epoch: 26 \t Loss: -0.005918704694421548\nTRAIN: \t Epoch: 26 \t Loss: -0.005919801435849972\nTRAIN: \t Epoch: 26 \t Loss: -0.005975204959395341\nTRAIN: \t Epoch: 26 \t Loss: -0.005999456700468946\nVALD: \t Epoch: 26 \t Loss: -0.006282529793679714\nVALD: \t Epoch: 26 \t Loss: -0.006585138849914074\nVALD: \t Epoch: 26 \t Loss: -0.005680987611413002\nVALD: \t Epoch: 26 \t Loss: -0.00646283570677042\nVALD: \t Epoch: 26 \t Loss: -0.006258788316146187\n******************************\nEpoch: social-tag : 26\ntrain_loss -0.005999456700468946\nval_loss -0.006258788316146187\n{'min_val_epoch': 21, 'min_val_loss': -0.007741330710394179}\n******************************\nTRAIN: \t Epoch: 27 \t Loss: -0.007735565770417452\nTRAIN: \t Epoch: 27 \t Loss: -0.0074049297254532576\nTRAIN: \t Epoch: 27 \t Loss: -0.007667247671633959\nTRAIN: \t Epoch: 27 \t Loss: -0.007543337880633771\nTRAIN: \t Epoch: 27 \t Loss: -0.007438670285046101\nTRAIN: \t Epoch: 27 \t Loss: -0.007458270605032642\nTRAIN: \t Epoch: 27 \t Loss: -0.007266011953886066\nTRAIN: \t Epoch: 27 \t Loss: -0.006968535657506436\nTRAIN: \t Epoch: 27 \t Loss: -0.006831178131202857\nTRAIN: \t Epoch: 27 \t Loss: -0.006880742218345404\nTRAIN: \t Epoch: 27 \t Loss: -0.006979993036524816\nTRAIN: \t Epoch: 27 \t Loss: -0.007021271662476162\nTRAIN: \t Epoch: 27 \t Loss: -0.007049632652734335\nTRAIN: \t Epoch: 27 \t Loss: -0.006903592397325805\nTRAIN: \t Epoch: 27 \t Loss: -0.006747567777832349\nTRAIN: \t Epoch: 27 \t Loss: -0.006690085952868685\nTRAIN: \t Epoch: 27 \t Loss: -0.006721534684081288\nTRAIN: \t Epoch: 27 \t Loss: -0.0067828687994430465\nTRAIN: \t Epoch: 27 \t Loss: -0.006887345354219801\nTRAIN: \t Epoch: 27 \t Loss: -0.006948923203162849\nTRAIN: \t Epoch: 27 \t Loss: -0.006950518031623975\nVALD: \t Epoch: 27 \t Loss: -0.007932944223284721\nVALD: \t Epoch: 27 \t Loss: -0.00766442297026515\nVALD: \t Epoch: 27 \t Loss: -0.007354999581972758\nVALD: \t Epoch: 27 \t Loss: -0.00808979058638215\nVALD: \t Epoch: 27 \t Loss: -0.007591171061167202\n******************************\nEpoch: social-tag : 27\ntrain_loss -0.006950518031623975\nval_loss -0.007591171061167202\n{'min_val_epoch': 21, 'min_val_loss': -0.007741330710394179}\n******************************\nTRAIN: \t Epoch: 28 \t Loss: -0.008856928907334805\nTRAIN: \t Epoch: 28 \t Loss: -0.008265169570222497\nTRAIN: \t Epoch: 28 \t Loss: -0.006796110343808929\nTRAIN: \t Epoch: 28 \t Loss: -0.005737008003052324\nTRAIN: \t Epoch: 28 \t Loss: -0.005687139229848981\nTRAIN: \t Epoch: 28 \t Loss: -0.005921726309073468\nTRAIN: \t Epoch: 28 \t Loss: -0.006163654017395207\nTRAIN: \t Epoch: 28 \t Loss: -0.006465140468208119\nTRAIN: \t Epoch: 28 \t Loss: -0.006642710495119293\nTRAIN: \t Epoch: 28 \t Loss: -0.00681825818028301\nTRAIN: \t Epoch: 28 \t Loss: -0.00673807160505517\nTRAIN: \t Epoch: 28 \t Loss: -0.006443265398653845\nTRAIN: \t Epoch: 28 \t Loss: -0.006350099467314207\nTRAIN: \t Epoch: 28 \t Loss: -0.00641143970590617\nTRAIN: \t Epoch: 28 \t Loss: -0.0065068442684908705\nTRAIN: \t Epoch: 28 \t Loss: -0.006641710438998416\nTRAIN: \t Epoch: 28 \t Loss: -0.006722308558357113\nTRAIN: \t Epoch: 28 \t Loss: -0.006820764418484436\nTRAIN: \t Epoch: 28 \t Loss: -0.006823748448177388\nTRAIN: \t Epoch: 28 \t Loss: -0.006747471890412271\nTRAIN: \t Epoch: 28 \t Loss: -0.0067270846381128615\nVALD: \t Epoch: 28 \t Loss: -0.0076478710398077965\nVALD: \t Epoch: 28 \t Loss: -0.007581287994980812\nVALD: \t Epoch: 28 \t Loss: -0.007000260365506013\nVALD: \t Epoch: 28 \t Loss: -0.007418151246383786\nVALD: \t Epoch: 28 \t Loss: -0.007042080211946545\n******************************\nEpoch: social-tag : 28\ntrain_loss -0.0067270846381128615\nval_loss -0.007042080211946545\n{'min_val_epoch': 21, 'min_val_loss': -0.007741330710394179}\n******************************\nTRAIN: \t Epoch: 29 \t Loss: -0.006583868060261011\nTRAIN: \t Epoch: 29 \t Loss: -0.007245562272146344\nTRAIN: \t Epoch: 29 \t Loss: -0.007436188403517008\nTRAIN: \t Epoch: 29 \t Loss: -0.007692673825658858\nTRAIN: \t Epoch: 29 \t Loss: -0.007504507619887591\nTRAIN: \t Epoch: 29 \t Loss: -0.007133485283702612\nTRAIN: \t Epoch: 29 \t Loss: -0.006889085019273418\nTRAIN: \t Epoch: 29 \t Loss: -0.006985717685893178\nTRAIN: \t Epoch: 29 \t Loss: -0.007059743007024129\nTRAIN: \t Epoch: 29 \t Loss: -0.007167063560336829\nTRAIN: \t Epoch: 29 \t Loss: -0.007324747741222382\nTRAIN: \t Epoch: 29 \t Loss: -0.007416762101153533\nTRAIN: \t Epoch: 29 \t Loss: -0.007420199565016306\nTRAIN: \t Epoch: 29 \t Loss: -0.007299823686480522\nTRAIN: \t Epoch: 29 \t Loss: -0.007134575893481572\nTRAIN: \t Epoch: 29 \t Loss: -0.007109243771992624\nTRAIN: \t Epoch: 29 \t Loss: -0.007083341616260654\nTRAIN: \t Epoch: 29 \t Loss: -0.0071312113302863306\nTRAIN: \t Epoch: 29 \t Loss: -0.007172512454225829\nTRAIN: \t Epoch: 29 \t Loss: -0.007193530676886439\nTRAIN: \t Epoch: 29 \t Loss: -0.007173632807977951\nVALD: \t Epoch: 29 \t Loss: -0.008073797449469566\nVALD: \t Epoch: 29 \t Loss: -0.008267112076282501\nVALD: \t Epoch: 29 \t Loss: -0.007255607129385074\nVALD: \t Epoch: 29 \t Loss: -0.007753462181426585\nVALD: \t Epoch: 29 \t Loss: -0.007408010402931299\n******************************\nEpoch: social-tag : 29\ntrain_loss -0.007173632807977951\nval_loss -0.007408010402931299\n{'min_val_epoch': 21, 'min_val_loss': -0.007741330710394179}\n******************************\nTRAIN: \t Epoch: 30 \t Loss: -0.008406147360801697\nTRAIN: \t Epoch: 30 \t Loss: -0.00838917400687933\nTRAIN: \t Epoch: 30 \t Loss: -0.007947600601861874\nTRAIN: \t Epoch: 30 \t Loss: -0.007232705014757812\nTRAIN: \t Epoch: 30 \t Loss: -0.006469882233068347\nTRAIN: \t Epoch: 30 \t Loss: -0.006334811643076439\nTRAIN: \t Epoch: 30 \t Loss: -0.006455447691093598\nTRAIN: \t Epoch: 30 \t Loss: -0.006656917656073347\nTRAIN: \t Epoch: 30 \t Loss: -0.0067771990110890735\nTRAIN: \t Epoch: 30 \t Loss: -0.006851104157976806\nTRAIN: \t Epoch: 30 \t Loss: -0.006989236645908518\nTRAIN: \t Epoch: 30 \t Loss: -0.007048460557901611\nTRAIN: \t Epoch: 30 \t Loss: -0.0070680793052395946\nTRAIN: \t Epoch: 30 \t Loss: -0.006985165029098945\nTRAIN: \t Epoch: 30 \t Loss: -0.006929197643573085\nTRAIN: \t Epoch: 30 \t Loss: -0.006929135342943482\nTRAIN: \t Epoch: 30 \t Loss: -0.007029178702984662\nTRAIN: \t Epoch: 30 \t Loss: -0.007080571350848509\nTRAIN: \t Epoch: 30 \t Loss: -0.007105982727616241\nTRAIN: \t Epoch: 30 \t Loss: -0.007068505731876939\nTRAIN: \t Epoch: 30 \t Loss: -0.007031885420209919\nVALD: \t Epoch: 30 \t Loss: -0.0018190135015174747\nVALD: \t Epoch: 30 \t Loss: -0.0032714802655391395\nVALD: \t Epoch: 30 \t Loss: -0.0009855778189375997\nVALD: \t Epoch: 30 \t Loss: -0.002383383078267798\nVALD: \t Epoch: 30 \t Loss: -0.002844994074864472\n******************************\nEpoch: social-tag : 30\ntrain_loss -0.007031885420209919\nval_loss -0.002844994074864472\n{'min_val_epoch': 21, 'min_val_loss': -0.007741330710394179}\n******************************\nTRAIN: \t Epoch: 31 \t Loss: -0.006848213262856007\nTRAIN: \t Epoch: 31 \t Loss: -0.007047373568639159\nTRAIN: \t Epoch: 31 \t Loss: -0.0072533794057865935\nTRAIN: \t Epoch: 31 \t Loss: -0.007386235753074288\nTRAIN: \t Epoch: 31 \t Loss: -0.007169609516859054\nTRAIN: \t Epoch: 31 \t Loss: -0.007069448241963983\nTRAIN: \t Epoch: 31 \t Loss: -0.00711349058630211\nTRAIN: \t Epoch: 31 \t Loss: -0.007128728844691068\nTRAIN: \t Epoch: 31 \t Loss: -0.007217663392010663\nTRAIN: \t Epoch: 31 \t Loss: -0.007225693389773369\nTRAIN: \t Epoch: 31 \t Loss: -0.00714426360685717\nTRAIN: \t Epoch: 31 \t Loss: -0.007075691712088883\nTRAIN: \t Epoch: 31 \t Loss: -0.0070932491037708064\nTRAIN: \t Epoch: 31 \t Loss: -0.007112991231094513\nTRAIN: \t Epoch: 31 \t Loss: -0.007190204504877329\nTRAIN: \t Epoch: 31 \t Loss: -0.007221410138299689\nTRAIN: \t Epoch: 31 \t Loss: -0.007217859137145912\nTRAIN: \t Epoch: 31 \t Loss: -0.007130037165350384\nTRAIN: \t Epoch: 31 \t Loss: -0.007039949542989856\nTRAIN: \t Epoch: 31 \t Loss: -0.007046230672858656\nTRAIN: \t Epoch: 31 \t Loss: -0.007070485453653446\nVALD: \t Epoch: 31 \t Loss: -0.006361325271427631\nVALD: \t Epoch: 31 \t Loss: -0.006779985269531608\nVALD: \t Epoch: 31 \t Loss: -0.005573983769863844\nVALD: \t Epoch: 31 \t Loss: -0.006547156604938209\nVALD: \t Epoch: 31 \t Loss: -0.006443390427772166\n******************************\nEpoch: social-tag : 31\ntrain_loss -0.007070485453653446\nval_loss -0.006443390427772166\n{'min_val_epoch': 21, 'min_val_loss': -0.007741330710394179}\n******************************\nTRAIN: \t Epoch: 32 \t Loss: -0.008432370610535145\nTRAIN: \t Epoch: 32 \t Loss: -0.008509052451699972\nTRAIN: \t Epoch: 32 \t Loss: -0.008644600088397661\nTRAIN: \t Epoch: 32 \t Loss: -0.008437811164185405\nTRAIN: \t Epoch: 32 \t Loss: -0.008375834487378598\nTRAIN: \t Epoch: 32 \t Loss: -0.00844928560157617\nTRAIN: \t Epoch: 32 \t Loss: -0.008213432971388102\nTRAIN: \t Epoch: 32 \t Loss: -0.007549585745437071\nTRAIN: \t Epoch: 32 \t Loss: -0.007255978959922989\nTRAIN: \t Epoch: 32 \t Loss: -0.007199154631234705\nTRAIN: \t Epoch: 32 \t Loss: -0.007280924971300093\nTRAIN: \t Epoch: 32 \t Loss: -0.0074121808672013385\nTRAIN: \t Epoch: 32 \t Loss: -0.0075213829091248605\nTRAIN: \t Epoch: 32 \t Loss: -0.007563689673718598\nTRAIN: \t Epoch: 32 \t Loss: -0.007520374516025186\nTRAIN: \t Epoch: 32 \t Loss: -0.007202030814369209\nTRAIN: \t Epoch: 32 \t Loss: -0.007042327224660446\nTRAIN: \t Epoch: 32 \t Loss: -0.007037093229074445\nTRAIN: \t Epoch: 32 \t Loss: -0.007108500705247647\nTRAIN: \t Epoch: 32 \t Loss: -0.007161723810713738\nTRAIN: \t Epoch: 32 \t Loss: -0.007165159447062632\nVALD: \t Epoch: 32 \t Loss: -0.0069520724937319756\nVALD: \t Epoch: 32 \t Loss: -0.007599750999361277\nVALD: \t Epoch: 32 \t Loss: -0.007007712343086799\nVALD: \t Epoch: 32 \t Loss: -0.00786743953358382\nVALD: \t Epoch: 32 \t Loss: -0.0075031223312476214\n******************************\nEpoch: social-tag : 32\ntrain_loss -0.007165159447062632\nval_loss -0.0075031223312476214\n{'min_val_epoch': 21, 'min_val_loss': -0.007741330710394179}\n******************************\nTRAIN: \t Epoch: 33 \t Loss: -0.008669204078614712\nTRAIN: \t Epoch: 33 \t Loss: -0.008517120964825153\nTRAIN: \t Epoch: 33 \t Loss: -0.0084507263575991\nTRAIN: \t Epoch: 33 \t Loss: -0.008609359385445714\nTRAIN: \t Epoch: 33 \t Loss: -0.008609724789857864\nTRAIN: \t Epoch: 33 \t Loss: -0.008286750953023633\nTRAIN: \t Epoch: 33 \t Loss: -0.007764798921665975\nTRAIN: \t Epoch: 33 \t Loss: -0.007497962622437626\nTRAIN: \t Epoch: 33 \t Loss: -0.007532902372380097\nTRAIN: \t Epoch: 33 \t Loss: -0.00758266905322671\nTRAIN: \t Epoch: 33 \t Loss: -0.007655453190884807\nTRAIN: \t Epoch: 33 \t Loss: -0.007717295084148645\nTRAIN: \t Epoch: 33 \t Loss: -0.007767544557841925\nTRAIN: \t Epoch: 33 \t Loss: -0.007766862300091556\nTRAIN: \t Epoch: 33 \t Loss: -0.007606050030638774\nTRAIN: \t Epoch: 33 \t Loss: -0.007438657165039331\nTRAIN: \t Epoch: 33 \t Loss: -0.007442301166627337\nTRAIN: \t Epoch: 33 \t Loss: -0.007502005202695727\nTRAIN: \t Epoch: 33 \t Loss: -0.007569372923554559\nTRAIN: \t Epoch: 33 \t Loss: -0.007607287098653615\nTRAIN: \t Epoch: 33 \t Loss: -0.007615015151965774\nVALD: \t Epoch: 33 \t Loss: -0.0079684192314744\nVALD: \t Epoch: 33 \t Loss: -0.008683365769684315\nVALD: \t Epoch: 33 \t Loss: -0.007887375230590502\nVALD: \t Epoch: 33 \t Loss: -0.00850203912705183\nVALD: \t Epoch: 33 \t Loss: -0.008091993473962117\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.31it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.7674473092174826  FDE: 1.2506499160476154\n**************************************************\n******************************\nEpoch: social-tag : 33\ntrain_loss -0.007615015151965774\nval_loss -0.008091993473962117\n{'min_val_epoch': 33, 'min_val_loss': -0.008091993473962117}\n******************************\nTRAIN: \t Epoch: 34 \t Loss: -0.009271780960261822\nTRAIN: \t Epoch: 34 \t Loss: -0.008857781998813152\nTRAIN: \t Epoch: 34 \t Loss: -0.008234582686175903\nTRAIN: \t Epoch: 34 \t Loss: -0.00776655925437808\nTRAIN: \t Epoch: 34 \t Loss: -0.0070054358802735806\nTRAIN: \t Epoch: 34 \t Loss: -0.006874963873997331\nTRAIN: \t Epoch: 34 \t Loss: -0.006982837138431413\nTRAIN: \t Epoch: 34 \t Loss: -0.007157433778047562\nTRAIN: \t Epoch: 34 \t Loss: -0.007335331808361743\nTRAIN: \t Epoch: 34 \t Loss: -0.007502904441207647\nTRAIN: \t Epoch: 34 \t Loss: -0.00757431679151275\nTRAIN: \t Epoch: 34 \t Loss: -0.00752885981152455\nTRAIN: \t Epoch: 34 \t Loss: -0.007336229694863925\nTRAIN: \t Epoch: 34 \t Loss: -0.007284184718238456\nTRAIN: \t Epoch: 34 \t Loss: -0.0073656778161724406\nTRAIN: \t Epoch: 34 \t Loss: -0.007482510060071945\nTRAIN: \t Epoch: 34 \t Loss: -0.0075609614415203825\nTRAIN: \t Epoch: 34 \t Loss: -0.007572356094088819\nTRAIN: \t Epoch: 34 \t Loss: -0.007518653145157977\nTRAIN: \t Epoch: 34 \t Loss: -0.007447401364333928\nTRAIN: \t Epoch: 34 \t Loss: -0.0074437366096258345\nVALD: \t Epoch: 34 \t Loss: -0.008850536309182644\nVALD: \t Epoch: 34 \t Loss: -0.008905252907425165\nVALD: \t Epoch: 34 \t Loss: -0.008565662118295828\nVALD: \t Epoch: 34 \t Loss: -0.008958749007433653\nVALD: \t Epoch: 34 \t Loss: -0.008355810350458018\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.22it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.7165734997761172  FDE: 1.1157292970422927\n**************************************************\n******************************\nEpoch: social-tag : 34\ntrain_loss -0.0074437366096258345\nval_loss -0.008355810350458018\n{'min_val_epoch': 34, 'min_val_loss': -0.008355810350458018}\n******************************\nTRAIN: \t Epoch: 35 \t Loss: -0.007352857384830713\nTRAIN: \t Epoch: 35 \t Loss: -0.007989647099748254\nTRAIN: \t Epoch: 35 \t Loss: -0.008522243704646826\nTRAIN: \t Epoch: 35 \t Loss: -0.008424763684161007\nTRAIN: \t Epoch: 35 \t Loss: -0.008175818715244532\nTRAIN: \t Epoch: 35 \t Loss: -0.008057291541869441\nTRAIN: \t Epoch: 35 \t Loss: -0.008056814449706249\nTRAIN: \t Epoch: 35 \t Loss: -0.008105107641313225\nTRAIN: \t Epoch: 35 \t Loss: -0.008223749024586545\nTRAIN: \t Epoch: 35 \t Loss: -0.0081800343003124\nTRAIN: \t Epoch: 35 \t Loss: -0.00806296422061595\nTRAIN: \t Epoch: 35 \t Loss: -0.007952087985662123\nTRAIN: \t Epoch: 35 \t Loss: -0.007874696837881437\nTRAIN: \t Epoch: 35 \t Loss: -0.007875067946900214\nTRAIN: \t Epoch: 35 \t Loss: -0.007931236767520507\nTRAIN: \t Epoch: 35 \t Loss: -0.00791280166595243\nTRAIN: \t Epoch: 35 \t Loss: -0.007825955775949885\nTRAIN: \t Epoch: 35 \t Loss: -0.0076840290437555974\nTRAIN: \t Epoch: 35 \t Loss: -0.007675499116119586\nTRAIN: \t Epoch: 35 \t Loss: -0.007721813814714551\nTRAIN: \t Epoch: 35 \t Loss: -0.007722519975767378\nVALD: \t Epoch: 35 \t Loss: 0.0014771793503314257\nVALD: \t Epoch: 35 \t Loss: -0.0007785229245200753\nVALD: \t Epoch: 35 \t Loss: 0.003124571948622664\nVALD: \t Epoch: 35 \t Loss: 0.00026656402042135596\nVALD: \t Epoch: 35 \t Loss: -0.0005234403598711687\n******************************\nEpoch: social-tag : 35\ntrain_loss -0.007722519975767378\nval_loss -0.0005234403598711687\n{'min_val_epoch': 34, 'min_val_loss': -0.008355810350458018}\n******************************\nTRAIN: \t Epoch: 36 \t Loss: -0.008859081193804741\nTRAIN: \t Epoch: 36 \t Loss: -0.009066428523510695\nTRAIN: \t Epoch: 36 \t Loss: -0.008506326160083214\nTRAIN: \t Epoch: 36 \t Loss: -0.008465988445095718\nTRAIN: \t Epoch: 36 \t Loss: -0.008427937794476748\nTRAIN: \t Epoch: 36 \t Loss: -0.008350291987881064\nTRAIN: \t Epoch: 36 \t Loss: -0.007853663659521512\nTRAIN: \t Epoch: 36 \t Loss: -0.007596244686283171\nTRAIN: \t Epoch: 36 \t Loss: -0.007590101359205114\nTRAIN: \t Epoch: 36 \t Loss: -0.0077400813344866036\nTRAIN: \t Epoch: 36 \t Loss: -0.007865275129337202\nTRAIN: \t Epoch: 36 \t Loss: -0.007974937985030314\nTRAIN: \t Epoch: 36 \t Loss: -0.008046179185979642\nTRAIN: \t Epoch: 36 \t Loss: -0.008057507586532406\nTRAIN: \t Epoch: 36 \t Loss: -0.00812776315336426\nTRAIN: \t Epoch: 36 \t Loss: -0.008181464887456968\nTRAIN: \t Epoch: 36 \t Loss: -0.008022372035638374\nTRAIN: \t Epoch: 36 \t Loss: -0.007778377078163127\nTRAIN: \t Epoch: 36 \t Loss: -0.007710192683397939\nTRAIN: \t Epoch: 36 \t Loss: -0.007751571608241648\nTRAIN: \t Epoch: 36 \t Loss: -0.007759013310155229\nVALD: \t Epoch: 36 \t Loss: -0.008297275751829147\nVALD: \t Epoch: 36 \t Loss: -0.008804758079349995\nVALD: \t Epoch: 36 \t Loss: -0.0083377487026155\nVALD: \t Epoch: 36 \t Loss: -0.008814202737994492\nVALD: \t Epoch: 36 \t Loss: -0.008406455774046181\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.34it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.7371680384189961  FDE: 1.178609532068826\n**************************************************\n******************************\nEpoch: social-tag : 36\ntrain_loss -0.007759013310155229\nval_loss -0.008406455774046181\n{'min_val_epoch': 36, 'min_val_loss': -0.008406455774046181}\n******************************\nTRAIN: \t Epoch: 37 \t Loss: -0.009092560037970543\nTRAIN: \t Epoch: 37 \t Loss: -0.008905978873372078\nTRAIN: \t Epoch: 37 \t Loss: -0.008585566033919653\nTRAIN: \t Epoch: 37 \t Loss: -0.008567249868065119\nTRAIN: \t Epoch: 37 \t Loss: -0.008346003573387861\nTRAIN: \t Epoch: 37 \t Loss: -0.008045754628255963\nTRAIN: \t Epoch: 37 \t Loss: -0.007639272138476372\nTRAIN: \t Epoch: 37 \t Loss: -0.0076504924218170345\nTRAIN: \t Epoch: 37 \t Loss: -0.007822505560600095\nTRAIN: \t Epoch: 37 \t Loss: -0.007961660763248802\nTRAIN: \t Epoch: 37 \t Loss: -0.00803764575076374\nTRAIN: \t Epoch: 37 \t Loss: -0.008061807331008216\nTRAIN: \t Epoch: 37 \t Loss: -0.007849305903968902\nTRAIN: \t Epoch: 37 \t Loss: -0.007702816843188235\nTRAIN: \t Epoch: 37 \t Loss: -0.007706115115433931\nTRAIN: \t Epoch: 37 \t Loss: -0.007762074732454494\nTRAIN: \t Epoch: 37 \t Loss: -0.007863855674205458\nTRAIN: \t Epoch: 37 \t Loss: -0.007970585358432598\nTRAIN: \t Epoch: 37 \t Loss: -0.008008440294744153\nTRAIN: \t Epoch: 37 \t Loss: -0.008040212909691036\nTRAIN: \t Epoch: 37 \t Loss: -0.008037261176587254\nVALD: \t Epoch: 37 \t Loss: -0.009248643182218075\nVALD: \t Epoch: 37 \t Loss: -0.009310510940849781\nVALD: \t Epoch: 37 \t Loss: -0.008768632852782806\nVALD: \t Epoch: 37 \t Loss: -0.009374598157592118\nVALD: \t Epoch: 37 \t Loss: -0.0088182606174942\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.41it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.6815273735804251  FDE: 1.092554606627805\n**************************************************\n******************************\nEpoch: social-tag : 37\ntrain_loss -0.008037261176587254\nval_loss -0.0088182606174942\n{'min_val_epoch': 37, 'min_val_loss': -0.0088182606174942}\n******************************\nTRAIN: \t Epoch: 38 \t Loss: -0.008184236474335194\nTRAIN: \t Epoch: 38 \t Loss: -0.008088801521807909\nTRAIN: \t Epoch: 38 \t Loss: -0.007629181258380413\nTRAIN: \t Epoch: 38 \t Loss: -0.007281174534000456\nTRAIN: \t Epoch: 38 \t Loss: -0.00733216293156147\nTRAIN: \t Epoch: 38 \t Loss: -0.007532545520613591\nTRAIN: \t Epoch: 38 \t Loss: -0.007761325953262193\nTRAIN: \t Epoch: 38 \t Loss: -0.00787436927203089\nTRAIN: \t Epoch: 38 \t Loss: -0.007987952687674098\nTRAIN: \t Epoch: 38 \t Loss: -0.007914482476189732\nTRAIN: \t Epoch: 38 \t Loss: -0.007856939114968885\nTRAIN: \t Epoch: 38 \t Loss: -0.0077776884427294135\nTRAIN: \t Epoch: 38 \t Loss: -0.007847379212482618\nTRAIN: \t Epoch: 38 \t Loss: -0.00793465948663652\nTRAIN: \t Epoch: 38 \t Loss: -0.00795633920157949\nTRAIN: \t Epoch: 38 \t Loss: -0.00795669955550693\nTRAIN: \t Epoch: 38 \t Loss: -0.00794811883722158\nTRAIN: \t Epoch: 38 \t Loss: -0.00785620655450556\nTRAIN: \t Epoch: 38 \t Loss: -0.007837639067714152\nTRAIN: \t Epoch: 38 \t Loss: -0.007872940390370786\nTRAIN: \t Epoch: 38 \t Loss: -0.007888159652168051\nVALD: \t Epoch: 38 \t Loss: -0.007757371291518211\nVALD: \t Epoch: 38 \t Loss: -0.008678349200636148\nVALD: \t Epoch: 38 \t Loss: -0.008073374163359404\nVALD: \t Epoch: 38 \t Loss: -0.00878772244323045\nVALD: \t Epoch: 38 \t Loss: -0.008361108061196148\n******************************\nEpoch: social-tag : 38\ntrain_loss -0.007888159652168051\nval_loss -0.008361108061196148\n{'min_val_epoch': 37, 'min_val_loss': -0.0088182606174942}\n******************************\nTRAIN: \t Epoch: 39 \t Loss: -0.008611221797764301\nTRAIN: \t Epoch: 39 \t Loss: -0.00894438149407506\nTRAIN: \t Epoch: 39 \t Loss: -0.008997950082023939\nTRAIN: \t Epoch: 39 \t Loss: -0.008801273768767715\nTRAIN: \t Epoch: 39 \t Loss: -0.00782369151711464\nTRAIN: \t Epoch: 39 \t Loss: -0.006971677454809348\nTRAIN: \t Epoch: 39 \t Loss: -0.006852695252746344\nTRAIN: \t Epoch: 39 \t Loss: -0.0069343820214271545\nTRAIN: \t Epoch: 39 \t Loss: -0.007054803996450371\nTRAIN: \t Epoch: 39 \t Loss: -0.007240988034754992\nTRAIN: \t Epoch: 39 \t Loss: -0.007386946796693585\nTRAIN: \t Epoch: 39 \t Loss: -0.007440430345013738\nTRAIN: \t Epoch: 39 \t Loss: -0.007575357691026651\nTRAIN: \t Epoch: 39 \t Loss: -0.00769426766782999\nTRAIN: \t Epoch: 39 \t Loss: -0.007577479692796866\nTRAIN: \t Epoch: 39 \t Loss: -0.007432647602399811\nTRAIN: \t Epoch: 39 \t Loss: -0.007399107302155565\nTRAIN: \t Epoch: 39 \t Loss: -0.007490771693281001\nTRAIN: \t Epoch: 39 \t Loss: -0.007589298521021479\nTRAIN: \t Epoch: 39 \t Loss: -0.007663819310255349\nTRAIN: \t Epoch: 39 \t Loss: -0.007696262797026241\nVALD: \t Epoch: 39 \t Loss: -0.009519156068563461\nVALD: \t Epoch: 39 \t Loss: -0.010051894932985306\nVALD: \t Epoch: 39 \t Loss: -0.009669963891307512\nVALD: \t Epoch: 39 \t Loss: -0.010225208243355155\nVALD: \t Epoch: 39 \t Loss: -0.00952784297162976\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.41it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.6805635618516168  FDE: 1.11973500983818\n**************************************************\n******************************\nEpoch: social-tag : 39\ntrain_loss -0.007696262797026241\nval_loss -0.00952784297162976\n{'min_val_epoch': 39, 'min_val_loss': -0.00952784297162976}\n******************************\nTRAIN: \t Epoch: 40 \t Loss: -0.009372030384838581\nTRAIN: \t Epoch: 40 \t Loss: -0.009186124894768\nTRAIN: \t Epoch: 40 \t Loss: -0.008969690650701523\nTRAIN: \t Epoch: 40 \t Loss: -0.00825718161650002\nTRAIN: \t Epoch: 40 \t Loss: -0.007883088663220406\nTRAIN: \t Epoch: 40 \t Loss: -0.00798356036345164\nTRAIN: \t Epoch: 40 \t Loss: -0.008192393529628004\nTRAIN: \t Epoch: 40 \t Loss: -0.008241438306868076\nTRAIN: \t Epoch: 40 \t Loss: -0.008308787726693682\nTRAIN: \t Epoch: 40 \t Loss: -0.008377820812165737\nTRAIN: \t Epoch: 40 \t Loss: -0.008397608064115047\nTRAIN: \t Epoch: 40 \t Loss: -0.008209674075866738\nTRAIN: \t Epoch: 40 \t Loss: -0.008017416721066603\nTRAIN: \t Epoch: 40 \t Loss: -0.008005799160205893\nTRAIN: \t Epoch: 40 \t Loss: -0.008100847247987986\nTRAIN: \t Epoch: 40 \t Loss: -0.008189010637579486\nTRAIN: \t Epoch: 40 \t Loss: -0.008267919122077087\nTRAIN: \t Epoch: 40 \t Loss: -0.008285303470782109\nTRAIN: \t Epoch: 40 \t Loss: -0.008049127866366976\nTRAIN: \t Epoch: 40 \t Loss: -0.007805750949773938\nTRAIN: \t Epoch: 40 \t Loss: -0.007797252521803495\nVALD: \t Epoch: 40 \t Loss: -0.0061263954266905785\nVALD: \t Epoch: 40 \t Loss: -0.007042993325740099\nVALD: \t Epoch: 40 \t Loss: -0.006763117543111245\nVALD: \t Epoch: 40 \t Loss: -0.007302784943021834\nVALD: \t Epoch: 40 \t Loss: -0.00700940995592618\n******************************\nEpoch: social-tag : 40\ntrain_loss -0.007797252521803495\nval_loss -0.00700940995592618\n{'min_val_epoch': 39, 'min_val_loss': -0.00952784297162976}\n******************************\nTRAIN: \t Epoch: 41 \t Loss: -0.006408324930816889\nTRAIN: \t Epoch: 41 \t Loss: -0.007743751397356391\nTRAIN: \t Epoch: 41 \t Loss: -0.008244098940243324\nTRAIN: \t Epoch: 41 \t Loss: -0.008746286039240658\nTRAIN: \t Epoch: 41 \t Loss: -0.00889250198379159\nTRAIN: \t Epoch: 41 \t Loss: -0.008868665356809894\nTRAIN: \t Epoch: 41 \t Loss: -0.008744193440569299\nTRAIN: \t Epoch: 41 \t Loss: -0.008555524284020066\nTRAIN: \t Epoch: 41 \t Loss: -0.008329703938215971\nTRAIN: \t Epoch: 41 \t Loss: -0.00834526908583939\nTRAIN: \t Epoch: 41 \t Loss: -0.008342213124375452\nTRAIN: \t Epoch: 41 \t Loss: -0.008409860427491367\nTRAIN: \t Epoch: 41 \t Loss: -0.008437694802593727\nTRAIN: \t Epoch: 41 \t Loss: -0.008419605537450739\nTRAIN: \t Epoch: 41 \t Loss: -0.008281875029206276\nTRAIN: \t Epoch: 41 \t Loss: -0.008179622003808618\nTRAIN: \t Epoch: 41 \t Loss: -0.00816844655748676\nTRAIN: \t Epoch: 41 \t Loss: -0.00824120526926385\nTRAIN: \t Epoch: 41 \t Loss: -0.008290093784269533\nTRAIN: \t Epoch: 41 \t Loss: -0.008333047106862067\nTRAIN: \t Epoch: 41 \t Loss: -0.008345139823974604\nVALD: \t Epoch: 41 \t Loss: -0.009271831251680851\nVALD: \t Epoch: 41 \t Loss: -0.009944293648004532\nVALD: \t Epoch: 41 \t Loss: -0.009515378313759962\nVALD: \t Epoch: 41 \t Loss: -0.01012654509395361\nVALD: \t Epoch: 41 \t Loss: -0.009525944357332975\n******************************\nEpoch: social-tag : 41\ntrain_loss -0.008345139823974604\nval_loss -0.009525944357332975\n{'min_val_epoch': 39, 'min_val_loss': -0.00952784297162976}\n******************************\nTRAIN: \t Epoch: 42 \t Loss: -0.00961516797542572\nTRAIN: \t Epoch: 42 \t Loss: -0.00943355681374669\nTRAIN: \t Epoch: 42 \t Loss: -0.00927987756828467\nTRAIN: \t Epoch: 42 \t Loss: -0.009019318269565701\nTRAIN: \t Epoch: 42 \t Loss: -0.00829945495352149\nTRAIN: \t Epoch: 42 \t Loss: -0.007857419938469926\nTRAIN: \t Epoch: 42 \t Loss: -0.007907047136021512\nTRAIN: \t Epoch: 42 \t Loss: -0.007979448477271944\nTRAIN: \t Epoch: 42 \t Loss: -0.008189197577950027\nTRAIN: \t Epoch: 42 \t Loss: -0.008142366306856275\nTRAIN: \t Epoch: 42 \t Loss: -0.008226449643685059\nTRAIN: \t Epoch: 42 \t Loss: -0.00831112137529999\nTRAIN: \t Epoch: 42 \t Loss: -0.008299302488852005\nTRAIN: \t Epoch: 42 \t Loss: -0.008269402231755001\nTRAIN: \t Epoch: 42 \t Loss: -0.008182899902264278\nTRAIN: \t Epoch: 42 \t Loss: -0.008161548350472003\nTRAIN: \t Epoch: 42 \t Loss: -0.00816851613276145\nTRAIN: \t Epoch: 42 \t Loss: -0.008221853524446487\nTRAIN: \t Epoch: 42 \t Loss: -0.008264152039038507\nTRAIN: \t Epoch: 42 \t Loss: -0.008332177717238664\nTRAIN: \t Epoch: 42 \t Loss: -0.008318570829531552\nVALD: \t Epoch: 42 \t Loss: -0.005440401379019022\nVALD: \t Epoch: 42 \t Loss: -0.00767852202989161\nVALD: \t Epoch: 42 \t Loss: -0.006760680116713047\nVALD: \t Epoch: 42 \t Loss: -0.007762847701087594\nVALD: \t Epoch: 42 \t Loss: -0.007511292870685866\n******************************\nEpoch: social-tag : 42\ntrain_loss -0.008318570829531552\nval_loss -0.007511292870685866\n{'min_val_epoch': 39, 'min_val_loss': -0.00952784297162976}\n******************************\nTRAIN: \t Epoch: 43 \t Loss: -0.008627895265817642\nTRAIN: \t Epoch: 43 \t Loss: -0.008913157507777214\nTRAIN: \t Epoch: 43 \t Loss: -0.008816740475594997\nTRAIN: \t Epoch: 43 \t Loss: -0.008671518880873919\nTRAIN: \t Epoch: 43 \t Loss: -0.00819010566920042\nTRAIN: \t Epoch: 43 \t Loss: -0.008012065275882682\nTRAIN: \t Epoch: 43 \t Loss: -0.008172947048608745\nTRAIN: \t Epoch: 43 \t Loss: -0.008304275863338262\nTRAIN: \t Epoch: 43 \t Loss: -0.008409462196545469\nTRAIN: \t Epoch: 43 \t Loss: -0.008422265341505409\nTRAIN: \t Epoch: 43 \t Loss: -0.008196038363332098\nTRAIN: \t Epoch: 43 \t Loss: -0.008166409640883407\nTRAIN: \t Epoch: 43 \t Loss: -0.008214404734854516\nTRAIN: \t Epoch: 43 \t Loss: -0.008312541459287916\nTRAIN: \t Epoch: 43 \t Loss: -0.008363293794294199\nTRAIN: \t Epoch: 43 \t Loss: -0.008393879397772253\nTRAIN: \t Epoch: 43 \t Loss: -0.008279831340426909\nTRAIN: \t Epoch: 43 \t Loss: -0.00810272632063263\nTRAIN: \t Epoch: 43 \t Loss: -0.008046546240190142\nTRAIN: \t Epoch: 43 \t Loss: -0.008088341145776213\nTRAIN: \t Epoch: 43 \t Loss: -0.008071216160780482\nVALD: \t Epoch: 43 \t Loss: -0.008428344503045082\nVALD: \t Epoch: 43 \t Loss: -0.009361258242279291\nVALD: \t Epoch: 43 \t Loss: -0.008917772521575293\nVALD: \t Epoch: 43 \t Loss: -0.009585947263985872\nVALD: \t Epoch: 43 \t Loss: -0.009072034159336304\n******************************\nEpoch: social-tag : 43\ntrain_loss -0.008071216160780482\nval_loss -0.009072034159336304\n{'min_val_epoch': 39, 'min_val_loss': -0.00952784297162976}\n******************************\nTRAIN: \t Epoch: 44 \t Loss: -0.00882840808480978\nTRAIN: \t Epoch: 44 \t Loss: -0.009358021896332502\nTRAIN: \t Epoch: 44 \t Loss: -0.009346618937949339\nTRAIN: \t Epoch: 44 \t Loss: -0.008972381241619587\nTRAIN: \t Epoch: 44 \t Loss: -0.008508675079792739\nTRAIN: \t Epoch: 44 \t Loss: -0.008221131982281804\nTRAIN: \t Epoch: 44 \t Loss: -0.008205960837325879\nTRAIN: \t Epoch: 44 \t Loss: -0.008391902956645936\nTRAIN: \t Epoch: 44 \t Loss: -0.008605626256515583\nTRAIN: \t Epoch: 44 \t Loss: -0.008684357767924667\nTRAIN: \t Epoch: 44 \t Loss: -0.008731153666634451\nTRAIN: \t Epoch: 44 \t Loss: -0.00875812431331724\nTRAIN: \t Epoch: 44 \t Loss: -0.008656840771436691\nTRAIN: \t Epoch: 44 \t Loss: -0.008405935085777725\nTRAIN: \t Epoch: 44 \t Loss: -0.00828892265756925\nTRAIN: \t Epoch: 44 \t Loss: -0.008290943806059659\nTRAIN: \t Epoch: 44 \t Loss: -0.008333898313781795\nTRAIN: \t Epoch: 44 \t Loss: -0.008373427256527875\nTRAIN: \t Epoch: 44 \t Loss: -0.008481644584160102\nTRAIN: \t Epoch: 44 \t Loss: -0.008527622604742647\nTRAIN: \t Epoch: 44 \t Loss: -0.008539477356048207\nVALD: \t Epoch: 44 \t Loss: -0.011085966601967812\nVALD: \t Epoch: 44 \t Loss: -0.011154228821396828\nVALD: \t Epoch: 44 \t Loss: -0.010768066781262556\nVALD: \t Epoch: 44 \t Loss: -0.011156858410686255\nVALD: \t Epoch: 44 \t Loss: -0.010375550595650542\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.13it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.6148678118959295  FDE: 0.9846855459226235\n**************************************************\n******************************\nEpoch: social-tag : 44\ntrain_loss -0.008539477356048207\nval_loss -0.010375550595650542\n{'min_val_epoch': 44, 'min_val_loss': -0.010375550595650542}\n******************************\nTRAIN: \t Epoch: 45 \t Loss: -0.010336332954466343\nTRAIN: \t Epoch: 45 \t Loss: -0.010328061413019896\nTRAIN: \t Epoch: 45 \t Loss: -0.009623796679079533\nTRAIN: \t Epoch: 45 \t Loss: -0.008249082951806486\nTRAIN: \t Epoch: 45 \t Loss: -0.007882850430905818\nTRAIN: \t Epoch: 45 \t Loss: -0.007998842280358076\nTRAIN: \t Epoch: 45 \t Loss: -0.008171935831861836\nTRAIN: \t Epoch: 45 \t Loss: -0.008276746841147542\nTRAIN: \t Epoch: 45 \t Loss: -0.008455796270734735\nTRAIN: \t Epoch: 45 \t Loss: -0.008484463579952717\nTRAIN: \t Epoch: 45 \t Loss: -0.008454578674652359\nTRAIN: \t Epoch: 45 \t Loss: -0.008406111970543861\nTRAIN: \t Epoch: 45 \t Loss: -0.008323521389124485\nTRAIN: \t Epoch: 45 \t Loss: -0.008316053443455271\nTRAIN: \t Epoch: 45 \t Loss: -0.00829879054799676\nTRAIN: \t Epoch: 45 \t Loss: -0.008319782180478796\nTRAIN: \t Epoch: 45 \t Loss: -0.008311639676856645\nTRAIN: \t Epoch: 45 \t Loss: -0.00834425505147212\nTRAIN: \t Epoch: 45 \t Loss: -0.008351622860094434\nTRAIN: \t Epoch: 45 \t Loss: -0.008361925440840422\nTRAIN: \t Epoch: 45 \t Loss: -0.00835509470727872\nVALD: \t Epoch: 45 \t Loss: -0.0030641548801213503\nVALD: \t Epoch: 45 \t Loss: -0.006351869669742882\nVALD: \t Epoch: 45 \t Loss: -0.006141225108876824\nVALD: \t Epoch: 45 \t Loss: -0.007398250687401742\nVALD: \t Epoch: 45 \t Loss: -0.0071817707996245555\n******************************\nEpoch: social-tag : 45\ntrain_loss -0.00835509470727872\nval_loss -0.0071817707996245555\n{'min_val_epoch': 44, 'min_val_loss': -0.010375550595650542}\n******************************\nTRAIN: \t Epoch: 46 \t Loss: -0.008084137924015522\nTRAIN: \t Epoch: 46 \t Loss: -0.009113442152738571\nTRAIN: \t Epoch: 46 \t Loss: -0.009514509389797846\nTRAIN: \t Epoch: 46 \t Loss: -0.009387485450133681\nTRAIN: \t Epoch: 46 \t Loss: -0.008961887564510107\nTRAIN: \t Epoch: 46 \t Loss: -0.008488562268515428\nTRAIN: \t Epoch: 46 \t Loss: -0.008408197867018836\nTRAIN: \t Epoch: 46 \t Loss: -0.008509516017511487\nTRAIN: \t Epoch: 46 \t Loss: -0.008630834105941985\nTRAIN: \t Epoch: 46 \t Loss: -0.008713953569531441\nTRAIN: \t Epoch: 46 \t Loss: -0.008745814334939827\nTRAIN: \t Epoch: 46 \t Loss: -0.008625941739107171\nTRAIN: \t Epoch: 46 \t Loss: -0.008503809284705382\nTRAIN: \t Epoch: 46 \t Loss: -0.008487038646957703\nTRAIN: \t Epoch: 46 \t Loss: -0.008545699653526147\nTRAIN: \t Epoch: 46 \t Loss: -0.008610253513325006\nTRAIN: \t Epoch: 46 \t Loss: -0.008630224229658352\nTRAIN: \t Epoch: 46 \t Loss: -0.00860260540826453\nTRAIN: \t Epoch: 46 \t Loss: -0.008511622117734268\nTRAIN: \t Epoch: 46 \t Loss: -0.008468946442008018\nTRAIN: \t Epoch: 46 \t Loss: -0.008456844408510278\nVALD: \t Epoch: 46 \t Loss: -0.007048599887639284\nVALD: \t Epoch: 46 \t Loss: -0.008753405651077628\nVALD: \t Epoch: 46 \t Loss: -0.00884116580709815\nVALD: \t Epoch: 46 \t Loss: -0.009593501570634544\nVALD: \t Epoch: 46 \t Loss: -0.009095329017454875\n******************************\nEpoch: social-tag : 46\ntrain_loss -0.008456844408510278\nval_loss -0.009095329017454875\n{'min_val_epoch': 44, 'min_val_loss': -0.010375550595650542}\n******************************\nTRAIN: \t Epoch: 47 \t Loss: -0.009427168406546116\nTRAIN: \t Epoch: 47 \t Loss: -0.009452543687075377\nTRAIN: \t Epoch: 47 \t Loss: -0.009282557604213556\nTRAIN: \t Epoch: 47 \t Loss: -0.009444944560527802\nTRAIN: \t Epoch: 47 \t Loss: -0.009235012345016003\nTRAIN: \t Epoch: 47 \t Loss: -0.00861512574677666\nTRAIN: \t Epoch: 47 \t Loss: -0.008183277544698544\nTRAIN: \t Epoch: 47 \t Loss: -0.008256432425696403\nTRAIN: \t Epoch: 47 \t Loss: -0.00839530878389875\nTRAIN: \t Epoch: 47 \t Loss: -0.008510579774156212\nTRAIN: \t Epoch: 47 \t Loss: -0.008566424546932632\nTRAIN: \t Epoch: 47 \t Loss: -0.008699614515838524\nTRAIN: \t Epoch: 47 \t Loss: -0.008786539009843882\nTRAIN: \t Epoch: 47 \t Loss: -0.008782182121649384\nTRAIN: \t Epoch: 47 \t Loss: -0.008804128101716439\nTRAIN: \t Epoch: 47 \t Loss: -0.008768670115387067\nTRAIN: \t Epoch: 47 \t Loss: -0.008695672485320008\nTRAIN: \t Epoch: 47 \t Loss: -0.00862680864520371\nTRAIN: \t Epoch: 47 \t Loss: -0.008646248668236168\nTRAIN: \t Epoch: 47 \t Loss: -0.008703728136606515\nTRAIN: \t Epoch: 47 \t Loss: -0.008709894399874562\nVALD: \t Epoch: 47 \t Loss: -0.008733591996133327\nVALD: \t Epoch: 47 \t Loss: -0.009598230011761189\nVALD: \t Epoch: 47 \t Loss: -0.008934154020001491\nVALD: \t Epoch: 47 \t Loss: -0.009691909071989357\nVALD: \t Epoch: 47 \t Loss: -0.009191233178843622\n******************************\nEpoch: social-tag : 47\ntrain_loss -0.008709894399874562\nval_loss -0.009191233178843622\n{'min_val_epoch': 44, 'min_val_loss': -0.010375550595650542}\n******************************\nTRAIN: \t Epoch: 48 \t Loss: -0.009884468279778957\nTRAIN: \t Epoch: 48 \t Loss: -0.00966074038296938\nTRAIN: \t Epoch: 48 \t Loss: -0.009615209884941578\nTRAIN: \t Epoch: 48 \t Loss: -0.009111930965445936\nTRAIN: \t Epoch: 48 \t Loss: -0.008498381357640028\nTRAIN: \t Epoch: 48 \t Loss: -0.008537695199872056\nTRAIN: \t Epoch: 48 \t Loss: -0.008701693466199296\nTRAIN: \t Epoch: 48 \t Loss: -0.008839403453748673\nTRAIN: \t Epoch: 48 \t Loss: -0.008962223564998971\nTRAIN: \t Epoch: 48 \t Loss: -0.009095774358138442\nTRAIN: \t Epoch: 48 \t Loss: -0.008949254639446735\nTRAIN: \t Epoch: 48 \t Loss: -0.008705771411769092\nTRAIN: \t Epoch: 48 \t Loss: -0.00855432364803094\nTRAIN: \t Epoch: 48 \t Loss: -0.00859363077740584\nTRAIN: \t Epoch: 48 \t Loss: -0.008660081028938293\nTRAIN: \t Epoch: 48 \t Loss: -0.008761882316321135\nTRAIN: \t Epoch: 48 \t Loss: -0.00884732132887139\nTRAIN: \t Epoch: 48 \t Loss: -0.00881118844780657\nTRAIN: \t Epoch: 48 \t Loss: -0.008681152555111208\nTRAIN: \t Epoch: 48 \t Loss: -0.008569358312524856\nTRAIN: \t Epoch: 48 \t Loss: -0.008559534629216633\nVALD: \t Epoch: 48 \t Loss: -0.001129631418734789\nVALD: \t Epoch: 48 \t Loss: -0.004617980914190412\nVALD: \t Epoch: 48 \t Loss: -0.003912187414243817\nVALD: \t Epoch: 48 \t Loss: -0.005416417436208576\nVALD: \t Epoch: 48 \t Loss: -0.005445727259640532\n******************************\nEpoch: social-tag : 48\ntrain_loss -0.008559534629216633\nval_loss -0.005445727259640532\n{'min_val_epoch': 44, 'min_val_loss': -0.010375550595650542}\n******************************\nTRAIN: \t Epoch: 49 \t Loss: -0.008744991384446621\nTRAIN: \t Epoch: 49 \t Loss: -0.009121263399720192\nTRAIN: \t Epoch: 49 \t Loss: -0.009304802864789963\nTRAIN: \t Epoch: 49 \t Loss: -0.009222491178661585\nTRAIN: \t Epoch: 49 \t Loss: -0.009156310185790061\nTRAIN: \t Epoch: 49 \t Loss: -0.009277849458158016\nTRAIN: \t Epoch: 49 \t Loss: -0.009053419809788465\nTRAIN: \t Epoch: 49 \t Loss: -0.008752879744861275\nTRAIN: \t Epoch: 49 \t Loss: -0.008614621280382076\nTRAIN: \t Epoch: 49 \t Loss: -0.008740988792851567\nTRAIN: \t Epoch: 49 \t Loss: -0.008856443137946453\nTRAIN: \t Epoch: 49 \t Loss: -0.008946700138039887\nTRAIN: \t Epoch: 49 \t Loss: -0.008944551854466017\nTRAIN: \t Epoch: 49 \t Loss: -0.008982788605083312\nTRAIN: \t Epoch: 49 \t Loss: -0.008916957334925731\nTRAIN: \t Epoch: 49 \t Loss: -0.008834552601911128\nTRAIN: \t Epoch: 49 \t Loss: -0.008821279413121589\nTRAIN: \t Epoch: 49 \t Loss: -0.008867892488423321\nTRAIN: \t Epoch: 49 \t Loss: -0.008922899877162356\nTRAIN: \t Epoch: 49 \t Loss: -0.008959606289863586\nTRAIN: \t Epoch: 49 \t Loss: -0.008950767442641483\nVALD: \t Epoch: 49 \t Loss: -0.008513685315847397\nVALD: \t Epoch: 49 \t Loss: -0.009443199262022972\nVALD: \t Epoch: 49 \t Loss: -0.008850050158798695\nVALD: \t Epoch: 49 \t Loss: -0.009687145240604877\nVALD: \t Epoch: 49 \t Loss: -0.009222880749694774\n******************************\nEpoch: social-tag : 49\ntrain_loss -0.008950767442641483\nval_loss -0.009222880749694774\n{'min_val_epoch': 44, 'min_val_loss': -0.010375550595650542}\n******************************\nTRAIN: \t Epoch: 50 \t Loss: -0.00889684446156025\nTRAIN: \t Epoch: 50 \t Loss: -0.009507325477898121\nTRAIN: \t Epoch: 50 \t Loss: -0.009553437121212482\nTRAIN: \t Epoch: 50 \t Loss: -0.00959797971881926\nTRAIN: \t Epoch: 50 \t Loss: -0.009481849521398545\nTRAIN: \t Epoch: 50 \t Loss: -0.00936038171251615\nTRAIN: \t Epoch: 50 \t Loss: -0.008925064799508877\nTRAIN: \t Epoch: 50 \t Loss: -0.00880696001695469\nTRAIN: \t Epoch: 50 \t Loss: -0.008837833121005032\nTRAIN: \t Epoch: 50 \t Loss: -0.008860033424571156\nTRAIN: \t Epoch: 50 \t Loss: -0.009022299073297869\nTRAIN: \t Epoch: 50 \t Loss: -0.00904648615202556\nTRAIN: \t Epoch: 50 \t Loss: -0.008946927586713662\nTRAIN: \t Epoch: 50 \t Loss: -0.008768502523058228\nTRAIN: \t Epoch: 50 \t Loss: -0.008757792444278796\nTRAIN: \t Epoch: 50 \t Loss: -0.008802429802017286\nTRAIN: \t Epoch: 50 \t Loss: -0.008848852286224855\nTRAIN: \t Epoch: 50 \t Loss: -0.008879650099616911\nTRAIN: \t Epoch: 50 \t Loss: -0.00885944649283039\nTRAIN: \t Epoch: 50 \t Loss: -0.008885252452455462\nTRAIN: \t Epoch: 50 \t Loss: -0.00887819478947471\nVALD: \t Epoch: 50 \t Loss: -0.005116876680403948\nVALD: \t Epoch: 50 \t Loss: -0.007638814626261592\nVALD: \t Epoch: 50 \t Loss: -0.007663360020766656\nVALD: \t Epoch: 50 \t Loss: -0.008708494366146624\nVALD: \t Epoch: 50 \t Loss: -0.008332527682017206\n******************************\nEpoch: social-tag : 50\ntrain_loss -0.00887819478947471\nval_loss -0.008332527682017206\n{'min_val_epoch': 44, 'min_val_loss': -0.010375550595650542}\n******************************\nTRAIN: \t Epoch: 51 \t Loss: -0.009131094440817833\nTRAIN: \t Epoch: 51 \t Loss: -0.009451097808778286\nTRAIN: \t Epoch: 51 \t Loss: -0.009064223306874434\nTRAIN: \t Epoch: 51 \t Loss: -0.00889477995224297\nTRAIN: \t Epoch: 51 \t Loss: -0.008860654756426811\nTRAIN: \t Epoch: 51 \t Loss: -0.009060225759943327\nTRAIN: \t Epoch: 51 \t Loss: -0.009189211497349399\nTRAIN: \t Epoch: 51 \t Loss: -0.009035803959704936\nTRAIN: \t Epoch: 51 \t Loss: -0.00878808026512464\nTRAIN: \t Epoch: 51 \t Loss: -0.008754743169993163\nTRAIN: \t Epoch: 51 \t Loss: -0.008875768801028078\nTRAIN: \t Epoch: 51 \t Loss: -0.008992569986730814\nTRAIN: \t Epoch: 51 \t Loss: -0.009027457724397\nTRAIN: \t Epoch: 51 \t Loss: -0.009007513323532683\nTRAIN: \t Epoch: 51 \t Loss: -0.009003487663964431\nTRAIN: \t Epoch: 51 \t Loss: -0.00898172875167802\nTRAIN: \t Epoch: 51 \t Loss: -0.00895462924724116\nTRAIN: \t Epoch: 51 \t Loss: -0.00888023768655128\nTRAIN: \t Epoch: 51 \t Loss: -0.00884134003794507\nTRAIN: \t Epoch: 51 \t Loss: -0.008854650473222137\nTRAIN: \t Epoch: 51 \t Loss: -0.008867507256171109\nVALD: \t Epoch: 51 \t Loss: -0.005047767423093319\nVALD: \t Epoch: 51 \t Loss: -0.007415427826344967\nVALD: \t Epoch: 51 \t Loss: -0.0062161257956177\nVALD: \t Epoch: 51 \t Loss: -0.007495300320442766\nVALD: \t Epoch: 51 \t Loss: -0.007241724145969139\n******************************\nEpoch: social-tag : 51\ntrain_loss -0.008867507256171109\nval_loss -0.007241724145969139\n{'min_val_epoch': 44, 'min_val_loss': -0.010375550595650542}\n******************************\nTRAIN: \t Epoch: 52 \t Loss: -0.010469573549926281\nTRAIN: \t Epoch: 52 \t Loss: -0.009775107260793447\nTRAIN: \t Epoch: 52 \t Loss: -0.008705114014446735\nTRAIN: \t Epoch: 52 \t Loss: -0.008219694020226598\nTRAIN: \t Epoch: 52 \t Loss: -0.008350388891994954\nTRAIN: \t Epoch: 52 \t Loss: -0.008688796311616898\nTRAIN: \t Epoch: 52 \t Loss: -0.008953292854130268\nTRAIN: \t Epoch: 52 \t Loss: -0.00901143066585064\nTRAIN: \t Epoch: 52 \t Loss: -0.008938202427493202\nTRAIN: \t Epoch: 52 \t Loss: -0.00874783149920404\nTRAIN: \t Epoch: 52 \t Loss: -0.008545793846926907\nTRAIN: \t Epoch: 52 \t Loss: -0.008605012049277624\nTRAIN: \t Epoch: 52 \t Loss: -0.008714588573918892\nTRAIN: \t Epoch: 52 \t Loss: -0.008807078003883362\nTRAIN: \t Epoch: 52 \t Loss: -0.00887633574505647\nTRAIN: \t Epoch: 52 \t Loss: -0.008802723081316799\nTRAIN: \t Epoch: 52 \t Loss: -0.008694245041731526\nTRAIN: \t Epoch: 52 \t Loss: -0.008663081460528903\nTRAIN: \t Epoch: 52 \t Loss: -0.00871242852391381\nTRAIN: \t Epoch: 52 \t Loss: -0.008792536053806543\nTRAIN: \t Epoch: 52 \t Loss: -0.008782453978622337\nVALD: \t Epoch: 52 \t Loss: -0.005685990676283836\nVALD: \t Epoch: 52 \t Loss: -0.008273288141936064\nVALD: \t Epoch: 52 \t Loss: -0.007665224839001894\nVALD: \t Epoch: 52 \t Loss: -0.008710623835213482\nVALD: \t Epoch: 52 \t Loss: -0.008322720942289932\n******************************\nEpoch: social-tag : 52\ntrain_loss -0.008782453978622337\nval_loss -0.008322720942289932\n{'min_val_epoch': 44, 'min_val_loss': -0.010375550595650542}\n******************************\nTRAIN: \t Epoch: 53 \t Loss: -0.010667105205357075\nTRAIN: \t Epoch: 53 \t Loss: -0.010709794238209724\nTRAIN: \t Epoch: 53 \t Loss: -0.009927365618447462\nTRAIN: \t Epoch: 53 \t Loss: -0.009291995083913207\nTRAIN: \t Epoch: 53 \t Loss: -0.008902544993907214\nTRAIN: \t Epoch: 53 \t Loss: -0.008896403713151813\nTRAIN: \t Epoch: 53 \t Loss: -0.008999294002673455\nTRAIN: \t Epoch: 53 \t Loss: -0.009144387498963624\nTRAIN: \t Epoch: 53 \t Loss: -0.00923165673803952\nTRAIN: \t Epoch: 53 \t Loss: -0.009144523506984114\nTRAIN: \t Epoch: 53 \t Loss: -0.009083141361109236\nTRAIN: \t Epoch: 53 \t Loss: -0.00892278835332642\nTRAIN: \t Epoch: 53 \t Loss: -0.0089072878472507\nTRAIN: \t Epoch: 53 \t Loss: -0.009018090520320194\nTRAIN: \t Epoch: 53 \t Loss: -0.00907509181027611\nTRAIN: \t Epoch: 53 \t Loss: -0.009102739713853225\nTRAIN: \t Epoch: 53 \t Loss: -0.009087121086742948\nTRAIN: \t Epoch: 53 \t Loss: -0.009076915758972367\nTRAIN: \t Epoch: 53 \t Loss: -0.00905186425600397\nTRAIN: \t Epoch: 53 \t Loss: -0.009019469679333269\nTRAIN: \t Epoch: 53 \t Loss: -0.009028509718781724\nVALD: \t Epoch: 53 \t Loss: -0.007372449152171612\nVALD: \t Epoch: 53 \t Loss: -0.00894953915849328\nVALD: \t Epoch: 53 \t Loss: -0.008780977688729763\nVALD: \t Epoch: 53 \t Loss: -0.009510711999610066\nVALD: \t Epoch: 53 \t Loss: -0.00906516081277107\n******************************\nEpoch: social-tag : 53\ntrain_loss -0.009028509718781724\nval_loss -0.00906516081277107\n{'min_val_epoch': 44, 'min_val_loss': -0.010375550595650542}\n******************************\nTRAIN: \t Epoch: 54 \t Loss: -0.010582348331809044\nTRAIN: \t Epoch: 54 \t Loss: -0.010514513589441776\nTRAIN: \t Epoch: 54 \t Loss: -0.009652803962429365\nTRAIN: \t Epoch: 54 \t Loss: -0.009209056617692113\nTRAIN: \t Epoch: 54 \t Loss: -0.009097247570753097\nTRAIN: \t Epoch: 54 \t Loss: -0.009164151269942522\nTRAIN: \t Epoch: 54 \t Loss: -0.009172102436423302\nTRAIN: \t Epoch: 54 \t Loss: -0.009096020250581205\nTRAIN: \t Epoch: 54 \t Loss: -0.009090348353816403\nTRAIN: \t Epoch: 54 \t Loss: -0.008937029540538788\nTRAIN: \t Epoch: 54 \t Loss: -0.008935424075885252\nTRAIN: \t Epoch: 54 \t Loss: -0.008984157970796028\nTRAIN: \t Epoch: 54 \t Loss: -0.009085221932484554\nTRAIN: \t Epoch: 54 \t Loss: -0.009182869789323636\nTRAIN: \t Epoch: 54 \t Loss: -0.009133918397128581\nTRAIN: \t Epoch: 54 \t Loss: -0.009041382640134543\nTRAIN: \t Epoch: 54 \t Loss: -0.008964211844346103\nTRAIN: \t Epoch: 54 \t Loss: -0.00898819649592042\nTRAIN: \t Epoch: 54 \t Loss: -0.009037242810192862\nTRAIN: \t Epoch: 54 \t Loss: -0.009070009179413318\nTRAIN: \t Epoch: 54 \t Loss: -0.009065034004386425\nVALD: \t Epoch: 54 \t Loss: -0.008721495047211647\nVALD: \t Epoch: 54 \t Loss: -0.009515529498457909\nVALD: \t Epoch: 54 \t Loss: -0.009088514993588129\nVALD: \t Epoch: 54 \t Loss: -0.009594793664291501\nVALD: \t Epoch: 54 \t Loss: -0.009108234718801895\n******************************\nEpoch: social-tag : 54\ntrain_loss -0.009065034004386425\nval_loss -0.009108234718801895\n{'min_val_epoch': 44, 'min_val_loss': -0.010375550595650542}\n******************************\nTRAIN: \t Epoch: 55 \t Loss: -0.009776913560926914\nTRAIN: \t Epoch: 55 \t Loss: -0.009829076007008553\nTRAIN: \t Epoch: 55 \t Loss: -0.009914077818393707\nTRAIN: \t Epoch: 55 \t Loss: -0.01004738942719996\nTRAIN: \t Epoch: 55 \t Loss: -0.009879145398736\nTRAIN: \t Epoch: 55 \t Loss: -0.009251931527008614\nTRAIN: \t Epoch: 55 \t Loss: -0.008914108321602856\nTRAIN: \t Epoch: 55 \t Loss: -0.009007631975691766\nTRAIN: \t Epoch: 55 \t Loss: -0.009105126839131117\nTRAIN: \t Epoch: 55 \t Loss: -0.009112933138385414\nTRAIN: \t Epoch: 55 \t Loss: -0.009215084708888422\nTRAIN: \t Epoch: 55 \t Loss: -0.009242630757701894\nTRAIN: \t Epoch: 55 \t Loss: -0.009172157635195898\nTRAIN: \t Epoch: 55 \t Loss: -0.009065288418371762\nTRAIN: \t Epoch: 55 \t Loss: -0.009110977221280337\nTRAIN: \t Epoch: 55 \t Loss: -0.00917244321317412\nTRAIN: \t Epoch: 55 \t Loss: -0.009153931264710776\nTRAIN: \t Epoch: 55 \t Loss: -0.00919694735461639\nTRAIN: \t Epoch: 55 \t Loss: -0.009216354645200465\nTRAIN: \t Epoch: 55 \t Loss: -0.009236804512329399\nTRAIN: \t Epoch: 55 \t Loss: -0.009215764201597332\nVALD: \t Epoch: 55 \t Loss: -0.003406578442081809\nVALD: \t Epoch: 55 \t Loss: -0.006678346428088844\nVALD: \t Epoch: 55 \t Loss: -0.005554123626401027\nVALD: \t Epoch: 55 \t Loss: -0.007018222124315798\nVALD: \t Epoch: 55 \t Loss: -0.006851505638128702\n******************************\nEpoch: social-tag : 55\ntrain_loss -0.009215764201597332\nval_loss -0.006851505638128702\n{'min_val_epoch': 44, 'min_val_loss': -0.010375550595650542}\n******************************\nTRAIN: \t Epoch: 56 \t Loss: -0.009752473793923855\nTRAIN: \t Epoch: 56 \t Loss: -0.00997196277603507\nTRAIN: \t Epoch: 56 \t Loss: -0.009854322920242945\nTRAIN: \t Epoch: 56 \t Loss: -0.009770199423655868\nTRAIN: \t Epoch: 56 \t Loss: -0.009519224986433983\nTRAIN: \t Epoch: 56 \t Loss: -0.009283111741145452\nTRAIN: \t Epoch: 56 \t Loss: -0.009299549008054393\nTRAIN: \t Epoch: 56 \t Loss: -0.009367227554321289\nTRAIN: \t Epoch: 56 \t Loss: -0.00947924962060319\nTRAIN: \t Epoch: 56 \t Loss: -0.009531408082693815\nTRAIN: \t Epoch: 56 \t Loss: -0.009602541950615969\nTRAIN: \t Epoch: 56 \t Loss: -0.009676820831373334\nTRAIN: \t Epoch: 56 \t Loss: -0.009590782296772186\nTRAIN: \t Epoch: 56 \t Loss: -0.009334224076675517\nTRAIN: \t Epoch: 56 \t Loss: -0.00911185188839833\nTRAIN: \t Epoch: 56 \t Loss: -0.009116197470575571\nTRAIN: \t Epoch: 56 \t Loss: -0.009201546144836089\nTRAIN: \t Epoch: 56 \t Loss: -0.009256406480239497\nTRAIN: \t Epoch: 56 \t Loss: -0.009361135675326773\nTRAIN: \t Epoch: 56 \t Loss: -0.0094258151948452\nTRAIN: \t Epoch: 56 \t Loss: -0.009433959871415643\nVALD: \t Epoch: 56 \t Loss: -0.005178417544811964\nVALD: \t Epoch: 56 \t Loss: -0.007278994424268603\nVALD: \t Epoch: 56 \t Loss: -0.006577359512448311\nVALD: \t Epoch: 56 \t Loss: -0.007902317214757204\nVALD: \t Epoch: 56 \t Loss: -0.007674075459128608\n******************************\nEpoch: social-tag : 56\ntrain_loss -0.009433959871415643\nval_loss -0.007674075459128608\n{'min_val_epoch': 44, 'min_val_loss': -0.010375550595650542}\n******************************\nTRAIN: \t Epoch: 57 \t Loss: -0.011113965883851051\nTRAIN: \t Epoch: 57 \t Loss: -0.01073914347216487\nTRAIN: \t Epoch: 57 \t Loss: -0.010669013174871603\nTRAIN: \t Epoch: 57 \t Loss: -0.010264608077704906\nTRAIN: \t Epoch: 57 \t Loss: -0.009742486663162709\nTRAIN: \t Epoch: 57 \t Loss: -0.0094587003501753\nTRAIN: \t Epoch: 57 \t Loss: -0.009525756085557597\nTRAIN: \t Epoch: 57 \t Loss: -0.009608862339518964\nTRAIN: \t Epoch: 57 \t Loss: -0.009562780873643028\nTRAIN: \t Epoch: 57 \t Loss: -0.009596770443022252\nTRAIN: \t Epoch: 57 \t Loss: -0.009663561105050823\nTRAIN: \t Epoch: 57 \t Loss: -0.009641597125058373\nTRAIN: \t Epoch: 57 \t Loss: -0.009384590201079845\nTRAIN: \t Epoch: 57 \t Loss: -0.009274294466844626\nTRAIN: \t Epoch: 57 \t Loss: -0.009300672014554342\nTRAIN: \t Epoch: 57 \t Loss: -0.009388479171320796\nTRAIN: \t Epoch: 57 \t Loss: -0.009425495422500022\nTRAIN: \t Epoch: 57 \t Loss: -0.009447812775356902\nTRAIN: \t Epoch: 57 \t Loss: -0.00949142500758171\nTRAIN: \t Epoch: 57 \t Loss: -0.009401252609677613\nTRAIN: \t Epoch: 57 \t Loss: -0.0093669265037605\nVALD: \t Epoch: 57 \t Loss: 0.0001085629264707677\nVALD: \t Epoch: 57 \t Loss: -0.00406563069918775\nVALD: \t Epoch: 57 \t Loss: -0.0019080477286479436\nVALD: \t Epoch: 57 \t Loss: -0.0035548296164051862\nVALD: \t Epoch: 57 \t Loss: -0.0037814969880785344\n******************************\nEpoch: social-tag : 57\ntrain_loss -0.0093669265037605\nval_loss -0.0037814969880785344\n{'min_val_epoch': 44, 'min_val_loss': -0.010375550595650542}\n******************************\nTRAIN: \t Epoch: 58 \t Loss: -0.008670317009091377\nTRAIN: \t Epoch: 58 \t Loss: -0.009487330447882414\nTRAIN: \t Epoch: 58 \t Loss: -0.009765383477012316\nTRAIN: \t Epoch: 58 \t Loss: -0.009828620357438922\nTRAIN: \t Epoch: 58 \t Loss: -0.009393627103418112\nTRAIN: \t Epoch: 58 \t Loss: -0.008990276061619321\nTRAIN: \t Epoch: 58 \t Loss: -0.008989209375743355\nTRAIN: \t Epoch: 58 \t Loss: -0.00912663241615519\nTRAIN: \t Epoch: 58 \t Loss: -0.009283102686620422\nTRAIN: \t Epoch: 58 \t Loss: -0.009285265160724521\nTRAIN: \t Epoch: 58 \t Loss: -0.009215037888762627\nTRAIN: \t Epoch: 58 \t Loss: -0.009068002148220936\nTRAIN: \t Epoch: 58 \t Loss: -0.009047535104820361\nTRAIN: \t Epoch: 58 \t Loss: -0.009104970177369458\nTRAIN: \t Epoch: 58 \t Loss: -0.009178451821208\nTRAIN: \t Epoch: 58 \t Loss: -0.009225922462064773\nTRAIN: \t Epoch: 58 \t Loss: -0.009222809992292347\nTRAIN: \t Epoch: 58 \t Loss: -0.00916571987585889\nTRAIN: \t Epoch: 58 \t Loss: -0.009095943826985987\nTRAIN: \t Epoch: 58 \t Loss: -0.009113960387185215\nTRAIN: \t Epoch: 58 \t Loss: -0.009144009768089701\nVALD: \t Epoch: 58 \t Loss: -0.005391835235059261\nVALD: \t Epoch: 58 \t Loss: -0.008293349761515856\nVALD: \t Epoch: 58 \t Loss: -0.007519897849609454\nVALD: \t Epoch: 58 \t Loss: -0.008725729421712458\nVALD: \t Epoch: 58 \t Loss: -0.008390682427011634\n******************************\nEpoch: social-tag : 58\ntrain_loss -0.009144009768089701\nval_loss -0.008390682427011634\n{'min_val_epoch': 44, 'min_val_loss': -0.010375550595650542}\n******************************\nTRAIN: \t Epoch: 59 \t Loss: -0.010090476833283901\nTRAIN: \t Epoch: 59 \t Loss: -0.009631713852286339\nTRAIN: \t Epoch: 59 \t Loss: -0.009952660029133161\nTRAIN: \t Epoch: 59 \t Loss: -0.009546770714223385\nTRAIN: \t Epoch: 59 \t Loss: -0.00930630173534155\nTRAIN: \t Epoch: 59 \t Loss: -0.009078367458035549\nTRAIN: \t Epoch: 59 \t Loss: -0.009069779887795448\nTRAIN: \t Epoch: 59 \t Loss: -0.009234320488758385\nTRAIN: \t Epoch: 59 \t Loss: -0.00927445489085383\nTRAIN: \t Epoch: 59 \t Loss: -0.009375824872404338\nTRAIN: \t Epoch: 59 \t Loss: -0.009459935128688812\nTRAIN: \t Epoch: 59 \t Loss: -0.009401390717054406\nTRAIN: \t Epoch: 59 \t Loss: -0.009344050302528419\nTRAIN: \t Epoch: 59 \t Loss: -0.009390149930758136\nTRAIN: \t Epoch: 59 \t Loss: -0.009487612607578437\nTRAIN: \t Epoch: 59 \t Loss: -0.009525907109491527\nTRAIN: \t Epoch: 59 \t Loss: -0.009554355076568969\nTRAIN: \t Epoch: 59 \t Loss: -0.009493324750413498\nTRAIN: \t Epoch: 59 \t Loss: -0.009507380033794203\nTRAIN: \t Epoch: 59 \t Loss: -0.00948812197893858\nTRAIN: \t Epoch: 59 \t Loss: -0.009489859676214026\nVALD: \t Epoch: 59 \t Loss: -0.011476417072117329\nVALD: \t Epoch: 59 \t Loss: -0.011635920498520136\nVALD: \t Epoch: 59 \t Loss: -0.011365339159965515\nVALD: \t Epoch: 59 \t Loss: -0.01171016925945878\nVALD: \t Epoch: 59 \t Loss: -0.01101397652173004\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.42it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.5426655611110321  FDE: 0.8654347340303001\n**************************************************\n******************************\nEpoch: social-tag : 59\ntrain_loss -0.009489859676214026\nval_loss -0.01101397652173004\n{'min_val_epoch': 59, 'min_val_loss': -0.01101397652173004}\n******************************\nTRAIN: \t Epoch: 60 \t Loss: -0.01063277292996645\nTRAIN: \t Epoch: 60 \t Loss: -0.010846616234630346\nTRAIN: \t Epoch: 60 \t Loss: -0.010680208603541056\nTRAIN: \t Epoch: 60 \t Loss: -0.01012587221339345\nTRAIN: \t Epoch: 60 \t Loss: -0.009426198434084654\nTRAIN: \t Epoch: 60 \t Loss: -0.009126029598216215\nTRAIN: \t Epoch: 60 \t Loss: -0.009185344246881348\nTRAIN: \t Epoch: 60 \t Loss: -0.009224554407410324\nTRAIN: \t Epoch: 60 \t Loss: -0.009382255892786715\nTRAIN: \t Epoch: 60 \t Loss: -0.009406957589089871\nTRAIN: \t Epoch: 60 \t Loss: -0.009328798644921997\nTRAIN: \t Epoch: 60 \t Loss: -0.009086371011411151\nTRAIN: \t Epoch: 60 \t Loss: -0.009041187854913564\nTRAIN: \t Epoch: 60 \t Loss: -0.009154507424682379\nTRAIN: \t Epoch: 60 \t Loss: -0.009254050503174464\nTRAIN: \t Epoch: 60 \t Loss: -0.009293658251408488\nTRAIN: \t Epoch: 60 \t Loss: -0.00928157480324016\nTRAIN: \t Epoch: 60 \t Loss: -0.00925106591441565\nTRAIN: \t Epoch: 60 \t Loss: -0.009251873989246394\nTRAIN: \t Epoch: 60 \t Loss: -0.00922125019133091\nTRAIN: \t Epoch: 60 \t Loss: -0.00923479049134089\nVALD: \t Epoch: 60 \t Loss: -0.005812743678689003\nVALD: \t Epoch: 60 \t Loss: -0.008296878077089787\nVALD: \t Epoch: 60 \t Loss: -0.008106018416583538\nVALD: \t Epoch: 60 \t Loss: -0.00910729425959289\nVALD: \t Epoch: 60 \t Loss: -0.008736494370704688\n******************************\nEpoch: social-tag : 60\ntrain_loss -0.00923479049134089\nval_loss -0.008736494370704688\n{'min_val_epoch': 59, 'min_val_loss': -0.01101397652173004}\n******************************\nTRAIN: \t Epoch: 61 \t Loss: -0.009595580399036407\nTRAIN: \t Epoch: 61 \t Loss: -0.0098330769687891\nTRAIN: \t Epoch: 61 \t Loss: -0.009449831520517668\nTRAIN: \t Epoch: 61 \t Loss: -0.009089639410376549\nTRAIN: \t Epoch: 61 \t Loss: -0.008961538970470428\nTRAIN: \t Epoch: 61 \t Loss: -0.009041829345126947\nTRAIN: \t Epoch: 61 \t Loss: -0.009116466556276594\nTRAIN: \t Epoch: 61 \t Loss: -0.009174947976134717\nTRAIN: \t Epoch: 61 \t Loss: -0.00923934020102024\nTRAIN: \t Epoch: 61 \t Loss: -0.009310324676334858\nTRAIN: \t Epoch: 61 \t Loss: -0.009366096759384329\nTRAIN: \t Epoch: 61 \t Loss: -0.009298943060760697\nTRAIN: \t Epoch: 61 \t Loss: -0.009322970675734373\nTRAIN: \t Epoch: 61 \t Loss: -0.00935966381803155\nTRAIN: \t Epoch: 61 \t Loss: -0.009381770404676597\nTRAIN: \t Epoch: 61 \t Loss: -0.009398862486705184\nTRAIN: \t Epoch: 61 \t Loss: -0.009284087703289354\nTRAIN: \t Epoch: 61 \t Loss: -0.009256625573875176\nTRAIN: \t Epoch: 61 \t Loss: -0.009292703734612778\nTRAIN: \t Epoch: 61 \t Loss: -0.00935804054606706\nTRAIN: \t Epoch: 61 \t Loss: -0.009365408934807907\nVALD: \t Epoch: 61 \t Loss: -0.006792230997234583\nVALD: \t Epoch: 61 \t Loss: -0.008333610137924552\nVALD: \t Epoch: 61 \t Loss: -0.007384357042610645\nVALD: \t Epoch: 61 \t Loss: -0.008310612058266997\nVALD: \t Epoch: 61 \t Loss: -0.008048070225738673\n******************************\nEpoch: social-tag : 61\ntrain_loss -0.009365408934807907\nval_loss -0.008048070225738673\n{'min_val_epoch': 59, 'min_val_loss': -0.01101397652173004}\n******************************\nTRAIN: \t Epoch: 62 \t Loss: -0.00968717597424984\nTRAIN: \t Epoch: 62 \t Loss: -0.00963400723412633\nTRAIN: \t Epoch: 62 \t Loss: -0.00989916455000639\nTRAIN: \t Epoch: 62 \t Loss: -0.010092170210555196\nTRAIN: \t Epoch: 62 \t Loss: -0.010040961392223834\nTRAIN: \t Epoch: 62 \t Loss: -0.009801537574579319\nTRAIN: \t Epoch: 62 \t Loss: -0.009466362784483604\nTRAIN: \t Epoch: 62 \t Loss: -0.009444001887459308\nTRAIN: \t Epoch: 62 \t Loss: -0.009565782371080585\nTRAIN: \t Epoch: 62 \t Loss: -0.009685980016365647\nTRAIN: \t Epoch: 62 \t Loss: -0.00977148466997526\nTRAIN: \t Epoch: 62 \t Loss: -0.009792465444964668\nTRAIN: \t Epoch: 62 \t Loss: -0.009674241873793878\nTRAIN: \t Epoch: 62 \t Loss: -0.00944244715252093\nTRAIN: \t Epoch: 62 \t Loss: -0.009356802329421043\nTRAIN: \t Epoch: 62 \t Loss: -0.0094026931328699\nTRAIN: \t Epoch: 62 \t Loss: -0.009404452024575542\nTRAIN: \t Epoch: 62 \t Loss: -0.00943614294131597\nTRAIN: \t Epoch: 62 \t Loss: -0.009479770878035771\nTRAIN: \t Epoch: 62 \t Loss: -0.009446550579741596\nTRAIN: \t Epoch: 62 \t Loss: -0.009453860090710883\nVALD: \t Epoch: 62 \t Loss: -0.010017036460340023\nVALD: \t Epoch: 62 \t Loss: -0.010523698758333921\nVALD: \t Epoch: 62 \t Loss: -0.009524665462474028\nVALD: \t Epoch: 62 \t Loss: -0.009930817410349846\nVALD: \t Epoch: 62 \t Loss: -0.009463216563545755\n******************************\nEpoch: social-tag : 62\ntrain_loss -0.009453860090710883\nval_loss -0.009463216563545755\n{'min_val_epoch': 59, 'min_val_loss': -0.01101397652173004}\n******************************\nTRAIN: \t Epoch: 63 \t Loss: -0.009437518194317818\nTRAIN: \t Epoch: 63 \t Loss: -0.01016858359798789\nTRAIN: \t Epoch: 63 \t Loss: -0.009908063026765982\nTRAIN: \t Epoch: 63 \t Loss: -0.009514900855720043\nTRAIN: \t Epoch: 63 \t Loss: -0.00940257329493761\nTRAIN: \t Epoch: 63 \t Loss: -0.009270469968517622\nTRAIN: \t Epoch: 63 \t Loss: -0.00929551784481321\nTRAIN: \t Epoch: 63 \t Loss: -0.009480008506216109\nTRAIN: \t Epoch: 63 \t Loss: -0.009437737262083424\nTRAIN: \t Epoch: 63 \t Loss: -0.009315903205424548\nTRAIN: \t Epoch: 63 \t Loss: -0.00919543257491155\nTRAIN: \t Epoch: 63 \t Loss: -0.009200158451373378\nTRAIN: \t Epoch: 63 \t Loss: -0.009340335328418475\nTRAIN: \t Epoch: 63 \t Loss: -0.009395884416465248\nTRAIN: \t Epoch: 63 \t Loss: -0.009503890015184879\nTRAIN: \t Epoch: 63 \t Loss: -0.009503614564891905\nTRAIN: \t Epoch: 63 \t Loss: -0.009473373620387386\nTRAIN: \t Epoch: 63 \t Loss: -0.009488140284601185\nTRAIN: \t Epoch: 63 \t Loss: -0.009502508775576166\nTRAIN: \t Epoch: 63 \t Loss: -0.009507439797744155\nTRAIN: \t Epoch: 63 \t Loss: -0.009502647940066564\nVALD: \t Epoch: 63 \t Loss: -0.011545089073479176\nVALD: \t Epoch: 63 \t Loss: -0.011368710082024336\nVALD: \t Epoch: 63 \t Loss: -0.010836822601656118\nVALD: \t Epoch: 63 \t Loss: -0.011165992822498083\nVALD: \t Epoch: 63 \t Loss: -0.01055040797173689\n******************************\nEpoch: social-tag : 63\ntrain_loss -0.009502647940066564\nval_loss -0.01055040797173689\n{'min_val_epoch': 59, 'min_val_loss': -0.01101397652173004}\n******************************\nTRAIN: \t Epoch: 64 \t Loss: -0.009242283180356026\nTRAIN: \t Epoch: 64 \t Loss: -0.009510345291346312\nTRAIN: \t Epoch: 64 \t Loss: -0.00964700368543466\nTRAIN: \t Epoch: 64 \t Loss: -0.009410145925357938\nTRAIN: \t Epoch: 64 \t Loss: -0.009273815713822842\nTRAIN: \t Epoch: 64 \t Loss: -0.009052035554001728\nTRAIN: \t Epoch: 64 \t Loss: -0.009171726980379649\nTRAIN: \t Epoch: 64 \t Loss: -0.00931154319550842\nTRAIN: \t Epoch: 64 \t Loss: -0.009429556214147143\nTRAIN: \t Epoch: 64 \t Loss: -0.009520794171839953\nTRAIN: \t Epoch: 64 \t Loss: -0.009477659467269074\nTRAIN: \t Epoch: 64 \t Loss: -0.009354104908804098\nTRAIN: \t Epoch: 64 \t Loss: -0.009339710482611107\nTRAIN: \t Epoch: 64 \t Loss: -0.009408885546560799\nTRAIN: \t Epoch: 64 \t Loss: -0.009465871006250381\nTRAIN: \t Epoch: 64 \t Loss: -0.009476552775595337\nTRAIN: \t Epoch: 64 \t Loss: -0.009441616506699254\nTRAIN: \t Epoch: 64 \t Loss: -0.009388576830840774\nTRAIN: \t Epoch: 64 \t Loss: -0.009418739888228868\nTRAIN: \t Epoch: 64 \t Loss: -0.009448305098339915\nTRAIN: \t Epoch: 64 \t Loss: -0.00946092424065495\nVALD: \t Epoch: 64 \t Loss: -0.011888697743415833\nVALD: \t Epoch: 64 \t Loss: -0.011666864156723022\nVALD: \t Epoch: 64 \t Loss: -0.010970736853778362\nVALD: \t Epoch: 64 \t Loss: -0.011300094658508897\nVALD: \t Epoch: 64 \t Loss: -0.01061655031502151\n******************************\nEpoch: social-tag : 64\ntrain_loss -0.00946092424065495\nval_loss -0.01061655031502151\n{'min_val_epoch': 59, 'min_val_loss': -0.01101397652173004}\n******************************\nTRAIN: \t Epoch: 65 \t Loss: -0.01191676314920187\nTRAIN: \t Epoch: 65 \t Loss: -0.01173899369314313\nTRAIN: \t Epoch: 65 \t Loss: -0.010695791492859522\nTRAIN: \t Epoch: 65 \t Loss: -0.010389748029410839\nTRAIN: \t Epoch: 65 \t Loss: -0.010386050678789616\nTRAIN: \t Epoch: 65 \t Loss: -0.010174584109336138\nTRAIN: \t Epoch: 65 \t Loss: -0.009933923371136189\nTRAIN: \t Epoch: 65 \t Loss: -0.009870396577753127\nTRAIN: \t Epoch: 65 \t Loss: -0.009711508949597677\nTRAIN: \t Epoch: 65 \t Loss: -0.009738163836300374\nTRAIN: \t Epoch: 65 \t Loss: -0.009770103306932882\nTRAIN: \t Epoch: 65 \t Loss: -0.009696619119495153\nTRAIN: \t Epoch: 65 \t Loss: -0.009656993457331108\nTRAIN: \t Epoch: 65 \t Loss: -0.009657870179840497\nTRAIN: \t Epoch: 65 \t Loss: -0.00965593953927358\nTRAIN: \t Epoch: 65 \t Loss: -0.009665089659392834\nTRAIN: \t Epoch: 65 \t Loss: -0.009697773822528474\nTRAIN: \t Epoch: 65 \t Loss: -0.009673740456087722\nTRAIN: \t Epoch: 65 \t Loss: -0.00968137785400215\nTRAIN: \t Epoch: 65 \t Loss: -0.009717713296413421\nTRAIN: \t Epoch: 65 \t Loss: -0.009718198602072717\nVALD: \t Epoch: 65 \t Loss: -0.008286896161735058\nVALD: \t Epoch: 65 \t Loss: -0.009144752286374569\nVALD: \t Epoch: 65 \t Loss: -0.008219352457672358\nVALD: \t Epoch: 65 \t Loss: -0.008944768575020134\nVALD: \t Epoch: 65 \t Loss: -0.008647147392113235\n******************************\nEpoch: social-tag : 65\ntrain_loss -0.009718198602072717\nval_loss -0.008647147392113235\n{'min_val_epoch': 59, 'min_val_loss': -0.01101397652173004}\n******************************\nTRAIN: \t Epoch: 66 \t Loss: -0.01121419295668602\nTRAIN: \t Epoch: 66 \t Loss: -0.010927347000688314\nTRAIN: \t Epoch: 66 \t Loss: -0.010577590515216192\nTRAIN: \t Epoch: 66 \t Loss: -0.010147127555683255\nTRAIN: \t Epoch: 66 \t Loss: -0.009442096669226884\nTRAIN: \t Epoch: 66 \t Loss: -0.009398949410145482\nTRAIN: \t Epoch: 66 \t Loss: -0.009529981296509504\nTRAIN: \t Epoch: 66 \t Loss: -0.00969199399696663\nTRAIN: \t Epoch: 66 \t Loss: -0.009790568043374352\nTRAIN: \t Epoch: 66 \t Loss: -0.009917219309136272\nTRAIN: \t Epoch: 66 \t Loss: -0.009984180796891451\nTRAIN: \t Epoch: 66 \t Loss: -0.009994050759511689\nTRAIN: \t Epoch: 66 \t Loss: -0.010011842151960501\nTRAIN: \t Epoch: 66 \t Loss: -0.010028649820014834\nTRAIN: \t Epoch: 66 \t Loss: -0.009961022281398375\nTRAIN: \t Epoch: 66 \t Loss: -0.00981207177392207\nTRAIN: \t Epoch: 66 \t Loss: -0.009773620267343871\nTRAIN: \t Epoch: 66 \t Loss: -0.00981049350876775\nTRAIN: \t Epoch: 66 \t Loss: -0.00981739587395599\nTRAIN: \t Epoch: 66 \t Loss: -0.009859315422363579\nTRAIN: \t Epoch: 66 \t Loss: -0.009857425631333426\nVALD: \t Epoch: 66 \t Loss: -0.01171739399433136\nVALD: \t Epoch: 66 \t Loss: -0.01175432512536645\nVALD: \t Epoch: 66 \t Loss: -0.011315115417043367\nVALD: \t Epoch: 66 \t Loss: -0.011730749625712633\nVALD: \t Epoch: 66 \t Loss: -0.011040905823454189\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.26it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.47812031116293024  FDE: 0.7446598802695676\n**************************************************\n******************************\nEpoch: social-tag : 66\ntrain_loss -0.009857425631333426\nval_loss -0.011040905823454189\n{'min_val_epoch': 66, 'min_val_loss': -0.011040905823454189}\n******************************\nTRAIN: \t Epoch: 67 \t Loss: -0.010533461347222328\nTRAIN: \t Epoch: 67 \t Loss: -0.010841996874660254\nTRAIN: \t Epoch: 67 \t Loss: -0.01084024210770925\nTRAIN: \t Epoch: 67 \t Loss: -0.010523784905672073\nTRAIN: \t Epoch: 67 \t Loss: -0.01046770066022873\nTRAIN: \t Epoch: 67 \t Loss: -0.010098838557799658\nTRAIN: \t Epoch: 67 \t Loss: -0.009615808459264892\nTRAIN: \t Epoch: 67 \t Loss: -0.009528322727419436\nTRAIN: \t Epoch: 67 \t Loss: -0.009666028018626902\nTRAIN: \t Epoch: 67 \t Loss: -0.009828389529138804\nTRAIN: \t Epoch: 67 \t Loss: -0.009916355748745527\nTRAIN: \t Epoch: 67 \t Loss: -0.009933860196421543\nTRAIN: \t Epoch: 67 \t Loss: -0.00983401294797659\nTRAIN: \t Epoch: 67 \t Loss: -0.00962929563995983\nTRAIN: \t Epoch: 67 \t Loss: -0.009598736992726724\nTRAIN: \t Epoch: 67 \t Loss: -0.009666252910392359\nTRAIN: \t Epoch: 67 \t Loss: -0.009775340694057591\nTRAIN: \t Epoch: 67 \t Loss: -0.009806801073460115\nTRAIN: \t Epoch: 67 \t Loss: -0.009850977944504274\nTRAIN: \t Epoch: 67 \t Loss: -0.009839524258859455\nTRAIN: \t Epoch: 67 \t Loss: -0.009797584170686711\nVALD: \t Epoch: 67 \t Loss: -0.011381550692021847\nVALD: \t Epoch: 67 \t Loss: -0.011367547325789928\nVALD: \t Epoch: 67 \t Loss: -0.011184831460316976\nVALD: \t Epoch: 67 \t Loss: -0.011470796773210168\nVALD: \t Epoch: 67 \t Loss: -0.01075507947596183\n******************************\nEpoch: social-tag : 67\ntrain_loss -0.009797584170686711\nval_loss -0.01075507947596183\n{'min_val_epoch': 66, 'min_val_loss': -0.011040905823454189}\n******************************\nTRAIN: \t Epoch: 68 \t Loss: -0.010363242588937283\nTRAIN: \t Epoch: 68 \t Loss: -0.010651163291186094\nTRAIN: \t Epoch: 68 \t Loss: -0.010687329495946566\nTRAIN: \t Epoch: 68 \t Loss: -0.00995373260229826\nTRAIN: \t Epoch: 68 \t Loss: -0.009527395479381084\nTRAIN: \t Epoch: 68 \t Loss: -0.009515689841161171\nTRAIN: \t Epoch: 68 \t Loss: -0.009641892675842558\nTRAIN: \t Epoch: 68 \t Loss: -0.009714445914141834\nTRAIN: \t Epoch: 68 \t Loss: -0.00983017362240288\nTRAIN: \t Epoch: 68 \t Loss: -0.009777017589658498\nTRAIN: \t Epoch: 68 \t Loss: -0.009569621094587174\nTRAIN: \t Epoch: 68 \t Loss: -0.009406487147013346\nTRAIN: \t Epoch: 68 \t Loss: -0.009364019362972332\nTRAIN: \t Epoch: 68 \t Loss: -0.009475368673780135\nTRAIN: \t Epoch: 68 \t Loss: -0.0095790671184659\nTRAIN: \t Epoch: 68 \t Loss: -0.009711924416478723\nTRAIN: \t Epoch: 68 \t Loss: -0.009787503246437101\nTRAIN: \t Epoch: 68 \t Loss: -0.009732897031224437\nTRAIN: \t Epoch: 68 \t Loss: -0.009490364476254112\nTRAIN: \t Epoch: 68 \t Loss: -0.009435994364321233\nTRAIN: \t Epoch: 68 \t Loss: -0.009437711276085265\nVALD: \t Epoch: 68 \t Loss: -0.009419544599950314\nVALD: \t Epoch: 68 \t Loss: -0.010058987885713577\nVALD: \t Epoch: 68 \t Loss: -0.00996933194498221\nVALD: \t Epoch: 68 \t Loss: -0.010390204610303044\nVALD: \t Epoch: 68 \t Loss: -0.009917491684021389\n******************************\nEpoch: social-tag : 68\ntrain_loss -0.009437711276085265\nval_loss -0.009917491684021389\n{'min_val_epoch': 66, 'min_val_loss': -0.011040905823454189}\n******************************\nTRAIN: \t Epoch: 69 \t Loss: -0.009793402627110481\nTRAIN: \t Epoch: 69 \t Loss: -0.01049800356850028\nTRAIN: \t Epoch: 69 \t Loss: -0.010640863018731276\nTRAIN: \t Epoch: 69 \t Loss: -0.010502709541469812\nTRAIN: \t Epoch: 69 \t Loss: -0.010240020975470544\nTRAIN: \t Epoch: 69 \t Loss: -0.009907767176628113\nTRAIN: \t Epoch: 69 \t Loss: -0.009872926931296076\nTRAIN: \t Epoch: 69 \t Loss: -0.009933210676535964\nTRAIN: \t Epoch: 69 \t Loss: -0.009994546365406778\nTRAIN: \t Epoch: 69 \t Loss: -0.00991960298269987\nTRAIN: \t Epoch: 69 \t Loss: -0.009745548563924704\nTRAIN: \t Epoch: 69 \t Loss: -0.009703776488701502\nTRAIN: \t Epoch: 69 \t Loss: -0.009740269671265896\nTRAIN: \t Epoch: 69 \t Loss: -0.009795767388173513\nTRAIN: \t Epoch: 69 \t Loss: -0.009861748665571213\nTRAIN: \t Epoch: 69 \t Loss: -0.009770867240149528\nTRAIN: \t Epoch: 69 \t Loss: -0.009639125536469853\nTRAIN: \t Epoch: 69 \t Loss: -0.00966254792486628\nTRAIN: \t Epoch: 69 \t Loss: -0.009756137126762616\nTRAIN: \t Epoch: 69 \t Loss: -0.00984183712862432\nTRAIN: \t Epoch: 69 \t Loss: -0.009849210174247312\nVALD: \t Epoch: 69 \t Loss: -0.011384757235646248\nVALD: \t Epoch: 69 \t Loss: -0.011479910928755999\nVALD: \t Epoch: 69 \t Loss: -0.011088307326038679\nVALD: \t Epoch: 69 \t Loss: -0.011472642188891768\nVALD: \t Epoch: 69 \t Loss: -0.010766130906564218\n******************************\nEpoch: social-tag : 69\ntrain_loss -0.009849210174247312\nval_loss -0.010766130906564218\n{'min_val_epoch': 66, 'min_val_loss': -0.011040905823454189}\n******************************\nTRAIN: \t Epoch: 70 \t Loss: -0.011097078211605549\nTRAIN: \t Epoch: 70 \t Loss: -0.011312512215226889\nTRAIN: \t Epoch: 70 \t Loss: -0.010718132058779398\nTRAIN: \t Epoch: 70 \t Loss: -0.010608327109366655\nTRAIN: \t Epoch: 70 \t Loss: -0.010631619580090046\nTRAIN: \t Epoch: 70 \t Loss: -0.010370093397796154\nTRAIN: \t Epoch: 70 \t Loss: -0.010151079456721033\nTRAIN: \t Epoch: 70 \t Loss: -0.009985331329517066\nTRAIN: \t Epoch: 70 \t Loss: -0.009928307806452116\nTRAIN: \t Epoch: 70 \t Loss: -0.009911690559238195\nTRAIN: \t Epoch: 70 \t Loss: -0.010010921396315098\nTRAIN: \t Epoch: 70 \t Loss: -0.010111486228803793\nTRAIN: \t Epoch: 70 \t Loss: -0.010064898560253473\nTRAIN: \t Epoch: 70 \t Loss: -0.009897905302100949\nTRAIN: \t Epoch: 70 \t Loss: -0.00983843980357051\nTRAIN: \t Epoch: 70 \t Loss: -0.00983236005413346\nTRAIN: \t Epoch: 70 \t Loss: -0.009927829097518149\nTRAIN: \t Epoch: 70 \t Loss: -0.009948752898102006\nTRAIN: \t Epoch: 70 \t Loss: -0.009944953588082603\nTRAIN: \t Epoch: 70 \t Loss: -0.009970499225892127\nTRAIN: \t Epoch: 70 \t Loss: -0.009982046317208613\nVALD: \t Epoch: 70 \t Loss: -0.004474543500691652\nVALD: \t Epoch: 70 \t Loss: -0.007060023257508874\nVALD: \t Epoch: 70 \t Loss: -0.005782839919750889\nVALD: \t Epoch: 70 \t Loss: -0.007168403652030975\nVALD: \t Epoch: 70 \t Loss: -0.007130472508413588\n******************************\nEpoch: social-tag : 70\ntrain_loss -0.009982046317208613\nval_loss -0.007130472508413588\n{'min_val_epoch': 66, 'min_val_loss': -0.011040905823454189}\n******************************\nTRAIN: \t Epoch: 71 \t Loss: -0.010988951660692692\nTRAIN: \t Epoch: 71 \t Loss: -0.010985849890857935\nTRAIN: \t Epoch: 71 \t Loss: -0.01087510927269856\nTRAIN: \t Epoch: 71 \t Loss: -0.010398893849924207\nTRAIN: \t Epoch: 71 \t Loss: -0.010235467366874218\nTRAIN: \t Epoch: 71 \t Loss: -0.010305977116028467\nTRAIN: \t Epoch: 71 \t Loss: -0.010133527485387666\nTRAIN: \t Epoch: 71 \t Loss: -0.010005266405642033\nTRAIN: \t Epoch: 71 \t Loss: -0.009958003647625446\nTRAIN: \t Epoch: 71 \t Loss: -0.010008075460791587\nTRAIN: \t Epoch: 71 \t Loss: -0.010036563331430609\nTRAIN: \t Epoch: 71 \t Loss: -0.009929309211050471\nTRAIN: \t Epoch: 71 \t Loss: -0.009735833925123397\nTRAIN: \t Epoch: 71 \t Loss: -0.009705649821885995\nTRAIN: \t Epoch: 71 \t Loss: -0.00979905283699433\nTRAIN: \t Epoch: 71 \t Loss: -0.009813092183321714\nTRAIN: \t Epoch: 71 \t Loss: -0.009800482870024793\nTRAIN: \t Epoch: 71 \t Loss: -0.009783548354688618\nTRAIN: \t Epoch: 71 \t Loss: -0.009783609280068623\nTRAIN: \t Epoch: 71 \t Loss: -0.009807971445843578\nTRAIN: \t Epoch: 71 \t Loss: -0.00981290016894903\nVALD: \t Epoch: 71 \t Loss: -0.003884619567543268\nVALD: \t Epoch: 71 \t Loss: -0.00686431978829205\nVALD: \t Epoch: 71 \t Loss: -0.005628531488279502\nVALD: \t Epoch: 71 \t Loss: -0.006901553366333246\nVALD: \t Epoch: 71 \t Loss: -0.006919801235198975\n******************************\nEpoch: social-tag : 71\ntrain_loss -0.00981290016894903\nval_loss -0.006919801235198975\n{'min_val_epoch': 66, 'min_val_loss': -0.011040905823454189}\n******************************\nTRAIN: \t Epoch: 72 \t Loss: -0.011023001745343208\nTRAIN: \t Epoch: 72 \t Loss: -0.011166614945977926\nTRAIN: \t Epoch: 72 \t Loss: -0.011101996526122093\nTRAIN: \t Epoch: 72 \t Loss: -0.010673539713025093\nTRAIN: \t Epoch: 72 \t Loss: -0.010202161967754364\nTRAIN: \t Epoch: 72 \t Loss: -0.01006564668690165\nTRAIN: \t Epoch: 72 \t Loss: -0.01021236354219062\nTRAIN: \t Epoch: 72 \t Loss: -0.010224531753920019\nTRAIN: \t Epoch: 72 \t Loss: -0.01023979764431715\nTRAIN: \t Epoch: 72 \t Loss: -0.010148202627897262\nTRAIN: \t Epoch: 72 \t Loss: -0.009963565167378296\nTRAIN: \t Epoch: 72 \t Loss: -0.009971301925058166\nTRAIN: \t Epoch: 72 \t Loss: -0.01002978404554037\nTRAIN: \t Epoch: 72 \t Loss: -0.010047284952764\nTRAIN: \t Epoch: 72 \t Loss: -0.010062117502093315\nTRAIN: \t Epoch: 72 \t Loss: -0.010115131561178714\nTRAIN: \t Epoch: 72 \t Loss: -0.01015236573841642\nTRAIN: \t Epoch: 72 \t Loss: -0.010216598295503192\nTRAIN: \t Epoch: 72 \t Loss: -0.010219858371113477\nTRAIN: \t Epoch: 72 \t Loss: -0.010079499171115457\nTRAIN: \t Epoch: 72 \t Loss: -0.010020894707168352\nVALD: \t Epoch: 72 \t Loss: -0.004695243202149868\nVALD: \t Epoch: 72 \t Loss: -0.0057928666938096285\nVALD: \t Epoch: 72 \t Loss: -0.005465805996209383\nVALD: \t Epoch: 72 \t Loss: -0.006215446977876127\nVALD: \t Epoch: 72 \t Loss: -0.00605286607419811\n******************************\nEpoch: social-tag : 72\ntrain_loss -0.010020894707168352\nval_loss -0.00605286607419811\n{'min_val_epoch': 66, 'min_val_loss': -0.011040905823454189}\n******************************\nTRAIN: \t Epoch: 73 \t Loss: -0.007468413561582565\nTRAIN: \t Epoch: 73 \t Loss: -0.00871154572814703\nTRAIN: \t Epoch: 73 \t Loss: -0.009545306985576948\nTRAIN: \t Epoch: 73 \t Loss: -0.009891815949231386\nTRAIN: \t Epoch: 73 \t Loss: -0.010117772780358792\nTRAIN: \t Epoch: 73 \t Loss: -0.010017129437377056\nTRAIN: \t Epoch: 73 \t Loss: -0.009876937605440617\nTRAIN: \t Epoch: 73 \t Loss: -0.009695147979073226\nTRAIN: \t Epoch: 73 \t Loss: -0.009733186620804999\nTRAIN: \t Epoch: 73 \t Loss: -0.009765523858368397\nTRAIN: \t Epoch: 73 \t Loss: -0.009792734191498974\nTRAIN: \t Epoch: 73 \t Loss: -0.009806672809645534\nTRAIN: \t Epoch: 73 \t Loss: -0.009797824331774162\nTRAIN: \t Epoch: 73 \t Loss: -0.009820258311395134\nTRAIN: \t Epoch: 73 \t Loss: -0.009846352475384871\nTRAIN: \t Epoch: 73 \t Loss: -0.00987295201048255\nTRAIN: \t Epoch: 73 \t Loss: -0.009896631133468711\nTRAIN: \t Epoch: 73 \t Loss: -0.009906222060736682\nTRAIN: \t Epoch: 73 \t Loss: -0.009870829011656736\nTRAIN: \t Epoch: 73 \t Loss: -0.009864514181390404\nTRAIN: \t Epoch: 73 \t Loss: -0.00986754639202021\nVALD: \t Epoch: 73 \t Loss: -0.00993734784424305\nVALD: \t Epoch: 73 \t Loss: -0.010710150003433228\nVALD: \t Epoch: 73 \t Loss: -0.010489623372753462\nVALD: \t Epoch: 73 \t Loss: -0.011005151085555553\nVALD: \t Epoch: 73 \t Loss: -0.01045042209195246\n******************************\nEpoch: social-tag : 73\ntrain_loss -0.00986754639202021\nval_loss -0.01045042209195246\n{'min_val_epoch': 66, 'min_val_loss': -0.011040905823454189}\n******************************\nTRAIN: \t Epoch: 74 \t Loss: -0.010002936236560345\nTRAIN: \t Epoch: 74 \t Loss: -0.010051195044070482\nTRAIN: \t Epoch: 74 \t Loss: -0.010324017765621344\nTRAIN: \t Epoch: 74 \t Loss: -0.010485756676644087\nTRAIN: \t Epoch: 74 \t Loss: -0.010479696653783321\nTRAIN: \t Epoch: 74 \t Loss: -0.010161204108347496\nTRAIN: \t Epoch: 74 \t Loss: -0.009855263999530248\nTRAIN: \t Epoch: 74 \t Loss: -0.009845547494478524\nTRAIN: \t Epoch: 74 \t Loss: -0.009936644281778071\nTRAIN: \t Epoch: 74 \t Loss: -0.010078446101397276\nTRAIN: \t Epoch: 74 \t Loss: -0.010206178508021614\nTRAIN: \t Epoch: 74 \t Loss: -0.010295375638330976\nTRAIN: \t Epoch: 74 \t Loss: -0.010115977878180834\nTRAIN: \t Epoch: 74 \t Loss: -0.009937515548829521\nTRAIN: \t Epoch: 74 \t Loss: -0.009940585928658645\nTRAIN: \t Epoch: 74 \t Loss: -0.010053282952867448\nTRAIN: \t Epoch: 74 \t Loss: -0.01017102675841135\nTRAIN: \t Epoch: 74 \t Loss: -0.010169842125227055\nTRAIN: \t Epoch: 74 \t Loss: -0.010146610144721834\nTRAIN: \t Epoch: 74 \t Loss: -0.01018889001570642\nTRAIN: \t Epoch: 74 \t Loss: -0.010171483756949925\nVALD: \t Epoch: 74 \t Loss: -0.003590258304029703\nVALD: \t Epoch: 74 \t Loss: -0.007201948901638389\nVALD: \t Epoch: 74 \t Loss: -0.006688273356606563\nVALD: \t Epoch: 74 \t Loss: -0.00781178695615381\nVALD: \t Epoch: 74 \t Loss: -0.007818036609225802\n******************************\nEpoch: social-tag : 74\ntrain_loss -0.010171483756949925\nval_loss -0.007818036609225802\n{'min_val_epoch': 66, 'min_val_loss': -0.011040905823454189}\n******************************\nTRAIN: \t Epoch: 75 \t Loss: -0.010562916286289692\nTRAIN: \t Epoch: 75 \t Loss: -0.010748524218797684\nTRAIN: \t Epoch: 75 \t Loss: -0.010524755654235681\nTRAIN: \t Epoch: 75 \t Loss: -0.009878985118120909\nTRAIN: \t Epoch: 75 \t Loss: -0.009367704577744006\nTRAIN: \t Epoch: 75 \t Loss: -0.009432191494852304\nTRAIN: \t Epoch: 75 \t Loss: -0.009642837569117546\nTRAIN: \t Epoch: 75 \t Loss: -0.009683073265478015\nTRAIN: \t Epoch: 75 \t Loss: -0.009851663062969843\nTRAIN: \t Epoch: 75 \t Loss: -0.009923496842384338\nTRAIN: \t Epoch: 75 \t Loss: -0.010049224746498194\nTRAIN: \t Epoch: 75 \t Loss: -0.010189553489908576\nTRAIN: \t Epoch: 75 \t Loss: -0.010156315034971787\nTRAIN: \t Epoch: 75 \t Loss: -0.010148256884089537\nTRAIN: \t Epoch: 75 \t Loss: -0.010150734211007754\nTRAIN: \t Epoch: 75 \t Loss: -0.010142610350158066\nTRAIN: \t Epoch: 75 \t Loss: -0.010101507856127094\nTRAIN: \t Epoch: 75 \t Loss: -0.010060402564704418\nTRAIN: \t Epoch: 75 \t Loss: -0.010082137819967772\nTRAIN: \t Epoch: 75 \t Loss: -0.010069954674690962\nTRAIN: \t Epoch: 75 \t Loss: -0.010087011636176656\nVALD: \t Epoch: 75 \t Loss: -0.011499282903969288\nVALD: \t Epoch: 75 \t Loss: -0.01170332171022892\nVALD: \t Epoch: 75 \t Loss: -0.011532409116625786\nVALD: \t Epoch: 75 \t Loss: -0.01181028294377029\nVALD: \t Epoch: 75 \t Loss: -0.01114812738077652\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:25<00:00, 11.81it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.49631529601359675  FDE: 0.7969133955332549\n**************************************************\n******************************\nEpoch: social-tag : 75\ntrain_loss -0.010087011636176656\nval_loss -0.01114812738077652\n{'min_val_epoch': 75, 'min_val_loss': -0.01114812738077652}\n******************************\nTRAIN: \t Epoch: 76 \t Loss: -0.011528604663908482\nTRAIN: \t Epoch: 76 \t Loss: -0.011394917964935303\nTRAIN: \t Epoch: 76 \t Loss: -0.010965421485404173\nTRAIN: \t Epoch: 76 \t Loss: -0.010857220040634274\nTRAIN: \t Epoch: 76 \t Loss: -0.01074088029563427\nTRAIN: \t Epoch: 76 \t Loss: -0.010341161396354437\nTRAIN: \t Epoch: 76 \t Loss: -0.01011618519467967\nTRAIN: \t Epoch: 76 \t Loss: -0.010185018414631486\nTRAIN: \t Epoch: 76 \t Loss: -0.010278935647673078\nTRAIN: \t Epoch: 76 \t Loss: -0.010370849259197712\nTRAIN: \t Epoch: 76 \t Loss: -0.01038446378978816\nTRAIN: \t Epoch: 76 \t Loss: -0.01033740242322286\nTRAIN: \t Epoch: 76 \t Loss: -0.010168964759661602\nTRAIN: \t Epoch: 76 \t Loss: -0.010106366194252456\nTRAIN: \t Epoch: 76 \t Loss: -0.010141788174708684\nTRAIN: \t Epoch: 76 \t Loss: -0.01019268692471087\nTRAIN: \t Epoch: 76 \t Loss: -0.01021984959130778\nTRAIN: \t Epoch: 76 \t Loss: -0.010218757515152296\nTRAIN: \t Epoch: 76 \t Loss: -0.010148429066727036\nTRAIN: \t Epoch: 76 \t Loss: -0.010051999893039464\nTRAIN: \t Epoch: 76 \t Loss: -0.010059487353220478\nVALD: \t Epoch: 76 \t Loss: -0.00839379895478487\nVALD: \t Epoch: 76 \t Loss: -0.009989073500037193\nVALD: \t Epoch: 76 \t Loss: -0.009763451293110847\nVALD: \t Epoch: 76 \t Loss: -0.010403146501630545\nVALD: \t Epoch: 76 \t Loss: -0.010024917970340993\n******************************\nEpoch: social-tag : 76\ntrain_loss -0.010059487353220478\nval_loss -0.010024917970340993\n{'min_val_epoch': 75, 'min_val_loss': -0.01114812738077652}\n******************************\nTRAIN: \t Epoch: 77 \t Loss: -0.011197616346180439\nTRAIN: \t Epoch: 77 \t Loss: -0.011447777971625328\nTRAIN: \t Epoch: 77 \t Loss: -0.011529051388303438\nTRAIN: \t Epoch: 77 \t Loss: -0.011033404618501663\nTRAIN: \t Epoch: 77 \t Loss: -0.010907732136547565\nTRAIN: \t Epoch: 77 \t Loss: -0.010861425970991453\nTRAIN: \t Epoch: 77 \t Loss: -0.010803090275398322\nTRAIN: \t Epoch: 77 \t Loss: -0.010639360756613314\nTRAIN: \t Epoch: 77 \t Loss: -0.010403339337143633\nTRAIN: \t Epoch: 77 \t Loss: -0.010354739613831043\nTRAIN: \t Epoch: 77 \t Loss: -0.010433097623965958\nTRAIN: \t Epoch: 77 \t Loss: -0.010529288090765476\nTRAIN: \t Epoch: 77 \t Loss: -0.010491700699696174\nTRAIN: \t Epoch: 77 \t Loss: -0.010188206531373518\nTRAIN: \t Epoch: 77 \t Loss: -0.010052745696157217\nTRAIN: \t Epoch: 77 \t Loss: -0.010079681378556415\nTRAIN: \t Epoch: 77 \t Loss: -0.010178060572156134\nTRAIN: \t Epoch: 77 \t Loss: -0.010271460878559284\nTRAIN: \t Epoch: 77 \t Loss: -0.010301067224262576\nTRAIN: \t Epoch: 77 \t Loss: -0.010335345775820315\nTRAIN: \t Epoch: 77 \t Loss: -0.010341856418983148\nVALD: \t Epoch: 77 \t Loss: -0.004698906093835831\nVALD: \t Epoch: 77 \t Loss: -0.008231047075241804\nVALD: \t Epoch: 77 \t Loss: -0.00799101140970985\nVALD: \t Epoch: 77 \t Loss: -0.00904770998749882\nVALD: \t Epoch: 77 \t Loss: -0.008747631704941656\n******************************\nEpoch: social-tag : 77\ntrain_loss -0.010341856418983148\nval_loss -0.008747631704941656\n{'min_val_epoch': 75, 'min_val_loss': -0.01114812738077652}\n******************************\nTRAIN: \t Epoch: 78 \t Loss: -0.011338833719491959\nTRAIN: \t Epoch: 78 \t Loss: -0.011260556988418102\nTRAIN: \t Epoch: 78 \t Loss: -0.011124829140802225\nTRAIN: \t Epoch: 78 \t Loss: -0.010946534341201186\nTRAIN: \t Epoch: 78 \t Loss: -0.010880948975682259\nTRAIN: \t Epoch: 78 \t Loss: -0.010874188194672266\nTRAIN: \t Epoch: 78 \t Loss: -0.010827434515314443\nTRAIN: \t Epoch: 78 \t Loss: -0.010659481165930629\nTRAIN: \t Epoch: 78 \t Loss: -0.010422883348332511\nTRAIN: \t Epoch: 78 \t Loss: -0.010421546548604966\nTRAIN: \t Epoch: 78 \t Loss: -0.010492469946091825\nTRAIN: \t Epoch: 78 \t Loss: -0.0105056616012007\nTRAIN: \t Epoch: 78 \t Loss: -0.010553478263318539\nTRAIN: \t Epoch: 78 \t Loss: -0.01053434849849769\nTRAIN: \t Epoch: 78 \t Loss: -0.010493183881044388\nTRAIN: \t Epoch: 78 \t Loss: -0.010459859506227076\nTRAIN: \t Epoch: 78 \t Loss: -0.010445127099314156\nTRAIN: \t Epoch: 78 \t Loss: -0.010475666469169987\nTRAIN: \t Epoch: 78 \t Loss: -0.010480809956789017\nTRAIN: \t Epoch: 78 \t Loss: -0.010435343580320477\nTRAIN: \t Epoch: 78 \t Loss: -0.010405796049370983\nVALD: \t Epoch: 78 \t Loss: -0.008466973900794983\nVALD: \t Epoch: 78 \t Loss: -0.009398265276104212\nVALD: \t Epoch: 78 \t Loss: -0.009458157854775587\nVALD: \t Epoch: 78 \t Loss: -0.009888317435979843\nVALD: \t Epoch: 78 \t Loss: -0.009571076015343412\n******************************\nEpoch: social-tag : 78\ntrain_loss -0.010405796049370983\nval_loss -0.009571076015343412\n{'min_val_epoch': 75, 'min_val_loss': -0.01114812738077652}\n******************************\nTRAIN: \t Epoch: 79 \t Loss: -0.009654508903622627\nTRAIN: \t Epoch: 79 \t Loss: -0.010260873008519411\nTRAIN: \t Epoch: 79 \t Loss: -0.010599215825398764\nTRAIN: \t Epoch: 79 \t Loss: -0.01081676478497684\nTRAIN: \t Epoch: 79 \t Loss: -0.010271232947707177\nTRAIN: \t Epoch: 79 \t Loss: -0.009738305894037088\nTRAIN: \t Epoch: 79 \t Loss: -0.009738033105220114\nTRAIN: \t Epoch: 79 \t Loss: -0.009950093226507306\nTRAIN: \t Epoch: 79 \t Loss: -0.010076123393244214\nTRAIN: \t Epoch: 79 \t Loss: -0.010210376232862473\nTRAIN: \t Epoch: 79 \t Loss: -0.010251922617581758\nTRAIN: \t Epoch: 79 \t Loss: -0.010253568179905415\nTRAIN: \t Epoch: 79 \t Loss: -0.01019996118087035\nTRAIN: \t Epoch: 79 \t Loss: -0.01011647782953722\nTRAIN: \t Epoch: 79 \t Loss: -0.010110076827307543\nTRAIN: \t Epoch: 79 \t Loss: -0.010100786341354251\nTRAIN: \t Epoch: 79 \t Loss: -0.010121582240304527\nTRAIN: \t Epoch: 79 \t Loss: -0.010160494647506211\nTRAIN: \t Epoch: 79 \t Loss: -0.010186871954877125\nTRAIN: \t Epoch: 79 \t Loss: -0.010189100774005056\nTRAIN: \t Epoch: 79 \t Loss: -0.010157124586719316\nVALD: \t Epoch: 79 \t Loss: -0.008177435956895351\nVALD: \t Epoch: 79 \t Loss: -0.00970075186342001\nVALD: \t Epoch: 79 \t Loss: -0.009730402690668901\nVALD: \t Epoch: 79 \t Loss: -0.010341174434870481\nVALD: \t Epoch: 79 \t Loss: -0.009853475623660617\n******************************\nEpoch: social-tag : 79\ntrain_loss -0.010157124586719316\nval_loss -0.009853475623660617\n{'min_val_epoch': 75, 'min_val_loss': -0.01114812738077652}\n******************************\nTRAIN: \t Epoch: 80 \t Loss: -0.009888886474072933\nTRAIN: \t Epoch: 80 \t Loss: -0.010239400900900364\nTRAIN: \t Epoch: 80 \t Loss: -0.010723150024811426\nTRAIN: \t Epoch: 80 \t Loss: -0.010719585232436657\nTRAIN: \t Epoch: 80 \t Loss: -0.010294593311846257\nTRAIN: \t Epoch: 80 \t Loss: -0.01007756202792128\nTRAIN: \t Epoch: 80 \t Loss: -0.010209607359554087\nTRAIN: \t Epoch: 80 \t Loss: -0.010260002687573433\nTRAIN: \t Epoch: 80 \t Loss: -0.010287865375479063\nTRAIN: \t Epoch: 80 \t Loss: -0.010405381955206395\nTRAIN: \t Epoch: 80 \t Loss: -0.01043950097466057\nTRAIN: \t Epoch: 80 \t Loss: -0.01034764937746028\nTRAIN: \t Epoch: 80 \t Loss: -0.010264764826458234\nTRAIN: \t Epoch: 80 \t Loss: -0.010275168676993676\nTRAIN: \t Epoch: 80 \t Loss: -0.010243815618256729\nTRAIN: \t Epoch: 80 \t Loss: -0.01024008117383346\nTRAIN: \t Epoch: 80 \t Loss: -0.01028352471835473\nTRAIN: \t Epoch: 80 \t Loss: -0.010309454881482653\nTRAIN: \t Epoch: 80 \t Loss: -0.010232932522500815\nTRAIN: \t Epoch: 80 \t Loss: -0.01021555638872087\nTRAIN: \t Epoch: 80 \t Loss: -0.010218930444353437\nVALD: \t Epoch: 80 \t Loss: -0.00947037898004055\nVALD: \t Epoch: 80 \t Loss: -0.0105767622590065\nVALD: \t Epoch: 80 \t Loss: -0.010573995610078176\nVALD: \t Epoch: 80 \t Loss: -0.011147720972076058\nVALD: \t Epoch: 80 \t Loss: -0.010639032399980727\n******************************\nEpoch: social-tag : 80\ntrain_loss -0.010218930444353437\nval_loss -0.010639032399980727\n{'min_val_epoch': 75, 'min_val_loss': -0.01114812738077652}\n******************************\nTRAIN: \t Epoch: 81 \t Loss: -0.011476890183985233\nTRAIN: \t Epoch: 81 \t Loss: -0.010993581730872393\nTRAIN: \t Epoch: 81 \t Loss: -0.010724283133943876\nTRAIN: \t Epoch: 81 \t Loss: -0.0109403389506042\nTRAIN: \t Epoch: 81 \t Loss: -0.010885767638683319\nTRAIN: \t Epoch: 81 \t Loss: -0.010408346851666769\nTRAIN: \t Epoch: 81 \t Loss: -0.009991202370396681\nTRAIN: \t Epoch: 81 \t Loss: -0.01003455650061369\nTRAIN: \t Epoch: 81 \t Loss: -0.010107195004820824\nTRAIN: \t Epoch: 81 \t Loss: -0.010279979836195708\nTRAIN: \t Epoch: 81 \t Loss: -0.010342174040322954\nTRAIN: \t Epoch: 81 \t Loss: -0.010496095831816396\nTRAIN: \t Epoch: 81 \t Loss: -0.010537944877376923\nTRAIN: \t Epoch: 81 \t Loss: -0.010514465759375266\nTRAIN: \t Epoch: 81 \t Loss: -0.010570592433214187\nTRAIN: \t Epoch: 81 \t Loss: -0.010550924460403621\nTRAIN: \t Epoch: 81 \t Loss: -0.01058183572090724\nTRAIN: \t Epoch: 81 \t Loss: -0.01053329138085246\nTRAIN: \t Epoch: 81 \t Loss: -0.010443764101517828\nTRAIN: \t Epoch: 81 \t Loss: -0.0103884635027498\nTRAIN: \t Epoch: 81 \t Loss: -0.010391623976603783\nVALD: \t Epoch: 81 \t Loss: -0.0030234598089009523\nVALD: \t Epoch: 81 \t Loss: -0.00710240111220628\nVALD: \t Epoch: 81 \t Loss: -0.007172339052582781\nVALD: \t Epoch: 81 \t Loss: -0.00859456235775724\nVALD: \t Epoch: 81 \t Loss: -0.008478923264333015\n******************************\nEpoch: social-tag : 81\ntrain_loss -0.010391623976603783\nval_loss -0.008478923264333015\n{'min_val_epoch': 75, 'min_val_loss': -0.01114812738077652}\n******************************\nTRAIN: \t Epoch: 82 \t Loss: -0.011455507017672062\nTRAIN: \t Epoch: 82 \t Loss: -0.011348168831318617\nTRAIN: \t Epoch: 82 \t Loss: -0.011433041033645472\nTRAIN: \t Epoch: 82 \t Loss: -0.011277260025963187\nTRAIN: \t Epoch: 82 \t Loss: -0.011278042383491994\nTRAIN: \t Epoch: 82 \t Loss: -0.011189609455565611\nTRAIN: \t Epoch: 82 \t Loss: -0.010916241710739476\nTRAIN: \t Epoch: 82 \t Loss: -0.010661263950169086\nTRAIN: \t Epoch: 82 \t Loss: -0.010657349187466834\nTRAIN: \t Epoch: 82 \t Loss: -0.010696015227586031\nTRAIN: \t Epoch: 82 \t Loss: -0.010690681043673645\nTRAIN: \t Epoch: 82 \t Loss: -0.01060586500292023\nTRAIN: \t Epoch: 82 \t Loss: -0.010639962453681689\nTRAIN: \t Epoch: 82 \t Loss: -0.010596996106739556\nTRAIN: \t Epoch: 82 \t Loss: -0.010435721712807815\nTRAIN: \t Epoch: 82 \t Loss: -0.010400476166978478\nTRAIN: \t Epoch: 82 \t Loss: -0.010449237628456424\nTRAIN: \t Epoch: 82 \t Loss: -0.010466215614643361\nTRAIN: \t Epoch: 82 \t Loss: -0.010505571863368937\nTRAIN: \t Epoch: 82 \t Loss: -0.010571189690381289\nTRAIN: \t Epoch: 82 \t Loss: -0.010571124076659439\nVALD: \t Epoch: 82 \t Loss: -0.00636145006865263\nVALD: \t Epoch: 82 \t Loss: -0.00930110877379775\nVALD: \t Epoch: 82 \t Loss: -0.008831088741620382\nVALD: \t Epoch: 82 \t Loss: -0.009776424383744597\nVALD: \t Epoch: 82 \t Loss: -0.00950786639334883\n******************************\nEpoch: social-tag : 82\ntrain_loss -0.010571124076659439\nval_loss -0.00950786639334883\n{'min_val_epoch': 75, 'min_val_loss': -0.01114812738077652}\n******************************\nTRAIN: \t Epoch: 83 \t Loss: -0.011901915073394775\nTRAIN: \t Epoch: 83 \t Loss: -0.011832729913294315\nTRAIN: \t Epoch: 83 \t Loss: -0.011266066382328669\nTRAIN: \t Epoch: 83 \t Loss: -0.01096436963416636\nTRAIN: \t Epoch: 83 \t Loss: -0.010704758949577809\nTRAIN: \t Epoch: 83 \t Loss: -0.010650974077483019\nTRAIN: \t Epoch: 83 \t Loss: -0.01066618984831231\nTRAIN: \t Epoch: 83 \t Loss: -0.010685761808417737\nTRAIN: \t Epoch: 83 \t Loss: -0.010517225290338198\nTRAIN: \t Epoch: 83 \t Loss: -0.010465654451400042\nTRAIN: \t Epoch: 83 \t Loss: -0.010500240393660286\nTRAIN: \t Epoch: 83 \t Loss: -0.010485798042888442\nTRAIN: \t Epoch: 83 \t Loss: -0.010513941972301556\nTRAIN: \t Epoch: 83 \t Loss: -0.010547628759273462\nTRAIN: \t Epoch: 83 \t Loss: -0.010589775380988915\nTRAIN: \t Epoch: 83 \t Loss: -0.010520366951823235\nTRAIN: \t Epoch: 83 \t Loss: -0.01043972181265845\nTRAIN: \t Epoch: 83 \t Loss: -0.010478330724355247\nTRAIN: \t Epoch: 83 \t Loss: -0.010546219976324784\nTRAIN: \t Epoch: 83 \t Loss: -0.010556264687329531\nTRAIN: \t Epoch: 83 \t Loss: -0.010556780313103586\nVALD: \t Epoch: 83 \t Loss: -0.01139131374657154\nVALD: \t Epoch: 83 \t Loss: -0.011837387923151255\nVALD: \t Epoch: 83 \t Loss: -0.011741769189635912\nVALD: \t Epoch: 83 \t Loss: -0.012082459637895226\nVALD: \t Epoch: 83 \t Loss: -0.011447085274590386\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:25<00:00, 11.69it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.4635077179346191  FDE: 0.7482333968372046\n**************************************************\n******************************\nEpoch: social-tag : 83\ntrain_loss -0.010556780313103586\nval_loss -0.011447085274590386\n{'min_val_epoch': 83, 'min_val_loss': -0.011447085274590386}\n******************************\nTRAIN: \t Epoch: 84 \t Loss: -0.011651309207081795\nTRAIN: \t Epoch: 84 \t Loss: -0.011307243257761002\nTRAIN: \t Epoch: 84 \t Loss: -0.011287343067427477\nTRAIN: \t Epoch: 84 \t Loss: -0.010871637845411897\nTRAIN: \t Epoch: 84 \t Loss: -0.010250464733690023\nTRAIN: \t Epoch: 84 \t Loss: -0.010089295528208217\nTRAIN: \t Epoch: 84 \t Loss: -0.010299825708248786\nTRAIN: \t Epoch: 84 \t Loss: -0.010406281624455005\nTRAIN: \t Epoch: 84 \t Loss: -0.010410927494780885\nTRAIN: \t Epoch: 84 \t Loss: -0.01034920192323625\nTRAIN: \t Epoch: 84 \t Loss: -0.010357305280525576\nTRAIN: \t Epoch: 84 \t Loss: -0.010310652626988789\nTRAIN: \t Epoch: 84 \t Loss: -0.010273638647049665\nTRAIN: \t Epoch: 84 \t Loss: -0.0102328896589045\nTRAIN: \t Epoch: 84 \t Loss: -0.010253616763899723\nTRAIN: \t Epoch: 84 \t Loss: -0.010329601675039157\nTRAIN: \t Epoch: 84 \t Loss: -0.010310445008251597\nTRAIN: \t Epoch: 84 \t Loss: -0.010181339897422327\nTRAIN: \t Epoch: 84 \t Loss: -0.010153738855335274\nTRAIN: \t Epoch: 84 \t Loss: -0.010208877758122981\nTRAIN: \t Epoch: 84 \t Loss: -0.010194362657476409\nVALD: \t Epoch: 84 \t Loss: -0.008346936665475368\nVALD: \t Epoch: 84 \t Loss: -0.010288187302649021\nVALD: \t Epoch: 84 \t Loss: -0.010130411945283413\nVALD: \t Epoch: 84 \t Loss: -0.010843863477930427\nVALD: \t Epoch: 84 \t Loss: -0.0103482896771024\n******************************\nEpoch: social-tag : 84\ntrain_loss -0.010194362657476409\nval_loss -0.0103482896771024\n{'min_val_epoch': 83, 'min_val_loss': -0.011447085274590386}\n******************************\nTRAIN: \t Epoch: 85 \t Loss: -0.011354746297001839\nTRAIN: \t Epoch: 85 \t Loss: -0.011319463141262531\nTRAIN: \t Epoch: 85 \t Loss: -0.010848343993226687\nTRAIN: \t Epoch: 85 \t Loss: -0.010500326287001371\nTRAIN: \t Epoch: 85 \t Loss: -0.01033372599631548\nTRAIN: \t Epoch: 85 \t Loss: -0.010373693269987902\nTRAIN: \t Epoch: 85 \t Loss: -0.010562482822154249\nTRAIN: \t Epoch: 85 \t Loss: -0.01042178412899375\nTRAIN: \t Epoch: 85 \t Loss: -0.01036426549156507\nTRAIN: \t Epoch: 85 \t Loss: -0.010407471749931573\nTRAIN: \t Epoch: 85 \t Loss: -0.010439628819850359\nTRAIN: \t Epoch: 85 \t Loss: -0.010484070982784033\nTRAIN: \t Epoch: 85 \t Loss: -0.010476630467634935\nTRAIN: \t Epoch: 85 \t Loss: -0.010505600100649255\nTRAIN: \t Epoch: 85 \t Loss: -0.010491873820622761\nTRAIN: \t Epoch: 85 \t Loss: -0.01048227958381176\nTRAIN: \t Epoch: 85 \t Loss: -0.010386322405846679\nTRAIN: \t Epoch: 85 \t Loss: -0.01034170233954986\nTRAIN: \t Epoch: 85 \t Loss: -0.010393603949954635\nTRAIN: \t Epoch: 85 \t Loss: -0.010463901283219456\nTRAIN: \t Epoch: 85 \t Loss: -0.010479982876281326\nVALD: \t Epoch: 85 \t Loss: -0.006137209944427013\nVALD: \t Epoch: 85 \t Loss: -0.008747180458158255\nVALD: \t Epoch: 85 \t Loss: -0.009017465946575006\nVALD: \t Epoch: 85 \t Loss: -0.00999650009907782\nVALD: \t Epoch: 85 \t Loss: -0.009670453946947476\n******************************\nEpoch: social-tag : 85\ntrain_loss -0.010479982876281326\nval_loss -0.009670453946947476\n{'min_val_epoch': 83, 'min_val_loss': -0.011447085274590386}\n******************************\nTRAIN: \t Epoch: 86 \t Loss: -0.012360494583845139\nTRAIN: \t Epoch: 86 \t Loss: -0.011255376040935516\nTRAIN: \t Epoch: 86 \t Loss: -0.010601342345277468\nTRAIN: \t Epoch: 86 \t Loss: -0.010570885380730033\nTRAIN: \t Epoch: 86 \t Loss: -0.010688954405486584\nTRAIN: \t Epoch: 86 \t Loss: -0.010829520101348558\nTRAIN: \t Epoch: 86 \t Loss: -0.010820064693689346\nTRAIN: \t Epoch: 86 \t Loss: -0.010696174344047904\nTRAIN: \t Epoch: 86 \t Loss: -0.010549762907127539\nTRAIN: \t Epoch: 86 \t Loss: -0.010548293311148881\nTRAIN: \t Epoch: 86 \t Loss: -0.010621527633206411\nTRAIN: \t Epoch: 86 \t Loss: -0.010692890655870238\nTRAIN: \t Epoch: 86 \t Loss: -0.010662678270958938\nTRAIN: \t Epoch: 86 \t Loss: -0.010540848464838095\nTRAIN: \t Epoch: 86 \t Loss: -0.010578871394197146\nTRAIN: \t Epoch: 86 \t Loss: -0.01058977033244446\nTRAIN: \t Epoch: 86 \t Loss: -0.010599610669647945\nTRAIN: \t Epoch: 86 \t Loss: -0.010517215718411736\nTRAIN: \t Epoch: 86 \t Loss: -0.010479370034054705\nTRAIN: \t Epoch: 86 \t Loss: -0.010475391754880548\nTRAIN: \t Epoch: 86 \t Loss: -0.010484794228005244\nVALD: \t Epoch: 86 \t Loss: -0.006633366458117962\nVALD: \t Epoch: 86 \t Loss: -0.00924278050661087\nVALD: \t Epoch: 86 \t Loss: -0.009312981429199377\nVALD: \t Epoch: 86 \t Loss: -0.010272747837007046\nVALD: \t Epoch: 86 \t Loss: -0.009810896119441772\n******************************\nEpoch: social-tag : 86\ntrain_loss -0.010484794228005244\nval_loss -0.009810896119441772\n{'min_val_epoch': 83, 'min_val_loss': -0.011447085274590386}\n******************************\nTRAIN: \t Epoch: 87 \t Loss: -0.011170078068971634\nTRAIN: \t Epoch: 87 \t Loss: -0.010789836291223764\nTRAIN: \t Epoch: 87 \t Loss: -0.010788221222658953\nTRAIN: \t Epoch: 87 \t Loss: -0.010587296448647976\nTRAIN: \t Epoch: 87 \t Loss: -0.010419159010052682\nTRAIN: \t Epoch: 87 \t Loss: -0.010497743574281534\nTRAIN: \t Epoch: 87 \t Loss: -0.010652695782482624\nTRAIN: \t Epoch: 87 \t Loss: -0.010702899191528559\nTRAIN: \t Epoch: 87 \t Loss: -0.01056048646569252\nTRAIN: \t Epoch: 87 \t Loss: -0.010566891450434924\nTRAIN: \t Epoch: 87 \t Loss: -0.010555538687516342\nTRAIN: \t Epoch: 87 \t Loss: -0.010574535699561238\nTRAIN: \t Epoch: 87 \t Loss: -0.01062283177788441\nTRAIN: \t Epoch: 87 \t Loss: -0.01070036000705191\nTRAIN: \t Epoch: 87 \t Loss: -0.010752410007019838\nTRAIN: \t Epoch: 87 \t Loss: -0.01074277568841353\nTRAIN: \t Epoch: 87 \t Loss: -0.010666024542468436\nTRAIN: \t Epoch: 87 \t Loss: -0.010598109569400549\nTRAIN: \t Epoch: 87 \t Loss: -0.010644397796376756\nTRAIN: \t Epoch: 87 \t Loss: -0.010704818228259683\nTRAIN: \t Epoch: 87 \t Loss: -0.010715332861670918\nVALD: \t Epoch: 87 \t Loss: -0.007728496100753546\nVALD: \t Epoch: 87 \t Loss: -0.009568815818056464\nVALD: \t Epoch: 87 \t Loss: -0.009243775314340988\nVALD: \t Epoch: 87 \t Loss: -0.010005121934227645\nVALD: \t Epoch: 87 \t Loss: -0.009686289202187948\n******************************\nEpoch: social-tag : 87\ntrain_loss -0.010715332861670918\nval_loss -0.009686289202187948\n{'min_val_epoch': 83, 'min_val_loss': -0.011447085274590386}\n******************************\nTRAIN: \t Epoch: 88 \t Loss: -0.011516780592501163\nTRAIN: \t Epoch: 88 \t Loss: -0.010956054087728262\nTRAIN: \t Epoch: 88 \t Loss: -0.010646964112917582\nTRAIN: \t Epoch: 88 \t Loss: -0.01061861077323556\nTRAIN: \t Epoch: 88 \t Loss: -0.010192767903208732\nTRAIN: \t Epoch: 88 \t Loss: -0.010033749043941498\nTRAIN: \t Epoch: 88 \t Loss: -0.010240588204136916\nTRAIN: \t Epoch: 88 \t Loss: -0.010477652424015105\nTRAIN: \t Epoch: 88 \t Loss: -0.010601708768970437\nTRAIN: \t Epoch: 88 \t Loss: -0.010503196157515049\nTRAIN: \t Epoch: 88 \t Loss: -0.010318171825598587\nTRAIN: \t Epoch: 88 \t Loss: -0.010266514805456003\nTRAIN: \t Epoch: 88 \t Loss: -0.0103345147978801\nTRAIN: \t Epoch: 88 \t Loss: -0.010412080878657954\nTRAIN: \t Epoch: 88 \t Loss: -0.010524038225412369\nTRAIN: \t Epoch: 88 \t Loss: -0.010488558968063444\nTRAIN: \t Epoch: 88 \t Loss: -0.01031494137885816\nTRAIN: \t Epoch: 88 \t Loss: -0.010290839321290454\nTRAIN: \t Epoch: 88 \t Loss: -0.010326608304718607\nTRAIN: \t Epoch: 88 \t Loss: -0.010398174007423221\nTRAIN: \t Epoch: 88 \t Loss: -0.010410831584871596\nVALD: \t Epoch: 88 \t Loss: -0.009194834157824516\nVALD: \t Epoch: 88 \t Loss: -0.010485436767339706\nVALD: \t Epoch: 88 \t Loss: -0.010366526121894518\nVALD: \t Epoch: 88 \t Loss: -0.011129728751257062\nVALD: \t Epoch: 88 \t Loss: -0.010631047489178546\n******************************\nEpoch: social-tag : 88\ntrain_loss -0.010410831584871596\nval_loss -0.010631047489178546\n{'min_val_epoch': 83, 'min_val_loss': -0.011447085274590386}\n******************************\nTRAIN: \t Epoch: 89 \t Loss: -0.011535790748894215\nTRAIN: \t Epoch: 89 \t Loss: -0.011588047258555889\nTRAIN: \t Epoch: 89 \t Loss: -0.011506437323987484\nTRAIN: \t Epoch: 89 \t Loss: -0.011470344848930836\nTRAIN: \t Epoch: 89 \t Loss: -0.011272473074495793\nTRAIN: \t Epoch: 89 \t Loss: -0.011085010599344969\nTRAIN: \t Epoch: 89 \t Loss: -0.010890947255705084\nTRAIN: \t Epoch: 89 \t Loss: -0.010891836951486766\nTRAIN: \t Epoch: 89 \t Loss: -0.010848721282349693\nTRAIN: \t Epoch: 89 \t Loss: -0.01084575690329075\nTRAIN: \t Epoch: 89 \t Loss: -0.010790207253938372\nTRAIN: \t Epoch: 89 \t Loss: -0.010693964237968126\nTRAIN: \t Epoch: 89 \t Loss: -0.010638016992463516\nTRAIN: \t Epoch: 89 \t Loss: -0.010678416384117944\nTRAIN: \t Epoch: 89 \t Loss: -0.010732452881832917\nTRAIN: \t Epoch: 89 \t Loss: -0.01073997386265546\nTRAIN: \t Epoch: 89 \t Loss: -0.010670111459844252\nTRAIN: \t Epoch: 89 \t Loss: -0.010662178250236643\nTRAIN: \t Epoch: 89 \t Loss: -0.010697927080879086\nTRAIN: \t Epoch: 89 \t Loss: -0.010738236457109451\nTRAIN: \t Epoch: 89 \t Loss: -0.010750526497707793\nVALD: \t Epoch: 89 \t Loss: -0.010998469777405262\nVALD: \t Epoch: 89 \t Loss: -0.01125627150759101\nVALD: \t Epoch: 89 \t Loss: -0.010727981415887674\nVALD: \t Epoch: 89 \t Loss: -0.011284944601356983\nVALD: \t Epoch: 89 \t Loss: -0.010679404900653734\n******************************\nEpoch: social-tag : 89\ntrain_loss -0.010750526497707793\nval_loss -0.010679404900653734\n{'min_val_epoch': 83, 'min_val_loss': -0.011447085274590386}\n******************************\nTRAIN: \t Epoch: 90 \t Loss: -0.012151413597166538\nTRAIN: \t Epoch: 90 \t Loss: -0.01197340153157711\nTRAIN: \t Epoch: 90 \t Loss: -0.011601420740286509\nTRAIN: \t Epoch: 90 \t Loss: -0.011584098683670163\nTRAIN: \t Epoch: 90 \t Loss: -0.011493514850735664\nTRAIN: \t Epoch: 90 \t Loss: -0.011282868838558594\nTRAIN: \t Epoch: 90 \t Loss: -0.010993385820516519\nTRAIN: \t Epoch: 90 \t Loss: -0.011036437819711864\nTRAIN: \t Epoch: 90 \t Loss: -0.011084515497916274\nTRAIN: \t Epoch: 90 \t Loss: -0.011029473319649697\nTRAIN: \t Epoch: 90 \t Loss: -0.010938021608374336\nTRAIN: \t Epoch: 90 \t Loss: -0.010936800856143236\nTRAIN: \t Epoch: 90 \t Loss: -0.010963903954969002\nTRAIN: \t Epoch: 90 \t Loss: -0.010962123916085278\nTRAIN: \t Epoch: 90 \t Loss: -0.01089756383250157\nTRAIN: \t Epoch: 90 \t Loss: -0.010813187924213707\nTRAIN: \t Epoch: 90 \t Loss: -0.010796467828399995\nTRAIN: \t Epoch: 90 \t Loss: -0.010837753613789877\nTRAIN: \t Epoch: 90 \t Loss: -0.010910468764211001\nTRAIN: \t Epoch: 90 \t Loss: -0.010951116820797325\nTRAIN: \t Epoch: 90 \t Loss: -0.010931125262561172\nVALD: \t Epoch: 90 \t Loss: -0.004941584076732397\nVALD: \t Epoch: 90 \t Loss: -0.008016543695703149\nVALD: \t Epoch: 90 \t Loss: -0.007692891638725996\nVALD: \t Epoch: 90 \t Loss: -0.008668609312735498\nVALD: \t Epoch: 90 \t Loss: -0.008492466620201073\n******************************\nEpoch: social-tag : 90\ntrain_loss -0.010931125262561172\nval_loss -0.008492466620201073\n{'min_val_epoch': 83, 'min_val_loss': -0.011447085274590386}\n******************************\nTRAIN: \t Epoch: 91 \t Loss: -0.011184561997652054\nTRAIN: \t Epoch: 91 \t Loss: -0.011182471178472042\nTRAIN: \t Epoch: 91 \t Loss: -0.011325609870254993\nTRAIN: \t Epoch: 91 \t Loss: -0.011449161684140563\nTRAIN: \t Epoch: 91 \t Loss: -0.010827554389834404\nTRAIN: \t Epoch: 91 \t Loss: -0.010452093246082464\nTRAIN: \t Epoch: 91 \t Loss: -0.01048657258174249\nTRAIN: \t Epoch: 91 \t Loss: -0.010548534570261836\nTRAIN: \t Epoch: 91 \t Loss: -0.010653742485576205\nTRAIN: \t Epoch: 91 \t Loss: -0.010688903741538525\nTRAIN: \t Epoch: 91 \t Loss: -0.010718506591563875\nTRAIN: \t Epoch: 91 \t Loss: -0.010660353659962615\nTRAIN: \t Epoch: 91 \t Loss: -0.01049607046521627\nTRAIN: \t Epoch: 91 \t Loss: -0.010442754054175956\nTRAIN: \t Epoch: 91 \t Loss: -0.01054297418644031\nTRAIN: \t Epoch: 91 \t Loss: -0.010610166180413216\nTRAIN: \t Epoch: 91 \t Loss: -0.01059668155058342\nTRAIN: \t Epoch: 91 \t Loss: -0.01061268337070942\nTRAIN: \t Epoch: 91 \t Loss: -0.010596677562908122\nTRAIN: \t Epoch: 91 \t Loss: -0.010585679020732642\nTRAIN: \t Epoch: 91 \t Loss: -0.010582277164977609\nVALD: \t Epoch: 91 \t Loss: -0.011346928775310516\nVALD: \t Epoch: 91 \t Loss: -0.011854622047394514\nVALD: \t Epoch: 91 \t Loss: -0.011864723637700081\nVALD: \t Epoch: 91 \t Loss: -0.012263765092939138\nVALD: \t Epoch: 91 \t Loss: -0.011584185171818388\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.15it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.46346215092339477  FDE: 0.7047318090761551\n**************************************************\n******************************\nEpoch: social-tag : 91\ntrain_loss -0.010582277164977609\nval_loss -0.011584185171818388\n{'min_val_epoch': 91, 'min_val_loss': -0.011584185171818388}\n******************************\nTRAIN: \t Epoch: 92 \t Loss: -0.011756443418562412\nTRAIN: \t Epoch: 92 \t Loss: -0.010965121444314718\nTRAIN: \t Epoch: 92 \t Loss: -0.010712553126116594\nTRAIN: \t Epoch: 92 \t Loss: -0.010785882361233234\nTRAIN: \t Epoch: 92 \t Loss: -0.010786776058375835\nTRAIN: \t Epoch: 92 \t Loss: -0.010658049335082373\nTRAIN: \t Epoch: 92 \t Loss: -0.01056073552795819\nTRAIN: \t Epoch: 92 \t Loss: -0.010597896296530962\nTRAIN: \t Epoch: 92 \t Loss: -0.01053007392005788\nTRAIN: \t Epoch: 92 \t Loss: -0.010542250145226717\nTRAIN: \t Epoch: 92 \t Loss: -0.0106227807700634\nTRAIN: \t Epoch: 92 \t Loss: -0.010627393377944827\nTRAIN: \t Epoch: 92 \t Loss: -0.010641023588295166\nTRAIN: \t Epoch: 92 \t Loss: -0.010609604218708617\nTRAIN: \t Epoch: 92 \t Loss: -0.010595984322329362\nTRAIN: \t Epoch: 92 \t Loss: -0.010619637905620039\nTRAIN: \t Epoch: 92 \t Loss: -0.010646633529926048\nTRAIN: \t Epoch: 92 \t Loss: -0.01062764972448349\nTRAIN: \t Epoch: 92 \t Loss: -0.010641470453456828\nTRAIN: \t Epoch: 92 \t Loss: -0.010597776621580124\nTRAIN: \t Epoch: 92 \t Loss: -0.01060877635833752\nVALD: \t Epoch: 92 \t Loss: -0.006253366358578205\nVALD: \t Epoch: 92 \t Loss: -0.008357608690857887\nVALD: \t Epoch: 92 \t Loss: -0.007744974456727505\nVALD: \t Epoch: 92 \t Loss: -0.008907467825338244\nVALD: \t Epoch: 92 \t Loss: -0.008734543542355155\n******************************\nEpoch: social-tag : 92\ntrain_loss -0.01060877635833752\nval_loss -0.008734543542355155\n{'min_val_epoch': 91, 'min_val_loss': -0.011584185171818388}\n******************************\nTRAIN: \t Epoch: 93 \t Loss: -0.011348886415362358\nTRAIN: \t Epoch: 93 \t Loss: -0.011408410500735044\nTRAIN: \t Epoch: 93 \t Loss: -0.011421540131171545\nTRAIN: \t Epoch: 93 \t Loss: -0.011140875518321991\nTRAIN: \t Epoch: 93 \t Loss: -0.010521426610648633\nTRAIN: \t Epoch: 93 \t Loss: -0.010500762766848007\nTRAIN: \t Epoch: 93 \t Loss: -0.010544727157269205\nTRAIN: \t Epoch: 93 \t Loss: -0.010728192515671253\nTRAIN: \t Epoch: 93 \t Loss: -0.0106356262953745\nTRAIN: \t Epoch: 93 \t Loss: -0.010682975221425296\nTRAIN: \t Epoch: 93 \t Loss: -0.010772608559239994\nTRAIN: \t Epoch: 93 \t Loss: -0.010796552912021676\nTRAIN: \t Epoch: 93 \t Loss: -0.01077753513191755\nTRAIN: \t Epoch: 93 \t Loss: -0.010710989590734243\nTRAIN: \t Epoch: 93 \t Loss: -0.010679800994694233\nTRAIN: \t Epoch: 93 \t Loss: -0.010763338359538466\nTRAIN: \t Epoch: 93 \t Loss: -0.01079129274277126\nTRAIN: \t Epoch: 93 \t Loss: -0.010740969019631544\nTRAIN: \t Epoch: 93 \t Loss: -0.010661616137153223\nTRAIN: \t Epoch: 93 \t Loss: -0.010597562277689576\nTRAIN: \t Epoch: 93 \t Loss: -0.010606090966884963\nVALD: \t Epoch: 93 \t Loss: -0.011623342521488667\nVALD: \t Epoch: 93 \t Loss: -0.012161898892372847\nVALD: \t Epoch: 93 \t Loss: -0.012297467328608036\nVALD: \t Epoch: 93 \t Loss: -0.012701836414635181\nVALD: \t Epoch: 93 \t Loss: -0.011914497988235547\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.10it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.4977620061269262  FDE: 0.8115770826416079\n**************************************************\n******************************\nEpoch: social-tag : 93\ntrain_loss -0.010606090966884963\nval_loss -0.011914497988235547\n{'min_val_epoch': 93, 'min_val_loss': -0.011914497988235547}\n******************************\nTRAIN: \t Epoch: 94 \t Loss: -0.011717640794813633\nTRAIN: \t Epoch: 94 \t Loss: -0.011574115138500929\nTRAIN: \t Epoch: 94 \t Loss: -0.011539886395136515\nTRAIN: \t Epoch: 94 \t Loss: -0.011652773711830378\nTRAIN: \t Epoch: 94 \t Loss: -0.011768431775271893\nTRAIN: \t Epoch: 94 \t Loss: -0.011556572591265043\nTRAIN: \t Epoch: 94 \t Loss: -0.011373968129711491\nTRAIN: \t Epoch: 94 \t Loss: -0.011339516844600439\nTRAIN: \t Epoch: 94 \t Loss: -0.011320492149227195\nTRAIN: \t Epoch: 94 \t Loss: -0.011320741288363934\nTRAIN: \t Epoch: 94 \t Loss: -0.011283357712355528\nTRAIN: \t Epoch: 94 \t Loss: -0.011119428866853317\nTRAIN: \t Epoch: 94 \t Loss: -0.0110350249048609\nTRAIN: \t Epoch: 94 \t Loss: -0.011052315322948354\nTRAIN: \t Epoch: 94 \t Loss: -0.011049843579530715\nTRAIN: \t Epoch: 94 \t Loss: -0.01105250942055136\nTRAIN: \t Epoch: 94 \t Loss: -0.011099029661101453\nTRAIN: \t Epoch: 94 \t Loss: -0.01103978573034207\nTRAIN: \t Epoch: 94 \t Loss: -0.010888230888859221\nTRAIN: \t Epoch: 94 \t Loss: -0.010842303326353431\nTRAIN: \t Epoch: 94 \t Loss: -0.010853373059972764\nVALD: \t Epoch: 94 \t Loss: -0.010333736427128315\nVALD: \t Epoch: 94 \t Loss: -0.01110745593905449\nVALD: \t Epoch: 94 \t Loss: -0.010888939102490744\nVALD: \t Epoch: 94 \t Loss: -0.0115434096660465\nVALD: \t Epoch: 94 \t Loss: -0.010957595326858251\n******************************\nEpoch: social-tag : 94\ntrain_loss -0.010853373059972764\nval_loss -0.010957595326858251\n{'min_val_epoch': 93, 'min_val_loss': -0.011914497988235547}\n******************************\nTRAIN: \t Epoch: 95 \t Loss: -0.01218273863196373\nTRAIN: \t Epoch: 95 \t Loss: -0.012438285164535046\nTRAIN: \t Epoch: 95 \t Loss: -0.012082689752181372\nTRAIN: \t Epoch: 95 \t Loss: -0.011767624877393246\nTRAIN: \t Epoch: 95 \t Loss: -0.011671194434165954\nTRAIN: \t Epoch: 95 \t Loss: -0.011577312524120012\nTRAIN: \t Epoch: 95 \t Loss: -0.011029356691454138\nTRAIN: \t Epoch: 95 \t Loss: -0.010889714350923896\nTRAIN: \t Epoch: 95 \t Loss: -0.01099736305574576\nTRAIN: \t Epoch: 95 \t Loss: -0.011001564655452966\nTRAIN: \t Epoch: 95 \t Loss: -0.011021086607467045\nTRAIN: \t Epoch: 95 \t Loss: -0.010919371231769523\nTRAIN: \t Epoch: 95 \t Loss: -0.010807916378745666\nTRAIN: \t Epoch: 95 \t Loss: -0.010856958638344492\nTRAIN: \t Epoch: 95 \t Loss: -0.010940676430861156\nTRAIN: \t Epoch: 95 \t Loss: -0.01092608436010778\nTRAIN: \t Epoch: 95 \t Loss: -0.010950662688735653\nTRAIN: \t Epoch: 95 \t Loss: -0.010870693872372309\nTRAIN: \t Epoch: 95 \t Loss: -0.010857798355190377\nTRAIN: \t Epoch: 95 \t Loss: -0.010874112788587808\nTRAIN: \t Epoch: 95 \t Loss: -0.01086759205358986\nVALD: \t Epoch: 95 \t Loss: -0.011233500204980373\nVALD: \t Epoch: 95 \t Loss: -0.011717056855559349\nVALD: \t Epoch: 95 \t Loss: -0.01155533983061711\nVALD: \t Epoch: 95 \t Loss: -0.011927090119570494\nVALD: \t Epoch: 95 \t Loss: -0.011362646512939159\n******************************\nEpoch: social-tag : 95\ntrain_loss -0.01086759205358986\nval_loss -0.011362646512939159\n{'min_val_epoch': 93, 'min_val_loss': -0.011914497988235547}\n******************************\nTRAIN: \t Epoch: 96 \t Loss: -0.011809451505541801\nTRAIN: \t Epoch: 96 \t Loss: -0.012146595865488052\nTRAIN: \t Epoch: 96 \t Loss: -0.011801865883171558\nTRAIN: \t Epoch: 96 \t Loss: -0.010969911701977253\nTRAIN: \t Epoch: 96 \t Loss: -0.010556793212890625\nTRAIN: \t Epoch: 96 \t Loss: -0.010665528941899538\nTRAIN: \t Epoch: 96 \t Loss: -0.010911173719380583\nTRAIN: \t Epoch: 96 \t Loss: -0.01101106044370681\nTRAIN: \t Epoch: 96 \t Loss: -0.010849080565902922\nTRAIN: \t Epoch: 96 \t Loss: -0.010574496630579234\nTRAIN: \t Epoch: 96 \t Loss: -0.010549200275404886\nTRAIN: \t Epoch: 96 \t Loss: -0.010713393101468682\nTRAIN: \t Epoch: 96 \t Loss: -0.010821183856863242\nTRAIN: \t Epoch: 96 \t Loss: -0.010913284695042031\nTRAIN: \t Epoch: 96 \t Loss: -0.010897898736099402\nTRAIN: \t Epoch: 96 \t Loss: -0.0108462845091708\nTRAIN: \t Epoch: 96 \t Loss: -0.010790439925211318\nTRAIN: \t Epoch: 96 \t Loss: -0.010814339802083042\nTRAIN: \t Epoch: 96 \t Loss: -0.010839298465534261\nTRAIN: \t Epoch: 96 \t Loss: -0.01082722689025104\nTRAIN: \t Epoch: 96 \t Loss: -0.010826654381814514\nVALD: \t Epoch: 96 \t Loss: -0.0077105020172894\nVALD: \t Epoch: 96 \t Loss: -0.00939849647693336\nVALD: \t Epoch: 96 \t Loss: -0.00892954490457972\nVALD: \t Epoch: 96 \t Loss: -0.009775565122254193\nVALD: \t Epoch: 96 \t Loss: -0.009399979394799078\n******************************\nEpoch: social-tag : 96\ntrain_loss -0.010826654381814514\nval_loss -0.009399979394799078\n{'min_val_epoch': 93, 'min_val_loss': -0.011914497988235547}\n******************************\nTRAIN: \t Epoch: 97 \t Loss: -0.012098205275833607\nTRAIN: \t Epoch: 97 \t Loss: -0.011768363416194916\nTRAIN: \t Epoch: 97 \t Loss: -0.011540280344585577\nTRAIN: \t Epoch: 97 \t Loss: -0.011232342105358839\nTRAIN: \t Epoch: 97 \t Loss: -0.011051802150905132\nTRAIN: \t Epoch: 97 \t Loss: -0.01102258792767922\nTRAIN: \t Epoch: 97 \t Loss: -0.011066419737679618\nTRAIN: \t Epoch: 97 \t Loss: -0.011176482657901943\nTRAIN: \t Epoch: 97 \t Loss: -0.011233133677807119\nTRAIN: \t Epoch: 97 \t Loss: -0.011053360160440206\nTRAIN: \t Epoch: 97 \t Loss: -0.010916607115756382\nTRAIN: \t Epoch: 97 \t Loss: -0.01094615381831924\nTRAIN: \t Epoch: 97 \t Loss: -0.011040340822476607\nTRAIN: \t Epoch: 97 \t Loss: -0.011124261255775179\nTRAIN: \t Epoch: 97 \t Loss: -0.011061018084486325\nTRAIN: \t Epoch: 97 \t Loss: -0.011044821178074926\nTRAIN: \t Epoch: 97 \t Loss: -0.01101605232585879\nTRAIN: \t Epoch: 97 \t Loss: -0.010983439421074258\nTRAIN: \t Epoch: 97 \t Loss: -0.010972004530853346\nTRAIN: \t Epoch: 97 \t Loss: -0.01097646770067513\nTRAIN: \t Epoch: 97 \t Loss: -0.010994730610615856\nVALD: \t Epoch: 97 \t Loss: -0.011649509891867638\nVALD: \t Epoch: 97 \t Loss: -0.012007638812065125\nVALD: \t Epoch: 97 \t Loss: -0.011753964858750502\nVALD: \t Epoch: 97 \t Loss: -0.012241893447935581\nVALD: \t Epoch: 97 \t Loss: -0.011521279907840845\n******************************\nEpoch: social-tag : 97\ntrain_loss -0.010994730610615856\nval_loss -0.011521279907840845\n{'min_val_epoch': 93, 'min_val_loss': -0.011914497988235547}\n******************************\nTRAIN: \t Epoch: 98 \t Loss: -0.012560028582811356\nTRAIN: \t Epoch: 98 \t Loss: -0.012205047532916069\nTRAIN: \t Epoch: 98 \t Loss: -0.011705911407868067\nTRAIN: \t Epoch: 98 \t Loss: -0.011299189180135727\nTRAIN: \t Epoch: 98 \t Loss: -0.011023229546844959\nTRAIN: \t Epoch: 98 \t Loss: -0.011139757931232452\nTRAIN: \t Epoch: 98 \t Loss: -0.011287288607231208\nTRAIN: \t Epoch: 98 \t Loss: -0.011168503435328603\nTRAIN: \t Epoch: 98 \t Loss: -0.01123588884042369\nTRAIN: \t Epoch: 98 \t Loss: -0.011187405698001385\nTRAIN: \t Epoch: 98 \t Loss: -0.011199470524760809\nTRAIN: \t Epoch: 98 \t Loss: -0.011120608542114496\nTRAIN: \t Epoch: 98 \t Loss: -0.011066798049096879\nTRAIN: \t Epoch: 98 \t Loss: -0.011002205378775085\nTRAIN: \t Epoch: 98 \t Loss: -0.01098105168590943\nTRAIN: \t Epoch: 98 \t Loss: -0.01101824635406956\nTRAIN: \t Epoch: 98 \t Loss: -0.01101453251698438\nTRAIN: \t Epoch: 98 \t Loss: -0.010954743975566493\nTRAIN: \t Epoch: 98 \t Loss: -0.010882683931604811\nTRAIN: \t Epoch: 98 \t Loss: -0.010901763197034597\nTRAIN: \t Epoch: 98 \t Loss: -0.010905593518762286\nVALD: \t Epoch: 98 \t Loss: -0.010558108799159527\nVALD: \t Epoch: 98 \t Loss: -0.01122679328545928\nVALD: \t Epoch: 98 \t Loss: -0.01099007266263167\nVALD: \t Epoch: 98 \t Loss: -0.011539364699274302\nVALD: \t Epoch: 98 \t Loss: -0.011034133545634444\n******************************\nEpoch: social-tag : 98\ntrain_loss -0.010905593518762286\nval_loss -0.011034133545634444\n{'min_val_epoch': 93, 'min_val_loss': -0.011914497988235547}\n******************************\nTRAIN: \t Epoch: 99 \t Loss: -0.01254146546125412\nTRAIN: \t Epoch: 99 \t Loss: -0.012230464722961187\nTRAIN: \t Epoch: 99 \t Loss: -0.01206558725486199\nTRAIN: \t Epoch: 99 \t Loss: -0.012110482202842832\nTRAIN: \t Epoch: 99 \t Loss: -0.01192107778042555\nTRAIN: \t Epoch: 99 \t Loss: -0.011719836387783289\nTRAIN: \t Epoch: 99 \t Loss: -0.011385569082839149\nTRAIN: \t Epoch: 99 \t Loss: -0.011223908513784409\nTRAIN: \t Epoch: 99 \t Loss: -0.01123164076772001\nTRAIN: \t Epoch: 99 \t Loss: -0.01127364169806242\nTRAIN: \t Epoch: 99 \t Loss: -0.011230809952725063\nTRAIN: \t Epoch: 99 \t Loss: -0.011122184805572033\nTRAIN: \t Epoch: 99 \t Loss: -0.011001268903223367\nTRAIN: \t Epoch: 99 \t Loss: -0.011048382281192712\nTRAIN: \t Epoch: 99 \t Loss: -0.011100681746999424\nTRAIN: \t Epoch: 99 \t Loss: -0.01106561382766813\nTRAIN: \t Epoch: 99 \t Loss: -0.010931757278740406\nTRAIN: \t Epoch: 99 \t Loss: -0.010874670257584916\nTRAIN: \t Epoch: 99 \t Loss: -0.010954021525226142\nTRAIN: \t Epoch: 99 \t Loss: -0.011047613807022572\nTRAIN: \t Epoch: 99 \t Loss: -0.011067863430256281\nVALD: \t Epoch: 99 \t Loss: -0.011952574364840984\nVALD: \t Epoch: 99 \t Loss: -0.012437368743121624\nVALD: \t Epoch: 99 \t Loss: -0.012548808318873247\nVALD: \t Epoch: 99 \t Loss: -0.012895340099930763\nVALD: \t Epoch: 99 \t Loss: -0.01213796232825508\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.14it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.4312004316695127  FDE: 0.7080009029249303\n**************************************************\n******************************\nEpoch: social-tag : 99\ntrain_loss -0.011067863430256281\nval_loss -0.01213796232825508\n{'min_val_epoch': 99, 'min_val_loss': -0.01213796232825508}\n******************************\nTRAIN: \t Epoch: 100 \t Loss: -0.012459633871912956\nTRAIN: \t Epoch: 100 \t Loss: -0.012301938142627478\nTRAIN: \t Epoch: 100 \t Loss: -0.012116878914336363\nTRAIN: \t Epoch: 100 \t Loss: -0.011926560197025537\nTRAIN: \t Epoch: 100 \t Loss: -0.011792971193790436\nTRAIN: \t Epoch: 100 \t Loss: -0.011601903010159731\nTRAIN: \t Epoch: 100 \t Loss: -0.011438439467123576\nTRAIN: \t Epoch: 100 \t Loss: -0.011541434214450419\nTRAIN: \t Epoch: 100 \t Loss: -0.011516680320103964\nTRAIN: \t Epoch: 100 \t Loss: -0.011347003933042287\nTRAIN: \t Epoch: 100 \t Loss: -0.011216973801228132\nTRAIN: \t Epoch: 100 \t Loss: -0.011097802237297097\nTRAIN: \t Epoch: 100 \t Loss: -0.011083590296598582\nTRAIN: \t Epoch: 100 \t Loss: -0.011167934696589197\nTRAIN: \t Epoch: 100 \t Loss: -0.011200230134030183\nTRAIN: \t Epoch: 100 \t Loss: -0.011145873402711004\nTRAIN: \t Epoch: 100 \t Loss: -0.011067656526232468\nTRAIN: \t Epoch: 100 \t Loss: -0.011094204460581144\nTRAIN: \t Epoch: 100 \t Loss: -0.011126043274998665\nTRAIN: \t Epoch: 100 \t Loss: -0.011114773666486144\nTRAIN: \t Epoch: 100 \t Loss: -0.011133495278788605\nVALD: \t Epoch: 100 \t Loss: -0.010595213621854782\nVALD: \t Epoch: 100 \t Loss: -0.011391257401555777\nVALD: \t Epoch: 100 \t Loss: -0.01129709059993426\nVALD: \t Epoch: 100 \t Loss: -0.011846539098769426\nVALD: \t Epoch: 100 \t Loss: -0.011288359929206099\n******************************\nEpoch: social-tag : 100\ntrain_loss -0.011133495278788605\nval_loss -0.011288359929206099\n{'min_val_epoch': 99, 'min_val_loss': -0.01213796232825508}\n******************************\nTRAIN: \t Epoch: 101 \t Loss: -0.012023883871734142\nTRAIN: \t Epoch: 101 \t Loss: -0.012418818194419146\nTRAIN: \t Epoch: 101 \t Loss: -0.011974010616540909\nTRAIN: \t Epoch: 101 \t Loss: -0.012048825388774276\nTRAIN: \t Epoch: 101 \t Loss: -0.01187041737139225\nTRAIN: \t Epoch: 101 \t Loss: -0.011225936779131493\nTRAIN: \t Epoch: 101 \t Loss: -0.010949653573334217\nTRAIN: \t Epoch: 101 \t Loss: -0.010969147318974137\nTRAIN: \t Epoch: 101 \t Loss: -0.011052187428706221\nTRAIN: \t Epoch: 101 \t Loss: -0.011123245675116777\nTRAIN: \t Epoch: 101 \t Loss: -0.011216346089812841\nTRAIN: \t Epoch: 101 \t Loss: -0.011202687319989005\nTRAIN: \t Epoch: 101 \t Loss: -0.011109129525721073\nTRAIN: \t Epoch: 101 \t Loss: -0.011059991882315703\nTRAIN: \t Epoch: 101 \t Loss: -0.011053049253920715\nTRAIN: \t Epoch: 101 \t Loss: -0.01109960745088756\nTRAIN: \t Epoch: 101 \t Loss: -0.01110818752032869\nTRAIN: \t Epoch: 101 \t Loss: -0.011063551219801107\nTRAIN: \t Epoch: 101 \t Loss: -0.011011225367455106\nTRAIN: \t Epoch: 101 \t Loss: -0.011060911696404219\nTRAIN: \t Epoch: 101 \t Loss: -0.011069853372912088\nVALD: \t Epoch: 101 \t Loss: -0.005096091888844967\nVALD: \t Epoch: 101 \t Loss: -0.008122968021780252\nVALD: \t Epoch: 101 \t Loss: -0.007437866802016894\nVALD: \t Epoch: 101 \t Loss: -0.008823706535622478\nVALD: \t Epoch: 101 \t Loss: -0.008692924335191024\n******************************\nEpoch: social-tag : 101\ntrain_loss -0.011069853372912088\nval_loss -0.008692924335191024\n{'min_val_epoch': 99, 'min_val_loss': -0.01213796232825508}\n******************************\nTRAIN: \t Epoch: 102 \t Loss: -0.012842071242630482\nTRAIN: \t Epoch: 102 \t Loss: -0.012378766667097807\nTRAIN: \t Epoch: 102 \t Loss: -0.01231886725872755\nTRAIN: \t Epoch: 102 \t Loss: -0.011842326493933797\nTRAIN: \t Epoch: 102 \t Loss: -0.01142975352704525\nTRAIN: \t Epoch: 102 \t Loss: -0.011174939572811127\nTRAIN: \t Epoch: 102 \t Loss: -0.011307742978845323\nTRAIN: \t Epoch: 102 \t Loss: -0.011384635814465582\nTRAIN: \t Epoch: 102 \t Loss: -0.011428526188764308\nTRAIN: \t Epoch: 102 \t Loss: -0.011355311051011086\nTRAIN: \t Epoch: 102 \t Loss: -0.011097634973173792\nTRAIN: \t Epoch: 102 \t Loss: -0.0110457056046774\nTRAIN: \t Epoch: 102 \t Loss: -0.011131530412687706\nTRAIN: \t Epoch: 102 \t Loss: -0.011188767783875977\nTRAIN: \t Epoch: 102 \t Loss: -0.011258546449244022\nTRAIN: \t Epoch: 102 \t Loss: -0.011251120187807828\nTRAIN: \t Epoch: 102 \t Loss: -0.011182846227551209\nTRAIN: \t Epoch: 102 \t Loss: -0.01107928870866696\nTRAIN: \t Epoch: 102 \t Loss: -0.011073264489440541\nTRAIN: \t Epoch: 102 \t Loss: -0.011089111818000675\nTRAIN: \t Epoch: 102 \t Loss: -0.011111385999685447\nVALD: \t Epoch: 102 \t Loss: -0.0102530587464571\nVALD: \t Epoch: 102 \t Loss: -0.011394196189939976\nVALD: \t Epoch: 102 \t Loss: -0.011158418220778307\nVALD: \t Epoch: 102 \t Loss: -0.011688417755067348\nVALD: \t Epoch: 102 \t Loss: -0.011201246733058862\n******************************\nEpoch: social-tag : 102\ntrain_loss -0.011111385999685447\nval_loss -0.011201246733058862\n{'min_val_epoch': 99, 'min_val_loss': -0.01213796232825508}\n******************************\nTRAIN: \t Epoch: 103 \t Loss: -0.012369551695883274\nTRAIN: \t Epoch: 103 \t Loss: -0.0118324626237154\nTRAIN: \t Epoch: 103 \t Loss: -0.011437425079445044\nTRAIN: \t Epoch: 103 \t Loss: -0.011378814466297626\nTRAIN: \t Epoch: 103 \t Loss: -0.0111937765032053\nTRAIN: \t Epoch: 103 \t Loss: -0.011097995564341545\nTRAIN: \t Epoch: 103 \t Loss: -0.011154405639639922\nTRAIN: \t Epoch: 103 \t Loss: -0.011141793453134596\nTRAIN: \t Epoch: 103 \t Loss: -0.010953925963905122\nTRAIN: \t Epoch: 103 \t Loss: -0.010850766208022833\nTRAIN: \t Epoch: 103 \t Loss: -0.010890773074193434\nTRAIN: \t Epoch: 103 \t Loss: -0.011029878631234169\nTRAIN: \t Epoch: 103 \t Loss: -0.011160507726554688\nTRAIN: \t Epoch: 103 \t Loss: -0.011212880057947976\nTRAIN: \t Epoch: 103 \t Loss: -0.011242806725203992\nTRAIN: \t Epoch: 103 \t Loss: -0.011270954215433449\nTRAIN: \t Epoch: 103 \t Loss: -0.011132175376748337\nTRAIN: \t Epoch: 103 \t Loss: -0.011030305332193771\nTRAIN: \t Epoch: 103 \t Loss: -0.011031307770233405\nTRAIN: \t Epoch: 103 \t Loss: -0.011078417394310236\nTRAIN: \t Epoch: 103 \t Loss: -0.0111027191534535\nVALD: \t Epoch: 103 \t Loss: -0.00738921482115984\nVALD: \t Epoch: 103 \t Loss: -0.010019147302955389\nVALD: \t Epoch: 103 \t Loss: -0.010234980843961239\nVALD: \t Epoch: 103 \t Loss: -0.011063857702538371\nVALD: \t Epoch: 103 \t Loss: -0.010643838396394887\n******************************\nEpoch: social-tag : 103\ntrain_loss -0.0111027191534535\nval_loss -0.010643838396394887\n{'min_val_epoch': 99, 'min_val_loss': -0.01213796232825508}\n******************************\nTRAIN: \t Epoch: 104 \t Loss: -0.012995574623346329\nTRAIN: \t Epoch: 104 \t Loss: -0.012296827044337988\nTRAIN: \t Epoch: 104 \t Loss: -0.012319125855962435\nTRAIN: \t Epoch: 104 \t Loss: -0.012304611969739199\nTRAIN: \t Epoch: 104 \t Loss: -0.012244397960603236\nTRAIN: \t Epoch: 104 \t Loss: -0.011998494466145834\nTRAIN: \t Epoch: 104 \t Loss: -0.01141539788139718\nTRAIN: \t Epoch: 104 \t Loss: -0.011156380525790155\nTRAIN: \t Epoch: 104 \t Loss: -0.011176000866625044\nTRAIN: \t Epoch: 104 \t Loss: -0.011274814698845148\nTRAIN: \t Epoch: 104 \t Loss: -0.011347984387116\nTRAIN: \t Epoch: 104 \t Loss: -0.01131735237625738\nTRAIN: \t Epoch: 104 \t Loss: -0.011233935992305096\nTRAIN: \t Epoch: 104 \t Loss: -0.011167502230299371\nTRAIN: \t Epoch: 104 \t Loss: -0.011113274656236172\nTRAIN: \t Epoch: 104 \t Loss: -0.01114150503417477\nTRAIN: \t Epoch: 104 \t Loss: -0.01109146502088098\nTRAIN: \t Epoch: 104 \t Loss: -0.011028990408198701\nTRAIN: \t Epoch: 104 \t Loss: -0.011067642781295274\nTRAIN: \t Epoch: 104 \t Loss: -0.011093200696632267\nTRAIN: \t Epoch: 104 \t Loss: -0.011104031903245583\nVALD: \t Epoch: 104 \t Loss: -0.012016383931040764\nVALD: \t Epoch: 104 \t Loss: -0.011300510261207819\nVALD: \t Epoch: 104 \t Loss: -0.00971921750654777\nVALD: \t Epoch: 104 \t Loss: -0.010165922809392214\nVALD: \t Epoch: 104 \t Loss: -0.00976514509142693\n******************************\nEpoch: social-tag : 104\ntrain_loss -0.011104031903245583\nval_loss -0.00976514509142693\n{'min_val_epoch': 99, 'min_val_loss': -0.01213796232825508}\n******************************\nTRAIN: \t Epoch: 105 \t Loss: -0.011178561486303806\nTRAIN: \t Epoch: 105 \t Loss: -0.011529494542628527\nTRAIN: \t Epoch: 105 \t Loss: -0.011415997830530008\nTRAIN: \t Epoch: 105 \t Loss: -0.011376319685950875\nTRAIN: \t Epoch: 105 \t Loss: -0.011235206387937069\nTRAIN: \t Epoch: 105 \t Loss: -0.011270760092884302\nTRAIN: \t Epoch: 105 \t Loss: -0.011169875439788614\nTRAIN: \t Epoch: 105 \t Loss: -0.011225967085920274\nTRAIN: \t Epoch: 105 \t Loss: -0.011289328647156557\nTRAIN: \t Epoch: 105 \t Loss: -0.011122054792940617\nTRAIN: \t Epoch: 105 \t Loss: -0.010992083190516993\nTRAIN: \t Epoch: 105 \t Loss: -0.011052968678995967\nTRAIN: \t Epoch: 105 \t Loss: -0.011117660583784947\nTRAIN: \t Epoch: 105 \t Loss: -0.011144713631698064\nTRAIN: \t Epoch: 105 \t Loss: -0.01103317483017842\nTRAIN: \t Epoch: 105 \t Loss: -0.010984857275616378\nTRAIN: \t Epoch: 105 \t Loss: -0.010998922345392844\nTRAIN: \t Epoch: 105 \t Loss: -0.010990647702581353\nTRAIN: \t Epoch: 105 \t Loss: -0.010964111130880682\nTRAIN: \t Epoch: 105 \t Loss: -0.011004103161394597\nTRAIN: \t Epoch: 105 \t Loss: -0.010986646483655884\nVALD: \t Epoch: 105 \t Loss: -0.012991558760404587\nVALD: \t Epoch: 105 \t Loss: -0.012831541243940592\nVALD: \t Epoch: 105 \t Loss: -0.012641722025970617\nVALD: \t Epoch: 105 \t Loss: -0.012824926525354385\nVALD: \t Epoch: 105 \t Loss: -0.012140887562971376\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.18it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.4224906105262293  FDE: 0.6250815060674604\n**************************************************\n******************************\nEpoch: social-tag : 105\ntrain_loss -0.010986646483655884\nval_loss -0.012140887562971376\n{'min_val_epoch': 105, 'min_val_loss': -0.012140887562971376}\n******************************\nTRAIN: \t Epoch: 106 \t Loss: -0.012455463409423828\nTRAIN: \t Epoch: 106 \t Loss: -0.012559692841023207\nTRAIN: \t Epoch: 106 \t Loss: -0.012243264354765415\nTRAIN: \t Epoch: 106 \t Loss: -0.011681226780638099\nTRAIN: \t Epoch: 106 \t Loss: -0.01127698291093111\nTRAIN: \t Epoch: 106 \t Loss: -0.011238467569152514\nTRAIN: \t Epoch: 106 \t Loss: -0.011262425354548864\nTRAIN: \t Epoch: 106 \t Loss: -0.011168387602083385\nTRAIN: \t Epoch: 106 \t Loss: -0.011070901010599401\nTRAIN: \t Epoch: 106 \t Loss: -0.01099747084081173\nTRAIN: \t Epoch: 106 \t Loss: -0.010969575579193506\nTRAIN: \t Epoch: 106 \t Loss: -0.011012881062924862\nTRAIN: \t Epoch: 106 \t Loss: -0.011079245461867405\nTRAIN: \t Epoch: 106 \t Loss: -0.0111367304011115\nTRAIN: \t Epoch: 106 \t Loss: -0.011114859332640966\nTRAIN: \t Epoch: 106 \t Loss: -0.01109681325033307\nTRAIN: \t Epoch: 106 \t Loss: -0.011066148824551526\nTRAIN: \t Epoch: 106 \t Loss: -0.011047252143422762\nTRAIN: \t Epoch: 106 \t Loss: -0.011050280428638584\nTRAIN: \t Epoch: 106 \t Loss: -0.011074579274281859\nTRAIN: \t Epoch: 106 \t Loss: -0.011077073657613767\nVALD: \t Epoch: 106 \t Loss: -0.01155494712293148\nVALD: \t Epoch: 106 \t Loss: -0.011774678248912096\nVALD: \t Epoch: 106 \t Loss: -0.011507281102240086\nVALD: \t Epoch: 106 \t Loss: -0.011916941730305552\nVALD: \t Epoch: 106 \t Loss: -0.011320131316469103\n******************************\nEpoch: social-tag : 106\ntrain_loss -0.011077073657613767\nval_loss -0.011320131316469103\n{'min_val_epoch': 105, 'min_val_loss': -0.012140887562971376}\n******************************\nTRAIN: \t Epoch: 107 \t Loss: -0.011834830977022648\nTRAIN: \t Epoch: 107 \t Loss: -0.011644956190139055\nTRAIN: \t Epoch: 107 \t Loss: -0.01121997150282065\nTRAIN: \t Epoch: 107 \t Loss: -0.011155684245750308\nTRAIN: \t Epoch: 107 \t Loss: -0.011170347593724728\nTRAIN: \t Epoch: 107 \t Loss: -0.011228016732881466\nTRAIN: \t Epoch: 107 \t Loss: -0.011351499174322401\nTRAIN: \t Epoch: 107 \t Loss: -0.011484366026706994\nTRAIN: \t Epoch: 107 \t Loss: -0.011387371665073765\nTRAIN: \t Epoch: 107 \t Loss: -0.011065571196377277\nTRAIN: \t Epoch: 107 \t Loss: -0.010964066128839146\nTRAIN: \t Epoch: 107 \t Loss: -0.011081363772973418\nTRAIN: \t Epoch: 107 \t Loss: -0.011219778169806186\nTRAIN: \t Epoch: 107 \t Loss: -0.011242237075098924\nTRAIN: \t Epoch: 107 \t Loss: -0.011229455719391506\nTRAIN: \t Epoch: 107 \t Loss: -0.011225760739762336\nTRAIN: \t Epoch: 107 \t Loss: -0.011214956093360396\nTRAIN: \t Epoch: 107 \t Loss: -0.011186695243749354\nTRAIN: \t Epoch: 107 \t Loss: -0.011155833539209868\nTRAIN: \t Epoch: 107 \t Loss: -0.011147587653249502\nTRAIN: \t Epoch: 107 \t Loss: -0.011134935176730983\nVALD: \t Epoch: 107 \t Loss: -0.011088386178016663\nVALD: \t Epoch: 107 \t Loss: -0.010631456039845943\nVALD: \t Epoch: 107 \t Loss: -0.009594969761868319\nVALD: \t Epoch: 107 \t Loss: -0.010215327376499772\nVALD: \t Epoch: 107 \t Loss: -0.009699775784871812\n******************************\nEpoch: social-tag : 107\ntrain_loss -0.011134935176730983\nval_loss -0.009699775784871812\n{'min_val_epoch': 105, 'min_val_loss': -0.012140887562971376}\n******************************\nTRAIN: \t Epoch: 108 \t Loss: -0.011211554519832134\nTRAIN: \t Epoch: 108 \t Loss: -0.01119565637782216\nTRAIN: \t Epoch: 108 \t Loss: -0.011373442287246386\nTRAIN: \t Epoch: 108 \t Loss: -0.011340366210788488\nTRAIN: \t Epoch: 108 \t Loss: -0.011064326390624046\nTRAIN: \t Epoch: 108 \t Loss: -0.010911106131970882\nTRAIN: \t Epoch: 108 \t Loss: -0.010998030193150043\nTRAIN: \t Epoch: 108 \t Loss: -0.01114917069207877\nTRAIN: \t Epoch: 108 \t Loss: -0.011167516828411155\nTRAIN: \t Epoch: 108 \t Loss: -0.011275851354002952\nTRAIN: \t Epoch: 108 \t Loss: -0.011196990158747543\nTRAIN: \t Epoch: 108 \t Loss: -0.011091551122566065\nTRAIN: \t Epoch: 108 \t Loss: -0.011191864569599811\nTRAIN: \t Epoch: 108 \t Loss: -0.011199276561715774\nTRAIN: \t Epoch: 108 \t Loss: -0.011084884280959766\nTRAIN: \t Epoch: 108 \t Loss: -0.01107171073090285\nTRAIN: \t Epoch: 108 \t Loss: -0.011073087944703944\nTRAIN: \t Epoch: 108 \t Loss: -0.011098382011469867\nTRAIN: \t Epoch: 108 \t Loss: -0.011117787590544475\nTRAIN: \t Epoch: 108 \t Loss: -0.011124689411371947\nTRAIN: \t Epoch: 108 \t Loss: -0.011101620189943219\nVALD: \t Epoch: 108 \t Loss: -0.01089137140661478\nVALD: \t Epoch: 108 \t Loss: -0.011106033809483051\nVALD: \t Epoch: 108 \t Loss: -0.010299752466380596\nVALD: \t Epoch: 108 \t Loss: -0.010628805495798588\nVALD: \t Epoch: 108 \t Loss: -0.010186942207256569\n******************************\nEpoch: social-tag : 108\ntrain_loss -0.011101620189943219\nval_loss -0.010186942207256569\n{'min_val_epoch': 105, 'min_val_loss': -0.012140887562971376}\n******************************\nTRAIN: \t Epoch: 109 \t Loss: -0.011433850042521954\nTRAIN: \t Epoch: 109 \t Loss: -0.011592365801334381\nTRAIN: \t Epoch: 109 \t Loss: -0.011856256052851677\nTRAIN: \t Epoch: 109 \t Loss: -0.011689159087836742\nTRAIN: \t Epoch: 109 \t Loss: -0.011293010972440242\nTRAIN: \t Epoch: 109 \t Loss: -0.011174116594096025\nTRAIN: \t Epoch: 109 \t Loss: -0.011346637670482908\nTRAIN: \t Epoch: 109 \t Loss: -0.011367730679921806\nTRAIN: \t Epoch: 109 \t Loss: -0.011339583123723665\nTRAIN: \t Epoch: 109 \t Loss: -0.01136922910809517\nTRAIN: \t Epoch: 109 \t Loss: -0.01143379450183023\nTRAIN: \t Epoch: 109 \t Loss: -0.011433925324430069\nTRAIN: \t Epoch: 109 \t Loss: -0.011340143230672065\nTRAIN: \t Epoch: 109 \t Loss: -0.011222890644733394\nTRAIN: \t Epoch: 109 \t Loss: -0.01124759924908479\nTRAIN: \t Epoch: 109 \t Loss: -0.011305453313980252\nTRAIN: \t Epoch: 109 \t Loss: -0.0113207572413718\nTRAIN: \t Epoch: 109 \t Loss: -0.011358283046219084\nTRAIN: \t Epoch: 109 \t Loss: -0.011339378004011354\nTRAIN: \t Epoch: 109 \t Loss: -0.011251243390142917\nTRAIN: \t Epoch: 109 \t Loss: -0.011246910842915361\nVALD: \t Epoch: 109 \t Loss: -0.0063075413927435875\nVALD: \t Epoch: 109 \t Loss: -0.009035664144903421\nVALD: \t Epoch: 109 \t Loss: -0.008397732550899187\nVALD: \t Epoch: 109 \t Loss: -0.009441865608096123\nVALD: \t Epoch: 109 \t Loss: -0.009139910508277144\n******************************\nEpoch: social-tag : 109\ntrain_loss -0.011246910842915361\nval_loss -0.009139910508277144\n{'min_val_epoch': 105, 'min_val_loss': -0.012140887562971376}\n******************************\nTRAIN: \t Epoch: 110 \t Loss: -0.011862681247293949\nTRAIN: \t Epoch: 110 \t Loss: -0.011753787752240896\nTRAIN: \t Epoch: 110 \t Loss: -0.011713745693365732\nTRAIN: \t Epoch: 110 \t Loss: -0.01157207228243351\nTRAIN: \t Epoch: 110 \t Loss: -0.011420955322682858\nTRAIN: \t Epoch: 110 \t Loss: -0.011351320737351974\nTRAIN: \t Epoch: 110 \t Loss: -0.011285112106374331\nTRAIN: \t Epoch: 110 \t Loss: -0.011177540989592671\nTRAIN: \t Epoch: 110 \t Loss: -0.011114950912694136\nTRAIN: \t Epoch: 110 \t Loss: -0.011231384892016649\nTRAIN: \t Epoch: 110 \t Loss: -0.011254106394269249\nTRAIN: \t Epoch: 110 \t Loss: -0.011250480078160763\nTRAIN: \t Epoch: 110 \t Loss: -0.011230391235305713\nTRAIN: \t Epoch: 110 \t Loss: -0.011192603901560818\nTRAIN: \t Epoch: 110 \t Loss: -0.011197817139327526\nTRAIN: \t Epoch: 110 \t Loss: -0.011232196702621877\nTRAIN: \t Epoch: 110 \t Loss: -0.011267471675048856\nTRAIN: \t Epoch: 110 \t Loss: -0.011324514790127674\nTRAIN: \t Epoch: 110 \t Loss: -0.011287344962750611\nTRAIN: \t Epoch: 110 \t Loss: -0.011235814122483135\nTRAIN: \t Epoch: 110 \t Loss: -0.011229874380938164\nVALD: \t Epoch: 110 \t Loss: -0.005148554686456919\nVALD: \t Epoch: 110 \t Loss: -0.008549886057153344\nVALD: \t Epoch: 110 \t Loss: -0.008313362952321768\nVALD: \t Epoch: 110 \t Loss: -0.009574606665410101\nVALD: \t Epoch: 110 \t Loss: -0.009324031079640903\n******************************\nEpoch: social-tag : 110\ntrain_loss -0.011229874380938164\nval_loss -0.009324031079640903\n{'min_val_epoch': 105, 'min_val_loss': -0.012140887562971376}\n******************************\nTRAIN: \t Epoch: 111 \t Loss: -0.011354560032486916\nTRAIN: \t Epoch: 111 \t Loss: -0.011952542699873447\nTRAIN: \t Epoch: 111 \t Loss: -0.012115100088218847\nTRAIN: \t Epoch: 111 \t Loss: -0.012109221424907446\nTRAIN: \t Epoch: 111 \t Loss: -0.011390768736600877\nTRAIN: \t Epoch: 111 \t Loss: -0.011025481236477694\nTRAIN: \t Epoch: 111 \t Loss: -0.011080732303006309\nTRAIN: \t Epoch: 111 \t Loss: -0.01128701667767018\nTRAIN: \t Epoch: 111 \t Loss: -0.011480962443682883\nTRAIN: \t Epoch: 111 \t Loss: -0.011530130449682475\nTRAIN: \t Epoch: 111 \t Loss: -0.011266435157846321\nTRAIN: \t Epoch: 111 \t Loss: -0.011145449864367643\nTRAIN: \t Epoch: 111 \t Loss: -0.011244226318712417\nTRAIN: \t Epoch: 111 \t Loss: -0.01130752225539514\nTRAIN: \t Epoch: 111 \t Loss: -0.011393042715887229\nTRAIN: \t Epoch: 111 \t Loss: -0.011380190553609282\nTRAIN: \t Epoch: 111 \t Loss: -0.01144460029900074\nTRAIN: \t Epoch: 111 \t Loss: -0.011470843468689255\nTRAIN: \t Epoch: 111 \t Loss: -0.011406986385976014\nTRAIN: \t Epoch: 111 \t Loss: -0.011316755088046193\nTRAIN: \t Epoch: 111 \t Loss: -0.01128188671749559\nVALD: \t Epoch: 111 \t Loss: -0.011627575382590294\nVALD: \t Epoch: 111 \t Loss: -0.011485263239592314\nVALD: \t Epoch: 111 \t Loss: -0.011440505273640156\nVALD: \t Epoch: 111 \t Loss: -0.011816770769655704\nVALD: \t Epoch: 111 \t Loss: -0.011185541149114834\n******************************\nEpoch: social-tag : 111\ntrain_loss -0.01128188671749559\nval_loss -0.011185541149114834\n{'min_val_epoch': 105, 'min_val_loss': -0.012140887562971376}\n******************************\nTRAIN: \t Epoch: 112 \t Loss: -0.010666619054973125\nTRAIN: \t Epoch: 112 \t Loss: -0.011193768121302128\nTRAIN: \t Epoch: 112 \t Loss: -0.011444014807542166\nTRAIN: \t Epoch: 112 \t Loss: -0.011568146292120218\nTRAIN: \t Epoch: 112 \t Loss: -0.0114954873919487\nTRAIN: \t Epoch: 112 \t Loss: -0.011358602748562893\nTRAIN: \t Epoch: 112 \t Loss: -0.011298165523580142\nTRAIN: \t Epoch: 112 \t Loss: -0.011368982028216124\nTRAIN: \t Epoch: 112 \t Loss: -0.011537012747592397\nTRAIN: \t Epoch: 112 \t Loss: -0.011494442727416754\nTRAIN: \t Epoch: 112 \t Loss: -0.011367729780348864\nTRAIN: \t Epoch: 112 \t Loss: -0.011157073080539703\nTRAIN: \t Epoch: 112 \t Loss: -0.011215748050465034\nTRAIN: \t Epoch: 112 \t Loss: -0.011306319984474353\nTRAIN: \t Epoch: 112 \t Loss: -0.011351095822950205\nTRAIN: \t Epoch: 112 \t Loss: -0.01142620260361582\nTRAIN: \t Epoch: 112 \t Loss: -0.011439109768937616\nTRAIN: \t Epoch: 112 \t Loss: -0.01137924851435754\nTRAIN: \t Epoch: 112 \t Loss: -0.01135911346462212\nTRAIN: \t Epoch: 112 \t Loss: -0.011377698834985494\nTRAIN: \t Epoch: 112 \t Loss: -0.011379014232476675\nVALD: \t Epoch: 112 \t Loss: -0.010762716643512249\nVALD: \t Epoch: 112 \t Loss: -0.011684937868267298\nVALD: \t Epoch: 112 \t Loss: -0.01148065272718668\nVALD: \t Epoch: 112 \t Loss: -0.012016369495540857\nVALD: \t Epoch: 112 \t Loss: -0.011419513855195467\n******************************\nEpoch: social-tag : 112\ntrain_loss -0.011379014232476675\nval_loss -0.011419513855195467\n{'min_val_epoch': 105, 'min_val_loss': -0.012140887562971376}\n******************************\nTRAIN: \t Epoch: 113 \t Loss: -0.01171173620969057\nTRAIN: \t Epoch: 113 \t Loss: -0.01208008173853159\nTRAIN: \t Epoch: 113 \t Loss: -0.012050341814756393\nTRAIN: \t Epoch: 113 \t Loss: -0.011616060743108392\nTRAIN: \t Epoch: 113 \t Loss: -0.011172873713076115\nTRAIN: \t Epoch: 113 \t Loss: -0.011114896895984808\nTRAIN: \t Epoch: 113 \t Loss: -0.01135319285094738\nTRAIN: \t Epoch: 113 \t Loss: -0.01137493026908487\nTRAIN: \t Epoch: 113 \t Loss: -0.011400050897565153\nTRAIN: \t Epoch: 113 \t Loss: -0.011464085057377815\nTRAIN: \t Epoch: 113 \t Loss: -0.011482784778557041\nTRAIN: \t Epoch: 113 \t Loss: -0.011294724109272162\nTRAIN: \t Epoch: 113 \t Loss: -0.011107605165587021\nTRAIN: \t Epoch: 113 \t Loss: -0.011123564360397202\nTRAIN: \t Epoch: 113 \t Loss: -0.01125315527121226\nTRAIN: \t Epoch: 113 \t Loss: -0.011277641460765153\nTRAIN: \t Epoch: 113 \t Loss: -0.01133403620299171\nTRAIN: \t Epoch: 113 \t Loss: -0.011415715898490615\nTRAIN: \t Epoch: 113 \t Loss: -0.011396067805196109\nTRAIN: \t Epoch: 113 \t Loss: -0.011289046984165907\nTRAIN: \t Epoch: 113 \t Loss: -0.011273282440056135\nVALD: \t Epoch: 113 \t Loss: -0.011273098178207874\nVALD: \t Epoch: 113 \t Loss: -0.011729761958122253\nVALD: \t Epoch: 113 \t Loss: -0.01136160579820474\nVALD: \t Epoch: 113 \t Loss: -0.011782497633248568\nVALD: \t Epoch: 113 \t Loss: -0.011144042878911116\n******************************\nEpoch: social-tag : 113\ntrain_loss -0.011273282440056135\nval_loss -0.011144042878911116\n{'min_val_epoch': 105, 'min_val_loss': -0.012140887562971376}\n******************************\nTRAIN: \t Epoch: 114 \t Loss: -0.011115191504359245\nTRAIN: \t Epoch: 114 \t Loss: -0.011878658551722765\nTRAIN: \t Epoch: 114 \t Loss: -0.01202246950318416\nTRAIN: \t Epoch: 114 \t Loss: -0.012002198258414865\nTRAIN: \t Epoch: 114 \t Loss: -0.011716673895716668\nTRAIN: \t Epoch: 114 \t Loss: -0.011553344782441854\nTRAIN: \t Epoch: 114 \t Loss: -0.011385422199964523\nTRAIN: \t Epoch: 114 \t Loss: -0.011441451380960643\nTRAIN: \t Epoch: 114 \t Loss: -0.011512819160189893\nTRAIN: \t Epoch: 114 \t Loss: -0.011586432345211506\nTRAIN: \t Epoch: 114 \t Loss: -0.011575463397259062\nTRAIN: \t Epoch: 114 \t Loss: -0.011461566279952725\nTRAIN: \t Epoch: 114 \t Loss: -0.011392582566119157\nTRAIN: \t Epoch: 114 \t Loss: -0.011410320975950785\nTRAIN: \t Epoch: 114 \t Loss: -0.011370229596892992\nTRAIN: \t Epoch: 114 \t Loss: -0.01132058136863634\nTRAIN: \t Epoch: 114 \t Loss: -0.011296095764812301\nTRAIN: \t Epoch: 114 \t Loss: -0.011300098254448838\nTRAIN: \t Epoch: 114 \t Loss: -0.011255826654010698\nTRAIN: \t Epoch: 114 \t Loss: -0.011267215339466929\nTRAIN: \t Epoch: 114 \t Loss: -0.011267432763471728\nVALD: \t Epoch: 114 \t Loss: -0.013536566868424416\nVALD: \t Epoch: 114 \t Loss: -0.013511316385120153\nVALD: \t Epoch: 114 \t Loss: -0.013224204691747824\nVALD: \t Epoch: 114 \t Loss: -0.013467237120494246\nVALD: \t Epoch: 114 \t Loss: -0.012693095418374128\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.10it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.4123755887658096  FDE: 0.6491169726373547\n**************************************************\n******************************\nEpoch: social-tag : 114\ntrain_loss -0.011267432763471728\nval_loss -0.012693095418374128\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 115 \t Loss: -0.012778106145560741\nTRAIN: \t Epoch: 115 \t Loss: -0.01295167813077569\nTRAIN: \t Epoch: 115 \t Loss: -0.011766777063409487\nTRAIN: \t Epoch: 115 \t Loss: -0.011373081244528294\nTRAIN: \t Epoch: 115 \t Loss: -0.011480257660150529\nTRAIN: \t Epoch: 115 \t Loss: -0.011476770509034395\nTRAIN: \t Epoch: 115 \t Loss: -0.011467146820255689\nTRAIN: \t Epoch: 115 \t Loss: -0.01156295696273446\nTRAIN: \t Epoch: 115 \t Loss: -0.011565490936239561\nTRAIN: \t Epoch: 115 \t Loss: -0.011441839952021837\nTRAIN: \t Epoch: 115 \t Loss: -0.01134545635432005\nTRAIN: \t Epoch: 115 \t Loss: -0.011365724339460334\nTRAIN: \t Epoch: 115 \t Loss: -0.011429683591883916\nTRAIN: \t Epoch: 115 \t Loss: -0.011430705697940928\nTRAIN: \t Epoch: 115 \t Loss: -0.011427349224686623\nTRAIN: \t Epoch: 115 \t Loss: -0.011421109549701214\nTRAIN: \t Epoch: 115 \t Loss: -0.011341069058022079\nTRAIN: \t Epoch: 115 \t Loss: -0.011288033694856696\nTRAIN: \t Epoch: 115 \t Loss: -0.01129825256372753\nTRAIN: \t Epoch: 115 \t Loss: -0.0113625168800354\nTRAIN: \t Epoch: 115 \t Loss: -0.01135802795175598\nVALD: \t Epoch: 115 \t Loss: -0.01055323239415884\nVALD: \t Epoch: 115 \t Loss: -0.010863491334021091\nVALD: \t Epoch: 115 \t Loss: -0.010096936486661434\nVALD: \t Epoch: 115 \t Loss: -0.010793916182592511\nVALD: \t Epoch: 115 \t Loss: -0.010381185010243536\n******************************\nEpoch: social-tag : 115\ntrain_loss -0.01135802795175598\nval_loss -0.010381185010243536\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 116 \t Loss: -0.012895057909190655\nTRAIN: \t Epoch: 116 \t Loss: -0.012672503013163805\nTRAIN: \t Epoch: 116 \t Loss: -0.01236094068735838\nTRAIN: \t Epoch: 116 \t Loss: -0.01223911833949387\nTRAIN: \t Epoch: 116 \t Loss: -0.011805763654410838\nTRAIN: \t Epoch: 116 \t Loss: -0.011627615739901861\nTRAIN: \t Epoch: 116 \t Loss: -0.011665863650185722\nTRAIN: \t Epoch: 116 \t Loss: -0.011503739864565432\nTRAIN: \t Epoch: 116 \t Loss: -0.011492573242220614\nTRAIN: \t Epoch: 116 \t Loss: -0.011610511224716902\nTRAIN: \t Epoch: 116 \t Loss: -0.011682244017720222\nTRAIN: \t Epoch: 116 \t Loss: -0.011613825336098671\nTRAIN: \t Epoch: 116 \t Loss: -0.011500207635645684\nTRAIN: \t Epoch: 116 \t Loss: -0.011505355725863151\nTRAIN: \t Epoch: 116 \t Loss: -0.011525750532746315\nTRAIN: \t Epoch: 116 \t Loss: -0.01154670346295461\nTRAIN: \t Epoch: 116 \t Loss: -0.011539342758410117\nTRAIN: \t Epoch: 116 \t Loss: -0.011488938859353462\nTRAIN: \t Epoch: 116 \t Loss: -0.011457872822096473\nTRAIN: \t Epoch: 116 \t Loss: -0.011475275736302137\nTRAIN: \t Epoch: 116 \t Loss: -0.011454355503010952\nVALD: \t Epoch: 116 \t Loss: -0.012913279235363007\nVALD: \t Epoch: 116 \t Loss: -0.013016294222325087\nVALD: \t Epoch: 116 \t Loss: -0.012682334830363592\nVALD: \t Epoch: 116 \t Loss: -0.01297851582057774\nVALD: \t Epoch: 116 \t Loss: -0.012232046484371314\n******************************\nEpoch: social-tag : 116\ntrain_loss -0.011454355503010952\nval_loss -0.012232046484371314\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 117 \t Loss: -0.01326935924589634\nTRAIN: \t Epoch: 117 \t Loss: -0.012980657629668713\nTRAIN: \t Epoch: 117 \t Loss: -0.012727907548348108\nTRAIN: \t Epoch: 117 \t Loss: -0.01252933288924396\nTRAIN: \t Epoch: 117 \t Loss: -0.012008016556501388\nTRAIN: \t Epoch: 117 \t Loss: -0.011660857436557611\nTRAIN: \t Epoch: 117 \t Loss: -0.011684747811939036\nTRAIN: \t Epoch: 117 \t Loss: -0.011828570044599473\nTRAIN: \t Epoch: 117 \t Loss: -0.011885277409520414\nTRAIN: \t Epoch: 117 \t Loss: -0.011731369700282813\nTRAIN: \t Epoch: 117 \t Loss: -0.011584295975891027\nTRAIN: \t Epoch: 117 \t Loss: -0.011607908876612782\nTRAIN: \t Epoch: 117 \t Loss: -0.011653061024844646\nTRAIN: \t Epoch: 117 \t Loss: -0.011597314716449805\nTRAIN: \t Epoch: 117 \t Loss: -0.01158669429520766\nTRAIN: \t Epoch: 117 \t Loss: -0.011606568179558963\nTRAIN: \t Epoch: 117 \t Loss: -0.011553030187154518\nTRAIN: \t Epoch: 117 \t Loss: -0.011514668973783651\nTRAIN: \t Epoch: 117 \t Loss: -0.011563178740049663\nTRAIN: \t Epoch: 117 \t Loss: -0.011589190736413003\nTRAIN: \t Epoch: 117 \t Loss: -0.011605615225680167\nVALD: \t Epoch: 117 \t Loss: -0.01323313731700182\nVALD: \t Epoch: 117 \t Loss: -0.013051401358097792\nVALD: \t Epoch: 117 \t Loss: -0.012720167326430479\nVALD: \t Epoch: 117 \t Loss: -0.013065416133031249\nVALD: \t Epoch: 117 \t Loss: -0.012303584631706398\n******************************\nEpoch: social-tag : 117\ntrain_loss -0.011605615225680167\nval_loss -0.012303584631706398\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 118 \t Loss: -0.012484095990657806\nTRAIN: \t Epoch: 118 \t Loss: -0.012725191656500101\nTRAIN: \t Epoch: 118 \t Loss: -0.012508374017973741\nTRAIN: \t Epoch: 118 \t Loss: -0.012128924252465367\nTRAIN: \t Epoch: 118 \t Loss: -0.01184421479701996\nTRAIN: \t Epoch: 118 \t Loss: -0.011669772366682688\nTRAIN: \t Epoch: 118 \t Loss: -0.011610618526382106\nTRAIN: \t Epoch: 118 \t Loss: -0.011591640417464077\nTRAIN: \t Epoch: 118 \t Loss: -0.01161065904630555\nTRAIN: \t Epoch: 118 \t Loss: -0.011589673813432455\nTRAIN: \t Epoch: 118 \t Loss: -0.011466003446416422\nTRAIN: \t Epoch: 118 \t Loss: -0.011468767343709866\nTRAIN: \t Epoch: 118 \t Loss: -0.01139189274265216\nTRAIN: \t Epoch: 118 \t Loss: -0.011485656523810965\nTRAIN: \t Epoch: 118 \t Loss: -0.011551526250938575\nTRAIN: \t Epoch: 118 \t Loss: -0.011496190913021564\nTRAIN: \t Epoch: 118 \t Loss: -0.01147672303897493\nTRAIN: \t Epoch: 118 \t Loss: -0.011547214817255735\nTRAIN: \t Epoch: 118 \t Loss: -0.011504753835891423\nTRAIN: \t Epoch: 118 \t Loss: -0.01147392694838345\nTRAIN: \t Epoch: 118 \t Loss: -0.011471062312699688\nVALD: \t Epoch: 118 \t Loss: -0.010848346166312695\nVALD: \t Epoch: 118 \t Loss: -0.010934801306575537\nVALD: \t Epoch: 118 \t Loss: -0.01035774809618791\nVALD: \t Epoch: 118 \t Loss: -0.010962872998788953\nVALD: \t Epoch: 118 \t Loss: -0.01061538212157294\n******************************\nEpoch: social-tag : 118\ntrain_loss -0.011471062312699688\nval_loss -0.01061538212157294\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 119 \t Loss: -0.010941767133772373\nTRAIN: \t Epoch: 119 \t Loss: -0.011357524432241917\nTRAIN: \t Epoch: 119 \t Loss: -0.011460785443584124\nTRAIN: \t Epoch: 119 \t Loss: -0.011400400195270777\nTRAIN: \t Epoch: 119 \t Loss: -0.011293129064142704\nTRAIN: \t Epoch: 119 \t Loss: -0.011249431408941746\nTRAIN: \t Epoch: 119 \t Loss: -0.011197593595300401\nTRAIN: \t Epoch: 119 \t Loss: -0.01123460242524743\nTRAIN: \t Epoch: 119 \t Loss: -0.011377352082894908\nTRAIN: \t Epoch: 119 \t Loss: -0.01134024215862155\nTRAIN: \t Epoch: 119 \t Loss: -0.011343172243372961\nTRAIN: \t Epoch: 119 \t Loss: -0.011344974239667257\nTRAIN: \t Epoch: 119 \t Loss: -0.011343230493366718\nTRAIN: \t Epoch: 119 \t Loss: -0.011338207205491406\nTRAIN: \t Epoch: 119 \t Loss: -0.011299888168772062\nTRAIN: \t Epoch: 119 \t Loss: -0.011403772747144103\nTRAIN: \t Epoch: 119 \t Loss: -0.011477876125889666\nTRAIN: \t Epoch: 119 \t Loss: -0.011398584271470705\nTRAIN: \t Epoch: 119 \t Loss: -0.011330719449018178\nTRAIN: \t Epoch: 119 \t Loss: -0.011325868032872676\nTRAIN: \t Epoch: 119 \t Loss: -0.011315521889681438\nVALD: \t Epoch: 119 \t Loss: -0.01308052521198988\nVALD: \t Epoch: 119 \t Loss: -0.012869099620729685\nVALD: \t Epoch: 119 \t Loss: -0.01253320494045814\nVALD: \t Epoch: 119 \t Loss: -0.01290710037574172\nVALD: \t Epoch: 119 \t Loss: -0.012215523136218773\n******************************\nEpoch: social-tag : 119\ntrain_loss -0.011315521889681438\nval_loss -0.012215523136218773\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 120 \t Loss: -0.012531444430351257\nTRAIN: \t Epoch: 120 \t Loss: -0.012240310665220022\nTRAIN: \t Epoch: 120 \t Loss: -0.012033960471550623\nTRAIN: \t Epoch: 120 \t Loss: -0.012000527465716004\nTRAIN: \t Epoch: 120 \t Loss: -0.011865740083158017\nTRAIN: \t Epoch: 120 \t Loss: -0.011625157669186592\nTRAIN: \t Epoch: 120 \t Loss: -0.011408402318400996\nTRAIN: \t Epoch: 120 \t Loss: -0.01139651716221124\nTRAIN: \t Epoch: 120 \t Loss: -0.011490896654625734\nTRAIN: \t Epoch: 120 \t Loss: -0.011603405885398387\nTRAIN: \t Epoch: 120 \t Loss: -0.011712274090810255\nTRAIN: \t Epoch: 120 \t Loss: -0.01172756776213646\nTRAIN: \t Epoch: 120 \t Loss: -0.011732588163935222\nTRAIN: \t Epoch: 120 \t Loss: -0.011581328298364366\nTRAIN: \t Epoch: 120 \t Loss: -0.01154747356971105\nTRAIN: \t Epoch: 120 \t Loss: -0.011566940404009074\nTRAIN: \t Epoch: 120 \t Loss: -0.01169041303150794\nTRAIN: \t Epoch: 120 \t Loss: -0.01172305964347389\nTRAIN: \t Epoch: 120 \t Loss: -0.01169391152890105\nTRAIN: \t Epoch: 120 \t Loss: -0.01165631553158164\nTRAIN: \t Epoch: 120 \t Loss: -0.011658635487350575\nVALD: \t Epoch: 120 \t Loss: -0.013370419852435589\nVALD: \t Epoch: 120 \t Loss: -0.013173299841582775\nVALD: \t Epoch: 120 \t Loss: -0.012946706575651964\nVALD: \t Epoch: 120 \t Loss: -0.013205706141889095\nVALD: \t Epoch: 120 \t Loss: -0.012450635049270162\n******************************\nEpoch: social-tag : 120\ntrain_loss -0.011658635487350575\nval_loss -0.012450635049270162\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 121 \t Loss: -0.012974618934094906\nTRAIN: \t Epoch: 121 \t Loss: -0.011860246304422617\nTRAIN: \t Epoch: 121 \t Loss: -0.01160946674644947\nTRAIN: \t Epoch: 121 \t Loss: -0.011813760502263904\nTRAIN: \t Epoch: 121 \t Loss: -0.01179313026368618\nTRAIN: \t Epoch: 121 \t Loss: -0.011749855087449154\nTRAIN: \t Epoch: 121 \t Loss: -0.011633035327707018\nTRAIN: \t Epoch: 121 \t Loss: -0.011567082023248076\nTRAIN: \t Epoch: 121 \t Loss: -0.011620157501763768\nTRAIN: \t Epoch: 121 \t Loss: -0.011676195356994868\nTRAIN: \t Epoch: 121 \t Loss: -0.0117131413214586\nTRAIN: \t Epoch: 121 \t Loss: -0.011659378884360194\nTRAIN: \t Epoch: 121 \t Loss: -0.011448039458348202\nTRAIN: \t Epoch: 121 \t Loss: -0.011379474241818701\nTRAIN: \t Epoch: 121 \t Loss: -0.011472924736638863\nTRAIN: \t Epoch: 121 \t Loss: -0.011515309917740524\nTRAIN: \t Epoch: 121 \t Loss: -0.011481753386118832\nTRAIN: \t Epoch: 121 \t Loss: -0.011498247221526172\nTRAIN: \t Epoch: 121 \t Loss: -0.011538638782344367\nTRAIN: \t Epoch: 121 \t Loss: -0.011537684127688408\nTRAIN: \t Epoch: 121 \t Loss: -0.01150541172959571\nVALD: \t Epoch: 121 \t Loss: -0.012207061983644962\nVALD: \t Epoch: 121 \t Loss: -0.011995033361017704\nVALD: \t Epoch: 121 \t Loss: -0.011851434906323751\nVALD: \t Epoch: 121 \t Loss: -0.012173511553555727\nVALD: \t Epoch: 121 \t Loss: -0.01160375929876994\n******************************\nEpoch: social-tag : 121\ntrain_loss -0.01150541172959571\nval_loss -0.01160375929876994\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 122 \t Loss: -0.01256493665277958\nTRAIN: \t Epoch: 122 \t Loss: -0.012295495253056288\nTRAIN: \t Epoch: 122 \t Loss: -0.011996431276202202\nTRAIN: \t Epoch: 122 \t Loss: -0.011359521420672536\nTRAIN: \t Epoch: 122 \t Loss: -0.011452962644398212\nTRAIN: \t Epoch: 122 \t Loss: -0.011563440474371115\nTRAIN: \t Epoch: 122 \t Loss: -0.011597305403224059\nTRAIN: \t Epoch: 122 \t Loss: -0.011696418281644583\nTRAIN: \t Epoch: 122 \t Loss: -0.011689678558872806\nTRAIN: \t Epoch: 122 \t Loss: -0.011502762977033853\nTRAIN: \t Epoch: 122 \t Loss: -0.011400385929102247\nTRAIN: \t Epoch: 122 \t Loss: -0.01146673372325798\nTRAIN: \t Epoch: 122 \t Loss: -0.01156341929275256\nTRAIN: \t Epoch: 122 \t Loss: -0.01154542840751154\nTRAIN: \t Epoch: 122 \t Loss: -0.011513291113078594\nTRAIN: \t Epoch: 122 \t Loss: -0.011468899145256728\nTRAIN: \t Epoch: 122 \t Loss: -0.011524776544641046\nTRAIN: \t Epoch: 122 \t Loss: -0.01157243394603332\nTRAIN: \t Epoch: 122 \t Loss: -0.011616244078858903\nTRAIN: \t Epoch: 122 \t Loss: -0.011560343345627189\nTRAIN: \t Epoch: 122 \t Loss: -0.011555310674787211\nVALD: \t Epoch: 122 \t Loss: -0.011485958471894264\nVALD: \t Epoch: 122 \t Loss: -0.011386335827410221\nVALD: \t Epoch: 122 \t Loss: -0.011500107434888681\nVALD: \t Epoch: 122 \t Loss: -0.01192272175103426\nVALD: \t Epoch: 122 \t Loss: -0.011247216215456166\n******************************\nEpoch: social-tag : 122\ntrain_loss -0.011555310674787211\nval_loss -0.011247216215456166\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 123 \t Loss: -0.011370500549674034\nTRAIN: \t Epoch: 123 \t Loss: -0.012139486614614725\nTRAIN: \t Epoch: 123 \t Loss: -0.012146536571284136\nTRAIN: \t Epoch: 123 \t Loss: -0.011799033964052796\nTRAIN: \t Epoch: 123 \t Loss: -0.01196185927838087\nTRAIN: \t Epoch: 123 \t Loss: -0.011967440601438284\nTRAIN: \t Epoch: 123 \t Loss: -0.011822135586823736\nTRAIN: \t Epoch: 123 \t Loss: -0.011762196430936456\nTRAIN: \t Epoch: 123 \t Loss: -0.011806531809270382\nTRAIN: \t Epoch: 123 \t Loss: -0.011802876461297273\nTRAIN: \t Epoch: 123 \t Loss: -0.011768591505560007\nTRAIN: \t Epoch: 123 \t Loss: -0.011850763810798526\nTRAIN: \t Epoch: 123 \t Loss: -0.011886467798971213\nTRAIN: \t Epoch: 123 \t Loss: -0.011755913629063539\nTRAIN: \t Epoch: 123 \t Loss: -0.011663198222716649\nTRAIN: \t Epoch: 123 \t Loss: -0.011682300304528326\nTRAIN: \t Epoch: 123 \t Loss: -0.011687754708177903\nTRAIN: \t Epoch: 123 \t Loss: -0.011659570969641209\nTRAIN: \t Epoch: 123 \t Loss: -0.011598766487287847\nTRAIN: \t Epoch: 123 \t Loss: -0.011533278226852416\nTRAIN: \t Epoch: 123 \t Loss: -0.0115339562997693\nVALD: \t Epoch: 123 \t Loss: -0.012914259918034077\nVALD: \t Epoch: 123 \t Loss: -0.012698195409029722\nVALD: \t Epoch: 123 \t Loss: -0.012651857609550158\nVALD: \t Epoch: 123 \t Loss: -0.012890680460259318\nVALD: \t Epoch: 123 \t Loss: -0.012261461328577113\n******************************\nEpoch: social-tag : 123\ntrain_loss -0.0115339562997693\nval_loss -0.012261461328577113\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 124 \t Loss: -0.012649363838136196\nTRAIN: \t Epoch: 124 \t Loss: -0.012606882955878973\nTRAIN: \t Epoch: 124 \t Loss: -0.01221333909779787\nTRAIN: \t Epoch: 124 \t Loss: -0.01162711507640779\nTRAIN: \t Epoch: 124 \t Loss: -0.011588967032730579\nTRAIN: \t Epoch: 124 \t Loss: -0.011619734577834606\nTRAIN: \t Epoch: 124 \t Loss: -0.011701769594635283\nTRAIN: \t Epoch: 124 \t Loss: -0.011786923976615071\nTRAIN: \t Epoch: 124 \t Loss: -0.011854379230903255\nTRAIN: \t Epoch: 124 \t Loss: -0.011810679268091918\nTRAIN: \t Epoch: 124 \t Loss: -0.011752100238068537\nTRAIN: \t Epoch: 124 \t Loss: -0.01172949691923956\nTRAIN: \t Epoch: 124 \t Loss: -0.011702285004922977\nTRAIN: \t Epoch: 124 \t Loss: -0.011712019863937582\nTRAIN: \t Epoch: 124 \t Loss: -0.011728170079489548\nTRAIN: \t Epoch: 124 \t Loss: -0.011754047649446875\nTRAIN: \t Epoch: 124 \t Loss: -0.01175377085147535\nTRAIN: \t Epoch: 124 \t Loss: -0.011664709024545219\nTRAIN: \t Epoch: 124 \t Loss: -0.011669894631363843\nTRAIN: \t Epoch: 124 \t Loss: -0.011628886219114065\nTRAIN: \t Epoch: 124 \t Loss: -0.011623339749062715\nVALD: \t Epoch: 124 \t Loss: -0.00646363478153944\nVALD: \t Epoch: 124 \t Loss: -0.008154179435223341\nVALD: \t Epoch: 124 \t Loss: -0.00703144430493315\nVALD: \t Epoch: 124 \t Loss: -0.008474004571326077\nVALD: \t Epoch: 124 \t Loss: -0.008519663231000424\n******************************\nEpoch: social-tag : 124\ntrain_loss -0.011623339749062715\nval_loss -0.008519663231000424\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 125 \t Loss: -0.012687207199633121\nTRAIN: \t Epoch: 125 \t Loss: -0.012847627513110638\nTRAIN: \t Epoch: 125 \t Loss: -0.012088034922877947\nTRAIN: \t Epoch: 125 \t Loss: -0.012186696054413915\nTRAIN: \t Epoch: 125 \t Loss: -0.012182291597127914\nTRAIN: \t Epoch: 125 \t Loss: -0.011956660387416681\nTRAIN: \t Epoch: 125 \t Loss: -0.01176848289157663\nTRAIN: \t Epoch: 125 \t Loss: -0.011848332826048136\nTRAIN: \t Epoch: 125 \t Loss: -0.01189466348538796\nTRAIN: \t Epoch: 125 \t Loss: -0.011966194864362479\nTRAIN: \t Epoch: 125 \t Loss: -0.011875517497008497\nTRAIN: \t Epoch: 125 \t Loss: -0.011689282720908523\nTRAIN: \t Epoch: 125 \t Loss: -0.011680850759148598\nTRAIN: \t Epoch: 125 \t Loss: -0.011814398890627282\nTRAIN: \t Epoch: 125 \t Loss: -0.01181339044123888\nTRAIN: \t Epoch: 125 \t Loss: -0.011666138772852719\nTRAIN: \t Epoch: 125 \t Loss: -0.01154784398043857\nTRAIN: \t Epoch: 125 \t Loss: -0.01160652418103483\nTRAIN: \t Epoch: 125 \t Loss: -0.01165996781109195\nTRAIN: \t Epoch: 125 \t Loss: -0.011685556313022972\nTRAIN: \t Epoch: 125 \t Loss: -0.011682990108348813\nVALD: \t Epoch: 125 \t Loss: -0.013476300053298473\nVALD: \t Epoch: 125 \t Loss: -0.013371773529797792\nVALD: \t Epoch: 125 \t Loss: -0.01313050091266632\nVALD: \t Epoch: 125 \t Loss: -0.013430350692942739\nVALD: \t Epoch: 125 \t Loss: -0.012642277899571662\n******************************\nEpoch: social-tag : 125\ntrain_loss -0.011682990108348813\nval_loss -0.012642277899571662\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 126 \t Loss: -0.012654241174459457\nTRAIN: \t Epoch: 126 \t Loss: -0.012779906392097473\nTRAIN: \t Epoch: 126 \t Loss: -0.012674703573187193\nTRAIN: \t Epoch: 126 \t Loss: -0.012590672355145216\nTRAIN: \t Epoch: 126 \t Loss: -0.0127023221924901\nTRAIN: \t Epoch: 126 \t Loss: -0.012711775799592337\nTRAIN: \t Epoch: 126 \t Loss: -0.012620385470134872\nTRAIN: \t Epoch: 126 \t Loss: -0.012390951160341501\nTRAIN: \t Epoch: 126 \t Loss: -0.012162549214230644\nTRAIN: \t Epoch: 126 \t Loss: -0.012145409733057022\nTRAIN: \t Epoch: 126 \t Loss: -0.012218421494418924\nTRAIN: \t Epoch: 126 \t Loss: -0.012140662098924318\nTRAIN: \t Epoch: 126 \t Loss: -0.011869226224147357\nTRAIN: \t Epoch: 126 \t Loss: -0.01178099594212004\nTRAIN: \t Epoch: 126 \t Loss: -0.011771773546934127\nTRAIN: \t Epoch: 126 \t Loss: -0.01184003020171076\nTRAIN: \t Epoch: 126 \t Loss: -0.01189823793795179\nTRAIN: \t Epoch: 126 \t Loss: -0.011885201920651726\nTRAIN: \t Epoch: 126 \t Loss: -0.01185706493101622\nTRAIN: \t Epoch: 126 \t Loss: -0.011903797881677747\nTRAIN: \t Epoch: 126 \t Loss: -0.011905037573234982\nVALD: \t Epoch: 126 \t Loss: -0.008565869182348251\nVALD: \t Epoch: 126 \t Loss: -0.010043622460216284\nVALD: \t Epoch: 126 \t Loss: -0.010003496582309404\nVALD: \t Epoch: 126 \t Loss: -0.010927311610430479\nVALD: \t Epoch: 126 \t Loss: -0.010485811701911275\n******************************\nEpoch: social-tag : 126\ntrain_loss -0.011905037573234982\nval_loss -0.010485811701911275\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 127 \t Loss: -0.013436286710202694\nTRAIN: \t Epoch: 127 \t Loss: -0.01319566136226058\nTRAIN: \t Epoch: 127 \t Loss: -0.012963742638627688\nTRAIN: \t Epoch: 127 \t Loss: -0.012400493491441011\nTRAIN: \t Epoch: 127 \t Loss: -0.01178901046514511\nTRAIN: \t Epoch: 127 \t Loss: -0.01152917044237256\nTRAIN: \t Epoch: 127 \t Loss: -0.011695055024964469\nTRAIN: \t Epoch: 127 \t Loss: -0.011775691178627312\nTRAIN: \t Epoch: 127 \t Loss: -0.011712626450591616\nTRAIN: \t Epoch: 127 \t Loss: -0.011569505091756582\nTRAIN: \t Epoch: 127 \t Loss: -0.011512731455943802\nTRAIN: \t Epoch: 127 \t Loss: -0.011512878894185027\nTRAIN: \t Epoch: 127 \t Loss: -0.011537528381897854\nTRAIN: \t Epoch: 127 \t Loss: -0.01155629162011402\nTRAIN: \t Epoch: 127 \t Loss: -0.011584830408294996\nTRAIN: \t Epoch: 127 \t Loss: -0.011562077153939754\nTRAIN: \t Epoch: 127 \t Loss: -0.011597837814513375\nTRAIN: \t Epoch: 127 \t Loss: -0.01152723737888866\nTRAIN: \t Epoch: 127 \t Loss: -0.011539444376371409\nTRAIN: \t Epoch: 127 \t Loss: -0.011606916040182113\nTRAIN: \t Epoch: 127 \t Loss: -0.01162656312183683\nVALD: \t Epoch: 127 \t Loss: -0.012728619389235973\nVALD: \t Epoch: 127 \t Loss: -0.012855805456638336\nVALD: \t Epoch: 127 \t Loss: -0.012521224406858286\nVALD: \t Epoch: 127 \t Loss: -0.012862595729529858\nVALD: \t Epoch: 127 \t Loss: -0.01218871544332704\n******************************\nEpoch: social-tag : 127\ntrain_loss -0.01162656312183683\nval_loss -0.01218871544332704\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 128 \t Loss: -0.01301339827477932\nTRAIN: \t Epoch: 128 \t Loss: -0.012986559886485338\nTRAIN: \t Epoch: 128 \t Loss: -0.012781814051171144\nTRAIN: \t Epoch: 128 \t Loss: -0.012300833594053984\nTRAIN: \t Epoch: 128 \t Loss: -0.012241363525390625\nTRAIN: \t Epoch: 128 \t Loss: -0.01213694317266345\nTRAIN: \t Epoch: 128 \t Loss: -0.012157539038785867\nTRAIN: \t Epoch: 128 \t Loss: -0.012162631377577782\nTRAIN: \t Epoch: 128 \t Loss: -0.012189894914627075\nTRAIN: \t Epoch: 128 \t Loss: -0.012048919405788183\nTRAIN: \t Epoch: 128 \t Loss: -0.011954591186209158\nTRAIN: \t Epoch: 128 \t Loss: -0.011976502525309721\nTRAIN: \t Epoch: 128 \t Loss: -0.012022551005849471\nTRAIN: \t Epoch: 128 \t Loss: -0.011951887314873082\nTRAIN: \t Epoch: 128 \t Loss: -0.011827694127957027\nTRAIN: \t Epoch: 128 \t Loss: -0.011777044564951211\nTRAIN: \t Epoch: 128 \t Loss: -0.011823848382953335\nTRAIN: \t Epoch: 128 \t Loss: -0.011792828742828634\nTRAIN: \t Epoch: 128 \t Loss: -0.011661132307429063\nTRAIN: \t Epoch: 128 \t Loss: -0.011555423028767109\nTRAIN: \t Epoch: 128 \t Loss: -0.01156234611385863\nVALD: \t Epoch: 128 \t Loss: -0.01240502204746008\nVALD: \t Epoch: 128 \t Loss: -0.012677059043198824\nVALD: \t Epoch: 128 \t Loss: -0.012461228296160698\nVALD: \t Epoch: 128 \t Loss: -0.012925234390422702\nVALD: \t Epoch: 128 \t Loss: -0.012103699351662407\n******************************\nEpoch: social-tag : 128\ntrain_loss -0.01156234611385863\nval_loss -0.012103699351662407\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 129 \t Loss: -0.0117907440289855\nTRAIN: \t Epoch: 129 \t Loss: -0.012183750979602337\nTRAIN: \t Epoch: 129 \t Loss: -0.012176102337737879\nTRAIN: \t Epoch: 129 \t Loss: -0.012099587358534336\nTRAIN: \t Epoch: 129 \t Loss: -0.01200324036180973\nTRAIN: \t Epoch: 129 \t Loss: -0.011898446828126907\nTRAIN: \t Epoch: 129 \t Loss: -0.011631630893264498\nTRAIN: \t Epoch: 129 \t Loss: -0.011657271883450449\nTRAIN: \t Epoch: 129 \t Loss: -0.011704209985004531\nTRAIN: \t Epoch: 129 \t Loss: -0.011780768819153308\nTRAIN: \t Epoch: 129 \t Loss: -0.011825128289108927\nTRAIN: \t Epoch: 129 \t Loss: -0.011850647938748201\nTRAIN: \t Epoch: 129 \t Loss: -0.011752213781269697\nTRAIN: \t Epoch: 129 \t Loss: -0.01163530901872686\nTRAIN: \t Epoch: 129 \t Loss: -0.011607247591018676\nTRAIN: \t Epoch: 129 \t Loss: -0.011631549685262144\nTRAIN: \t Epoch: 129 \t Loss: -0.011692644644747762\nTRAIN: \t Epoch: 129 \t Loss: -0.011744568434854349\nTRAIN: \t Epoch: 129 \t Loss: -0.011692461361618419\nTRAIN: \t Epoch: 129 \t Loss: -0.011692999256774783\nTRAIN: \t Epoch: 129 \t Loss: -0.011671527427439517\nVALD: \t Epoch: 129 \t Loss: -0.008711067959666252\nVALD: \t Epoch: 129 \t Loss: -0.010565119329839945\nVALD: \t Epoch: 129 \t Loss: -0.010156556343038877\nVALD: \t Epoch: 129 \t Loss: -0.011055751703679562\nVALD: \t Epoch: 129 \t Loss: -0.010651479980603892\n******************************\nEpoch: social-tag : 129\ntrain_loss -0.011671527427439517\nval_loss -0.010651479980603892\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 130 \t Loss: -0.012626580893993378\nTRAIN: \t Epoch: 130 \t Loss: -0.012531280983239412\nTRAIN: \t Epoch: 130 \t Loss: -0.0124573710684975\nTRAIN: \t Epoch: 130 \t Loss: -0.012173489900305867\nTRAIN: \t Epoch: 130 \t Loss: -0.01185524370521307\nTRAIN: \t Epoch: 130 \t Loss: -0.011832427699118853\nTRAIN: \t Epoch: 130 \t Loss: -0.011926295102706977\nTRAIN: \t Epoch: 130 \t Loss: -0.011884337989613414\nTRAIN: \t Epoch: 130 \t Loss: -0.01179471363623937\nTRAIN: \t Epoch: 130 \t Loss: -0.011870226357132196\nTRAIN: \t Epoch: 130 \t Loss: -0.01199180768294768\nTRAIN: \t Epoch: 130 \t Loss: -0.011902860598638654\nTRAIN: \t Epoch: 130 \t Loss: -0.011830813967837738\nTRAIN: \t Epoch: 130 \t Loss: -0.011768795483346497\nTRAIN: \t Epoch: 130 \t Loss: -0.011778395995497704\nTRAIN: \t Epoch: 130 \t Loss: -0.01184938446385786\nTRAIN: \t Epoch: 130 \t Loss: -0.011797789484262466\nTRAIN: \t Epoch: 130 \t Loss: -0.01175430758545796\nTRAIN: \t Epoch: 130 \t Loss: -0.011790222096207895\nTRAIN: \t Epoch: 130 \t Loss: -0.011806268198415636\nTRAIN: \t Epoch: 130 \t Loss: -0.011806975053472526\nVALD: \t Epoch: 130 \t Loss: -0.01215432584285736\nVALD: \t Epoch: 130 \t Loss: -0.012669175863265991\nVALD: \t Epoch: 130 \t Loss: -0.012518142660458883\nVALD: \t Epoch: 130 \t Loss: -0.01295605581253767\nVALD: \t Epoch: 130 \t Loss: -0.012246788315918902\n******************************\nEpoch: social-tag : 130\ntrain_loss -0.011806975053472526\nval_loss -0.012246788315918902\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 131 \t Loss: -0.012912633828818798\nTRAIN: \t Epoch: 131 \t Loss: -0.012783558573573828\nTRAIN: \t Epoch: 131 \t Loss: -0.0123258704940478\nTRAIN: \t Epoch: 131 \t Loss: -0.011695598950609565\nTRAIN: \t Epoch: 131 \t Loss: -0.011721559427678585\nTRAIN: \t Epoch: 131 \t Loss: -0.0117581474284331\nTRAIN: \t Epoch: 131 \t Loss: -0.011848302824156625\nTRAIN: \t Epoch: 131 \t Loss: -0.011736679705791175\nTRAIN: \t Epoch: 131 \t Loss: -0.01163756526592705\nTRAIN: \t Epoch: 131 \t Loss: -0.01153163556009531\nTRAIN: \t Epoch: 131 \t Loss: -0.01163567475635897\nTRAIN: \t Epoch: 131 \t Loss: -0.011692656514545282\nTRAIN: \t Epoch: 131 \t Loss: -0.011637887559258021\nTRAIN: \t Epoch: 131 \t Loss: -0.011653978111488479\nTRAIN: \t Epoch: 131 \t Loss: -0.011665855844815573\nTRAIN: \t Epoch: 131 \t Loss: -0.011684030061587691\nTRAIN: \t Epoch: 131 \t Loss: -0.011685996809426476\nTRAIN: \t Epoch: 131 \t Loss: -0.01168326697208815\nTRAIN: \t Epoch: 131 \t Loss: -0.011679536908080703\nTRAIN: \t Epoch: 131 \t Loss: -0.0116372374817729\nTRAIN: \t Epoch: 131 \t Loss: -0.011652471289417793\nVALD: \t Epoch: 131 \t Loss: -0.013136493042111397\nVALD: \t Epoch: 131 \t Loss: -0.012967515271157026\nVALD: \t Epoch: 131 \t Loss: -0.012476231902837753\nVALD: \t Epoch: 131 \t Loss: -0.0127372604329139\nVALD: \t Epoch: 131 \t Loss: -0.012063236340232517\n******************************\nEpoch: social-tag : 131\ntrain_loss -0.011652471289417793\nval_loss -0.012063236340232517\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 132 \t Loss: -0.012811084277927876\nTRAIN: \t Epoch: 132 \t Loss: -0.012710782699286938\nTRAIN: \t Epoch: 132 \t Loss: -0.012571096420288086\nTRAIN: \t Epoch: 132 \t Loss: -0.01230239518918097\nTRAIN: \t Epoch: 132 \t Loss: -0.012149962969124318\nTRAIN: \t Epoch: 132 \t Loss: -0.011940235582490763\nTRAIN: \t Epoch: 132 \t Loss: -0.011949773079582624\nTRAIN: \t Epoch: 132 \t Loss: -0.011898466851562262\nTRAIN: \t Epoch: 132 \t Loss: -0.011835142970085144\nTRAIN: \t Epoch: 132 \t Loss: -0.011752416659146548\nTRAIN: \t Epoch: 132 \t Loss: -0.011742431755093012\nTRAIN: \t Epoch: 132 \t Loss: -0.011747841878483692\nTRAIN: \t Epoch: 132 \t Loss: -0.011783591686533047\nTRAIN: \t Epoch: 132 \t Loss: -0.011746922042220831\nTRAIN: \t Epoch: 132 \t Loss: -0.011673367271820704\nTRAIN: \t Epoch: 132 \t Loss: -0.011662815522868186\nTRAIN: \t Epoch: 132 \t Loss: -0.011744582039468428\nTRAIN: \t Epoch: 132 \t Loss: -0.011763262769414319\nTRAIN: \t Epoch: 132 \t Loss: -0.011754717432746762\nTRAIN: \t Epoch: 132 \t Loss: -0.011689729476347565\nTRAIN: \t Epoch: 132 \t Loss: -0.011687098267727295\nVALD: \t Epoch: 132 \t Loss: -0.011849605478346348\nVALD: \t Epoch: 132 \t Loss: -0.011663861572742462\nVALD: \t Epoch: 132 \t Loss: -0.011014126551647982\nVALD: \t Epoch: 132 \t Loss: -0.011434539454057813\nVALD: \t Epoch: 132 \t Loss: -0.010911622677064364\n******************************\nEpoch: social-tag : 132\ntrain_loss -0.011687098267727295\nval_loss -0.010911622677064364\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 133 \t Loss: -0.011890721507370472\nTRAIN: \t Epoch: 133 \t Loss: -0.012242747470736504\nTRAIN: \t Epoch: 133 \t Loss: -0.012629628802339235\nTRAIN: \t Epoch: 133 \t Loss: -0.012052776524797082\nTRAIN: \t Epoch: 133 \t Loss: -0.011617857404053212\nTRAIN: \t Epoch: 133 \t Loss: -0.011697559927900633\nTRAIN: \t Epoch: 133 \t Loss: -0.01181980116026742\nTRAIN: \t Epoch: 133 \t Loss: -0.011823133914731443\nTRAIN: \t Epoch: 133 \t Loss: -0.011798996788760027\nTRAIN: \t Epoch: 133 \t Loss: -0.011818198394030333\nTRAIN: \t Epoch: 133 \t Loss: -0.011875917711718515\nTRAIN: \t Epoch: 133 \t Loss: -0.011882570416977009\nTRAIN: \t Epoch: 133 \t Loss: -0.01189006521151616\nTRAIN: \t Epoch: 133 \t Loss: -0.011873960295425994\nTRAIN: \t Epoch: 133 \t Loss: -0.011862448354562124\nTRAIN: \t Epoch: 133 \t Loss: -0.011822747008409351\nTRAIN: \t Epoch: 133 \t Loss: -0.01181637353318579\nTRAIN: \t Epoch: 133 \t Loss: -0.011797532335751586\nTRAIN: \t Epoch: 133 \t Loss: -0.011786631553580887\nTRAIN: \t Epoch: 133 \t Loss: -0.011765842139720917\nTRAIN: \t Epoch: 133 \t Loss: -0.011776018092148838\nVALD: \t Epoch: 133 \t Loss: -0.010055676102638245\nVALD: \t Epoch: 133 \t Loss: -0.011451276950538158\nVALD: \t Epoch: 133 \t Loss: -0.010639379421869913\nVALD: \t Epoch: 133 \t Loss: -0.011355253402143717\nVALD: \t Epoch: 133 \t Loss: -0.011016575610580075\n******************************\nEpoch: social-tag : 133\ntrain_loss -0.011776018092148838\nval_loss -0.011016575610580075\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 134 \t Loss: -0.013402733020484447\nTRAIN: \t Epoch: 134 \t Loss: -0.013332333415746689\nTRAIN: \t Epoch: 134 \t Loss: -0.01291165966540575\nTRAIN: \t Epoch: 134 \t Loss: -0.012366311624646187\nTRAIN: \t Epoch: 134 \t Loss: -0.0122057743370533\nTRAIN: \t Epoch: 134 \t Loss: -0.012276988476514816\nTRAIN: \t Epoch: 134 \t Loss: -0.012239234788077218\nTRAIN: \t Epoch: 134 \t Loss: -0.012226103222928941\nTRAIN: \t Epoch: 134 \t Loss: -0.012346947048273351\nTRAIN: \t Epoch: 134 \t Loss: -0.012248311005532742\nTRAIN: \t Epoch: 134 \t Loss: -0.012011339887976646\nTRAIN: \t Epoch: 134 \t Loss: -0.011908507362628976\nTRAIN: \t Epoch: 134 \t Loss: -0.011972911942463655\nTRAIN: \t Epoch: 134 \t Loss: -0.011990906005459172\nTRAIN: \t Epoch: 134 \t Loss: -0.012062370404601097\nTRAIN: \t Epoch: 134 \t Loss: -0.012009325088001788\nTRAIN: \t Epoch: 134 \t Loss: -0.011995379703448099\nTRAIN: \t Epoch: 134 \t Loss: -0.012007735203951597\nTRAIN: \t Epoch: 134 \t Loss: -0.012009858545896254\nTRAIN: \t Epoch: 134 \t Loss: -0.012021661223843693\nTRAIN: \t Epoch: 134 \t Loss: -0.01203091664321623\nVALD: \t Epoch: 134 \t Loss: -0.011851847171783447\nVALD: \t Epoch: 134 \t Loss: -0.012281775940209627\nVALD: \t Epoch: 134 \t Loss: -0.011724054502944151\nVALD: \t Epoch: 134 \t Loss: -0.012241469463333488\nVALD: \t Epoch: 134 \t Loss: -0.011685308626884423\n******************************\nEpoch: social-tag : 134\ntrain_loss -0.01203091664321623\nval_loss -0.011685308626884423\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 135 \t Loss: -0.012655064463615417\nTRAIN: \t Epoch: 135 \t Loss: -0.012118224054574966\nTRAIN: \t Epoch: 135 \t Loss: -0.01185425433019797\nTRAIN: \t Epoch: 135 \t Loss: -0.012033852515742183\nTRAIN: \t Epoch: 135 \t Loss: -0.012054717354476451\nTRAIN: \t Epoch: 135 \t Loss: -0.011923886369913816\nTRAIN: \t Epoch: 135 \t Loss: -0.011909302989287036\nTRAIN: \t Epoch: 135 \t Loss: -0.011937106028199196\nTRAIN: \t Epoch: 135 \t Loss: -0.011965499673452642\nTRAIN: \t Epoch: 135 \t Loss: -0.01190732503309846\nTRAIN: \t Epoch: 135 \t Loss: -0.01185286823998798\nTRAIN: \t Epoch: 135 \t Loss: -0.011774657138933739\nTRAIN: \t Epoch: 135 \t Loss: -0.011818927211257128\nTRAIN: \t Epoch: 135 \t Loss: -0.01188714915354337\nTRAIN: \t Epoch: 135 \t Loss: -0.011869106441736221\nTRAIN: \t Epoch: 135 \t Loss: -0.011806584661826491\nTRAIN: \t Epoch: 135 \t Loss: -0.011800126501304261\nTRAIN: \t Epoch: 135 \t Loss: -0.011857690703537729\nTRAIN: \t Epoch: 135 \t Loss: -0.011792239635006377\nTRAIN: \t Epoch: 135 \t Loss: -0.011752784112468362\nTRAIN: \t Epoch: 135 \t Loss: -0.011746512012198602\nVALD: \t Epoch: 135 \t Loss: -0.009421050548553467\nVALD: \t Epoch: 135 \t Loss: -0.010362464468926191\nVALD: \t Epoch: 135 \t Loss: -0.008719353626171747\nVALD: \t Epoch: 135 \t Loss: -0.009663315257057548\nVALD: \t Epoch: 135 \t Loss: -0.009524784034385005\n******************************\nEpoch: social-tag : 135\ntrain_loss -0.011746512012198602\nval_loss -0.009524784034385005\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 136 \t Loss: -0.01284746266901493\nTRAIN: \t Epoch: 136 \t Loss: -0.012598355300724506\nTRAIN: \t Epoch: 136 \t Loss: -0.012637161649763584\nTRAIN: \t Epoch: 136 \t Loss: -0.012791824759915471\nTRAIN: \t Epoch: 136 \t Loss: -0.012496163323521613\nTRAIN: \t Epoch: 136 \t Loss: -0.012125583831220865\nTRAIN: \t Epoch: 136 \t Loss: -0.012017200168754374\nTRAIN: \t Epoch: 136 \t Loss: -0.012037927517667413\nTRAIN: \t Epoch: 136 \t Loss: -0.012074888890816106\nTRAIN: \t Epoch: 136 \t Loss: -0.012106556911021471\nTRAIN: \t Epoch: 136 \t Loss: -0.012084237807853655\nTRAIN: \t Epoch: 136 \t Loss: -0.011952105599145094\nTRAIN: \t Epoch: 136 \t Loss: -0.011894246706595788\nTRAIN: \t Epoch: 136 \t Loss: -0.011926883499005012\nTRAIN: \t Epoch: 136 \t Loss: -0.011991404617826143\nTRAIN: \t Epoch: 136 \t Loss: -0.01202456490136683\nTRAIN: \t Epoch: 136 \t Loss: -0.01199634953895036\nTRAIN: \t Epoch: 136 \t Loss: -0.011952671325869031\nTRAIN: \t Epoch: 136 \t Loss: -0.011979725751045504\nTRAIN: \t Epoch: 136 \t Loss: -0.011945870239287615\nTRAIN: \t Epoch: 136 \t Loss: -0.0119327680597695\nVALD: \t Epoch: 136 \t Loss: -0.012578017078340054\nVALD: \t Epoch: 136 \t Loss: -0.012604698538780212\nVALD: \t Epoch: 136 \t Loss: -0.012322147687276205\nVALD: \t Epoch: 136 \t Loss: -0.012606461998075247\nVALD: \t Epoch: 136 \t Loss: -0.011954568146507522\n******************************\nEpoch: social-tag : 136\ntrain_loss -0.0119327680597695\nval_loss -0.011954568146507522\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 137 \t Loss: -0.012965619564056396\nTRAIN: \t Epoch: 137 \t Loss: -0.013051843736320734\nTRAIN: \t Epoch: 137 \t Loss: -0.01260661364843448\nTRAIN: \t Epoch: 137 \t Loss: -0.012482087593525648\nTRAIN: \t Epoch: 137 \t Loss: -0.012232613563537598\nTRAIN: \t Epoch: 137 \t Loss: -0.012113346407810846\nTRAIN: \t Epoch: 137 \t Loss: -0.012033381499350071\nTRAIN: \t Epoch: 137 \t Loss: -0.01206522062420845\nTRAIN: \t Epoch: 137 \t Loss: -0.012095786010225614\nTRAIN: \t Epoch: 137 \t Loss: -0.012087682634592057\nTRAIN: \t Epoch: 137 \t Loss: -0.011978409676389261\nTRAIN: \t Epoch: 137 \t Loss: -0.011832052453731498\nTRAIN: \t Epoch: 137 \t Loss: -0.011914241199310008\nTRAIN: \t Epoch: 137 \t Loss: -0.012048021624130862\nTRAIN: \t Epoch: 137 \t Loss: -0.012009307369589805\nTRAIN: \t Epoch: 137 \t Loss: -0.011978529626503587\nTRAIN: \t Epoch: 137 \t Loss: -0.012047952972352505\nTRAIN: \t Epoch: 137 \t Loss: -0.012087299643705288\nTRAIN: \t Epoch: 137 \t Loss: -0.011989409447108445\nTRAIN: \t Epoch: 137 \t Loss: -0.011908874800428747\nTRAIN: \t Epoch: 137 \t Loss: -0.01190609254557625\nVALD: \t Epoch: 137 \t Loss: -0.012580616399645805\nVALD: \t Epoch: 137 \t Loss: -0.013059618882834911\nVALD: \t Epoch: 137 \t Loss: -0.013025470077991486\nVALD: \t Epoch: 137 \t Loss: -0.013320413185283542\nVALD: \t Epoch: 137 \t Loss: -0.012634395593222405\n******************************\nEpoch: social-tag : 137\ntrain_loss -0.01190609254557625\nval_loss -0.012634395593222405\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 138 \t Loss: -0.011913346126675606\nTRAIN: \t Epoch: 138 \t Loss: -0.012103568762540817\nTRAIN: \t Epoch: 138 \t Loss: -0.011928517992297808\nTRAIN: \t Epoch: 138 \t Loss: -0.011566936504095793\nTRAIN: \t Epoch: 138 \t Loss: -0.011700415425002575\nTRAIN: \t Epoch: 138 \t Loss: -0.011551052487144867\nTRAIN: \t Epoch: 138 \t Loss: -0.01163256966641971\nTRAIN: \t Epoch: 138 \t Loss: -0.011654034606181085\nTRAIN: \t Epoch: 138 \t Loss: -0.011637223574022451\nTRAIN: \t Epoch: 138 \t Loss: -0.011759250052273273\nTRAIN: \t Epoch: 138 \t Loss: -0.011758354238488457\nTRAIN: \t Epoch: 138 \t Loss: -0.01164927170611918\nTRAIN: \t Epoch: 138 \t Loss: -0.01165590535562772\nTRAIN: \t Epoch: 138 \t Loss: -0.01173647859000734\nTRAIN: \t Epoch: 138 \t Loss: -0.011765270493924618\nTRAIN: \t Epoch: 138 \t Loss: -0.011823559529148042\nTRAIN: \t Epoch: 138 \t Loss: -0.011830390047501116\nTRAIN: \t Epoch: 138 \t Loss: -0.011797390774720244\nTRAIN: \t Epoch: 138 \t Loss: -0.011806464803061988\nTRAIN: \t Epoch: 138 \t Loss: -0.011846690438687801\nTRAIN: \t Epoch: 138 \t Loss: -0.011856023242313676\nVALD: \t Epoch: 138 \t Loss: -0.006269619334489107\nVALD: \t Epoch: 138 \t Loss: -0.009209776995703578\nVALD: \t Epoch: 138 \t Loss: -0.008305033513655266\nVALD: \t Epoch: 138 \t Loss: -0.009690524428151548\nVALD: \t Epoch: 138 \t Loss: -0.009442520122405219\n******************************\nEpoch: social-tag : 138\ntrain_loss -0.011856023242313676\nval_loss -0.009442520122405219\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 139 \t Loss: -0.013811532407999039\nTRAIN: \t Epoch: 139 \t Loss: -0.01319085294380784\nTRAIN: \t Epoch: 139 \t Loss: -0.013218093353013197\nTRAIN: \t Epoch: 139 \t Loss: -0.012396929087117314\nTRAIN: \t Epoch: 139 \t Loss: -0.011791232228279113\nTRAIN: \t Epoch: 139 \t Loss: -0.011838384593526522\nTRAIN: \t Epoch: 139 \t Loss: -0.01198287627526692\nTRAIN: \t Epoch: 139 \t Loss: -0.01211690902709961\nTRAIN: \t Epoch: 139 \t Loss: -0.012173903692099784\nTRAIN: \t Epoch: 139 \t Loss: -0.012119133584201336\nTRAIN: \t Epoch: 139 \t Loss: -0.01186799926852638\nTRAIN: \t Epoch: 139 \t Loss: -0.011865106566498676\nTRAIN: \t Epoch: 139 \t Loss: -0.011933297062149415\nTRAIN: \t Epoch: 139 \t Loss: -0.011982040546302284\nTRAIN: \t Epoch: 139 \t Loss: -0.012008302720884483\nTRAIN: \t Epoch: 139 \t Loss: -0.012066025810781866\nTRAIN: \t Epoch: 139 \t Loss: -0.01203776742605602\nTRAIN: \t Epoch: 139 \t Loss: -0.011994505663298898\nTRAIN: \t Epoch: 139 \t Loss: -0.012004312765049307\nTRAIN: \t Epoch: 139 \t Loss: -0.011987892584875226\nTRAIN: \t Epoch: 139 \t Loss: -0.01197544721428394\nVALD: \t Epoch: 139 \t Loss: -0.012995736673474312\nVALD: \t Epoch: 139 \t Loss: -0.0128398765809834\nVALD: \t Epoch: 139 \t Loss: -0.012556609076758226\nVALD: \t Epoch: 139 \t Loss: -0.01287115877494216\nVALD: \t Epoch: 139 \t Loss: -0.012216852770911323\n******************************\nEpoch: social-tag : 139\ntrain_loss -0.01197544721428394\nval_loss -0.012216852770911323\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 140 \t Loss: -0.012892249971628189\nTRAIN: \t Epoch: 140 \t Loss: -0.012748622335493565\nTRAIN: \t Epoch: 140 \t Loss: -0.01263076594720284\nTRAIN: \t Epoch: 140 \t Loss: -0.012394501827657223\nTRAIN: \t Epoch: 140 \t Loss: -0.011992951668798924\nTRAIN: \t Epoch: 140 \t Loss: -0.0119787254370749\nTRAIN: \t Epoch: 140 \t Loss: -0.012003886912550246\nTRAIN: \t Epoch: 140 \t Loss: -0.012038218788802624\nTRAIN: \t Epoch: 140 \t Loss: -0.012035781310664283\nTRAIN: \t Epoch: 140 \t Loss: -0.011974571458995342\nTRAIN: \t Epoch: 140 \t Loss: -0.011925519444048405\nTRAIN: \t Epoch: 140 \t Loss: -0.012067395184809962\nTRAIN: \t Epoch: 140 \t Loss: -0.012071274722424837\nTRAIN: \t Epoch: 140 \t Loss: -0.011994748642402036\nTRAIN: \t Epoch: 140 \t Loss: -0.01201479850957791\nTRAIN: \t Epoch: 140 \t Loss: -0.012038371176458895\nTRAIN: \t Epoch: 140 \t Loss: -0.011985261267160667\nTRAIN: \t Epoch: 140 \t Loss: -0.011984020937234163\nTRAIN: \t Epoch: 140 \t Loss: -0.011980079359521991\nTRAIN: \t Epoch: 140 \t Loss: -0.01199846570380032\nTRAIN: \t Epoch: 140 \t Loss: -0.012008440970179661\nVALD: \t Epoch: 140 \t Loss: -0.011681405827403069\nVALD: \t Epoch: 140 \t Loss: -0.01215709513053298\nVALD: \t Epoch: 140 \t Loss: -0.011715369919935862\nVALD: \t Epoch: 140 \t Loss: -0.012369044125080109\nVALD: \t Epoch: 140 \t Loss: -0.011714923017074904\n******************************\nEpoch: social-tag : 140\ntrain_loss -0.012008440970179661\nval_loss -0.011714923017074904\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 141 \t Loss: -0.012427289970219135\nTRAIN: \t Epoch: 141 \t Loss: -0.012911948375403881\nTRAIN: \t Epoch: 141 \t Loss: -0.013136035452286402\nTRAIN: \t Epoch: 141 \t Loss: -0.013111773179844022\nTRAIN: \t Epoch: 141 \t Loss: -0.012752590514719487\nTRAIN: \t Epoch: 141 \t Loss: -0.012426847281555334\nTRAIN: \t Epoch: 141 \t Loss: -0.012480326129921846\nTRAIN: \t Epoch: 141 \t Loss: -0.012427754467353225\nTRAIN: \t Epoch: 141 \t Loss: -0.012346499599516392\nTRAIN: \t Epoch: 141 \t Loss: -0.012330530025064946\nTRAIN: \t Epoch: 141 \t Loss: -0.01230287170884284\nTRAIN: \t Epoch: 141 \t Loss: -0.012231567641720176\nTRAIN: \t Epoch: 141 \t Loss: -0.012100689542981295\nTRAIN: \t Epoch: 141 \t Loss: -0.012030660906540496\nTRAIN: \t Epoch: 141 \t Loss: -0.01215339923898379\nTRAIN: \t Epoch: 141 \t Loss: -0.012195954914204776\nTRAIN: \t Epoch: 141 \t Loss: -0.012152691152604186\nTRAIN: \t Epoch: 141 \t Loss: -0.012116213173915943\nTRAIN: \t Epoch: 141 \t Loss: -0.01211075033796461\nTRAIN: \t Epoch: 141 \t Loss: -0.012019135477021336\nTRAIN: \t Epoch: 141 \t Loss: -0.012028032837689795\nVALD: \t Epoch: 141 \t Loss: -0.012843836098909378\nVALD: \t Epoch: 141 \t Loss: -0.012615230865776539\nVALD: \t Epoch: 141 \t Loss: -0.012352578341960907\nVALD: \t Epoch: 141 \t Loss: -0.012755591655150056\nVALD: \t Epoch: 141 \t Loss: -0.011976252047528007\n******************************\nEpoch: social-tag : 141\ntrain_loss -0.012028032837689795\nval_loss -0.011976252047528007\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 142 \t Loss: -0.012771418318152428\nTRAIN: \t Epoch: 142 \t Loss: -0.012734667863696814\nTRAIN: \t Epoch: 142 \t Loss: -0.012802252856393656\nTRAIN: \t Epoch: 142 \t Loss: -0.012662646127864718\nTRAIN: \t Epoch: 142 \t Loss: -0.012312101759016513\nTRAIN: \t Epoch: 142 \t Loss: -0.012244191486388445\nTRAIN: \t Epoch: 142 \t Loss: -0.01235450551445995\nTRAIN: \t Epoch: 142 \t Loss: -0.01214567048009485\nTRAIN: \t Epoch: 142 \t Loss: -0.011854356568720605\nTRAIN: \t Epoch: 142 \t Loss: -0.011874621734023094\nTRAIN: \t Epoch: 142 \t Loss: -0.011889267797497187\nTRAIN: \t Epoch: 142 \t Loss: -0.011943354814623794\nTRAIN: \t Epoch: 142 \t Loss: -0.012011039930467423\nTRAIN: \t Epoch: 142 \t Loss: -0.012116707528808288\nTRAIN: \t Epoch: 142 \t Loss: -0.01207510307431221\nTRAIN: \t Epoch: 142 \t Loss: -0.011986639874521643\nTRAIN: \t Epoch: 142 \t Loss: -0.012043352348401266\nTRAIN: \t Epoch: 142 \t Loss: -0.012071183675693141\nTRAIN: \t Epoch: 142 \t Loss: -0.01208885842444081\nTRAIN: \t Epoch: 142 \t Loss: -0.012118611438199877\nTRAIN: \t Epoch: 142 \t Loss: -0.012142904143013583\nVALD: \t Epoch: 142 \t Loss: -0.01040507759898901\nVALD: \t Epoch: 142 \t Loss: -0.011410294100642204\nVALD: \t Epoch: 142 \t Loss: -0.010139665411164364\nVALD: \t Epoch: 142 \t Loss: -0.01095756294671446\nVALD: \t Epoch: 142 \t Loss: -0.010525878212878094\n******************************\nEpoch: social-tag : 142\ntrain_loss -0.012142904143013583\nval_loss -0.010525878212878094\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 143 \t Loss: -0.012949895113706589\nTRAIN: \t Epoch: 143 \t Loss: -0.01299932086840272\nTRAIN: \t Epoch: 143 \t Loss: -0.013210411183536053\nTRAIN: \t Epoch: 143 \t Loss: -0.013017087243497372\nTRAIN: \t Epoch: 143 \t Loss: -0.012727567739784718\nTRAIN: \t Epoch: 143 \t Loss: -0.01233348793660601\nTRAIN: \t Epoch: 143 \t Loss: -0.012129821548504489\nTRAIN: \t Epoch: 143 \t Loss: -0.012195008457638323\nTRAIN: \t Epoch: 143 \t Loss: -0.01228742560164796\nTRAIN: \t Epoch: 143 \t Loss: -0.0122331403195858\nTRAIN: \t Epoch: 143 \t Loss: -0.012147561223669485\nTRAIN: \t Epoch: 143 \t Loss: -0.012105499549458424\nTRAIN: \t Epoch: 143 \t Loss: -0.012180207655406915\nTRAIN: \t Epoch: 143 \t Loss: -0.012187682424804993\nTRAIN: \t Epoch: 143 \t Loss: -0.012132687369982402\nTRAIN: \t Epoch: 143 \t Loss: -0.012104988389182836\nTRAIN: \t Epoch: 143 \t Loss: -0.012188107542255345\nTRAIN: \t Epoch: 143 \t Loss: -0.012173063069995906\nTRAIN: \t Epoch: 143 \t Loss: -0.012137115148729399\nTRAIN: \t Epoch: 143 \t Loss: -0.012147343996912242\nTRAIN: \t Epoch: 143 \t Loss: -0.012144446510853175\nVALD: \t Epoch: 143 \t Loss: -0.012482927180826664\nVALD: \t Epoch: 143 \t Loss: -0.012446608860045671\nVALD: \t Epoch: 143 \t Loss: -0.012158108254273733\nVALD: \t Epoch: 143 \t Loss: -0.012489423854276538\nVALD: \t Epoch: 143 \t Loss: -0.01198213188736527\n******************************\nEpoch: social-tag : 143\ntrain_loss -0.012144446510853175\nval_loss -0.01198213188736527\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 144 \t Loss: -0.012119350023567677\nTRAIN: \t Epoch: 144 \t Loss: -0.012787488754838705\nTRAIN: \t Epoch: 144 \t Loss: -0.012512481150527796\nTRAIN: \t Epoch: 144 \t Loss: -0.011753739323467016\nTRAIN: \t Epoch: 144 \t Loss: -0.01157257929444313\nTRAIN: \t Epoch: 144 \t Loss: -0.011839357204735279\nTRAIN: \t Epoch: 144 \t Loss: -0.011977946385741234\nTRAIN: \t Epoch: 144 \t Loss: -0.011920209159143269\nTRAIN: \t Epoch: 144 \t Loss: -0.011918375061617957\nTRAIN: \t Epoch: 144 \t Loss: -0.011781040392816067\nTRAIN: \t Epoch: 144 \t Loss: -0.011748924851417542\nTRAIN: \t Epoch: 144 \t Loss: -0.011766612296923995\nTRAIN: \t Epoch: 144 \t Loss: -0.011729357692484673\nTRAIN: \t Epoch: 144 \t Loss: -0.01172840255977852\nTRAIN: \t Epoch: 144 \t Loss: -0.01177763274560372\nTRAIN: \t Epoch: 144 \t Loss: -0.011839993996545672\nTRAIN: \t Epoch: 144 \t Loss: -0.011820727411438437\nTRAIN: \t Epoch: 144 \t Loss: -0.011801662233968576\nTRAIN: \t Epoch: 144 \t Loss: -0.011796892914724978\nTRAIN: \t Epoch: 144 \t Loss: -0.011847120756283403\nTRAIN: \t Epoch: 144 \t Loss: -0.011851752387990194\nVALD: \t Epoch: 144 \t Loss: -0.013468433171510696\nVALD: \t Epoch: 144 \t Loss: -0.013273041229695082\nVALD: \t Epoch: 144 \t Loss: -0.012939562710622946\nVALD: \t Epoch: 144 \t Loss: -0.013358703581616282\nVALD: \t Epoch: 144 \t Loss: -0.012638954724666578\n******************************\nEpoch: social-tag : 144\ntrain_loss -0.011851752387990194\nval_loss -0.012638954724666578\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 145 \t Loss: -0.013944897800683975\nTRAIN: \t Epoch: 145 \t Loss: -0.01361312298104167\nTRAIN: \t Epoch: 145 \t Loss: -0.013093400125702223\nTRAIN: \t Epoch: 145 \t Loss: -0.012950707226991653\nTRAIN: \t Epoch: 145 \t Loss: -0.012679030373692512\nTRAIN: \t Epoch: 145 \t Loss: -0.012401005097975334\nTRAIN: \t Epoch: 145 \t Loss: -0.012372286724192756\nTRAIN: \t Epoch: 145 \t Loss: -0.012474688002839684\nTRAIN: \t Epoch: 145 \t Loss: -0.012535777253409227\nTRAIN: \t Epoch: 145 \t Loss: -0.01245106542482972\nTRAIN: \t Epoch: 145 \t Loss: -0.012370094148950144\nTRAIN: \t Epoch: 145 \t Loss: -0.01231896768634518\nTRAIN: \t Epoch: 145 \t Loss: -0.012290610573612727\nTRAIN: \t Epoch: 145 \t Loss: -0.01229444651731423\nTRAIN: \t Epoch: 145 \t Loss: -0.012253082854052385\nTRAIN: \t Epoch: 145 \t Loss: -0.012216567352879792\nTRAIN: \t Epoch: 145 \t Loss: -0.012254339149769615\nTRAIN: \t Epoch: 145 \t Loss: -0.012226332755138477\nTRAIN: \t Epoch: 145 \t Loss: -0.01219096701396139\nTRAIN: \t Epoch: 145 \t Loss: -0.01215697624720633\nTRAIN: \t Epoch: 145 \t Loss: -0.01215223424601941\nVALD: \t Epoch: 145 \t Loss: -0.010132764466106892\nVALD: \t Epoch: 145 \t Loss: -0.010825006291270256\nVALD: \t Epoch: 145 \t Loss: -0.009635225714494785\nVALD: \t Epoch: 145 \t Loss: -0.01040893781464547\nVALD: \t Epoch: 145 \t Loss: -0.010194289415546853\n******************************\nEpoch: social-tag : 145\ntrain_loss -0.01215223424601941\nval_loss -0.010194289415546853\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 146 \t Loss: -0.013076587580144405\nTRAIN: \t Epoch: 146 \t Loss: -0.012973689008504152\nTRAIN: \t Epoch: 146 \t Loss: -0.01300790316114823\nTRAIN: \t Epoch: 146 \t Loss: -0.013041746569797397\nTRAIN: \t Epoch: 146 \t Loss: -0.012859411537647247\nTRAIN: \t Epoch: 146 \t Loss: -0.0126304862399896\nTRAIN: \t Epoch: 146 \t Loss: -0.012365691097719329\nTRAIN: \t Epoch: 146 \t Loss: -0.012367180897854269\nTRAIN: \t Epoch: 146 \t Loss: -0.012342014556957616\nTRAIN: \t Epoch: 146 \t Loss: -0.012380174454301595\nTRAIN: \t Epoch: 146 \t Loss: -0.012324134395881132\nTRAIN: \t Epoch: 146 \t Loss: -0.012208044063299894\nTRAIN: \t Epoch: 146 \t Loss: -0.012203933527836433\nTRAIN: \t Epoch: 146 \t Loss: -0.01223239947908691\nTRAIN: \t Epoch: 146 \t Loss: -0.012197724978129069\nTRAIN: \t Epoch: 146 \t Loss: -0.012172442919109017\nTRAIN: \t Epoch: 146 \t Loss: -0.012224836274981499\nTRAIN: \t Epoch: 146 \t Loss: -0.012269233508656422\nTRAIN: \t Epoch: 146 \t Loss: -0.012201841871597265\nTRAIN: \t Epoch: 146 \t Loss: -0.012104595405980944\nTRAIN: \t Epoch: 146 \t Loss: -0.012125100024222592\nVALD: \t Epoch: 146 \t Loss: -0.009813721291720867\nVALD: \t Epoch: 146 \t Loss: -0.01074565900489688\nVALD: \t Epoch: 146 \t Loss: -0.00957195150355498\nVALD: \t Epoch: 146 \t Loss: -0.010618163738399744\nVALD: \t Epoch: 146 \t Loss: -0.010325951084806531\n******************************\nEpoch: social-tag : 146\ntrain_loss -0.012125100024222592\nval_loss -0.010325951084806531\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 147 \t Loss: -0.013131399638950825\nTRAIN: \t Epoch: 147 \t Loss: -0.012730699963867664\nTRAIN: \t Epoch: 147 \t Loss: -0.012882344424724579\nTRAIN: \t Epoch: 147 \t Loss: -0.0122839929535985\nTRAIN: \t Epoch: 147 \t Loss: -0.012202947027981281\nTRAIN: \t Epoch: 147 \t Loss: -0.012156589111934105\nTRAIN: \t Epoch: 147 \t Loss: -0.012192282426570143\nTRAIN: \t Epoch: 147 \t Loss: -0.012273801607079804\nTRAIN: \t Epoch: 147 \t Loss: -0.012145195984178118\nTRAIN: \t Epoch: 147 \t Loss: -0.011916415952146053\nTRAIN: \t Epoch: 147 \t Loss: -0.011979162100363861\nTRAIN: \t Epoch: 147 \t Loss: -0.012098351881528894\nTRAIN: \t Epoch: 147 \t Loss: -0.012088445803293815\nTRAIN: \t Epoch: 147 \t Loss: -0.011993391106703452\nTRAIN: \t Epoch: 147 \t Loss: -0.011968943228324255\nTRAIN: \t Epoch: 147 \t Loss: -0.01204001292353496\nTRAIN: \t Epoch: 147 \t Loss: -0.012071824413450324\nTRAIN: \t Epoch: 147 \t Loss: -0.012006751520352231\nTRAIN: \t Epoch: 147 \t Loss: -0.011959677856219443\nTRAIN: \t Epoch: 147 \t Loss: -0.011998225376009941\nTRAIN: \t Epoch: 147 \t Loss: -0.012010743400640641\nVALD: \t Epoch: 147 \t Loss: -0.011215977370738983\nVALD: \t Epoch: 147 \t Loss: -0.011933252215385437\nVALD: \t Epoch: 147 \t Loss: -0.011305216699838638\nVALD: \t Epoch: 147 \t Loss: -0.011824555927887559\nVALD: \t Epoch: 147 \t Loss: -0.011345441982558\n******************************\nEpoch: social-tag : 147\ntrain_loss -0.012010743400640641\nval_loss -0.011345441982558\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 148 \t Loss: -0.013372926972806454\nTRAIN: \t Epoch: 148 \t Loss: -0.013435873668640852\nTRAIN: \t Epoch: 148 \t Loss: -0.013549776747822762\nTRAIN: \t Epoch: 148 \t Loss: -0.013020272133871913\nTRAIN: \t Epoch: 148 \t Loss: -0.012593554891645908\nTRAIN: \t Epoch: 148 \t Loss: -0.012513745576143265\nTRAIN: \t Epoch: 148 \t Loss: -0.012644302765173572\nTRAIN: \t Epoch: 148 \t Loss: -0.012632742989808321\nTRAIN: \t Epoch: 148 \t Loss: -0.01233882736414671\nTRAIN: \t Epoch: 148 \t Loss: -0.01224791631102562\nTRAIN: \t Epoch: 148 \t Loss: -0.012338176115669987\nTRAIN: \t Epoch: 148 \t Loss: -0.01242450593660275\nTRAIN: \t Epoch: 148 \t Loss: -0.01245116234685366\nTRAIN: \t Epoch: 148 \t Loss: -0.012405626675380128\nTRAIN: \t Epoch: 148 \t Loss: -0.012267102922002475\nTRAIN: \t Epoch: 148 \t Loss: -0.012152371113188565\nTRAIN: \t Epoch: 148 \t Loss: -0.01217877640224555\nTRAIN: \t Epoch: 148 \t Loss: -0.01219515497278836\nTRAIN: \t Epoch: 148 \t Loss: -0.012188922613859177\nTRAIN: \t Epoch: 148 \t Loss: -0.012220257427543402\nTRAIN: \t Epoch: 148 \t Loss: -0.012209327120273594\nVALD: \t Epoch: 148 \t Loss: -0.013500010594725609\nVALD: \t Epoch: 148 \t Loss: -0.013410569168627262\nVALD: \t Epoch: 148 \t Loss: -0.013076450986166796\nVALD: \t Epoch: 148 \t Loss: -0.013476610882207751\nVALD: \t Epoch: 148 \t Loss: -0.012655587875900638\n******************************\nEpoch: social-tag : 148\ntrain_loss -0.012209327120273594\nval_loss -0.012655587875900638\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 149 \t Loss: -0.012828157283365726\nTRAIN: \t Epoch: 149 \t Loss: -0.013081642799079418\nTRAIN: \t Epoch: 149 \t Loss: -0.01260914063702027\nTRAIN: \t Epoch: 149 \t Loss: -0.012279556598514318\nTRAIN: \t Epoch: 149 \t Loss: -0.012218269892036915\nTRAIN: \t Epoch: 149 \t Loss: -0.012163638758162657\nTRAIN: \t Epoch: 149 \t Loss: -0.01209042567227568\nTRAIN: \t Epoch: 149 \t Loss: -0.01208627293817699\nTRAIN: \t Epoch: 149 \t Loss: -0.012161005598803362\nTRAIN: \t Epoch: 149 \t Loss: -0.01218427661806345\nTRAIN: \t Epoch: 149 \t Loss: -0.012191658924249086\nTRAIN: \t Epoch: 149 \t Loss: -0.012067067592094341\nTRAIN: \t Epoch: 149 \t Loss: -0.012067342033753028\nTRAIN: \t Epoch: 149 \t Loss: -0.01215587489839111\nTRAIN: \t Epoch: 149 \t Loss: -0.012225849678119023\nTRAIN: \t Epoch: 149 \t Loss: -0.012195076327770948\nTRAIN: \t Epoch: 149 \t Loss: -0.012175614890806815\nTRAIN: \t Epoch: 149 \t Loss: -0.012126845980270041\nTRAIN: \t Epoch: 149 \t Loss: -0.01214935454098802\nTRAIN: \t Epoch: 149 \t Loss: -0.012216360308229923\nTRAIN: \t Epoch: 149 \t Loss: -0.012228640955323519\nVALD: \t Epoch: 149 \t Loss: -0.01141618937253952\nVALD: \t Epoch: 149 \t Loss: -0.012061524670571089\nVALD: \t Epoch: 149 \t Loss: -0.010912925004959106\nVALD: \t Epoch: 149 \t Loss: -0.011483973823487759\nVALD: \t Epoch: 149 \t Loss: -0.011123872033639806\n******************************\nEpoch: social-tag : 149\ntrain_loss -0.012228640955323519\nval_loss -0.011123872033639806\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 150 \t Loss: -0.013668588362634182\nTRAIN: \t Epoch: 150 \t Loss: -0.013393738772720098\nTRAIN: \t Epoch: 150 \t Loss: -0.013295643652478853\nTRAIN: \t Epoch: 150 \t Loss: -0.01328917546197772\nTRAIN: \t Epoch: 150 \t Loss: -0.013237453810870648\nTRAIN: \t Epoch: 150 \t Loss: -0.013319087990870079\nTRAIN: \t Epoch: 150 \t Loss: -0.0133219716538276\nTRAIN: \t Epoch: 150 \t Loss: -0.013381983037106693\nTRAIN: \t Epoch: 150 \t Loss: -0.013321741898026731\nTRAIN: \t Epoch: 150 \t Loss: -0.013345640804618596\nTRAIN: \t Epoch: 150 \t Loss: -0.013352417979728092\nTRAIN: \t Epoch: 150 \t Loss: -0.01338713134949406\nTRAIN: \t Epoch: 150 \t Loss: -0.013402253174437927\nTRAIN: \t Epoch: 150 \t Loss: -0.013496499841234513\nTRAIN: \t Epoch: 150 \t Loss: -0.013491345321138699\nTRAIN: \t Epoch: 150 \t Loss: -0.013438741269055754\nTRAIN: \t Epoch: 150 \t Loss: -0.013383858344134162\nTRAIN: \t Epoch: 150 \t Loss: -0.01337596494704485\nTRAIN: \t Epoch: 150 \t Loss: -0.013354960927053502\nTRAIN: \t Epoch: 150 \t Loss: -0.013313789200037719\nTRAIN: \t Epoch: 150 \t Loss: -0.013326156387534984\nVALD: \t Epoch: 150 \t Loss: -0.01337333396077156\nVALD: \t Epoch: 150 \t Loss: -0.013376264832913876\nVALD: \t Epoch: 150 \t Loss: -0.012931584070126215\nVALD: \t Epoch: 150 \t Loss: -0.013236698228865862\nVALD: \t Epoch: 150 \t Loss: -0.012583021572438607\n******************************\nEpoch: social-tag : 150\ntrain_loss -0.013326156387534984\nval_loss -0.012583021572438607\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 151 \t Loss: -0.013035982847213745\nTRAIN: \t Epoch: 151 \t Loss: -0.013586390763521194\nTRAIN: \t Epoch: 151 \t Loss: -0.013681759436925253\nTRAIN: \t Epoch: 151 \t Loss: -0.013968621380627155\nTRAIN: \t Epoch: 151 \t Loss: -0.014097063057124614\nTRAIN: \t Epoch: 151 \t Loss: -0.014097724109888077\nTRAIN: \t Epoch: 151 \t Loss: -0.014016403683594294\nTRAIN: \t Epoch: 151 \t Loss: -0.014048684621229768\nTRAIN: \t Epoch: 151 \t Loss: -0.013922417018976476\nTRAIN: \t Epoch: 151 \t Loss: -0.013748078886419535\nTRAIN: \t Epoch: 151 \t Loss: -0.013728307898749004\nTRAIN: \t Epoch: 151 \t Loss: -0.013691258694355687\nTRAIN: \t Epoch: 151 \t Loss: -0.013644129109497253\nTRAIN: \t Epoch: 151 \t Loss: -0.013671987556985446\nTRAIN: \t Epoch: 151 \t Loss: -0.013691447799404462\nTRAIN: \t Epoch: 151 \t Loss: -0.013732425344642252\nTRAIN: \t Epoch: 151 \t Loss: -0.013695572064641644\nTRAIN: \t Epoch: 151 \t Loss: -0.013583901441759534\nTRAIN: \t Epoch: 151 \t Loss: -0.01356293834549816\nTRAIN: \t Epoch: 151 \t Loss: -0.013493556156754493\nTRAIN: \t Epoch: 151 \t Loss: -0.01347242419822637\nVALD: \t Epoch: 151 \t Loss: -0.008942583575844765\nVALD: \t Epoch: 151 \t Loss: -0.010793129913508892\nVALD: \t Epoch: 151 \t Loss: -0.009548119734972715\nVALD: \t Epoch: 151 \t Loss: -0.0104464195901528\nVALD: \t Epoch: 151 \t Loss: -0.01027074115479818\n******************************\nEpoch: social-tag : 151\ntrain_loss -0.01347242419822637\nval_loss -0.01027074115479818\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 152 \t Loss: -0.013107959181070328\nTRAIN: \t Epoch: 152 \t Loss: -0.012944332789629698\nTRAIN: \t Epoch: 152 \t Loss: -0.013611673998335997\nTRAIN: \t Epoch: 152 \t Loss: -0.013696979731321335\nTRAIN: \t Epoch: 152 \t Loss: -0.01369144134223461\nTRAIN: \t Epoch: 152 \t Loss: -0.013621731816480557\nTRAIN: \t Epoch: 152 \t Loss: -0.013476913262690817\nTRAIN: \t Epoch: 152 \t Loss: -0.013487767311744392\nTRAIN: \t Epoch: 152 \t Loss: -0.01344901964896255\nTRAIN: \t Epoch: 152 \t Loss: -0.013557931408286095\nTRAIN: \t Epoch: 152 \t Loss: -0.013590906662019816\nTRAIN: \t Epoch: 152 \t Loss: -0.013482560326034823\nTRAIN: \t Epoch: 152 \t Loss: -0.013494098415741554\nTRAIN: \t Epoch: 152 \t Loss: -0.013514397798904352\nTRAIN: \t Epoch: 152 \t Loss: -0.013519807159900666\nTRAIN: \t Epoch: 152 \t Loss: -0.013525007292628288\nTRAIN: \t Epoch: 152 \t Loss: -0.013556018033448388\nTRAIN: \t Epoch: 152 \t Loss: -0.013506613257858489\nTRAIN: \t Epoch: 152 \t Loss: -0.013454340986515346\nTRAIN: \t Epoch: 152 \t Loss: -0.013471439806744456\nTRAIN: \t Epoch: 152 \t Loss: -0.013457876276124324\nVALD: \t Epoch: 152 \t Loss: -0.007059447467327118\nVALD: \t Epoch: 152 \t Loss: -0.009522486943751574\nVALD: \t Epoch: 152 \t Loss: -0.007417268352583051\nVALD: \t Epoch: 152 \t Loss: -0.008799852046649903\nVALD: \t Epoch: 152 \t Loss: -0.008861959988368306\n******************************\nEpoch: social-tag : 152\ntrain_loss -0.013457876276124324\nval_loss -0.008861959988368306\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 153 \t Loss: -0.013460117392241955\nTRAIN: \t Epoch: 153 \t Loss: -0.013117057736963034\nTRAIN: \t Epoch: 153 \t Loss: -0.013639056123793125\nTRAIN: \t Epoch: 153 \t Loss: -0.01360461045987904\nTRAIN: \t Epoch: 153 \t Loss: -0.0136129729449749\nTRAIN: \t Epoch: 153 \t Loss: -0.013709762133657932\nTRAIN: \t Epoch: 153 \t Loss: -0.013577779089765889\nTRAIN: \t Epoch: 153 \t Loss: -0.01359583344310522\nTRAIN: \t Epoch: 153 \t Loss: -0.01357806846499443\nTRAIN: \t Epoch: 153 \t Loss: -0.013582923356443644\nTRAIN: \t Epoch: 153 \t Loss: -0.013503112152896145\nTRAIN: \t Epoch: 153 \t Loss: -0.013502420003836354\nTRAIN: \t Epoch: 153 \t Loss: -0.013513743017728511\nTRAIN: \t Epoch: 153 \t Loss: -0.013507835899612733\nTRAIN: \t Epoch: 153 \t Loss: -0.013472109163800875\nTRAIN: \t Epoch: 153 \t Loss: -0.013483878807164729\nTRAIN: \t Epoch: 153 \t Loss: -0.013461841325111249\nTRAIN: \t Epoch: 153 \t Loss: -0.013502025097194646\nTRAIN: \t Epoch: 153 \t Loss: -0.013523356940009092\nTRAIN: \t Epoch: 153 \t Loss: -0.013498941948637367\nTRAIN: \t Epoch: 153 \t Loss: -0.013492713318297564\nVALD: \t Epoch: 153 \t Loss: -0.013333644717931747\nVALD: \t Epoch: 153 \t Loss: -0.013203433714807034\nVALD: \t Epoch: 153 \t Loss: -0.012849830711881319\nVALD: \t Epoch: 153 \t Loss: -0.013128867838531733\nVALD: \t Epoch: 153 \t Loss: -0.012499586587558814\n******************************\nEpoch: social-tag : 153\ntrain_loss -0.013492713318297564\nval_loss -0.012499586587558814\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 154 \t Loss: -0.014150718227028847\nTRAIN: \t Epoch: 154 \t Loss: -0.013905394356697798\nTRAIN: \t Epoch: 154 \t Loss: -0.01379232294857502\nTRAIN: \t Epoch: 154 \t Loss: -0.01371197821572423\nTRAIN: \t Epoch: 154 \t Loss: -0.013643934763967991\nTRAIN: \t Epoch: 154 \t Loss: -0.013678820338100195\nTRAIN: \t Epoch: 154 \t Loss: -0.013444881354059492\nTRAIN: \t Epoch: 154 \t Loss: -0.013475535670295358\nTRAIN: \t Epoch: 154 \t Loss: -0.013561709266569879\nTRAIN: \t Epoch: 154 \t Loss: -0.013577343989163637\nTRAIN: \t Epoch: 154 \t Loss: -0.013639090244065632\nTRAIN: \t Epoch: 154 \t Loss: -0.013620077321926752\nTRAIN: \t Epoch: 154 \t Loss: -0.013618127371256169\nTRAIN: \t Epoch: 154 \t Loss: -0.013545542157122068\nTRAIN: \t Epoch: 154 \t Loss: -0.01354807491103808\nTRAIN: \t Epoch: 154 \t Loss: -0.013538851635530591\nTRAIN: \t Epoch: 154 \t Loss: -0.013541330857311977\nTRAIN: \t Epoch: 154 \t Loss: -0.013537012558016513\nTRAIN: \t Epoch: 154 \t Loss: -0.013517505783391627\nTRAIN: \t Epoch: 154 \t Loss: -0.013531204825267196\nTRAIN: \t Epoch: 154 \t Loss: -0.013509899044643482\nVALD: \t Epoch: 154 \t Loss: -0.00834638625383377\nVALD: \t Epoch: 154 \t Loss: -0.010421468876302242\nVALD: \t Epoch: 154 \t Loss: -0.008877404810239872\nVALD: \t Epoch: 154 \t Loss: -0.009950982755981386\nVALD: \t Epoch: 154 \t Loss: -0.00981096100308085\n******************************\nEpoch: social-tag : 154\ntrain_loss -0.013509899044643482\nval_loss -0.00981096100308085\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 155 \t Loss: -0.012664984911680222\nTRAIN: \t Epoch: 155 \t Loss: -0.012741255108267069\nTRAIN: \t Epoch: 155 \t Loss: -0.01305572067697843\nTRAIN: \t Epoch: 155 \t Loss: -0.01305995718576014\nTRAIN: \t Epoch: 155 \t Loss: -0.012908940017223359\nTRAIN: \t Epoch: 155 \t Loss: -0.013148316647857428\nTRAIN: \t Epoch: 155 \t Loss: -0.013275324366986752\nTRAIN: \t Epoch: 155 \t Loss: -0.013339181547053158\nTRAIN: \t Epoch: 155 \t Loss: -0.013377864638136493\nTRAIN: \t Epoch: 155 \t Loss: -0.013400566857308149\nTRAIN: \t Epoch: 155 \t Loss: -0.013469446873800322\nTRAIN: \t Epoch: 155 \t Loss: -0.01342548585186402\nTRAIN: \t Epoch: 155 \t Loss: -0.013488659635186195\nTRAIN: \t Epoch: 155 \t Loss: -0.013540663889476232\nTRAIN: \t Epoch: 155 \t Loss: -0.01355516848464807\nTRAIN: \t Epoch: 155 \t Loss: -0.013516768638510257\nTRAIN: \t Epoch: 155 \t Loss: -0.013528466608156176\nTRAIN: \t Epoch: 155 \t Loss: -0.013529081104530228\nTRAIN: \t Epoch: 155 \t Loss: -0.013545742062361617\nTRAIN: \t Epoch: 155 \t Loss: -0.013545657461509108\nTRAIN: \t Epoch: 155 \t Loss: -0.013539556939508148\nVALD: \t Epoch: 155 \t Loss: -0.012192709371447563\nVALD: \t Epoch: 155 \t Loss: -0.01210590498521924\nVALD: \t Epoch: 155 \t Loss: -0.01088599426050981\nVALD: \t Epoch: 155 \t Loss: -0.011180442990735173\nVALD: \t Epoch: 155 \t Loss: -0.010816896499259076\n******************************\nEpoch: social-tag : 155\ntrain_loss -0.013539556939508148\nval_loss -0.010816896499259076\n{'min_val_epoch': 114, 'min_val_loss': -0.012693095418374128}\n******************************\nTRAIN: \t Epoch: 156 \t Loss: -0.013107741251587868\nTRAIN: \t Epoch: 156 \t Loss: -0.012689674738794565\nTRAIN: \t Epoch: 156 \t Loss: -0.013331466975311438\nTRAIN: \t Epoch: 156 \t Loss: -0.01336946221999824\nTRAIN: \t Epoch: 156 \t Loss: -0.013268345408141613\nTRAIN: \t Epoch: 156 \t Loss: -0.013555608379344145\nTRAIN: \t Epoch: 156 \t Loss: -0.013602613205356258\nTRAIN: \t Epoch: 156 \t Loss: -0.01366853853687644\nTRAIN: \t Epoch: 156 \t Loss: -0.013671184269090494\nTRAIN: \t Epoch: 156 \t Loss: -0.013623641338199377\nTRAIN: \t Epoch: 156 \t Loss: -0.013594786128537222\nTRAIN: \t Epoch: 156 \t Loss: -0.01362715382128954\nTRAIN: \t Epoch: 156 \t Loss: -0.013613933697342873\nTRAIN: \t Epoch: 156 \t Loss: -0.013568713994962829\nTRAIN: \t Epoch: 156 \t Loss: -0.013597204101582367\nTRAIN: \t Epoch: 156 \t Loss: -0.013586151064373553\nTRAIN: \t Epoch: 156 \t Loss: -0.013579573591842371\nTRAIN: \t Epoch: 156 \t Loss: -0.013598171787129508\nTRAIN: \t Epoch: 156 \t Loss: -0.01357564849680976\nTRAIN: \t Epoch: 156 \t Loss: -0.013594746962189675\nTRAIN: \t Epoch: 156 \t Loss: -0.01356558406362188\nVALD: \t Epoch: 156 \t Loss: -0.01385971438139677\nVALD: \t Epoch: 156 \t Loss: -0.013682225253432989\nVALD: \t Epoch: 156 \t Loss: -0.013456117361783981\nVALD: \t Epoch: 156 \t Loss: -0.013536300510168076\nVALD: \t Epoch: 156 \t Loss: -0.012862095510326146\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.41it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.34952415783577534  FDE: 0.5133900366759295\n**************************************************\n******************************\nEpoch: social-tag : 156\ntrain_loss -0.01356558406362188\nval_loss -0.012862095510326146\n{'min_val_epoch': 156, 'min_val_loss': -0.012862095510326146}\n******************************\nTRAIN: \t Epoch: 157 \t Loss: -0.01424929779022932\nTRAIN: \t Epoch: 157 \t Loss: -0.014234817586839199\nTRAIN: \t Epoch: 157 \t Loss: -0.01410453300923109\nTRAIN: \t Epoch: 157 \t Loss: -0.013668878702446818\nTRAIN: \t Epoch: 157 \t Loss: -0.013519737496972084\nTRAIN: \t Epoch: 157 \t Loss: -0.013505439274013042\nTRAIN: \t Epoch: 157 \t Loss: -0.013627540320158005\nTRAIN: \t Epoch: 157 \t Loss: -0.013507105177268386\nTRAIN: \t Epoch: 157 \t Loss: -0.013645840601788627\nTRAIN: \t Epoch: 157 \t Loss: -0.013648008275777102\nTRAIN: \t Epoch: 157 \t Loss: -0.013726999221200293\nTRAIN: \t Epoch: 157 \t Loss: -0.013701456288496653\nTRAIN: \t Epoch: 157 \t Loss: -0.013716217631903978\nTRAIN: \t Epoch: 157 \t Loss: -0.013644599688372441\nTRAIN: \t Epoch: 157 \t Loss: -0.013630205268661181\nTRAIN: \t Epoch: 157 \t Loss: -0.013566287641879171\nTRAIN: \t Epoch: 157 \t Loss: -0.013572331791853203\nTRAIN: \t Epoch: 157 \t Loss: -0.013564448441482253\nTRAIN: \t Epoch: 157 \t Loss: -0.013565008822632464\nTRAIN: \t Epoch: 157 \t Loss: -0.013571637263521553\nTRAIN: \t Epoch: 157 \t Loss: -0.013569002242941258\nVALD: \t Epoch: 157 \t Loss: -0.013672495260834694\nVALD: \t Epoch: 157 \t Loss: -0.013725720811635256\nVALD: \t Epoch: 157 \t Loss: -0.013642949673036734\nVALD: \t Epoch: 157 \t Loss: -0.013652142835780978\nVALD: \t Epoch: 157 \t Loss: -0.012929200742171774\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.14it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.37673412453159694  FDE: 0.5901803069526762\n**************************************************\n******************************\nEpoch: social-tag : 157\ntrain_loss -0.013569002242941258\nval_loss -0.012929200742171774\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 158 \t Loss: -0.013494055718183517\nTRAIN: \t Epoch: 158 \t Loss: -0.013446212280541658\nTRAIN: \t Epoch: 158 \t Loss: -0.013927657157182693\nTRAIN: \t Epoch: 158 \t Loss: -0.013957092305645347\nTRAIN: \t Epoch: 158 \t Loss: -0.013793038204312325\nTRAIN: \t Epoch: 158 \t Loss: -0.013962949470927319\nTRAIN: \t Epoch: 158 \t Loss: -0.013894646295479365\nTRAIN: \t Epoch: 158 \t Loss: -0.01392193092033267\nTRAIN: \t Epoch: 158 \t Loss: -0.013691699132323265\nTRAIN: \t Epoch: 158 \t Loss: -0.01367619289085269\nTRAIN: \t Epoch: 158 \t Loss: -0.013637560843066736\nTRAIN: \t Epoch: 158 \t Loss: -0.013529695648079118\nTRAIN: \t Epoch: 158 \t Loss: -0.01354643630866821\nTRAIN: \t Epoch: 158 \t Loss: -0.013565312139689922\nTRAIN: \t Epoch: 158 \t Loss: -0.013490775041282177\nTRAIN: \t Epoch: 158 \t Loss: -0.013499726657755673\nTRAIN: \t Epoch: 158 \t Loss: -0.013517741497386904\nTRAIN: \t Epoch: 158 \t Loss: -0.013541819838186106\nTRAIN: \t Epoch: 158 \t Loss: -0.013589184437143175\nTRAIN: \t Epoch: 158 \t Loss: -0.013599802646785975\nTRAIN: \t Epoch: 158 \t Loss: -0.013585709006674215\nVALD: \t Epoch: 158 \t Loss: -0.013219838961958885\nVALD: \t Epoch: 158 \t Loss: -0.013398422859609127\nVALD: \t Epoch: 158 \t Loss: -0.013370014106233915\nVALD: \t Epoch: 158 \t Loss: -0.013390498701483011\nVALD: \t Epoch: 158 \t Loss: -0.012740037099558758\n******************************\nEpoch: social-tag : 158\ntrain_loss -0.013585709006674215\nval_loss -0.012740037099558758\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 159 \t Loss: -0.012238439172506332\nTRAIN: \t Epoch: 159 \t Loss: -0.01298151770606637\nTRAIN: \t Epoch: 159 \t Loss: -0.012911943718791008\nTRAIN: \t Epoch: 159 \t Loss: -0.0129560143686831\nTRAIN: \t Epoch: 159 \t Loss: -0.012994817271828652\nTRAIN: \t Epoch: 159 \t Loss: -0.013140310222903887\nTRAIN: \t Epoch: 159 \t Loss: -0.013326048451874937\nTRAIN: \t Epoch: 159 \t Loss: -0.013262441265396774\nTRAIN: \t Epoch: 159 \t Loss: -0.013279246683749888\nTRAIN: \t Epoch: 159 \t Loss: -0.01334673399105668\nTRAIN: \t Epoch: 159 \t Loss: -0.013331529769030485\nTRAIN: \t Epoch: 159 \t Loss: -0.013339111969495812\nTRAIN: \t Epoch: 159 \t Loss: -0.01339979412463995\nTRAIN: \t Epoch: 159 \t Loss: -0.013405031191983394\nTRAIN: \t Epoch: 159 \t Loss: -0.013430411492784818\nTRAIN: \t Epoch: 159 \t Loss: -0.013469803147017956\nTRAIN: \t Epoch: 159 \t Loss: -0.013474233722423805\nTRAIN: \t Epoch: 159 \t Loss: -0.013468274898413155\nTRAIN: \t Epoch: 159 \t Loss: -0.013481365418747851\nTRAIN: \t Epoch: 159 \t Loss: -0.013580156862735749\nTRAIN: \t Epoch: 159 \t Loss: -0.013586549066817107\nVALD: \t Epoch: 159 \t Loss: -0.012318385764956474\nVALD: \t Epoch: 159 \t Loss: -0.012799642980098724\nVALD: \t Epoch: 159 \t Loss: -0.012228936577836672\nVALD: \t Epoch: 159 \t Loss: -0.012581817572936416\nVALD: \t Epoch: 159 \t Loss: -0.01202172703213162\n******************************\nEpoch: social-tag : 159\ntrain_loss -0.013586549066817107\nval_loss -0.01202172703213162\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 160 \t Loss: -0.014011459425091743\nTRAIN: \t Epoch: 160 \t Loss: -0.013929253444075584\nTRAIN: \t Epoch: 160 \t Loss: -0.013580660646160444\nTRAIN: \t Epoch: 160 \t Loss: -0.013513618148863316\nTRAIN: \t Epoch: 160 \t Loss: -0.013450357317924499\nTRAIN: \t Epoch: 160 \t Loss: -0.013637850837161144\nTRAIN: \t Epoch: 160 \t Loss: -0.013604083497609411\nTRAIN: \t Epoch: 160 \t Loss: -0.013581148581579328\nTRAIN: \t Epoch: 160 \t Loss: -0.013542268528706498\nTRAIN: \t Epoch: 160 \t Loss: -0.013398983608931303\nTRAIN: \t Epoch: 160 \t Loss: -0.013463327830488031\nTRAIN: \t Epoch: 160 \t Loss: -0.013458510007088384\nTRAIN: \t Epoch: 160 \t Loss: -0.013555487976051293\nTRAIN: \t Epoch: 160 \t Loss: -0.013575023106698478\nTRAIN: \t Epoch: 160 \t Loss: -0.013580072484910489\nTRAIN: \t Epoch: 160 \t Loss: -0.013592496223282069\nTRAIN: \t Epoch: 160 \t Loss: -0.013626196640817559\nTRAIN: \t Epoch: 160 \t Loss: -0.013613776717748906\nTRAIN: \t Epoch: 160 \t Loss: -0.01359923433904585\nTRAIN: \t Epoch: 160 \t Loss: -0.013590716570615769\nTRAIN: \t Epoch: 160 \t Loss: -0.013605040190030542\nVALD: \t Epoch: 160 \t Loss: -0.012884057126939297\nVALD: \t Epoch: 160 \t Loss: -0.0131680304184556\nVALD: \t Epoch: 160 \t Loss: -0.012903346680104733\nVALD: \t Epoch: 160 \t Loss: -0.013031413545832038\nVALD: \t Epoch: 160 \t Loss: -0.012431503302041267\n******************************\nEpoch: social-tag : 160\ntrain_loss -0.013605040190030542\nval_loss -0.012431503302041267\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 161 \t Loss: -0.015256276354193687\nTRAIN: \t Epoch: 161 \t Loss: -0.013935608323663473\nTRAIN: \t Epoch: 161 \t Loss: -0.013698748933772245\nTRAIN: \t Epoch: 161 \t Loss: -0.013733283383771777\nTRAIN: \t Epoch: 161 \t Loss: -0.013463428989052773\nTRAIN: \t Epoch: 161 \t Loss: -0.013677853314826885\nTRAIN: \t Epoch: 161 \t Loss: -0.013617809595806258\nTRAIN: \t Epoch: 161 \t Loss: -0.013671873020939529\nTRAIN: \t Epoch: 161 \t Loss: -0.013815225102007389\nTRAIN: \t Epoch: 161 \t Loss: -0.013766258396208287\nTRAIN: \t Epoch: 161 \t Loss: -0.013636943376199766\nTRAIN: \t Epoch: 161 \t Loss: -0.013646147058655819\nTRAIN: \t Epoch: 161 \t Loss: -0.013632229959162382\nTRAIN: \t Epoch: 161 \t Loss: -0.013653646955000502\nTRAIN: \t Epoch: 161 \t Loss: -0.013665069391330083\nTRAIN: \t Epoch: 161 \t Loss: -0.01368655898841098\nTRAIN: \t Epoch: 161 \t Loss: -0.013665037284440854\nTRAIN: \t Epoch: 161 \t Loss: -0.013645646472771963\nTRAIN: \t Epoch: 161 \t Loss: -0.01363632082939148\nTRAIN: \t Epoch: 161 \t Loss: -0.013626369973644614\nTRAIN: \t Epoch: 161 \t Loss: -0.013617218390922869\nVALD: \t Epoch: 161 \t Loss: -0.006121826823800802\nVALD: \t Epoch: 161 \t Loss: -0.008587826741859317\nVALD: \t Epoch: 161 \t Loss: -0.0052343242180844145\nVALD: \t Epoch: 161 \t Loss: -0.006798112648539245\nVALD: \t Epoch: 161 \t Loss: -0.007148956331077981\n******************************\nEpoch: social-tag : 161\ntrain_loss -0.013617218390922869\nval_loss -0.007148956331077981\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 162 \t Loss: -0.014092165045440197\nTRAIN: \t Epoch: 162 \t Loss: -0.01422575255855918\nTRAIN: \t Epoch: 162 \t Loss: -0.014172875322401524\nTRAIN: \t Epoch: 162 \t Loss: -0.014051339589059353\nTRAIN: \t Epoch: 162 \t Loss: -0.013718773424625397\nTRAIN: \t Epoch: 162 \t Loss: -0.013639812978605429\nTRAIN: \t Epoch: 162 \t Loss: -0.01361852205757584\nTRAIN: \t Epoch: 162 \t Loss: -0.013448951649479568\nTRAIN: \t Epoch: 162 \t Loss: -0.013457253575325012\nTRAIN: \t Epoch: 162 \t Loss: -0.013483916874974966\nTRAIN: \t Epoch: 162 \t Loss: -0.013525175862014294\nTRAIN: \t Epoch: 162 \t Loss: -0.013533354814474782\nTRAIN: \t Epoch: 162 \t Loss: -0.013564413270125022\nTRAIN: \t Epoch: 162 \t Loss: -0.013516501057893038\nTRAIN: \t Epoch: 162 \t Loss: -0.013514802046120166\nTRAIN: \t Epoch: 162 \t Loss: -0.013533496472518891\nTRAIN: \t Epoch: 162 \t Loss: -0.01356161730911802\nTRAIN: \t Epoch: 162 \t Loss: -0.01357647492032912\nTRAIN: \t Epoch: 162 \t Loss: -0.013630995968062627\nTRAIN: \t Epoch: 162 \t Loss: -0.013621987216174603\nTRAIN: \t Epoch: 162 \t Loss: -0.013621304422593981\nVALD: \t Epoch: 162 \t Loss: -0.013116752728819847\nVALD: \t Epoch: 162 \t Loss: -0.013226838316768408\nVALD: \t Epoch: 162 \t Loss: -0.012839116776982943\nVALD: \t Epoch: 162 \t Loss: -0.013105493737384677\nVALD: \t Epoch: 162 \t Loss: -0.012465068299605267\n******************************\nEpoch: social-tag : 162\ntrain_loss -0.013621304422593981\nval_loss -0.012465068299605267\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 163 \t Loss: -0.013727769255638123\nTRAIN: \t Epoch: 163 \t Loss: -0.013877486810088158\nTRAIN: \t Epoch: 163 \t Loss: -0.013934345915913582\nTRAIN: \t Epoch: 163 \t Loss: -0.013812181307002902\nTRAIN: \t Epoch: 163 \t Loss: -0.013773384876549244\nTRAIN: \t Epoch: 163 \t Loss: -0.013825159209469954\nTRAIN: \t Epoch: 163 \t Loss: -0.013829839948032583\nTRAIN: \t Epoch: 163 \t Loss: -0.013842721353285015\nTRAIN: \t Epoch: 163 \t Loss: -0.013830082077119086\nTRAIN: \t Epoch: 163 \t Loss: -0.013729959819465875\nTRAIN: \t Epoch: 163 \t Loss: -0.013723262416368181\nTRAIN: \t Epoch: 163 \t Loss: -0.013659068926547965\nTRAIN: \t Epoch: 163 \t Loss: -0.013642072892532898\nTRAIN: \t Epoch: 163 \t Loss: -0.013629741234970945\nTRAIN: \t Epoch: 163 \t Loss: -0.013671731886764368\nTRAIN: \t Epoch: 163 \t Loss: -0.013624323706608266\nTRAIN: \t Epoch: 163 \t Loss: -0.013630102388560772\nTRAIN: \t Epoch: 163 \t Loss: -0.01364376007889708\nTRAIN: \t Epoch: 163 \t Loss: -0.01361019242750971\nTRAIN: \t Epoch: 163 \t Loss: -0.013630838925018906\nTRAIN: \t Epoch: 163 \t Loss: -0.013631552963323012\nVALD: \t Epoch: 163 \t Loss: -0.009819005616009235\nVALD: \t Epoch: 163 \t Loss: -0.010853033512830734\nVALD: \t Epoch: 163 \t Loss: -0.009233224671334028\nVALD: \t Epoch: 163 \t Loss: -0.010237979353405535\nVALD: \t Epoch: 163 \t Loss: -0.010015961436641773\n******************************\nEpoch: social-tag : 163\ntrain_loss -0.013631552963323012\nval_loss -0.010015961436641773\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 164 \t Loss: -0.014042288064956665\nTRAIN: \t Epoch: 164 \t Loss: -0.013831175863742828\nTRAIN: \t Epoch: 164 \t Loss: -0.013493947063883146\nTRAIN: \t Epoch: 164 \t Loss: -0.013712642481550574\nTRAIN: \t Epoch: 164 \t Loss: -0.013630149886012077\nTRAIN: \t Epoch: 164 \t Loss: -0.013690660086770853\nTRAIN: \t Epoch: 164 \t Loss: -0.013739966654351779\nTRAIN: \t Epoch: 164 \t Loss: -0.013778084539808333\nTRAIN: \t Epoch: 164 \t Loss: -0.01366526105751594\nTRAIN: \t Epoch: 164 \t Loss: -0.013644157629460096\nTRAIN: \t Epoch: 164 \t Loss: -0.01353032531386072\nTRAIN: \t Epoch: 164 \t Loss: -0.013551235354195038\nTRAIN: \t Epoch: 164 \t Loss: -0.013526684461304775\nTRAIN: \t Epoch: 164 \t Loss: -0.013586890018944229\nTRAIN: \t Epoch: 164 \t Loss: -0.013659969655176003\nTRAIN: \t Epoch: 164 \t Loss: -0.01367710466729477\nTRAIN: \t Epoch: 164 \t Loss: -0.013682815191500327\nTRAIN: \t Epoch: 164 \t Loss: -0.01366233318630192\nTRAIN: \t Epoch: 164 \t Loss: -0.013637775898371873\nTRAIN: \t Epoch: 164 \t Loss: -0.013623461639508606\nTRAIN: \t Epoch: 164 \t Loss: -0.013642314611808466\nVALD: \t Epoch: 164 \t Loss: -0.011803806759417057\nVALD: \t Epoch: 164 \t Loss: -0.012024592608213425\nVALD: \t Epoch: 164 \t Loss: -0.011140201861659685\nVALD: \t Epoch: 164 \t Loss: -0.011747185606509447\nVALD: \t Epoch: 164 \t Loss: -0.011265357527371957\n******************************\nEpoch: social-tag : 164\ntrain_loss -0.013642314611808466\nval_loss -0.011265357527371957\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 165 \t Loss: -0.01347106322646141\nTRAIN: \t Epoch: 165 \t Loss: -0.013674575835466385\nTRAIN: \t Epoch: 165 \t Loss: -0.013487353610495726\nTRAIN: \t Epoch: 165 \t Loss: -0.013734841486439109\nTRAIN: \t Epoch: 165 \t Loss: -0.013510201498866082\nTRAIN: \t Epoch: 165 \t Loss: -0.01382206023360292\nTRAIN: \t Epoch: 165 \t Loss: -0.013655166806919234\nTRAIN: \t Epoch: 165 \t Loss: -0.01364918751642108\nTRAIN: \t Epoch: 165 \t Loss: -0.01369493506434891\nTRAIN: \t Epoch: 165 \t Loss: -0.013680387102067471\nTRAIN: \t Epoch: 165 \t Loss: -0.01368204690515995\nTRAIN: \t Epoch: 165 \t Loss: -0.013612618437036872\nTRAIN: \t Epoch: 165 \t Loss: -0.01362256591136639\nTRAIN: \t Epoch: 165 \t Loss: -0.01364025054499507\nTRAIN: \t Epoch: 165 \t Loss: -0.01361212922881047\nTRAIN: \t Epoch: 165 \t Loss: -0.013620209589134902\nTRAIN: \t Epoch: 165 \t Loss: -0.013646418667014907\nTRAIN: \t Epoch: 165 \t Loss: -0.013656677367786566\nTRAIN: \t Epoch: 165 \t Loss: -0.013685938313995538\nTRAIN: \t Epoch: 165 \t Loss: -0.013695125188678502\nTRAIN: \t Epoch: 165 \t Loss: -0.013676555435199782\nVALD: \t Epoch: 165 \t Loss: -0.012148895300924778\nVALD: \t Epoch: 165 \t Loss: -0.012165739201009274\nVALD: \t Epoch: 165 \t Loss: -0.011428586828211943\nVALD: \t Epoch: 165 \t Loss: -0.011612339410930872\nVALD: \t Epoch: 165 \t Loss: -0.011242818717219404\n******************************\nEpoch: social-tag : 165\ntrain_loss -0.013676555435199782\nval_loss -0.011242818717219404\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 166 \t Loss: -0.01333705335855484\nTRAIN: \t Epoch: 166 \t Loss: -0.013397379778325558\nTRAIN: \t Epoch: 166 \t Loss: -0.01320686750113964\nTRAIN: \t Epoch: 166 \t Loss: -0.013125298544764519\nTRAIN: \t Epoch: 166 \t Loss: -0.013102621026337146\nTRAIN: \t Epoch: 166 \t Loss: -0.013236296828836203\nTRAIN: \t Epoch: 166 \t Loss: -0.013286057460520948\nTRAIN: \t Epoch: 166 \t Loss: -0.013258286053314805\nTRAIN: \t Epoch: 166 \t Loss: -0.013343833283417754\nTRAIN: \t Epoch: 166 \t Loss: -0.013401780556887389\nTRAIN: \t Epoch: 166 \t Loss: -0.013524987988851288\nTRAIN: \t Epoch: 166 \t Loss: -0.013562096593280634\nTRAIN: \t Epoch: 166 \t Loss: -0.013561218976974487\nTRAIN: \t Epoch: 166 \t Loss: -0.013628346645938498\nTRAIN: \t Epoch: 166 \t Loss: -0.013684151011208694\nTRAIN: \t Epoch: 166 \t Loss: -0.013685809739399701\nTRAIN: \t Epoch: 166 \t Loss: -0.013668629450394827\nTRAIN: \t Epoch: 166 \t Loss: -0.013642971766077809\nTRAIN: \t Epoch: 166 \t Loss: -0.013653625194963655\nTRAIN: \t Epoch: 166 \t Loss: -0.0136446385178715\nTRAIN: \t Epoch: 166 \t Loss: -0.013669725671215249\nVALD: \t Epoch: 166 \t Loss: -0.011890367604792118\nVALD: \t Epoch: 166 \t Loss: -0.012225671205669641\nVALD: \t Epoch: 166 \t Loss: -0.011085476415852705\nVALD: \t Epoch: 166 \t Loss: -0.011488344287499785\nVALD: \t Epoch: 166 \t Loss: -0.01113421444731634\n******************************\nEpoch: social-tag : 166\ntrain_loss -0.013669725671215249\nval_loss -0.01113421444731634\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 167 \t Loss: -0.013971433974802494\nTRAIN: \t Epoch: 167 \t Loss: -0.013257470913231373\nTRAIN: \t Epoch: 167 \t Loss: -0.013564225907127062\nTRAIN: \t Epoch: 167 \t Loss: -0.013495357939973474\nTRAIN: \t Epoch: 167 \t Loss: -0.013408317230641842\nTRAIN: \t Epoch: 167 \t Loss: -0.013490996789187193\nTRAIN: \t Epoch: 167 \t Loss: -0.013476569205522537\nTRAIN: \t Epoch: 167 \t Loss: -0.013402843498624861\nTRAIN: \t Epoch: 167 \t Loss: -0.013421120846437084\nTRAIN: \t Epoch: 167 \t Loss: -0.013451312202960253\nTRAIN: \t Epoch: 167 \t Loss: -0.013485689647495747\nTRAIN: \t Epoch: 167 \t Loss: -0.013595764137183627\nTRAIN: \t Epoch: 167 \t Loss: -0.013625804406519119\nTRAIN: \t Epoch: 167 \t Loss: -0.013622661985989128\nTRAIN: \t Epoch: 167 \t Loss: -0.013625146510700384\nTRAIN: \t Epoch: 167 \t Loss: -0.013654941751156002\nTRAIN: \t Epoch: 167 \t Loss: -0.013668861130581182\nTRAIN: \t Epoch: 167 \t Loss: -0.013715626154508855\nTRAIN: \t Epoch: 167 \t Loss: -0.013701431208143109\nTRAIN: \t Epoch: 167 \t Loss: -0.013729480421170592\nTRAIN: \t Epoch: 167 \t Loss: -0.013726865535252996\nVALD: \t Epoch: 167 \t Loss: -0.013975012116134167\nVALD: \t Epoch: 167 \t Loss: -0.013706698082387447\nVALD: \t Epoch: 167 \t Loss: -0.013385422838230928\nVALD: \t Epoch: 167 \t Loss: -0.013484760886058211\nVALD: \t Epoch: 167 \t Loss: -0.012829063211278255\n******************************\nEpoch: social-tag : 167\ntrain_loss -0.013726865535252996\nval_loss -0.012829063211278255\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 168 \t Loss: -0.01394206564873457\nTRAIN: \t Epoch: 168 \t Loss: -0.01427161181345582\nTRAIN: \t Epoch: 168 \t Loss: -0.013747532851994038\nTRAIN: \t Epoch: 168 \t Loss: -0.013698700349777937\nTRAIN: \t Epoch: 168 \t Loss: -0.013746151700615883\nTRAIN: \t Epoch: 168 \t Loss: -0.013606514781713486\nTRAIN: \t Epoch: 168 \t Loss: -0.013517788079168116\nTRAIN: \t Epoch: 168 \t Loss: -0.013617424527183175\nTRAIN: \t Epoch: 168 \t Loss: -0.013630108079976506\nTRAIN: \t Epoch: 168 \t Loss: -0.013547563925385475\nTRAIN: \t Epoch: 168 \t Loss: -0.013562155802818861\nTRAIN: \t Epoch: 168 \t Loss: -0.013576343422755599\nTRAIN: \t Epoch: 168 \t Loss: -0.013556708868306417\nTRAIN: \t Epoch: 168 \t Loss: -0.013576704077422619\nTRAIN: \t Epoch: 168 \t Loss: -0.01360707717637221\nTRAIN: \t Epoch: 168 \t Loss: -0.013598683115560561\nTRAIN: \t Epoch: 168 \t Loss: -0.013669920811320053\nTRAIN: \t Epoch: 168 \t Loss: -0.01368705959369739\nTRAIN: \t Epoch: 168 \t Loss: -0.013683882738022428\nTRAIN: \t Epoch: 168 \t Loss: -0.013692667335271835\nTRAIN: \t Epoch: 168 \t Loss: -0.01369200052898851\nVALD: \t Epoch: 168 \t Loss: -0.009919865056872368\nVALD: \t Epoch: 168 \t Loss: -0.011262254789471626\nVALD: \t Epoch: 168 \t Loss: -0.010570037799576918\nVALD: \t Epoch: 168 \t Loss: -0.010861614253371954\nVALD: \t Epoch: 168 \t Loss: -0.010521369087907234\n******************************\nEpoch: social-tag : 168\ntrain_loss -0.01369200052898851\nval_loss -0.010521369087907234\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 169 \t Loss: -0.01411677710711956\nTRAIN: \t Epoch: 169 \t Loss: -0.014052466955035925\nTRAIN: \t Epoch: 169 \t Loss: -0.013492569948236147\nTRAIN: \t Epoch: 169 \t Loss: -0.01360887405462563\nTRAIN: \t Epoch: 169 \t Loss: -0.01362831201404333\nTRAIN: \t Epoch: 169 \t Loss: -0.01361305189008514\nTRAIN: \t Epoch: 169 \t Loss: -0.013630542771092482\nTRAIN: \t Epoch: 169 \t Loss: -0.013559065875597298\nTRAIN: \t Epoch: 169 \t Loss: -0.013566810430751907\nTRAIN: \t Epoch: 169 \t Loss: -0.013533035758882761\nTRAIN: \t Epoch: 169 \t Loss: -0.013604796830226074\nTRAIN: \t Epoch: 169 \t Loss: -0.013555904927973947\nTRAIN: \t Epoch: 169 \t Loss: -0.013518110347481875\nTRAIN: \t Epoch: 169 \t Loss: -0.01351437611239297\nTRAIN: \t Epoch: 169 \t Loss: -0.013527672427395979\nTRAIN: \t Epoch: 169 \t Loss: -0.013519478670787066\nTRAIN: \t Epoch: 169 \t Loss: -0.013531840132439838\nTRAIN: \t Epoch: 169 \t Loss: -0.013590449260340797\nTRAIN: \t Epoch: 169 \t Loss: -0.013653398932595002\nTRAIN: \t Epoch: 169 \t Loss: -0.013696338282898068\nTRAIN: \t Epoch: 169 \t Loss: -0.01372283471944981\nVALD: \t Epoch: 169 \t Loss: -0.013651059940457344\nVALD: \t Epoch: 169 \t Loss: -0.013317971490323544\nVALD: \t Epoch: 169 \t Loss: -0.012818211068709692\nVALD: \t Epoch: 169 \t Loss: -0.012906309450045228\nVALD: \t Epoch: 169 \t Loss: -0.01230255167649372\n******************************\nEpoch: social-tag : 169\ntrain_loss -0.01372283471944981\nval_loss -0.01230255167649372\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 170 \t Loss: -0.01338610053062439\nTRAIN: \t Epoch: 170 \t Loss: -0.013715522829443216\nTRAIN: \t Epoch: 170 \t Loss: -0.013546750570336977\nTRAIN: \t Epoch: 170 \t Loss: -0.01353461411781609\nTRAIN: \t Epoch: 170 \t Loss: -0.013820161670446396\nTRAIN: \t Epoch: 170 \t Loss: -0.013719622356196245\nTRAIN: \t Epoch: 170 \t Loss: -0.013749146567923682\nTRAIN: \t Epoch: 170 \t Loss: -0.013788688462227583\nTRAIN: \t Epoch: 170 \t Loss: -0.013746114447712898\nTRAIN: \t Epoch: 170 \t Loss: -0.013773890398442745\nTRAIN: \t Epoch: 170 \t Loss: -0.013825681060552597\nTRAIN: \t Epoch: 170 \t Loss: -0.013830244075506926\nTRAIN: \t Epoch: 170 \t Loss: -0.01370190864858719\nTRAIN: \t Epoch: 170 \t Loss: -0.013741448588137115\nTRAIN: \t Epoch: 170 \t Loss: -0.013680840594073136\nTRAIN: \t Epoch: 170 \t Loss: -0.013704357494134456\nTRAIN: \t Epoch: 170 \t Loss: -0.0136504721663454\nTRAIN: \t Epoch: 170 \t Loss: -0.0136774064352115\nTRAIN: \t Epoch: 170 \t Loss: -0.01370615075881544\nTRAIN: \t Epoch: 170 \t Loss: -0.01373539650812745\nTRAIN: \t Epoch: 170 \t Loss: -0.013740114241539743\nVALD: \t Epoch: 170 \t Loss: -0.012267055921256542\nVALD: \t Epoch: 170 \t Loss: -0.012574496679008007\nVALD: \t Epoch: 170 \t Loss: -0.012055153648058573\nVALD: \t Epoch: 170 \t Loss: -0.01222004508599639\nVALD: \t Epoch: 170 \t Loss: -0.011750504782426377\n******************************\nEpoch: social-tag : 170\ntrain_loss -0.013740114241539743\nval_loss -0.011750504782426377\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 171 \t Loss: -0.014514677226543427\nTRAIN: \t Epoch: 171 \t Loss: -0.014336029067635536\nTRAIN: \t Epoch: 171 \t Loss: -0.014270498106877008\nTRAIN: \t Epoch: 171 \t Loss: -0.014188385801389813\nTRAIN: \t Epoch: 171 \t Loss: -0.014013261720538139\nTRAIN: \t Epoch: 171 \t Loss: -0.013834057531009117\nTRAIN: \t Epoch: 171 \t Loss: -0.013881235915635313\nTRAIN: \t Epoch: 171 \t Loss: -0.013880809769034386\nTRAIN: \t Epoch: 171 \t Loss: -0.01390312011871073\nTRAIN: \t Epoch: 171 \t Loss: -0.013825303129851817\nTRAIN: \t Epoch: 171 \t Loss: -0.013914957557889547\nTRAIN: \t Epoch: 171 \t Loss: -0.013894268466780582\nTRAIN: \t Epoch: 171 \t Loss: -0.01389261156034011\nTRAIN: \t Epoch: 171 \t Loss: -0.013870895041951112\nTRAIN: \t Epoch: 171 \t Loss: -0.013801442335049312\nTRAIN: \t Epoch: 171 \t Loss: -0.013827023620251566\nTRAIN: \t Epoch: 171 \t Loss: -0.013748572153203627\nTRAIN: \t Epoch: 171 \t Loss: -0.013798263337877061\nTRAIN: \t Epoch: 171 \t Loss: -0.013743386750942782\nTRAIN: \t Epoch: 171 \t Loss: -0.013760295370593667\nTRAIN: \t Epoch: 171 \t Loss: -0.013742199105856138\nVALD: \t Epoch: 171 \t Loss: -0.012763089500367641\nVALD: \t Epoch: 171 \t Loss: -0.012953476049005985\nVALD: \t Epoch: 171 \t Loss: -0.012333402099708715\nVALD: \t Epoch: 171 \t Loss: -0.012557591777294874\nVALD: \t Epoch: 171 \t Loss: -0.012003819723636056\n******************************\nEpoch: social-tag : 171\ntrain_loss -0.013742199105856138\nval_loss -0.012003819723636056\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 172 \t Loss: -0.013532308861613274\nTRAIN: \t Epoch: 172 \t Loss: -0.01350529445335269\nTRAIN: \t Epoch: 172 \t Loss: -0.013682884785036245\nTRAIN: \t Epoch: 172 \t Loss: -0.013707402860745788\nTRAIN: \t Epoch: 172 \t Loss: -0.013826423138380051\nTRAIN: \t Epoch: 172 \t Loss: -0.013862362752358118\nTRAIN: \t Epoch: 172 \t Loss: -0.013866418041288853\nTRAIN: \t Epoch: 172 \t Loss: -0.013840723084285855\nTRAIN: \t Epoch: 172 \t Loss: -0.01384360519134336\nTRAIN: \t Epoch: 172 \t Loss: -0.013791368156671525\nTRAIN: \t Epoch: 172 \t Loss: -0.013793150979009542\nTRAIN: \t Epoch: 172 \t Loss: -0.013832690815130869\nTRAIN: \t Epoch: 172 \t Loss: -0.013777493570859615\nTRAIN: \t Epoch: 172 \t Loss: -0.013735841893191849\nTRAIN: \t Epoch: 172 \t Loss: -0.01375372614711523\nTRAIN: \t Epoch: 172 \t Loss: -0.013747509976383299\nTRAIN: \t Epoch: 172 \t Loss: -0.01376543374841704\nTRAIN: \t Epoch: 172 \t Loss: -0.013778305974685483\nTRAIN: \t Epoch: 172 \t Loss: -0.013794339850152793\nTRAIN: \t Epoch: 172 \t Loss: -0.013776666484773159\nTRAIN: \t Epoch: 172 \t Loss: -0.013781009036573706\nVALD: \t Epoch: 172 \t Loss: -0.0088669927790761\nVALD: \t Epoch: 172 \t Loss: -0.010785837657749653\nVALD: \t Epoch: 172 \t Loss: -0.009992755949497223\nVALD: \t Epoch: 172 \t Loss: -0.010676254285499454\nVALD: \t Epoch: 172 \t Loss: -0.010468539408439599\n******************************\nEpoch: social-tag : 172\ntrain_loss -0.013781009036573706\nval_loss -0.010468539408439599\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 173 \t Loss: -0.014557872898876667\nTRAIN: \t Epoch: 173 \t Loss: -0.013658538926392794\nTRAIN: \t Epoch: 173 \t Loss: -0.01383078284561634\nTRAIN: \t Epoch: 173 \t Loss: -0.014010365586727858\nTRAIN: \t Epoch: 173 \t Loss: -0.013982042670249939\nTRAIN: \t Epoch: 173 \t Loss: -0.014023295448472103\nTRAIN: \t Epoch: 173 \t Loss: -0.013983470653848988\nTRAIN: \t Epoch: 173 \t Loss: -0.013999033719301224\nTRAIN: \t Epoch: 173 \t Loss: -0.013933428666657872\nTRAIN: \t Epoch: 173 \t Loss: -0.013919597119092941\nTRAIN: \t Epoch: 173 \t Loss: -0.013901561752639034\nTRAIN: \t Epoch: 173 \t Loss: -0.013879707005495826\nTRAIN: \t Epoch: 173 \t Loss: -0.013830169677161254\nTRAIN: \t Epoch: 173 \t Loss: -0.013809397018381528\nTRAIN: \t Epoch: 173 \t Loss: -0.013817150456209978\nTRAIN: \t Epoch: 173 \t Loss: -0.013848887174390256\nTRAIN: \t Epoch: 173 \t Loss: -0.013836034299696194\nTRAIN: \t Epoch: 173 \t Loss: -0.01381723815575242\nTRAIN: \t Epoch: 173 \t Loss: -0.01380057985845365\nTRAIN: \t Epoch: 173 \t Loss: -0.013771832967177034\nTRAIN: \t Epoch: 173 \t Loss: -0.013783673908771141\nVALD: \t Epoch: 173 \t Loss: -0.00829806737601757\nVALD: \t Epoch: 173 \t Loss: -0.009570267982780933\nVALD: \t Epoch: 173 \t Loss: -0.007209679810330272\nVALD: \t Epoch: 173 \t Loss: -0.008351449912879616\nVALD: \t Epoch: 173 \t Loss: -0.008483020723729895\n******************************\nEpoch: social-tag : 173\ntrain_loss -0.013783673908771141\nval_loss -0.008483020723729895\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 174 \t Loss: -0.013326149433851242\nTRAIN: \t Epoch: 174 \t Loss: -0.013660718221217394\nTRAIN: \t Epoch: 174 \t Loss: -0.013452607207000256\nTRAIN: \t Epoch: 174 \t Loss: -0.013719410635530949\nTRAIN: \t Epoch: 174 \t Loss: -0.01360806804150343\nTRAIN: \t Epoch: 174 \t Loss: -0.013864934289207062\nTRAIN: \t Epoch: 174 \t Loss: -0.013797286631805556\nTRAIN: \t Epoch: 174 \t Loss: -0.01375612651463598\nTRAIN: \t Epoch: 174 \t Loss: -0.013826141444345316\nTRAIN: \t Epoch: 174 \t Loss: -0.013761468790471553\nTRAIN: \t Epoch: 174 \t Loss: -0.013730647127059374\nTRAIN: \t Epoch: 174 \t Loss: -0.013753774265448252\nTRAIN: \t Epoch: 174 \t Loss: -0.013774985805726968\nTRAIN: \t Epoch: 174 \t Loss: -0.013764098818813051\nTRAIN: \t Epoch: 174 \t Loss: -0.013720320723950862\nTRAIN: \t Epoch: 174 \t Loss: -0.013750171812716872\nTRAIN: \t Epoch: 174 \t Loss: -0.013818158713333747\nTRAIN: \t Epoch: 174 \t Loss: -0.013836389045334525\nTRAIN: \t Epoch: 174 \t Loss: -0.013830889016389847\nTRAIN: \t Epoch: 174 \t Loss: -0.013815419794991613\nTRAIN: \t Epoch: 174 \t Loss: -0.013804551044517787\nVALD: \t Epoch: 174 \t Loss: -0.009999661706387997\nVALD: \t Epoch: 174 \t Loss: -0.011176909320056438\nVALD: \t Epoch: 174 \t Loss: -0.009663684448848167\nVALD: \t Epoch: 174 \t Loss: -0.010472245165146887\nVALD: \t Epoch: 174 \t Loss: -0.010244697092427915\n******************************\nEpoch: social-tag : 174\ntrain_loss -0.013804551044517787\nval_loss -0.010244697092427915\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 175 \t Loss: -0.013981832191348076\nTRAIN: \t Epoch: 175 \t Loss: -0.01384276757016778\nTRAIN: \t Epoch: 175 \t Loss: -0.01387893594801426\nTRAIN: \t Epoch: 175 \t Loss: -0.013615138595923781\nTRAIN: \t Epoch: 175 \t Loss: -0.013763006031513213\nTRAIN: \t Epoch: 175 \t Loss: -0.013981877050052086\nTRAIN: \t Epoch: 175 \t Loss: -0.013900499258722578\nTRAIN: \t Epoch: 175 \t Loss: -0.013909403700381517\nTRAIN: \t Epoch: 175 \t Loss: -0.013939833268523216\nTRAIN: \t Epoch: 175 \t Loss: -0.013939123414456845\nTRAIN: \t Epoch: 175 \t Loss: -0.0139432412318208\nTRAIN: \t Epoch: 175 \t Loss: -0.013908212771639228\nTRAIN: \t Epoch: 175 \t Loss: -0.01386748433399659\nTRAIN: \t Epoch: 175 \t Loss: -0.013844730399016823\nTRAIN: \t Epoch: 175 \t Loss: -0.013834796908001105\nTRAIN: \t Epoch: 175 \t Loss: -0.01385321922134608\nTRAIN: \t Epoch: 175 \t Loss: -0.013891739880337435\nTRAIN: \t Epoch: 175 \t Loss: -0.013810213034351667\nTRAIN: \t Epoch: 175 \t Loss: -0.013802423141896725\nTRAIN: \t Epoch: 175 \t Loss: -0.013802884612232447\nTRAIN: \t Epoch: 175 \t Loss: -0.013806978578096919\nVALD: \t Epoch: 175 \t Loss: -0.010605012997984886\nVALD: \t Epoch: 175 \t Loss: -0.01128715556114912\nVALD: \t Epoch: 175 \t Loss: -0.010197875400384268\nVALD: \t Epoch: 175 \t Loss: -0.01081387815065682\nVALD: \t Epoch: 175 \t Loss: -0.010554441315348405\n******************************\nEpoch: social-tag : 175\ntrain_loss -0.013806978578096919\nval_loss -0.010554441315348405\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 176 \t Loss: -0.013771121390163898\nTRAIN: \t Epoch: 176 \t Loss: -0.013401239644736052\nTRAIN: \t Epoch: 176 \t Loss: -0.0137824018796285\nTRAIN: \t Epoch: 176 \t Loss: -0.013462606118991971\nTRAIN: \t Epoch: 176 \t Loss: -0.013517332635819912\nTRAIN: \t Epoch: 176 \t Loss: -0.013524210546165705\nTRAIN: \t Epoch: 176 \t Loss: -0.013570867744939668\nTRAIN: \t Epoch: 176 \t Loss: -0.013662262936122715\nTRAIN: \t Epoch: 176 \t Loss: -0.013642415300839476\nTRAIN: \t Epoch: 176 \t Loss: -0.013664759323000908\nTRAIN: \t Epoch: 176 \t Loss: -0.01371297799050808\nTRAIN: \t Epoch: 176 \t Loss: -0.01378639523560802\nTRAIN: \t Epoch: 176 \t Loss: -0.01380112344542375\nTRAIN: \t Epoch: 176 \t Loss: -0.013794260964329754\nTRAIN: \t Epoch: 176 \t Loss: -0.013804855880637963\nTRAIN: \t Epoch: 176 \t Loss: -0.013815268699545413\nTRAIN: \t Epoch: 176 \t Loss: -0.013802769648678163\nTRAIN: \t Epoch: 176 \t Loss: -0.013812159705493186\nTRAIN: \t Epoch: 176 \t Loss: -0.013848994956596902\nTRAIN: \t Epoch: 176 \t Loss: -0.013811844261363148\nTRAIN: \t Epoch: 176 \t Loss: -0.013811862436550438\nVALD: \t Epoch: 176 \t Loss: -0.010728243738412857\nVALD: \t Epoch: 176 \t Loss: -0.011124434880912304\nVALD: \t Epoch: 176 \t Loss: -0.009709161085387072\nVALD: \t Epoch: 176 \t Loss: -0.010325675597414374\nVALD: \t Epoch: 176 \t Loss: -0.010085751564990303\n******************************\nEpoch: social-tag : 176\ntrain_loss -0.013811862436550438\nval_loss -0.010085751564990303\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 177 \t Loss: -0.012699278071522713\nTRAIN: \t Epoch: 177 \t Loss: -0.013477987609803677\nTRAIN: \t Epoch: 177 \t Loss: -0.013446656366189321\nTRAIN: \t Epoch: 177 \t Loss: -0.013687666039913893\nTRAIN: \t Epoch: 177 \t Loss: -0.013630713894963264\nTRAIN: \t Epoch: 177 \t Loss: -0.013659149563560883\nTRAIN: \t Epoch: 177 \t Loss: -0.013654618257922786\nTRAIN: \t Epoch: 177 \t Loss: -0.013773387530818582\nTRAIN: \t Epoch: 177 \t Loss: -0.013869715440604422\nTRAIN: \t Epoch: 177 \t Loss: -0.013792338129132987\nTRAIN: \t Epoch: 177 \t Loss: -0.013820359398695555\nTRAIN: \t Epoch: 177 \t Loss: -0.013813761761412024\nTRAIN: \t Epoch: 177 \t Loss: -0.013786870699662428\nTRAIN: \t Epoch: 177 \t Loss: -0.013826817274093628\nTRAIN: \t Epoch: 177 \t Loss: -0.013762626859049002\nTRAIN: \t Epoch: 177 \t Loss: -0.013777189829852432\nTRAIN: \t Epoch: 177 \t Loss: -0.013821542975218856\nTRAIN: \t Epoch: 177 \t Loss: -0.01377455833264523\nTRAIN: \t Epoch: 177 \t Loss: -0.013758701419359759\nTRAIN: \t Epoch: 177 \t Loss: -0.013792888587340712\nTRAIN: \t Epoch: 177 \t Loss: -0.013810833279867768\nVALD: \t Epoch: 177 \t Loss: -0.00910281389951706\nVALD: \t Epoch: 177 \t Loss: -0.010380955878645182\nVALD: \t Epoch: 177 \t Loss: -0.008086769531170527\nVALD: \t Epoch: 177 \t Loss: -0.009014457231387496\nVALD: \t Epoch: 177 \t Loss: -0.008955633870645421\n******************************\nEpoch: social-tag : 177\ntrain_loss -0.013810833279867768\nval_loss -0.008955633870645421\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 178 \t Loss: -0.014582525007426739\nTRAIN: \t Epoch: 178 \t Loss: -0.014147407375276089\nTRAIN: \t Epoch: 178 \t Loss: -0.014463912695646286\nTRAIN: \t Epoch: 178 \t Loss: -0.0143775490578264\nTRAIN: \t Epoch: 178 \t Loss: -0.01421379428356886\nTRAIN: \t Epoch: 178 \t Loss: -0.01421502341205875\nTRAIN: \t Epoch: 178 \t Loss: -0.014214647933840752\nTRAIN: \t Epoch: 178 \t Loss: -0.014106279355473816\nTRAIN: \t Epoch: 178 \t Loss: -0.014147831540968683\nTRAIN: \t Epoch: 178 \t Loss: -0.01402089986950159\nTRAIN: \t Epoch: 178 \t Loss: -0.0140694673596458\nTRAIN: \t Epoch: 178 \t Loss: -0.014075769499565164\nTRAIN: \t Epoch: 178 \t Loss: -0.013993813871191097\nTRAIN: \t Epoch: 178 \t Loss: -0.013919812294521503\nTRAIN: \t Epoch: 178 \t Loss: -0.013978768326342105\nTRAIN: \t Epoch: 178 \t Loss: -0.013950520602520555\nTRAIN: \t Epoch: 178 \t Loss: -0.013904231326544987\nTRAIN: \t Epoch: 178 \t Loss: -0.013888813658720918\nTRAIN: \t Epoch: 178 \t Loss: -0.013936494624144152\nTRAIN: \t Epoch: 178 \t Loss: -0.013840632606297731\nTRAIN: \t Epoch: 178 \t Loss: -0.013828821198795057\nVALD: \t Epoch: 178 \t Loss: -0.013347381725907326\nVALD: \t Epoch: 178 \t Loss: -0.012889662757515907\nVALD: \t Epoch: 178 \t Loss: -0.012307230072716871\nVALD: \t Epoch: 178 \t Loss: -0.012401666957885027\nVALD: \t Epoch: 178 \t Loss: -0.011912738451443241\n******************************\nEpoch: social-tag : 178\ntrain_loss -0.013828821198795057\nval_loss -0.011912738451443241\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 179 \t Loss: -0.014036957174539566\nTRAIN: \t Epoch: 179 \t Loss: -0.013969045132398605\nTRAIN: \t Epoch: 179 \t Loss: -0.01375431939959526\nTRAIN: \t Epoch: 179 \t Loss: -0.013688922161236405\nTRAIN: \t Epoch: 179 \t Loss: -0.013732118345797063\nTRAIN: \t Epoch: 179 \t Loss: -0.013788541002819935\nTRAIN: \t Epoch: 179 \t Loss: -0.013785055838525295\nTRAIN: \t Epoch: 179 \t Loss: -0.013845527428202331\nTRAIN: \t Epoch: 179 \t Loss: -0.013772145120633973\nTRAIN: \t Epoch: 179 \t Loss: -0.013835285045206546\nTRAIN: \t Epoch: 179 \t Loss: -0.013895344327796589\nTRAIN: \t Epoch: 179 \t Loss: -0.013833123414466778\nTRAIN: \t Epoch: 179 \t Loss: -0.01379900647757145\nTRAIN: \t Epoch: 179 \t Loss: -0.0138335254575525\nTRAIN: \t Epoch: 179 \t Loss: -0.013839196972548961\nTRAIN: \t Epoch: 179 \t Loss: -0.013800220796838403\nTRAIN: \t Epoch: 179 \t Loss: -0.013811192749177708\nTRAIN: \t Epoch: 179 \t Loss: -0.013850530764708916\nTRAIN: \t Epoch: 179 \t Loss: -0.013823775378497024\nTRAIN: \t Epoch: 179 \t Loss: -0.013808166002854705\nTRAIN: \t Epoch: 179 \t Loss: -0.013784487762631319\nVALD: \t Epoch: 179 \t Loss: -0.006687636487185955\nVALD: \t Epoch: 179 \t Loss: -0.00821163272485137\nVALD: \t Epoch: 179 \t Loss: -0.004674784606322646\nVALD: \t Epoch: 179 \t Loss: -0.005546590138692409\nVALD: \t Epoch: 179 \t Loss: -0.006110865568769151\n******************************\nEpoch: social-tag : 179\ntrain_loss -0.013784487762631319\nval_loss -0.006110865568769151\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 180 \t Loss: -0.013886275701224804\nTRAIN: \t Epoch: 180 \t Loss: -0.013949078973382711\nTRAIN: \t Epoch: 180 \t Loss: -0.014205268894632658\nTRAIN: \t Epoch: 180 \t Loss: -0.014390196884050965\nTRAIN: \t Epoch: 180 \t Loss: -0.014156456291675567\nTRAIN: \t Epoch: 180 \t Loss: -0.014106942030290762\nTRAIN: \t Epoch: 180 \t Loss: -0.01395604679627078\nTRAIN: \t Epoch: 180 \t Loss: -0.0138934007845819\nTRAIN: \t Epoch: 180 \t Loss: -0.013885653267304102\nTRAIN: \t Epoch: 180 \t Loss: -0.013887524977326393\nTRAIN: \t Epoch: 180 \t Loss: -0.013894828713753006\nTRAIN: \t Epoch: 180 \t Loss: -0.013829665180916587\nTRAIN: \t Epoch: 180 \t Loss: -0.013794992620555254\nTRAIN: \t Epoch: 180 \t Loss: -0.01384736763845597\nTRAIN: \t Epoch: 180 \t Loss: -0.013849182551105817\nTRAIN: \t Epoch: 180 \t Loss: -0.013841760403010994\nTRAIN: \t Epoch: 180 \t Loss: -0.013827118584338357\nTRAIN: \t Epoch: 180 \t Loss: -0.013806008423368136\nTRAIN: \t Epoch: 180 \t Loss: -0.013797770303330924\nTRAIN: \t Epoch: 180 \t Loss: -0.013847774360328913\nTRAIN: \t Epoch: 180 \t Loss: -0.013861070344699928\nVALD: \t Epoch: 180 \t Loss: -0.011465710587799549\nVALD: \t Epoch: 180 \t Loss: -0.011815206613391638\nVALD: \t Epoch: 180 \t Loss: -0.010695249773561954\nVALD: \t Epoch: 180 \t Loss: -0.011262166080996394\nVALD: \t Epoch: 180 \t Loss: -0.010898070058960846\n******************************\nEpoch: social-tag : 180\ntrain_loss -0.013861070344699928\nval_loss -0.010898070058960846\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 181 \t Loss: -0.013220880180597305\nTRAIN: \t Epoch: 181 \t Loss: -0.01343480683863163\nTRAIN: \t Epoch: 181 \t Loss: -0.013398480912049612\nTRAIN: \t Epoch: 181 \t Loss: -0.013359226519241929\nTRAIN: \t Epoch: 181 \t Loss: -0.013486163690686226\nTRAIN: \t Epoch: 181 \t Loss: -0.013499436434358358\nTRAIN: \t Epoch: 181 \t Loss: -0.013469154946506023\nTRAIN: \t Epoch: 181 \t Loss: -0.013658288167789578\nTRAIN: \t Epoch: 181 \t Loss: -0.013803046921061145\nTRAIN: \t Epoch: 181 \t Loss: -0.013826977740973235\nTRAIN: \t Epoch: 181 \t Loss: -0.013880709778856148\nTRAIN: \t Epoch: 181 \t Loss: -0.013883231750999888\nTRAIN: \t Epoch: 181 \t Loss: -0.013841836355053462\nTRAIN: \t Epoch: 181 \t Loss: -0.013884061016142368\nTRAIN: \t Epoch: 181 \t Loss: -0.013891899523635705\nTRAIN: \t Epoch: 181 \t Loss: -0.013930556189734489\nTRAIN: \t Epoch: 181 \t Loss: -0.01387906726449728\nTRAIN: \t Epoch: 181 \t Loss: -0.013886253866884444\nTRAIN: \t Epoch: 181 \t Loss: -0.013824012277549818\nTRAIN: \t Epoch: 181 \t Loss: -0.013830142607912422\nTRAIN: \t Epoch: 181 \t Loss: -0.013829055894956097\nVALD: \t Epoch: 181 \t Loss: -0.00963252317160368\nVALD: \t Epoch: 181 \t Loss: -0.01079990854486823\nVALD: \t Epoch: 181 \t Loss: -0.009471921095003685\nVALD: \t Epoch: 181 \t Loss: -0.00992995931301266\nVALD: \t Epoch: 181 \t Loss: -0.009849910670816418\n******************************\nEpoch: social-tag : 181\ntrain_loss -0.013829055894956097\nval_loss -0.009849910670816418\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 182 \t Loss: -0.01378945168107748\nTRAIN: \t Epoch: 182 \t Loss: -0.013625905383378267\nTRAIN: \t Epoch: 182 \t Loss: -0.01379296276718378\nTRAIN: \t Epoch: 182 \t Loss: -0.013866553083062172\nTRAIN: \t Epoch: 182 \t Loss: -0.01380877960473299\nTRAIN: \t Epoch: 182 \t Loss: -0.013813020351032415\nTRAIN: \t Epoch: 182 \t Loss: -0.013876799094889845\nTRAIN: \t Epoch: 182 \t Loss: -0.013731674756854773\nTRAIN: \t Epoch: 182 \t Loss: -0.013750008100436794\nTRAIN: \t Epoch: 182 \t Loss: -0.013722228724509478\nTRAIN: \t Epoch: 182 \t Loss: -0.013760400682010433\nTRAIN: \t Epoch: 182 \t Loss: -0.013796045755346617\nTRAIN: \t Epoch: 182 \t Loss: -0.013797864102973388\nTRAIN: \t Epoch: 182 \t Loss: -0.013801998325756617\nTRAIN: \t Epoch: 182 \t Loss: -0.013828184083104133\nTRAIN: \t Epoch: 182 \t Loss: -0.01380347937811166\nTRAIN: \t Epoch: 182 \t Loss: -0.01383031636257382\nTRAIN: \t Epoch: 182 \t Loss: -0.013834898567034138\nTRAIN: \t Epoch: 182 \t Loss: -0.013853068855640135\nTRAIN: \t Epoch: 182 \t Loss: -0.013844495220109821\nTRAIN: \t Epoch: 182 \t Loss: -0.013849071394321821\nVALD: \t Epoch: 182 \t Loss: -0.00631422083824873\nVALD: \t Epoch: 182 \t Loss: -0.0093454010784626\nVALD: \t Epoch: 182 \t Loss: -0.007962969597429037\nVALD: \t Epoch: 182 \t Loss: -0.00914409093093127\nVALD: \t Epoch: 182 \t Loss: -0.009127842919262136\n******************************\nEpoch: social-tag : 182\ntrain_loss -0.013849071394321821\nval_loss -0.009127842919262136\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 183 \t Loss: -0.01378612406551838\nTRAIN: \t Epoch: 183 \t Loss: -0.014199989847838879\nTRAIN: \t Epoch: 183 \t Loss: -0.013886125447849432\nTRAIN: \t Epoch: 183 \t Loss: -0.01393589237704873\nTRAIN: \t Epoch: 183 \t Loss: -0.013731852173805237\nTRAIN: \t Epoch: 183 \t Loss: -0.013869975538303455\nTRAIN: \t Epoch: 183 \t Loss: -0.013950086065701075\nTRAIN: \t Epoch: 183 \t Loss: -0.013934775604866445\nTRAIN: \t Epoch: 183 \t Loss: -0.01405366613633103\nTRAIN: \t Epoch: 183 \t Loss: -0.01406020363792777\nTRAIN: \t Epoch: 183 \t Loss: -0.013977664268829605\nTRAIN: \t Epoch: 183 \t Loss: -0.013915159972384572\nTRAIN: \t Epoch: 183 \t Loss: -0.013830570575709526\nTRAIN: \t Epoch: 183 \t Loss: -0.013793603916253363\nTRAIN: \t Epoch: 183 \t Loss: -0.013776367468138536\nTRAIN: \t Epoch: 183 \t Loss: -0.013700743089430034\nTRAIN: \t Epoch: 183 \t Loss: -0.013721987276392825\nTRAIN: \t Epoch: 183 \t Loss: -0.013768171581129232\nTRAIN: \t Epoch: 183 \t Loss: -0.013813324909853307\nTRAIN: \t Epoch: 183 \t Loss: -0.013876204332336783\nTRAIN: \t Epoch: 183 \t Loss: -0.01385459286622479\nVALD: \t Epoch: 183 \t Loss: -0.010812236927449703\nVALD: \t Epoch: 183 \t Loss: -0.011535498779267073\nVALD: \t Epoch: 183 \t Loss: -0.010491674765944481\nVALD: \t Epoch: 183 \t Loss: -0.0111213109921664\nVALD: \t Epoch: 183 \t Loss: -0.010797060822135201\n******************************\nEpoch: social-tag : 183\ntrain_loss -0.01385459286622479\nval_loss -0.010797060822135201\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 184 \t Loss: -0.0151755902916193\nTRAIN: \t Epoch: 184 \t Loss: -0.014351025223731995\nTRAIN: \t Epoch: 184 \t Loss: -0.014094437782963118\nTRAIN: \t Epoch: 184 \t Loss: -0.013961415970697999\nTRAIN: \t Epoch: 184 \t Loss: -0.013996869511902332\nTRAIN: \t Epoch: 184 \t Loss: -0.01404121145606041\nTRAIN: \t Epoch: 184 \t Loss: -0.013950348964759282\nTRAIN: \t Epoch: 184 \t Loss: -0.014003716292791069\nTRAIN: \t Epoch: 184 \t Loss: -0.014011273367537392\nTRAIN: \t Epoch: 184 \t Loss: -0.01402915483340621\nTRAIN: \t Epoch: 184 \t Loss: -0.014003698036751965\nTRAIN: \t Epoch: 184 \t Loss: -0.0139784364340206\nTRAIN: \t Epoch: 184 \t Loss: -0.014020463809944116\nTRAIN: \t Epoch: 184 \t Loss: -0.014113814237394504\nTRAIN: \t Epoch: 184 \t Loss: -0.014116325539847215\nTRAIN: \t Epoch: 184 \t Loss: -0.014071739569772035\nTRAIN: \t Epoch: 184 \t Loss: -0.01396638439858661\nTRAIN: \t Epoch: 184 \t Loss: -0.013920131294677654\nTRAIN: \t Epoch: 184 \t Loss: -0.013911256174507895\nTRAIN: \t Epoch: 184 \t Loss: -0.013861826714128255\nTRAIN: \t Epoch: 184 \t Loss: -0.013861855774911074\nVALD: \t Epoch: 184 \t Loss: -0.01254323497414589\nVALD: \t Epoch: 184 \t Loss: -0.012261276599019766\nVALD: \t Epoch: 184 \t Loss: -0.01186654002716144\nVALD: \t Epoch: 184 \t Loss: -0.012210920685902238\nVALD: \t Epoch: 184 \t Loss: -0.01160529798163692\n******************************\nEpoch: social-tag : 184\ntrain_loss -0.013861855774911074\nval_loss -0.01160529798163692\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 185 \t Loss: -0.013473290018737316\nTRAIN: \t Epoch: 185 \t Loss: -0.0132737229578197\nTRAIN: \t Epoch: 185 \t Loss: -0.01364166879405578\nTRAIN: \t Epoch: 185 \t Loss: -0.013690515886992216\nTRAIN: \t Epoch: 185 \t Loss: -0.013616499863564968\nTRAIN: \t Epoch: 185 \t Loss: -0.013747243055452904\nTRAIN: \t Epoch: 185 \t Loss: -0.013635420107415743\nTRAIN: \t Epoch: 185 \t Loss: -0.013633129419758916\nTRAIN: \t Epoch: 185 \t Loss: -0.013607542030513287\nTRAIN: \t Epoch: 185 \t Loss: -0.013638393487781286\nTRAIN: \t Epoch: 185 \t Loss: -0.01376825317063115\nTRAIN: \t Epoch: 185 \t Loss: -0.013820365459347764\nTRAIN: \t Epoch: 185 \t Loss: -0.013884113600047735\nTRAIN: \t Epoch: 185 \t Loss: -0.01389584583895547\nTRAIN: \t Epoch: 185 \t Loss: -0.013849836029112338\nTRAIN: \t Epoch: 185 \t Loss: -0.013810435368213803\nTRAIN: \t Epoch: 185 \t Loss: -0.013820176725001895\nTRAIN: \t Epoch: 185 \t Loss: -0.013776347713751925\nTRAIN: \t Epoch: 185 \t Loss: -0.013809580257848689\nTRAIN: \t Epoch: 185 \t Loss: -0.01385262799449265\nTRAIN: \t Epoch: 185 \t Loss: -0.01383500169954028\nVALD: \t Epoch: 185 \t Loss: -0.0137016661465168\nVALD: \t Epoch: 185 \t Loss: -0.013678161893039942\nVALD: \t Epoch: 185 \t Loss: -0.013471903589864572\nVALD: \t Epoch: 185 \t Loss: -0.0136064481921494\nVALD: \t Epoch: 185 \t Loss: -0.012872899979976832\n******************************\nEpoch: social-tag : 185\ntrain_loss -0.01383500169954028\nval_loss -0.012872899979976832\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 186 \t Loss: -0.014123042114078999\nTRAIN: \t Epoch: 186 \t Loss: -0.013947359751909971\nTRAIN: \t Epoch: 186 \t Loss: -0.014042623030642668\nTRAIN: \t Epoch: 186 \t Loss: -0.013979757903143764\nTRAIN: \t Epoch: 186 \t Loss: -0.014046164602041245\nTRAIN: \t Epoch: 186 \t Loss: -0.014076117581377426\nTRAIN: \t Epoch: 186 \t Loss: -0.014149953877287251\nTRAIN: \t Epoch: 186 \t Loss: -0.013916945783421397\nTRAIN: \t Epoch: 186 \t Loss: -0.0137775508272979\nTRAIN: \t Epoch: 186 \t Loss: -0.013844425324350595\nTRAIN: \t Epoch: 186 \t Loss: -0.013823962008411234\nTRAIN: \t Epoch: 186 \t Loss: -0.013836426194757223\nTRAIN: \t Epoch: 186 \t Loss: -0.013827828905330254\nTRAIN: \t Epoch: 186 \t Loss: -0.013827190734446049\nTRAIN: \t Epoch: 186 \t Loss: -0.013876193575561047\nTRAIN: \t Epoch: 186 \t Loss: -0.013850455812644213\nTRAIN: \t Epoch: 186 \t Loss: -0.0138792392304715\nTRAIN: \t Epoch: 186 \t Loss: -0.013904375645021597\nTRAIN: \t Epoch: 186 \t Loss: -0.013888209813127392\nTRAIN: \t Epoch: 186 \t Loss: -0.01388852410018444\nTRAIN: \t Epoch: 186 \t Loss: -0.013852189631763568\nVALD: \t Epoch: 186 \t Loss: -0.008034562692046165\nVALD: \t Epoch: 186 \t Loss: -0.009959067683666945\nVALD: \t Epoch: 186 \t Loss: -0.008469867520034313\nVALD: \t Epoch: 186 \t Loss: -0.009421680122613907\nVALD: \t Epoch: 186 \t Loss: -0.009358475173729053\n******************************\nEpoch: social-tag : 186\ntrain_loss -0.013852189631763568\nval_loss -0.009358475173729053\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 187 \t Loss: -0.012924562208354473\nTRAIN: \t Epoch: 187 \t Loss: -0.013130111154168844\nTRAIN: \t Epoch: 187 \t Loss: -0.013335617259144783\nTRAIN: \t Epoch: 187 \t Loss: -0.013461192371323705\nTRAIN: \t Epoch: 187 \t Loss: -0.013515653274953365\nTRAIN: \t Epoch: 187 \t Loss: -0.013376353463778893\nTRAIN: \t Epoch: 187 \t Loss: -0.013296434389693397\nTRAIN: \t Epoch: 187 \t Loss: -0.013422009535133839\nTRAIN: \t Epoch: 187 \t Loss: -0.01352122902042336\nTRAIN: \t Epoch: 187 \t Loss: -0.01360576432198286\nTRAIN: \t Epoch: 187 \t Loss: -0.013591211458498781\nTRAIN: \t Epoch: 187 \t Loss: -0.013690292369574308\nTRAIN: \t Epoch: 187 \t Loss: -0.013748408366854373\nTRAIN: \t Epoch: 187 \t Loss: -0.013760782246078764\nTRAIN: \t Epoch: 187 \t Loss: -0.013745409809052945\nTRAIN: \t Epoch: 187 \t Loss: -0.013747173070441931\nTRAIN: \t Epoch: 187 \t Loss: -0.013748460504062036\nTRAIN: \t Epoch: 187 \t Loss: -0.013736820055378808\nTRAIN: \t Epoch: 187 \t Loss: -0.013802138304239825\nTRAIN: \t Epoch: 187 \t Loss: -0.01383182886056602\nTRAIN: \t Epoch: 187 \t Loss: -0.01383960651450646\nVALD: \t Epoch: 187 \t Loss: -0.013337623327970505\nVALD: \t Epoch: 187 \t Loss: -0.01321684243157506\nVALD: \t Epoch: 187 \t Loss: -0.012763527842859427\nVALD: \t Epoch: 187 \t Loss: -0.012933303834870458\nVALD: \t Epoch: 187 \t Loss: -0.012360538261524145\n******************************\nEpoch: social-tag : 187\ntrain_loss -0.01383960651450646\nval_loss -0.012360538261524145\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 188 \t Loss: -0.013194024562835693\nTRAIN: \t Epoch: 188 \t Loss: -0.013547121081501245\nTRAIN: \t Epoch: 188 \t Loss: -0.013661866076290607\nTRAIN: \t Epoch: 188 \t Loss: -0.013462919741868973\nTRAIN: \t Epoch: 188 \t Loss: -0.01325712576508522\nTRAIN: \t Epoch: 188 \t Loss: -0.013250434771180153\nTRAIN: \t Epoch: 188 \t Loss: -0.01344407509480204\nTRAIN: \t Epoch: 188 \t Loss: -0.013550435658544302\nTRAIN: \t Epoch: 188 \t Loss: -0.013568155157069365\nTRAIN: \t Epoch: 188 \t Loss: -0.013623819034546614\nTRAIN: \t Epoch: 188 \t Loss: -0.013690309429710562\nTRAIN: \t Epoch: 188 \t Loss: -0.01367615326307714\nTRAIN: \t Epoch: 188 \t Loss: -0.013728451700164722\nTRAIN: \t Epoch: 188 \t Loss: -0.013740265143236943\nTRAIN: \t Epoch: 188 \t Loss: -0.01373435699691375\nTRAIN: \t Epoch: 188 \t Loss: -0.01372044306481257\nTRAIN: \t Epoch: 188 \t Loss: -0.013780425740953754\nTRAIN: \t Epoch: 188 \t Loss: -0.01377317707778679\nTRAIN: \t Epoch: 188 \t Loss: -0.013772758312131228\nTRAIN: \t Epoch: 188 \t Loss: -0.013810746232047677\nTRAIN: \t Epoch: 188 \t Loss: -0.0138328301511366\nVALD: \t Epoch: 188 \t Loss: -0.011855312623083591\nVALD: \t Epoch: 188 \t Loss: -0.011784831993281841\nVALD: \t Epoch: 188 \t Loss: -0.010203358562042316\nVALD: \t Epoch: 188 \t Loss: -0.010544191696681082\nVALD: \t Epoch: 188 \t Loss: -0.010312846605328546\n******************************\nEpoch: social-tag : 188\ntrain_loss -0.0138328301511366\nval_loss -0.010312846605328546\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 189 \t Loss: -0.014007654041051865\nTRAIN: \t Epoch: 189 \t Loss: -0.013866939581930637\nTRAIN: \t Epoch: 189 \t Loss: -0.013838903357585272\nTRAIN: \t Epoch: 189 \t Loss: -0.01354506192728877\nTRAIN: \t Epoch: 189 \t Loss: -0.013770250603556633\nTRAIN: \t Epoch: 189 \t Loss: -0.013809683732688427\nTRAIN: \t Epoch: 189 \t Loss: -0.013872485341770309\nTRAIN: \t Epoch: 189 \t Loss: -0.013905758038163185\nTRAIN: \t Epoch: 189 \t Loss: -0.013982819496757455\nTRAIN: \t Epoch: 189 \t Loss: -0.013969121035188437\nTRAIN: \t Epoch: 189 \t Loss: -0.01393649930303747\nTRAIN: \t Epoch: 189 \t Loss: -0.013863585035627088\nTRAIN: \t Epoch: 189 \t Loss: -0.0139048990005484\nTRAIN: \t Epoch: 189 \t Loss: -0.013863501005939074\nTRAIN: \t Epoch: 189 \t Loss: -0.013870915211737157\nTRAIN: \t Epoch: 189 \t Loss: -0.013897815078962594\nTRAIN: \t Epoch: 189 \t Loss: -0.013926089686505935\nTRAIN: \t Epoch: 189 \t Loss: -0.013903406655622853\nTRAIN: \t Epoch: 189 \t Loss: -0.013892607665375659\nTRAIN: \t Epoch: 189 \t Loss: -0.013886251812800765\nTRAIN: \t Epoch: 189 \t Loss: -0.013875690922887856\nVALD: \t Epoch: 189 \t Loss: -0.012808740139007568\nVALD: \t Epoch: 189 \t Loss: -0.01252342015504837\nVALD: \t Epoch: 189 \t Loss: -0.012009321401516596\nVALD: \t Epoch: 189 \t Loss: -0.012109315488487482\nVALD: \t Epoch: 189 \t Loss: -0.011680903066183634\n******************************\nEpoch: social-tag : 189\ntrain_loss -0.013875690922887856\nval_loss -0.011680903066183634\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 190 \t Loss: -0.014144686050713062\nTRAIN: \t Epoch: 190 \t Loss: -0.014105554204434156\nTRAIN: \t Epoch: 190 \t Loss: -0.014040211526056131\nTRAIN: \t Epoch: 190 \t Loss: -0.0141401884611696\nTRAIN: \t Epoch: 190 \t Loss: -0.014314479567110539\nTRAIN: \t Epoch: 190 \t Loss: -0.014252965804189444\nTRAIN: \t Epoch: 190 \t Loss: -0.014131485218448299\nTRAIN: \t Epoch: 190 \t Loss: -0.014054636470973492\nTRAIN: \t Epoch: 190 \t Loss: -0.01395514721257819\nTRAIN: \t Epoch: 190 \t Loss: -0.013998985942453145\nTRAIN: \t Epoch: 190 \t Loss: -0.014024808918210592\nTRAIN: \t Epoch: 190 \t Loss: -0.014012389971564213\nTRAIN: \t Epoch: 190 \t Loss: -0.013984246680942865\nTRAIN: \t Epoch: 190 \t Loss: -0.014023163222840853\nTRAIN: \t Epoch: 190 \t Loss: -0.01405028613905112\nTRAIN: \t Epoch: 190 \t Loss: -0.01406337838852778\nTRAIN: \t Epoch: 190 \t Loss: -0.014018006951493375\nTRAIN: \t Epoch: 190 \t Loss: -0.013948144287698798\nTRAIN: \t Epoch: 190 \t Loss: -0.013855617787492903\nTRAIN: \t Epoch: 190 \t Loss: -0.013825486321002245\nTRAIN: \t Epoch: 190 \t Loss: -0.013837157416178247\nVALD: \t Epoch: 190 \t Loss: -0.0131241949275136\nVALD: \t Epoch: 190 \t Loss: -0.013107380829751492\nVALD: \t Epoch: 190 \t Loss: -0.012456047969559828\nVALD: \t Epoch: 190 \t Loss: -0.012580527225509286\nVALD: \t Epoch: 190 \t Loss: -0.012078111106454846\n******************************\nEpoch: social-tag : 190\ntrain_loss -0.013837157416178247\nval_loss -0.012078111106454846\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 191 \t Loss: -0.014044651761651039\nTRAIN: \t Epoch: 191 \t Loss: -0.013934811111539602\nTRAIN: \t Epoch: 191 \t Loss: -0.014183057472109795\nTRAIN: \t Epoch: 191 \t Loss: -0.014168396359309554\nTRAIN: \t Epoch: 191 \t Loss: -0.014194992557168006\nTRAIN: \t Epoch: 191 \t Loss: -0.014069112638632456\nTRAIN: \t Epoch: 191 \t Loss: -0.014005259743758611\nTRAIN: \t Epoch: 191 \t Loss: -0.013958303607068956\nTRAIN: \t Epoch: 191 \t Loss: -0.013945624232292175\nTRAIN: \t Epoch: 191 \t Loss: -0.013899798225611448\nTRAIN: \t Epoch: 191 \t Loss: -0.013896672901782122\nTRAIN: \t Epoch: 191 \t Loss: -0.013893337920308113\nTRAIN: \t Epoch: 191 \t Loss: -0.013956656392950278\nTRAIN: \t Epoch: 191 \t Loss: -0.013853061012923717\nTRAIN: \t Epoch: 191 \t Loss: -0.013798740319907665\nTRAIN: \t Epoch: 191 \t Loss: -0.013803304987959564\nTRAIN: \t Epoch: 191 \t Loss: -0.013847509125138031\nTRAIN: \t Epoch: 191 \t Loss: -0.013847202476527955\nTRAIN: \t Epoch: 191 \t Loss: -0.013820509328261801\nTRAIN: \t Epoch: 191 \t Loss: -0.013826612196862698\nTRAIN: \t Epoch: 191 \t Loss: -0.013853044099043034\nVALD: \t Epoch: 191 \t Loss: -0.006451963447034359\nVALD: \t Epoch: 191 \t Loss: -0.008655758574604988\nVALD: \t Epoch: 191 \t Loss: -0.0044317600938181085\nVALD: \t Epoch: 191 \t Loss: -0.006051035015843809\nVALD: \t Epoch: 191 \t Loss: -0.006555610904755032\n******************************\nEpoch: social-tag : 191\ntrain_loss -0.013853044099043034\nval_loss -0.006555610904755032\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 192 \t Loss: -0.01366747822612524\nTRAIN: \t Epoch: 192 \t Loss: -0.013792537152767181\nTRAIN: \t Epoch: 192 \t Loss: -0.014219063644607862\nTRAIN: \t Epoch: 192 \t Loss: -0.014250551350414753\nTRAIN: \t Epoch: 192 \t Loss: -0.014228207804262639\nTRAIN: \t Epoch: 192 \t Loss: -0.014300402253866196\nTRAIN: \t Epoch: 192 \t Loss: -0.01412601356527635\nTRAIN: \t Epoch: 192 \t Loss: -0.014152262010611594\nTRAIN: \t Epoch: 192 \t Loss: -0.014057245415945848\nTRAIN: \t Epoch: 192 \t Loss: -0.013896647561341523\nTRAIN: \t Epoch: 192 \t Loss: -0.013869061896746809\nTRAIN: \t Epoch: 192 \t Loss: -0.013851432440181574\nTRAIN: \t Epoch: 192 \t Loss: -0.013895267692322914\nTRAIN: \t Epoch: 192 \t Loss: -0.013936722784170083\nTRAIN: \t Epoch: 192 \t Loss: -0.01389402790615956\nTRAIN: \t Epoch: 192 \t Loss: -0.013925459992606193\nTRAIN: \t Epoch: 192 \t Loss: -0.013893905424458139\nTRAIN: \t Epoch: 192 \t Loss: -0.013902410192208158\nTRAIN: \t Epoch: 192 \t Loss: -0.01390695861099582\nTRAIN: \t Epoch: 192 \t Loss: -0.01386090717278421\nTRAIN: \t Epoch: 192 \t Loss: -0.0138650318936559\nVALD: \t Epoch: 192 \t Loss: -0.01256136316806078\nVALD: \t Epoch: 192 \t Loss: -0.012787073850631714\nVALD: \t Epoch: 192 \t Loss: -0.01225200699021419\nVALD: \t Epoch: 192 \t Loss: -0.012420563958585262\nVALD: \t Epoch: 192 \t Loss: -0.01195629572138886\n******************************\nEpoch: social-tag : 192\ntrain_loss -0.0138650318936559\nval_loss -0.01195629572138886\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 193 \t Loss: -0.014262424781918526\nTRAIN: \t Epoch: 193 \t Loss: -0.01405745791271329\nTRAIN: \t Epoch: 193 \t Loss: -0.013810173918803533\nTRAIN: \t Epoch: 193 \t Loss: -0.013709606602787971\nTRAIN: \t Epoch: 193 \t Loss: -0.013665762729942799\nTRAIN: \t Epoch: 193 \t Loss: -0.013718195880452791\nTRAIN: \t Epoch: 193 \t Loss: -0.013682483429355281\nTRAIN: \t Epoch: 193 \t Loss: -0.013727017911151052\nTRAIN: \t Epoch: 193 \t Loss: -0.013778513193958335\nTRAIN: \t Epoch: 193 \t Loss: -0.013783561624586582\nTRAIN: \t Epoch: 193 \t Loss: -0.013803297484462912\nTRAIN: \t Epoch: 193 \t Loss: -0.01381021939838926\nTRAIN: \t Epoch: 193 \t Loss: -0.01384780090302229\nTRAIN: \t Epoch: 193 \t Loss: -0.013790077662893705\nTRAIN: \t Epoch: 193 \t Loss: -0.013810617973407109\nTRAIN: \t Epoch: 193 \t Loss: -0.013847498572431505\nTRAIN: \t Epoch: 193 \t Loss: -0.013877429287223256\nTRAIN: \t Epoch: 193 \t Loss: -0.013862368598994281\nTRAIN: \t Epoch: 193 \t Loss: -0.013804178881017785\nTRAIN: \t Epoch: 193 \t Loss: -0.01385046299546957\nTRAIN: \t Epoch: 193 \t Loss: -0.013865968311577461\nVALD: \t Epoch: 193 \t Loss: -0.00976994726806879\nVALD: \t Epoch: 193 \t Loss: -0.010999782476574183\nVALD: \t Epoch: 193 \t Loss: -0.008942152839154005\nVALD: \t Epoch: 193 \t Loss: -0.009892290341667831\nVALD: \t Epoch: 193 \t Loss: -0.009776028745992172\n******************************\nEpoch: social-tag : 193\ntrain_loss -0.013865968311577461\nval_loss -0.009776028745992172\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 194 \t Loss: -0.014361787587404251\nTRAIN: \t Epoch: 194 \t Loss: -0.013903106097131968\nTRAIN: \t Epoch: 194 \t Loss: -0.01395670510828495\nTRAIN: \t Epoch: 194 \t Loss: -0.014293030370026827\nTRAIN: \t Epoch: 194 \t Loss: -0.014036532677710056\nTRAIN: \t Epoch: 194 \t Loss: -0.013982510504623255\nTRAIN: \t Epoch: 194 \t Loss: -0.014006545900234155\nTRAIN: \t Epoch: 194 \t Loss: -0.01401010958943516\nTRAIN: \t Epoch: 194 \t Loss: -0.01399012551539474\nTRAIN: \t Epoch: 194 \t Loss: -0.013977900706231594\nTRAIN: \t Epoch: 194 \t Loss: -0.013928045349364931\nTRAIN: \t Epoch: 194 \t Loss: -0.013948470043639341\nTRAIN: \t Epoch: 194 \t Loss: -0.013945899402292876\nTRAIN: \t Epoch: 194 \t Loss: -0.013964904272662742\nTRAIN: \t Epoch: 194 \t Loss: -0.013913063270350298\nTRAIN: \t Epoch: 194 \t Loss: -0.01393755862955004\nTRAIN: \t Epoch: 194 \t Loss: -0.013951688676195987\nTRAIN: \t Epoch: 194 \t Loss: -0.013925633858889341\nTRAIN: \t Epoch: 194 \t Loss: -0.013898865270771478\nTRAIN: \t Epoch: 194 \t Loss: -0.01392532978206873\nTRAIN: \t Epoch: 194 \t Loss: -0.01390663251200362\nVALD: \t Epoch: 194 \t Loss: -0.011929403059184551\nVALD: \t Epoch: 194 \t Loss: -0.012647988274693489\nVALD: \t Epoch: 194 \t Loss: -0.01229534981151422\nVALD: \t Epoch: 194 \t Loss: -0.012475154362618923\nVALD: \t Epoch: 194 \t Loss: -0.011928485787433126\n******************************\nEpoch: social-tag : 194\ntrain_loss -0.01390663251200362\nval_loss -0.011928485787433126\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 195 \t Loss: -0.014100410975515842\nTRAIN: \t Epoch: 195 \t Loss: -0.013852682430297136\nTRAIN: \t Epoch: 195 \t Loss: -0.013609236416717371\nTRAIN: \t Epoch: 195 \t Loss: -0.01361879101023078\nTRAIN: \t Epoch: 195 \t Loss: -0.013913104124367238\nTRAIN: \t Epoch: 195 \t Loss: -0.014127865278472504\nTRAIN: \t Epoch: 195 \t Loss: -0.014100816499974047\nTRAIN: \t Epoch: 195 \t Loss: -0.013990504434332252\nTRAIN: \t Epoch: 195 \t Loss: -0.013865740969777107\nTRAIN: \t Epoch: 195 \t Loss: -0.013926791679114103\nTRAIN: \t Epoch: 195 \t Loss: -0.013926698318259283\nTRAIN: \t Epoch: 195 \t Loss: -0.013945208940034112\nTRAIN: \t Epoch: 195 \t Loss: -0.013907844773851909\nTRAIN: \t Epoch: 195 \t Loss: -0.013914236266698157\nTRAIN: \t Epoch: 195 \t Loss: -0.013887248809138934\nTRAIN: \t Epoch: 195 \t Loss: -0.013841036998201162\nTRAIN: \t Epoch: 195 \t Loss: -0.013868252527626121\nTRAIN: \t Epoch: 195 \t Loss: -0.013910710604654418\nTRAIN: \t Epoch: 195 \t Loss: -0.013856802625875724\nTRAIN: \t Epoch: 195 \t Loss: -0.013845127960667013\nTRAIN: \t Epoch: 195 \t Loss: -0.013838193971741264\nVALD: \t Epoch: 195 \t Loss: -0.01202489621937275\nVALD: \t Epoch: 195 \t Loss: -0.01257701264694333\nVALD: \t Epoch: 195 \t Loss: -0.012197869208951792\nVALD: \t Epoch: 195 \t Loss: -0.01251371274702251\nVALD: \t Epoch: 195 \t Loss: -0.012005451220821067\n******************************\nEpoch: social-tag : 195\ntrain_loss -0.013838193971741264\nval_loss -0.012005451220821067\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 196 \t Loss: -0.013407447375357151\nTRAIN: \t Epoch: 196 \t Loss: -0.01330258371308446\nTRAIN: \t Epoch: 196 \t Loss: -0.013386337086558342\nTRAIN: \t Epoch: 196 \t Loss: -0.01367431622929871\nTRAIN: \t Epoch: 196 \t Loss: -0.013759183511137963\nTRAIN: \t Epoch: 196 \t Loss: -0.013689360115677118\nTRAIN: \t Epoch: 196 \t Loss: -0.013749200052448682\nTRAIN: \t Epoch: 196 \t Loss: -0.0137670140247792\nTRAIN: \t Epoch: 196 \t Loss: -0.013757794785002867\nTRAIN: \t Epoch: 196 \t Loss: -0.013846258446574212\nTRAIN: \t Epoch: 196 \t Loss: -0.013809535483067686\nTRAIN: \t Epoch: 196 \t Loss: -0.013879752407471338\nTRAIN: \t Epoch: 196 \t Loss: -0.013955699924666148\nTRAIN: \t Epoch: 196 \t Loss: -0.013937406042324645\nTRAIN: \t Epoch: 196 \t Loss: -0.013978829359014828\nTRAIN: \t Epoch: 196 \t Loss: -0.01390783692477271\nTRAIN: \t Epoch: 196 \t Loss: -0.013907549409743617\nTRAIN: \t Epoch: 196 \t Loss: -0.01389745617699292\nTRAIN: \t Epoch: 196 \t Loss: -0.01392146920491206\nTRAIN: \t Epoch: 196 \t Loss: -0.013917831983417273\nTRAIN: \t Epoch: 196 \t Loss: -0.01391693628008952\nVALD: \t Epoch: 196 \t Loss: -0.01181559357792139\nVALD: \t Epoch: 196 \t Loss: -0.01219049608334899\nVALD: \t Epoch: 196 \t Loss: -0.011708184145390987\nVALD: \t Epoch: 196 \t Loss: -0.012094534235075116\nVALD: \t Epoch: 196 \t Loss: -0.011636407670959375\n******************************\nEpoch: social-tag : 196\ntrain_loss -0.01391693628008952\nval_loss -0.011636407670959375\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 197 \t Loss: -0.0142603050917387\nTRAIN: \t Epoch: 197 \t Loss: -0.014385536778718233\nTRAIN: \t Epoch: 197 \t Loss: -0.01437078726788362\nTRAIN: \t Epoch: 197 \t Loss: -0.0140348169952631\nTRAIN: \t Epoch: 197 \t Loss: -0.013939310424029827\nTRAIN: \t Epoch: 197 \t Loss: -0.013893662796666225\nTRAIN: \t Epoch: 197 \t Loss: -0.013904300252241748\nTRAIN: \t Epoch: 197 \t Loss: -0.013856699923053384\nTRAIN: \t Epoch: 197 \t Loss: -0.013951460520426432\nTRAIN: \t Epoch: 197 \t Loss: -0.014020480494946242\nTRAIN: \t Epoch: 197 \t Loss: -0.014000970870256424\nTRAIN: \t Epoch: 197 \t Loss: -0.014065373145664731\nTRAIN: \t Epoch: 197 \t Loss: -0.014120069833902212\nTRAIN: \t Epoch: 197 \t Loss: -0.014089552951710565\nTRAIN: \t Epoch: 197 \t Loss: -0.014089168297747772\nTRAIN: \t Epoch: 197 \t Loss: -0.014061472902540118\nTRAIN: \t Epoch: 197 \t Loss: -0.013986128053682692\nTRAIN: \t Epoch: 197 \t Loss: -0.013990871970438294\nTRAIN: \t Epoch: 197 \t Loss: -0.013998148668753473\nTRAIN: \t Epoch: 197 \t Loss: -0.013982654502615332\nTRAIN: \t Epoch: 197 \t Loss: -0.013955384297562087\nVALD: \t Epoch: 197 \t Loss: -0.012643258087337017\nVALD: \t Epoch: 197 \t Loss: -0.012842448428273201\nVALD: \t Epoch: 197 \t Loss: -0.01224712934345007\nVALD: \t Epoch: 197 \t Loss: -0.012526378035545349\nVALD: \t Epoch: 197 \t Loss: -0.0120369758390958\n******************************\nEpoch: social-tag : 197\ntrain_loss -0.013955384297562087\nval_loss -0.0120369758390958\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 198 \t Loss: -0.01368701085448265\nTRAIN: \t Epoch: 198 \t Loss: -0.013755145482718945\nTRAIN: \t Epoch: 198 \t Loss: -0.014174904363850752\nTRAIN: \t Epoch: 198 \t Loss: -0.014176781522110105\nTRAIN: \t Epoch: 198 \t Loss: -0.0141950661316514\nTRAIN: \t Epoch: 198 \t Loss: -0.014265986469884714\nTRAIN: \t Epoch: 198 \t Loss: -0.014143522695771285\nTRAIN: \t Epoch: 198 \t Loss: -0.01402906165458262\nTRAIN: \t Epoch: 198 \t Loss: -0.014007349291609393\nTRAIN: \t Epoch: 198 \t Loss: -0.014002355094999076\nTRAIN: \t Epoch: 198 \t Loss: -0.01402239552275701\nTRAIN: \t Epoch: 198 \t Loss: -0.014004535895461837\nTRAIN: \t Epoch: 198 \t Loss: -0.013950847519131808\nTRAIN: \t Epoch: 198 \t Loss: -0.013946053106337786\nTRAIN: \t Epoch: 198 \t Loss: -0.013972118807335695\nTRAIN: \t Epoch: 198 \t Loss: -0.013953440356999636\nTRAIN: \t Epoch: 198 \t Loss: -0.013887287007973474\nTRAIN: \t Epoch: 198 \t Loss: -0.01387586041043202\nTRAIN: \t Epoch: 198 \t Loss: -0.013869980742272577\nTRAIN: \t Epoch: 198 \t Loss: -0.013914259430021048\nTRAIN: \t Epoch: 198 \t Loss: -0.013920502287659538\nVALD: \t Epoch: 198 \t Loss: -0.012900295667350292\nVALD: \t Epoch: 198 \t Loss: -0.01317244116216898\nVALD: \t Epoch: 198 \t Loss: -0.012465443772574266\nVALD: \t Epoch: 198 \t Loss: -0.012526082806289196\nVALD: \t Epoch: 198 \t Loss: -0.012018875607353862\n******************************\nEpoch: social-tag : 198\ntrain_loss -0.013920502287659538\nval_loss -0.012018875607353862\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 199 \t Loss: -0.01386952493339777\nTRAIN: \t Epoch: 199 \t Loss: -0.013939252588897943\nTRAIN: \t Epoch: 199 \t Loss: -0.014315780873099962\nTRAIN: \t Epoch: 199 \t Loss: -0.014066718053072691\nTRAIN: \t Epoch: 199 \t Loss: -0.013846954703330994\nTRAIN: \t Epoch: 199 \t Loss: -0.013927770468095938\nTRAIN: \t Epoch: 199 \t Loss: -0.013903756093766009\nTRAIN: \t Epoch: 199 \t Loss: -0.01396430330350995\nTRAIN: \t Epoch: 199 \t Loss: -0.013975289753741689\nTRAIN: \t Epoch: 199 \t Loss: -0.013967501744627953\nTRAIN: \t Epoch: 199 \t Loss: -0.013938723301345652\nTRAIN: \t Epoch: 199 \t Loss: -0.013901916720593968\nTRAIN: \t Epoch: 199 \t Loss: -0.013954144759246936\nTRAIN: \t Epoch: 199 \t Loss: -0.013951394108257123\nTRAIN: \t Epoch: 199 \t Loss: -0.01394826490432024\nTRAIN: \t Epoch: 199 \t Loss: -0.01395041769137606\nTRAIN: \t Epoch: 199 \t Loss: -0.013901797725873835\nTRAIN: \t Epoch: 199 \t Loss: -0.013931917492300272\nTRAIN: \t Epoch: 199 \t Loss: -0.013914974112259714\nTRAIN: \t Epoch: 199 \t Loss: -0.013916589552536607\nTRAIN: \t Epoch: 199 \t Loss: -0.013925659939152697\nVALD: \t Epoch: 199 \t Loss: -0.009997019544243813\nVALD: \t Epoch: 199 \t Loss: -0.011315382085740566\nVALD: \t Epoch: 199 \t Loss: -0.009471315890550613\nVALD: \t Epoch: 199 \t Loss: -0.01019232883118093\nVALD: \t Epoch: 199 \t Loss: -0.010055047878320667\n******************************\nEpoch: social-tag : 199\ntrain_loss -0.013925659939152697\nval_loss -0.010055047878320667\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 200 \t Loss: -0.013523881323635578\nTRAIN: \t Epoch: 200 \t Loss: -0.013448669109493494\nTRAIN: \t Epoch: 200 \t Loss: -0.013807090620199839\nTRAIN: \t Epoch: 200 \t Loss: -0.013353910064324737\nTRAIN: \t Epoch: 200 \t Loss: -0.013367888517677784\nTRAIN: \t Epoch: 200 \t Loss: -0.01351965901752313\nTRAIN: \t Epoch: 200 \t Loss: -0.013560439726071698\nTRAIN: \t Epoch: 200 \t Loss: -0.013515904895029962\nTRAIN: \t Epoch: 200 \t Loss: -0.01366537633455462\nTRAIN: \t Epoch: 200 \t Loss: -0.013661745935678482\nTRAIN: \t Epoch: 200 \t Loss: -0.013747999105941166\nTRAIN: \t Epoch: 200 \t Loss: -0.013781419567142924\nTRAIN: \t Epoch: 200 \t Loss: -0.013757245830045296\nTRAIN: \t Epoch: 200 \t Loss: -0.013775234842406852\nTRAIN: \t Epoch: 200 \t Loss: -0.01380548644810915\nTRAIN: \t Epoch: 200 \t Loss: -0.013842675951309502\nTRAIN: \t Epoch: 200 \t Loss: -0.013862250044065364\nTRAIN: \t Epoch: 200 \t Loss: -0.01387443719431758\nTRAIN: \t Epoch: 200 \t Loss: -0.013889137851564507\nTRAIN: \t Epoch: 200 \t Loss: -0.013938146643340588\nTRAIN: \t Epoch: 200 \t Loss: -0.013936470610964913\nVALD: \t Epoch: 200 \t Loss: -0.013078350573778152\nVALD: \t Epoch: 200 \t Loss: -0.013090655207633972\nVALD: \t Epoch: 200 \t Loss: -0.012071489046017328\nVALD: \t Epoch: 200 \t Loss: -0.012237618211656809\nVALD: \t Epoch: 200 \t Loss: -0.011778634527455206\n******************************\nEpoch: social-tag : 200\ntrain_loss -0.013936470610964913\nval_loss -0.011778634527455206\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 201 \t Loss: -0.014382587745785713\nTRAIN: \t Epoch: 201 \t Loss: -0.014300007838755846\nTRAIN: \t Epoch: 201 \t Loss: -0.01414318848401308\nTRAIN: \t Epoch: 201 \t Loss: -0.014276783214882016\nTRAIN: \t Epoch: 201 \t Loss: -0.014080941304564476\nTRAIN: \t Epoch: 201 \t Loss: -0.014010178546110788\nTRAIN: \t Epoch: 201 \t Loss: -0.014072019074644362\nTRAIN: \t Epoch: 201 \t Loss: -0.013932037283666432\nTRAIN: \t Epoch: 201 \t Loss: -0.013954343584676584\nTRAIN: \t Epoch: 201 \t Loss: -0.013882671296596528\nTRAIN: \t Epoch: 201 \t Loss: -0.013910427858883684\nTRAIN: \t Epoch: 201 \t Loss: -0.013885818887501955\nTRAIN: \t Epoch: 201 \t Loss: -0.013891197884312043\nTRAIN: \t Epoch: 201 \t Loss: -0.01394671421231968\nTRAIN: \t Epoch: 201 \t Loss: -0.013888639770448208\nTRAIN: \t Epoch: 201 \t Loss: -0.01392764929914847\nTRAIN: \t Epoch: 201 \t Loss: -0.01389933843165636\nTRAIN: \t Epoch: 201 \t Loss: -0.01389527739956975\nTRAIN: \t Epoch: 201 \t Loss: -0.013914244543564947\nTRAIN: \t Epoch: 201 \t Loss: -0.013905104296281934\nTRAIN: \t Epoch: 201 \t Loss: -0.013879235193374622\nVALD: \t Epoch: 201 \t Loss: -0.012848629616200924\nVALD: \t Epoch: 201 \t Loss: -0.012678740546107292\nVALD: \t Epoch: 201 \t Loss: -0.012271287230153879\nVALD: \t Epoch: 201 \t Loss: -0.012597407680004835\nVALD: \t Epoch: 201 \t Loss: -0.012042078419008118\n******************************\nEpoch: social-tag : 201\ntrain_loss -0.013879235193374622\nval_loss -0.012042078419008118\n{'min_val_epoch': 157, 'min_val_loss': -0.012929200742171774}\n******************************\nTRAIN: \t Epoch: 202 \t Loss: -0.013005354441702366\nTRAIN: \t Epoch: 202 \t Loss: -0.01376664498820901\nTRAIN: \t Epoch: 202 \t Loss: -0.0139044135188063\nTRAIN: \t Epoch: 202 \t Loss: -0.013761604903265834\nTRAIN: \t Epoch: 202 \t Loss: -0.013901382870972156\nTRAIN: \t Epoch: 202 \t Loss: -0.013976175182809433\nTRAIN: \t Epoch: 202 \t Loss: -0.013832243824643748\nTRAIN: \t Epoch: 202 \t Loss: -0.013808386051096022\nTRAIN: \t Epoch: 202 \t Loss: -0.01383064759688245\nTRAIN: \t Epoch: 202 \t Loss: -0.013816289603710175\nTRAIN: \t Epoch: 202 \t Loss: -0.013807025314732031\nTRAIN: \t Epoch: 202 \t Loss: -0.013816339041416844\nTRAIN: \t Epoch: 202 \t Loss: -0.013799685483368544\nTRAIN: \t Epoch: 202 \t Loss: -0.013793846060122763\nTRAIN: \t Epoch: 202 \t Loss: -0.013860766589641572\nTRAIN: \t Epoch: 202 \t Loss: -0.01392311044037342\nTRAIN: \t Epoch: 202 \t Loss: -0.013970223803292303\nTRAIN: \t Epoch: 202 \t Loss: -0.013990988540980551\nTRAIN: \t Epoch: 202 \t Loss: -0.013991952383596646\nTRAIN: \t Epoch: 202 \t Loss: -0.013954400736838578\nTRAIN: \t Epoch: 202 \t Loss: -0.013947719402191909\nVALD: \t Epoch: 202 \t Loss: -0.014140169136226177\nVALD: \t Epoch: 202 \t Loss: -0.013865968212485313\nVALD: \t Epoch: 202 \t Loss: -0.014019664997855822\nVALD: \t Epoch: 202 \t Loss: -0.01393941231071949\nVALD: \t Epoch: 202 \t Loss: -0.013223659589094817\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:24<00:00, 12.48it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.3587777940329964  FDE: 0.5329856783310291\n**************************************************\n******************************\nEpoch: social-tag : 202\ntrain_loss -0.013947719402191909\nval_loss -0.013223659589094817\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 203 \t Loss: -0.01456633023917675\nTRAIN: \t Epoch: 203 \t Loss: -0.014336977154016495\nTRAIN: \t Epoch: 203 \t Loss: -0.014043731614947319\nTRAIN: \t Epoch: 203 \t Loss: -0.014263628050684929\nTRAIN: \t Epoch: 203 \t Loss: -0.014306720532476901\nTRAIN: \t Epoch: 203 \t Loss: -0.014377231088777384\nTRAIN: \t Epoch: 203 \t Loss: -0.014406363080654825\nTRAIN: \t Epoch: 203 \t Loss: -0.014377532759681344\nTRAIN: \t Epoch: 203 \t Loss: -0.014364316335154904\nTRAIN: \t Epoch: 203 \t Loss: -0.014278998970985413\nTRAIN: \t Epoch: 203 \t Loss: -0.014250631782818924\nTRAIN: \t Epoch: 203 \t Loss: -0.014158098570381602\nTRAIN: \t Epoch: 203 \t Loss: -0.01409009607652059\nTRAIN: \t Epoch: 203 \t Loss: -0.01405945826055748\nTRAIN: \t Epoch: 203 \t Loss: -0.014102226433654627\nTRAIN: \t Epoch: 203 \t Loss: -0.014043577248230577\nTRAIN: \t Epoch: 203 \t Loss: -0.014078360160484034\nTRAIN: \t Epoch: 203 \t Loss: -0.014033183093286224\nTRAIN: \t Epoch: 203 \t Loss: -0.014004117122998363\nTRAIN: \t Epoch: 203 \t Loss: -0.01396860801614821\nTRAIN: \t Epoch: 203 \t Loss: -0.013940714041177547\nVALD: \t Epoch: 203 \t Loss: -0.012597705237567425\nVALD: \t Epoch: 203 \t Loss: -0.013003356754779816\nVALD: \t Epoch: 203 \t Loss: -0.012985214591026306\nVALD: \t Epoch: 203 \t Loss: -0.013137140078470111\nVALD: \t Epoch: 203 \t Loss: -0.012454284753968366\n******************************\nEpoch: social-tag : 203\ntrain_loss -0.013940714041177547\nval_loss -0.012454284753968366\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 204 \t Loss: -0.014188618399202824\nTRAIN: \t Epoch: 204 \t Loss: -0.014423318207263947\nTRAIN: \t Epoch: 204 \t Loss: -0.013962255169947943\nTRAIN: \t Epoch: 204 \t Loss: -0.014054850675165653\nTRAIN: \t Epoch: 204 \t Loss: -0.013980143703520298\nTRAIN: \t Epoch: 204 \t Loss: -0.013912145979702473\nTRAIN: \t Epoch: 204 \t Loss: -0.013871165524635996\nTRAIN: \t Epoch: 204 \t Loss: -0.013844607281498611\nTRAIN: \t Epoch: 204 \t Loss: -0.013843465492957167\nTRAIN: \t Epoch: 204 \t Loss: -0.013914362527430058\nTRAIN: \t Epoch: 204 \t Loss: -0.013847817243500189\nTRAIN: \t Epoch: 204 \t Loss: -0.013843914804359278\nTRAIN: \t Epoch: 204 \t Loss: -0.0137060907884286\nTRAIN: \t Epoch: 204 \t Loss: -0.013660354539752007\nTRAIN: \t Epoch: 204 \t Loss: -0.013679706305265427\nTRAIN: \t Epoch: 204 \t Loss: -0.0137485156301409\nTRAIN: \t Epoch: 204 \t Loss: -0.013814847258960499\nTRAIN: \t Epoch: 204 \t Loss: -0.013870946804268492\nTRAIN: \t Epoch: 204 \t Loss: -0.013876105413625115\nTRAIN: \t Epoch: 204 \t Loss: -0.013891929248347878\nTRAIN: \t Epoch: 204 \t Loss: -0.01389221739061263\nVALD: \t Epoch: 204 \t Loss: -0.012875216081738472\nVALD: \t Epoch: 204 \t Loss: -0.013081785291433334\nVALD: \t Epoch: 204 \t Loss: -0.012936611038943132\nVALD: \t Epoch: 204 \t Loss: -0.013056396041065454\nVALD: \t Epoch: 204 \t Loss: -0.012419458172747479\n******************************\nEpoch: social-tag : 204\ntrain_loss -0.01389221739061263\nval_loss -0.012419458172747479\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 205 \t Loss: -0.014163819141685963\nTRAIN: \t Epoch: 205 \t Loss: -0.014028930570930243\nTRAIN: \t Epoch: 205 \t Loss: -0.014130533362428347\nTRAIN: \t Epoch: 205 \t Loss: -0.013884876854717731\nTRAIN: \t Epoch: 205 \t Loss: -0.013752070441842078\nTRAIN: \t Epoch: 205 \t Loss: -0.013643710563580195\nTRAIN: \t Epoch: 205 \t Loss: -0.013617024357829775\nTRAIN: \t Epoch: 205 \t Loss: -0.013713236898183823\nTRAIN: \t Epoch: 205 \t Loss: -0.01372890847010745\nTRAIN: \t Epoch: 205 \t Loss: -0.013875584676861763\nTRAIN: \t Epoch: 205 \t Loss: -0.01391881670464169\nTRAIN: \t Epoch: 205 \t Loss: -0.013963536669810614\nTRAIN: \t Epoch: 205 \t Loss: -0.013987970323516773\nTRAIN: \t Epoch: 205 \t Loss: -0.013971626559006316\nTRAIN: \t Epoch: 205 \t Loss: -0.013956228892008463\nTRAIN: \t Epoch: 205 \t Loss: -0.013958156923763454\nTRAIN: \t Epoch: 205 \t Loss: -0.013997911420815131\nTRAIN: \t Epoch: 205 \t Loss: -0.013979485672381189\nTRAIN: \t Epoch: 205 \t Loss: -0.013965932554320284\nTRAIN: \t Epoch: 205 \t Loss: -0.013951451238244771\nTRAIN: \t Epoch: 205 \t Loss: -0.013950862697021909\nVALD: \t Epoch: 205 \t Loss: -0.012114300392568111\nVALD: \t Epoch: 205 \t Loss: -0.012486766092479229\nVALD: \t Epoch: 205 \t Loss: -0.011866197610894838\nVALD: \t Epoch: 205 \t Loss: -0.01198030007071793\nVALD: \t Epoch: 205 \t Loss: -0.011535239872341186\n******************************\nEpoch: social-tag : 205\ntrain_loss -0.013950862697021909\nval_loss -0.011535239872341186\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 206 \t Loss: -0.013687487691640854\nTRAIN: \t Epoch: 206 \t Loss: -0.013670604210346937\nTRAIN: \t Epoch: 206 \t Loss: -0.013603485810259977\nTRAIN: \t Epoch: 206 \t Loss: -0.01358968741260469\nTRAIN: \t Epoch: 206 \t Loss: -0.01369887888431549\nTRAIN: \t Epoch: 206 \t Loss: -0.013832663961996635\nTRAIN: \t Epoch: 206 \t Loss: -0.0138169348772083\nTRAIN: \t Epoch: 206 \t Loss: -0.013689086306840181\nTRAIN: \t Epoch: 206 \t Loss: -0.013718387215501733\nTRAIN: \t Epoch: 206 \t Loss: -0.013782788719981908\nTRAIN: \t Epoch: 206 \t Loss: -0.013773615810681473\nTRAIN: \t Epoch: 206 \t Loss: -0.013832183554768562\nTRAIN: \t Epoch: 206 \t Loss: -0.01388212049809786\nTRAIN: \t Epoch: 206 \t Loss: -0.013963524651314532\nTRAIN: \t Epoch: 206 \t Loss: -0.013940675184130669\nTRAIN: \t Epoch: 206 \t Loss: -0.013960511481855065\nTRAIN: \t Epoch: 206 \t Loss: -0.013983240351080894\nTRAIN: \t Epoch: 206 \t Loss: -0.013963278538237015\nTRAIN: \t Epoch: 206 \t Loss: -0.013971141049344288\nTRAIN: \t Epoch: 206 \t Loss: -0.013920809514820576\nTRAIN: \t Epoch: 206 \t Loss: -0.013925626805036364\nVALD: \t Epoch: 206 \t Loss: -0.011881621554493904\nVALD: \t Epoch: 206 \t Loss: -0.012237109243869781\nVALD: \t Epoch: 206 \t Loss: -0.011288043111562729\nVALD: \t Epoch: 206 \t Loss: -0.011842810083180666\nVALD: \t Epoch: 206 \t Loss: -0.01143322647481725\n******************************\nEpoch: social-tag : 206\ntrain_loss -0.013925626805036364\nval_loss -0.01143322647481725\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 207 \t Loss: -0.013692023232579231\nTRAIN: \t Epoch: 207 \t Loss: -0.013617397751659155\nTRAIN: \t Epoch: 207 \t Loss: -0.013631495336691538\nTRAIN: \t Epoch: 207 \t Loss: -0.01342298835515976\nTRAIN: \t Epoch: 207 \t Loss: -0.013617990538477898\nTRAIN: \t Epoch: 207 \t Loss: -0.013539673450092474\nTRAIN: \t Epoch: 207 \t Loss: -0.013749736627297742\nTRAIN: \t Epoch: 207 \t Loss: -0.013798369327560067\nTRAIN: \t Epoch: 207 \t Loss: -0.013865863904356956\nTRAIN: \t Epoch: 207 \t Loss: -0.013945406116545201\nTRAIN: \t Epoch: 207 \t Loss: -0.013912799429487099\nTRAIN: \t Epoch: 207 \t Loss: -0.013910525012761354\nTRAIN: \t Epoch: 207 \t Loss: -0.013918875716626644\nTRAIN: \t Epoch: 207 \t Loss: -0.013928427493997983\nTRAIN: \t Epoch: 207 \t Loss: -0.013900449499487877\nTRAIN: \t Epoch: 207 \t Loss: -0.013943938538432121\nTRAIN: \t Epoch: 207 \t Loss: -0.014000538846149164\nTRAIN: \t Epoch: 207 \t Loss: -0.013954156181878515\nTRAIN: \t Epoch: 207 \t Loss: -0.013988871813604706\nTRAIN: \t Epoch: 207 \t Loss: -0.013989195134490729\nTRAIN: \t Epoch: 207 \t Loss: -0.013981244131152962\nVALD: \t Epoch: 207 \t Loss: -0.011119618080556393\nVALD: \t Epoch: 207 \t Loss: -0.012147806119173765\nVALD: \t Epoch: 207 \t Loss: -0.011334088320533434\nVALD: \t Epoch: 207 \t Loss: -0.011772789992392063\nVALD: \t Epoch: 207 \t Loss: -0.011384340875966537\n******************************\nEpoch: social-tag : 207\ntrain_loss -0.013981244131152962\nval_loss -0.011384340875966537\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 208 \t Loss: -0.014190235175192356\nTRAIN: \t Epoch: 208 \t Loss: -0.014003518037497997\nTRAIN: \t Epoch: 208 \t Loss: -0.013662268407642841\nTRAIN: \t Epoch: 208 \t Loss: -0.01363403582945466\nTRAIN: \t Epoch: 208 \t Loss: -0.013487400300800801\nTRAIN: \t Epoch: 208 \t Loss: -0.013638081184277931\nTRAIN: \t Epoch: 208 \t Loss: -0.01374658995441028\nTRAIN: \t Epoch: 208 \t Loss: -0.013725821394473314\nTRAIN: \t Epoch: 208 \t Loss: -0.013638105243444443\nTRAIN: \t Epoch: 208 \t Loss: -0.013532927632331848\nTRAIN: \t Epoch: 208 \t Loss: -0.013576684096320108\nTRAIN: \t Epoch: 208 \t Loss: -0.013614984384427467\nTRAIN: \t Epoch: 208 \t Loss: -0.013673415765739404\nTRAIN: \t Epoch: 208 \t Loss: -0.013732927984425\nTRAIN: \t Epoch: 208 \t Loss: -0.013771504970888297\nTRAIN: \t Epoch: 208 \t Loss: -0.013881817751098424\nTRAIN: \t Epoch: 208 \t Loss: -0.01394304561921779\nTRAIN: \t Epoch: 208 \t Loss: -0.013934588939365413\nTRAIN: \t Epoch: 208 \t Loss: -0.013928153895233808\nTRAIN: \t Epoch: 208 \t Loss: -0.01393515602685511\nTRAIN: \t Epoch: 208 \t Loss: -0.01391011261673459\nVALD: \t Epoch: 208 \t Loss: -0.012238922528922558\nVALD: \t Epoch: 208 \t Loss: -0.012530494946986437\nVALD: \t Epoch: 208 \t Loss: -0.011567702206472555\nVALD: \t Epoch: 208 \t Loss: -0.011954132933169603\nVALD: \t Epoch: 208 \t Loss: -0.011457668122462029\n******************************\nEpoch: social-tag : 208\ntrain_loss -0.01391011261673459\nval_loss -0.011457668122462029\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 209 \t Loss: -0.014477361924946308\nTRAIN: \t Epoch: 209 \t Loss: -0.014756354503333569\nTRAIN: \t Epoch: 209 \t Loss: -0.014470014721155167\nTRAIN: \t Epoch: 209 \t Loss: -0.014265788486227393\nTRAIN: \t Epoch: 209 \t Loss: -0.014231900125741959\nTRAIN: \t Epoch: 209 \t Loss: -0.014353388144324223\nTRAIN: \t Epoch: 209 \t Loss: -0.014423183564628874\nTRAIN: \t Epoch: 209 \t Loss: -0.014156802673824131\nTRAIN: \t Epoch: 209 \t Loss: -0.014061585689584414\nTRAIN: \t Epoch: 209 \t Loss: -0.013978692796081304\nTRAIN: \t Epoch: 209 \t Loss: -0.01395578682422638\nTRAIN: \t Epoch: 209 \t Loss: -0.014004979360227784\nTRAIN: \t Epoch: 209 \t Loss: -0.013957771400992688\nTRAIN: \t Epoch: 209 \t Loss: -0.013921272807887622\nTRAIN: \t Epoch: 209 \t Loss: -0.013920371669034164\nTRAIN: \t Epoch: 209 \t Loss: -0.013928884116467088\nTRAIN: \t Epoch: 209 \t Loss: -0.014007789301959908\nTRAIN: \t Epoch: 209 \t Loss: -0.013973344272623459\nTRAIN: \t Epoch: 209 \t Loss: -0.014019051615736987\nTRAIN: \t Epoch: 209 \t Loss: -0.013940316531807185\nTRAIN: \t Epoch: 209 \t Loss: -0.01394995022206097\nVALD: \t Epoch: 209 \t Loss: -0.008315833285450935\nVALD: \t Epoch: 209 \t Loss: -0.01042353780940175\nVALD: \t Epoch: 209 \t Loss: -0.009112126814822355\nVALD: \t Epoch: 209 \t Loss: -0.010087568312883377\nVALD: \t Epoch: 209 \t Loss: -0.009966184359625726\n******************************\nEpoch: social-tag : 209\ntrain_loss -0.01394995022206097\nval_loss -0.009966184359625726\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 210 \t Loss: -0.015150372870266438\nTRAIN: \t Epoch: 210 \t Loss: -0.015102399047464132\nTRAIN: \t Epoch: 210 \t Loss: -0.01483438815921545\nTRAIN: \t Epoch: 210 \t Loss: -0.014409874333068728\nTRAIN: \t Epoch: 210 \t Loss: -0.014428037963807584\nTRAIN: \t Epoch: 210 \t Loss: -0.014560087273518244\nTRAIN: \t Epoch: 210 \t Loss: -0.014654901809990406\nTRAIN: \t Epoch: 210 \t Loss: -0.014588374295271933\nTRAIN: \t Epoch: 210 \t Loss: -0.014372506592836645\nTRAIN: \t Epoch: 210 \t Loss: -0.014319896418601274\nTRAIN: \t Epoch: 210 \t Loss: -0.014270372011444786\nTRAIN: \t Epoch: 210 \t Loss: -0.014230266058196625\nTRAIN: \t Epoch: 210 \t Loss: -0.014191288429384049\nTRAIN: \t Epoch: 210 \t Loss: -0.014222009706177883\nTRAIN: \t Epoch: 210 \t Loss: -0.014243326957027118\nTRAIN: \t Epoch: 210 \t Loss: -0.014201664191205055\nTRAIN: \t Epoch: 210 \t Loss: -0.014170539784519112\nTRAIN: \t Epoch: 210 \t Loss: -0.014097742995040284\nTRAIN: \t Epoch: 210 \t Loss: -0.014053372274103918\nTRAIN: \t Epoch: 210 \t Loss: -0.014022397715598345\nTRAIN: \t Epoch: 210 \t Loss: -0.014018684212023604\nVALD: \t Epoch: 210 \t Loss: -0.013057797215878963\nVALD: \t Epoch: 210 \t Loss: -0.013257044367492199\nVALD: \t Epoch: 210 \t Loss: -0.01285642416526874\nVALD: \t Epoch: 210 \t Loss: -0.012991365510970354\nVALD: \t Epoch: 210 \t Loss: -0.012390391454220585\n******************************\nEpoch: social-tag : 210\ntrain_loss -0.014018684212023604\nval_loss -0.012390391454220585\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 211 \t Loss: -0.014228831976652145\nTRAIN: \t Epoch: 211 \t Loss: -0.014425212517380714\nTRAIN: \t Epoch: 211 \t Loss: -0.014121057465672493\nTRAIN: \t Epoch: 211 \t Loss: -0.013953112764284015\nTRAIN: \t Epoch: 211 \t Loss: -0.01383655983954668\nTRAIN: \t Epoch: 211 \t Loss: -0.01390556866923968\nTRAIN: \t Epoch: 211 \t Loss: -0.0140079902485013\nTRAIN: \t Epoch: 211 \t Loss: -0.01395189668983221\nTRAIN: \t Epoch: 211 \t Loss: -0.013827367168333795\nTRAIN: \t Epoch: 211 \t Loss: -0.01381635880097747\nTRAIN: \t Epoch: 211 \t Loss: -0.013865766030820932\nTRAIN: \t Epoch: 211 \t Loss: -0.013905779303361973\nTRAIN: \t Epoch: 211 \t Loss: -0.013902924166849026\nTRAIN: \t Epoch: 211 \t Loss: -0.013944599644414015\nTRAIN: \t Epoch: 211 \t Loss: -0.013948509531716505\nTRAIN: \t Epoch: 211 \t Loss: -0.0139013328589499\nTRAIN: \t Epoch: 211 \t Loss: -0.01392480276305886\nTRAIN: \t Epoch: 211 \t Loss: -0.013940643260462416\nTRAIN: \t Epoch: 211 \t Loss: -0.013961563524054853\nTRAIN: \t Epoch: 211 \t Loss: -0.013947183359414339\nTRAIN: \t Epoch: 211 \t Loss: -0.013965643798744301\nVALD: \t Epoch: 211 \t Loss: -0.009198106825351715\nVALD: \t Epoch: 211 \t Loss: -0.010074479505419731\nVALD: \t Epoch: 211 \t Loss: -0.007900053324798742\nVALD: \t Epoch: 211 \t Loss: -0.008715198608115315\nVALD: \t Epoch: 211 \t Loss: -0.008582447174090695\n******************************\nEpoch: social-tag : 211\ntrain_loss -0.013965643798744301\nval_loss -0.008582447174090695\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 212 \t Loss: -0.014484405517578125\nTRAIN: \t Epoch: 212 \t Loss: -0.013562321662902832\nTRAIN: \t Epoch: 212 \t Loss: -0.01401837170124054\nTRAIN: \t Epoch: 212 \t Loss: -0.014145514462143183\nTRAIN: \t Epoch: 212 \t Loss: -0.014199859462678432\nTRAIN: \t Epoch: 212 \t Loss: -0.014115704533954462\nTRAIN: \t Epoch: 212 \t Loss: -0.014040053556007999\nTRAIN: \t Epoch: 212 \t Loss: -0.014067172887735069\nTRAIN: \t Epoch: 212 \t Loss: -0.014014644651777215\nTRAIN: \t Epoch: 212 \t Loss: -0.013974640145897865\nTRAIN: \t Epoch: 212 \t Loss: -0.014003151858394796\nTRAIN: \t Epoch: 212 \t Loss: -0.01404041664985319\nTRAIN: \t Epoch: 212 \t Loss: -0.01395544889741219\nTRAIN: \t Epoch: 212 \t Loss: -0.01395428300436054\nTRAIN: \t Epoch: 212 \t Loss: -0.014000904498000939\nTRAIN: \t Epoch: 212 \t Loss: -0.01403003465384245\nTRAIN: \t Epoch: 212 \t Loss: -0.013934764791937435\nTRAIN: \t Epoch: 212 \t Loss: -0.013959399579713741\nTRAIN: \t Epoch: 212 \t Loss: -0.013978197386390284\nTRAIN: \t Epoch: 212 \t Loss: -0.01398027059622109\nTRAIN: \t Epoch: 212 \t Loss: -0.013992957156165895\nVALD: \t Epoch: 212 \t Loss: -0.011457457207143307\nVALD: \t Epoch: 212 \t Loss: -0.011351010762155056\nVALD: \t Epoch: 212 \t Loss: -0.010033678418646256\nVALD: \t Epoch: 212 \t Loss: -0.01055644650477916\nVALD: \t Epoch: 212 \t Loss: -0.010202604407465593\n******************************\nEpoch: social-tag : 212\ntrain_loss -0.013992957156165895\nval_loss -0.010202604407465593\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 213 \t Loss: -0.013669096864759922\nTRAIN: \t Epoch: 213 \t Loss: -0.013421314302831888\nTRAIN: \t Epoch: 213 \t Loss: -0.013776837848126888\nTRAIN: \t Epoch: 213 \t Loss: -0.013870295835658908\nTRAIN: \t Epoch: 213 \t Loss: -0.01390560008585453\nTRAIN: \t Epoch: 213 \t Loss: -0.01375666648770372\nTRAIN: \t Epoch: 213 \t Loss: -0.013804479635187559\nTRAIN: \t Epoch: 213 \t Loss: -0.013927301857620478\nTRAIN: \t Epoch: 213 \t Loss: -0.013963732144070996\nTRAIN: \t Epoch: 213 \t Loss: -0.01401488846167922\nTRAIN: \t Epoch: 213 \t Loss: -0.013983273624696514\nTRAIN: \t Epoch: 213 \t Loss: -0.013991837933038672\nTRAIN: \t Epoch: 213 \t Loss: -0.013987268321216106\nTRAIN: \t Epoch: 213 \t Loss: -0.013984639530203171\nTRAIN: \t Epoch: 213 \t Loss: -0.013990946610768636\nTRAIN: \t Epoch: 213 \t Loss: -0.013963671051897109\nTRAIN: \t Epoch: 213 \t Loss: -0.013971228207297185\nTRAIN: \t Epoch: 213 \t Loss: -0.013948833362923728\nTRAIN: \t Epoch: 213 \t Loss: -0.013917015081173495\nTRAIN: \t Epoch: 213 \t Loss: -0.013957657665014268\nTRAIN: \t Epoch: 213 \t Loss: -0.013933053442672665\nVALD: \t Epoch: 213 \t Loss: -0.008840097114443779\nVALD: \t Epoch: 213 \t Loss: -0.010336152743548155\nVALD: \t Epoch: 213 \t Loss: -0.00942123852049311\nVALD: \t Epoch: 213 \t Loss: -0.010300621972419322\nVALD: \t Epoch: 213 \t Loss: -0.010017609154736554\n******************************\nEpoch: social-tag : 213\ntrain_loss -0.013933053442672665\nval_loss -0.010017609154736554\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 214 \t Loss: -0.014106912538409233\nTRAIN: \t Epoch: 214 \t Loss: -0.013989754486829042\nTRAIN: \t Epoch: 214 \t Loss: -0.013945108590026697\nTRAIN: \t Epoch: 214 \t Loss: -0.013919569784775376\nTRAIN: \t Epoch: 214 \t Loss: -0.01409082394093275\nTRAIN: \t Epoch: 214 \t Loss: -0.01411775111531218\nTRAIN: \t Epoch: 214 \t Loss: -0.014084658718534879\nTRAIN: \t Epoch: 214 \t Loss: -0.014105554320849478\nTRAIN: \t Epoch: 214 \t Loss: -0.014060901581413217\nTRAIN: \t Epoch: 214 \t Loss: -0.014121753536164761\nTRAIN: \t Epoch: 214 \t Loss: -0.014161758971485224\nTRAIN: \t Epoch: 214 \t Loss: -0.014167982464035353\nTRAIN: \t Epoch: 214 \t Loss: -0.014188434068973247\nTRAIN: \t Epoch: 214 \t Loss: -0.0142147823769067\nTRAIN: \t Epoch: 214 \t Loss: -0.01426012044151624\nTRAIN: \t Epoch: 214 \t Loss: -0.014177718840073794\nTRAIN: \t Epoch: 214 \t Loss: -0.014158444917377304\nTRAIN: \t Epoch: 214 \t Loss: -0.014091169979009364\nTRAIN: \t Epoch: 214 \t Loss: -0.014013262808715043\nTRAIN: \t Epoch: 214 \t Loss: -0.014016295364126564\nTRAIN: \t Epoch: 214 \t Loss: -0.014011631746336232\nVALD: \t Epoch: 214 \t Loss: -0.011233432218432426\nVALD: \t Epoch: 214 \t Loss: -0.011734045576304197\nVALD: \t Epoch: 214 \t Loss: -0.010091943821559349\nVALD: \t Epoch: 214 \t Loss: -0.010642431559972465\nVALD: \t Epoch: 214 \t Loss: -0.010345150115217371\n******************************\nEpoch: social-tag : 214\ntrain_loss -0.014011631746336232\nval_loss -0.010345150115217371\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 215 \t Loss: -0.014825972728431225\nTRAIN: \t Epoch: 215 \t Loss: -0.014570968225598335\nTRAIN: \t Epoch: 215 \t Loss: -0.014149455974499384\nTRAIN: \t Epoch: 215 \t Loss: -0.01377369207330048\nTRAIN: \t Epoch: 215 \t Loss: -0.01378614380955696\nTRAIN: \t Epoch: 215 \t Loss: -0.013869118876755238\nTRAIN: \t Epoch: 215 \t Loss: -0.013801900137748038\nTRAIN: \t Epoch: 215 \t Loss: -0.013776155305095017\nTRAIN: \t Epoch: 215 \t Loss: -0.013902903534471989\nTRAIN: \t Epoch: 215 \t Loss: -0.013852782920002938\nTRAIN: \t Epoch: 215 \t Loss: -0.013849956322122704\nTRAIN: \t Epoch: 215 \t Loss: -0.013884436416750153\nTRAIN: \t Epoch: 215 \t Loss: -0.01385905328564919\nTRAIN: \t Epoch: 215 \t Loss: -0.013940037028597934\nTRAIN: \t Epoch: 215 \t Loss: -0.013977682652572791\nTRAIN: \t Epoch: 215 \t Loss: -0.013997058616951108\nTRAIN: \t Epoch: 215 \t Loss: -0.014013650279273005\nTRAIN: \t Epoch: 215 \t Loss: -0.013973902393546369\nTRAIN: \t Epoch: 215 \t Loss: -0.014019962498231939\nTRAIN: \t Epoch: 215 \t Loss: -0.013986422121524811\nTRAIN: \t Epoch: 215 \t Loss: -0.013985521821398504\nVALD: \t Epoch: 215 \t Loss: -0.01319130975753069\nVALD: \t Epoch: 215 \t Loss: -0.01297573046758771\nVALD: \t Epoch: 215 \t Loss: -0.012395402106146017\nVALD: \t Epoch: 215 \t Loss: -0.012558511458337307\nVALD: \t Epoch: 215 \t Loss: -0.012071287958326356\n******************************\nEpoch: social-tag : 215\ntrain_loss -0.013985521821398504\nval_loss -0.012071287958326356\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 216 \t Loss: -0.014248212799429893\nTRAIN: \t Epoch: 216 \t Loss: -0.013597235083580017\nTRAIN: \t Epoch: 216 \t Loss: -0.013627781843145689\nTRAIN: \t Epoch: 216 \t Loss: -0.013611086644232273\nTRAIN: \t Epoch: 216 \t Loss: -0.013689973391592503\nTRAIN: \t Epoch: 216 \t Loss: -0.013838169941057762\nTRAIN: \t Epoch: 216 \t Loss: -0.013886599934526853\nTRAIN: \t Epoch: 216 \t Loss: -0.013791658449918032\nTRAIN: \t Epoch: 216 \t Loss: -0.013788237132959895\nTRAIN: \t Epoch: 216 \t Loss: -0.013726698327809572\nTRAIN: \t Epoch: 216 \t Loss: -0.013735336928882382\nTRAIN: \t Epoch: 216 \t Loss: -0.01378540190247198\nTRAIN: \t Epoch: 216 \t Loss: -0.013821392463376889\nTRAIN: \t Epoch: 216 \t Loss: -0.01385705532240016\nTRAIN: \t Epoch: 216 \t Loss: -0.013841987711687882\nTRAIN: \t Epoch: 216 \t Loss: -0.013901201193220913\nTRAIN: \t Epoch: 216 \t Loss: -0.013953371302169911\nTRAIN: \t Epoch: 216 \t Loss: -0.013945903215143416\nTRAIN: \t Epoch: 216 \t Loss: -0.013938210708530326\nTRAIN: \t Epoch: 216 \t Loss: -0.013918281299993396\nTRAIN: \t Epoch: 216 \t Loss: -0.013933177718586801\nVALD: \t Epoch: 216 \t Loss: -0.012290299870073795\nVALD: \t Epoch: 216 \t Loss: -0.012296482920646667\nVALD: \t Epoch: 216 \t Loss: -0.011502861976623535\nVALD: \t Epoch: 216 \t Loss: -0.011825052555650473\nVALD: \t Epoch: 216 \t Loss: -0.011419435246171198\n******************************\nEpoch: social-tag : 216\ntrain_loss -0.013933177718586801\nval_loss -0.011419435246171198\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 217 \t Loss: -0.014842716045677662\nTRAIN: \t Epoch: 217 \t Loss: -0.014716713223606348\nTRAIN: \t Epoch: 217 \t Loss: -0.014894047441581884\nTRAIN: \t Epoch: 217 \t Loss: -0.014735573437064886\nTRAIN: \t Epoch: 217 \t Loss: -0.014606887102127075\nTRAIN: \t Epoch: 217 \t Loss: -0.014606044006844362\nTRAIN: \t Epoch: 217 \t Loss: -0.014515322899179799\nTRAIN: \t Epoch: 217 \t Loss: -0.014400837360881269\nTRAIN: \t Epoch: 217 \t Loss: -0.014385976414713595\nTRAIN: \t Epoch: 217 \t Loss: -0.014308417309075594\nTRAIN: \t Epoch: 217 \t Loss: -0.014192388392984867\nTRAIN: \t Epoch: 217 \t Loss: -0.014107769277567664\nTRAIN: \t Epoch: 217 \t Loss: -0.01415530384446566\nTRAIN: \t Epoch: 217 \t Loss: -0.014170035320733274\nTRAIN: \t Epoch: 217 \t Loss: -0.014119829175372918\nTRAIN: \t Epoch: 217 \t Loss: -0.014088190568145365\nTRAIN: \t Epoch: 217 \t Loss: -0.01412035513888387\nTRAIN: \t Epoch: 217 \t Loss: -0.014102601860132482\nTRAIN: \t Epoch: 217 \t Loss: -0.014068982367844958\nTRAIN: \t Epoch: 217 \t Loss: -0.014053755905479193\nTRAIN: \t Epoch: 217 \t Loss: -0.01401835177941054\nVALD: \t Epoch: 217 \t Loss: -0.014043186791241169\nVALD: \t Epoch: 217 \t Loss: -0.013947255443781614\nVALD: \t Epoch: 217 \t Loss: -0.013683930349846682\nVALD: \t Epoch: 217 \t Loss: -0.013302868232131004\nVALD: \t Epoch: 217 \t Loss: -0.012658499289250029\n******************************\nEpoch: social-tag : 217\ntrain_loss -0.01401835177941054\nval_loss -0.012658499289250029\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 218 \t Loss: -0.01488533429801464\nTRAIN: \t Epoch: 218 \t Loss: -0.014372595120221376\nTRAIN: \t Epoch: 218 \t Loss: -0.014142354329427084\nTRAIN: \t Epoch: 218 \t Loss: -0.014284749748185277\nTRAIN: \t Epoch: 218 \t Loss: -0.01409066766500473\nTRAIN: \t Epoch: 218 \t Loss: -0.01393072105323275\nTRAIN: \t Epoch: 218 \t Loss: -0.014090959514890398\nTRAIN: \t Epoch: 218 \t Loss: -0.014045878080651164\nTRAIN: \t Epoch: 218 \t Loss: -0.014036294486787584\nTRAIN: \t Epoch: 218 \t Loss: -0.014109263755381107\nTRAIN: \t Epoch: 218 \t Loss: -0.014144496822899038\nTRAIN: \t Epoch: 218 \t Loss: -0.014186157224078974\nTRAIN: \t Epoch: 218 \t Loss: -0.014203878191228096\nTRAIN: \t Epoch: 218 \t Loss: -0.014139324559697084\nTRAIN: \t Epoch: 218 \t Loss: -0.014139316541453202\nTRAIN: \t Epoch: 218 \t Loss: -0.014118933642748743\nTRAIN: \t Epoch: 218 \t Loss: -0.014094671946676338\nTRAIN: \t Epoch: 218 \t Loss: -0.014047849043789837\nTRAIN: \t Epoch: 218 \t Loss: -0.014047874284810141\nTRAIN: \t Epoch: 218 \t Loss: -0.01403296496719122\nTRAIN: \t Epoch: 218 \t Loss: -0.014011333596733963\nVALD: \t Epoch: 218 \t Loss: -0.012743175029754639\nVALD: \t Epoch: 218 \t Loss: -0.012968685943633318\nVALD: \t Epoch: 218 \t Loss: -0.012467161441842714\nVALD: \t Epoch: 218 \t Loss: -0.012592982267960906\nVALD: \t Epoch: 218 \t Loss: -0.012023683907329172\n******************************\nEpoch: social-tag : 218\ntrain_loss -0.014011333596733963\nval_loss -0.012023683907329172\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 219 \t Loss: -0.014425724744796753\nTRAIN: \t Epoch: 219 \t Loss: -0.014022232964634895\nTRAIN: \t Epoch: 219 \t Loss: -0.01397106951723496\nTRAIN: \t Epoch: 219 \t Loss: -0.013962836936116219\nTRAIN: \t Epoch: 219 \t Loss: -0.01398865319788456\nTRAIN: \t Epoch: 219 \t Loss: -0.013988274615257978\nTRAIN: \t Epoch: 219 \t Loss: -0.01397575025579759\nTRAIN: \t Epoch: 219 \t Loss: -0.013959658332169056\nTRAIN: \t Epoch: 219 \t Loss: -0.014021265423960157\nTRAIN: \t Epoch: 219 \t Loss: -0.013804684951901436\nTRAIN: \t Epoch: 219 \t Loss: -0.013791862113231962\nTRAIN: \t Epoch: 219 \t Loss: -0.013765995778764287\nTRAIN: \t Epoch: 219 \t Loss: -0.013804896161533318\nTRAIN: \t Epoch: 219 \t Loss: -0.013815645527626787\nTRAIN: \t Epoch: 219 \t Loss: -0.013778517829875152\nTRAIN: \t Epoch: 219 \t Loss: -0.013876667886506766\nTRAIN: \t Epoch: 219 \t Loss: -0.013931189554140848\nTRAIN: \t Epoch: 219 \t Loss: -0.01391745705364479\nTRAIN: \t Epoch: 219 \t Loss: -0.013926263604509203\nTRAIN: \t Epoch: 219 \t Loss: -0.0139872204978019\nTRAIN: \t Epoch: 219 \t Loss: -0.013992776756757207\nVALD: \t Epoch: 219 \t Loss: -0.009046302177011967\nVALD: \t Epoch: 219 \t Loss: -0.009906538296490908\nVALD: \t Epoch: 219 \t Loss: -0.007396315922960639\nVALD: \t Epoch: 219 \t Loss: -0.008237260452006012\nVALD: \t Epoch: 219 \t Loss: -0.008275039002131342\n******************************\nEpoch: social-tag : 219\ntrain_loss -0.013992776756757207\nval_loss -0.008275039002131342\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 220 \t Loss: -0.014216491021215916\nTRAIN: \t Epoch: 220 \t Loss: -0.014030023012310266\nTRAIN: \t Epoch: 220 \t Loss: -0.013674651893476645\nTRAIN: \t Epoch: 220 \t Loss: -0.01410360960289836\nTRAIN: \t Epoch: 220 \t Loss: -0.014188349433243274\nTRAIN: \t Epoch: 220 \t Loss: -0.014183253360291323\nTRAIN: \t Epoch: 220 \t Loss: -0.014219156732516629\nTRAIN: \t Epoch: 220 \t Loss: -0.014212741632945836\nTRAIN: \t Epoch: 220 \t Loss: -0.014188309200108051\nTRAIN: \t Epoch: 220 \t Loss: -0.01414726972579956\nTRAIN: \t Epoch: 220 \t Loss: -0.014078842476010323\nTRAIN: \t Epoch: 220 \t Loss: -0.014088072658826908\nTRAIN: \t Epoch: 220 \t Loss: -0.014086915968129268\nTRAIN: \t Epoch: 220 \t Loss: -0.014028446416237525\nTRAIN: \t Epoch: 220 \t Loss: -0.014009807941814263\nTRAIN: \t Epoch: 220 \t Loss: -0.013973714376334101\nTRAIN: \t Epoch: 220 \t Loss: -0.014010153108221643\nTRAIN: \t Epoch: 220 \t Loss: -0.014050014368775818\nTRAIN: \t Epoch: 220 \t Loss: -0.013988403211298742\nTRAIN: \t Epoch: 220 \t Loss: -0.013989705126732588\nTRAIN: \t Epoch: 220 \t Loss: -0.014004680613140556\nVALD: \t Epoch: 220 \t Loss: -0.009245903231203556\nVALD: \t Epoch: 220 \t Loss: -0.01033214945346117\nVALD: \t Epoch: 220 \t Loss: -0.007811879428724448\nVALD: \t Epoch: 220 \t Loss: -0.00867582531645894\nVALD: \t Epoch: 220 \t Loss: -0.008630145767076772\n******************************\nEpoch: social-tag : 220\ntrain_loss -0.014004680613140556\nval_loss -0.008630145767076772\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 221 \t Loss: -0.014156848192214966\nTRAIN: \t Epoch: 221 \t Loss: -0.014080115593969822\nTRAIN: \t Epoch: 221 \t Loss: -0.014228973848124346\nTRAIN: \t Epoch: 221 \t Loss: -0.014222642406821251\nTRAIN: \t Epoch: 221 \t Loss: -0.014308907836675645\nTRAIN: \t Epoch: 221 \t Loss: -0.014383621669063965\nTRAIN: \t Epoch: 221 \t Loss: -0.014208511981580938\nTRAIN: \t Epoch: 221 \t Loss: -0.014185050851665437\nTRAIN: \t Epoch: 221 \t Loss: -0.014202279452648427\nTRAIN: \t Epoch: 221 \t Loss: -0.014211806654930114\nTRAIN: \t Epoch: 221 \t Loss: -0.014151529916985468\nTRAIN: \t Epoch: 221 \t Loss: -0.014174342698728045\nTRAIN: \t Epoch: 221 \t Loss: -0.014166964719501825\nTRAIN: \t Epoch: 221 \t Loss: -0.014178086072206497\nTRAIN: \t Epoch: 221 \t Loss: -0.014172583135465782\nTRAIN: \t Epoch: 221 \t Loss: -0.01416685339063406\nTRAIN: \t Epoch: 221 \t Loss: -0.014092006117982022\nTRAIN: \t Epoch: 221 \t Loss: -0.014085821652164062\nTRAIN: \t Epoch: 221 \t Loss: -0.014081007653945371\nTRAIN: \t Epoch: 221 \t Loss: -0.014071171963587403\nTRAIN: \t Epoch: 221 \t Loss: -0.014056218433306598\nVALD: \t Epoch: 221 \t Loss: -0.010110637173056602\nVALD: \t Epoch: 221 \t Loss: -0.010976497549563646\nVALD: \t Epoch: 221 \t Loss: -0.009548416516433159\nVALD: \t Epoch: 221 \t Loss: -0.010251245577819645\nVALD: \t Epoch: 221 \t Loss: -0.010056986325029015\n******************************\nEpoch: social-tag : 221\ntrain_loss -0.014056218433306598\nval_loss -0.010056986325029015\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 222 \t Loss: -0.013545965775847435\nTRAIN: \t Epoch: 222 \t Loss: -0.014053487684577703\nTRAIN: \t Epoch: 222 \t Loss: -0.013960164350767931\nTRAIN: \t Epoch: 222 \t Loss: -0.014146482106298208\nTRAIN: \t Epoch: 222 \t Loss: -0.013983888551592827\nTRAIN: \t Epoch: 222 \t Loss: -0.013916184349606434\nTRAIN: \t Epoch: 222 \t Loss: -0.013674626126885414\nTRAIN: \t Epoch: 222 \t Loss: -0.01375428494066\nTRAIN: \t Epoch: 222 \t Loss: -0.013749680067929957\nTRAIN: \t Epoch: 222 \t Loss: -0.013762016408145427\nTRAIN: \t Epoch: 222 \t Loss: -0.013779383068057623\nTRAIN: \t Epoch: 222 \t Loss: -0.013845241706197461\nTRAIN: \t Epoch: 222 \t Loss: -0.01385008765814396\nTRAIN: \t Epoch: 222 \t Loss: -0.01393516075664333\nTRAIN: \t Epoch: 222 \t Loss: -0.01393702756613493\nTRAIN: \t Epoch: 222 \t Loss: -0.013887616980355233\nTRAIN: \t Epoch: 222 \t Loss: -0.013943056247251876\nTRAIN: \t Epoch: 222 \t Loss: -0.013948944500750966\nTRAIN: \t Epoch: 222 \t Loss: -0.013977637906607828\nTRAIN: \t Epoch: 222 \t Loss: -0.0140049462672323\nTRAIN: \t Epoch: 222 \t Loss: -0.013996073796644335\nVALD: \t Epoch: 222 \t Loss: -0.008478655479848385\nVALD: \t Epoch: 222 \t Loss: -0.01008848799392581\nVALD: \t Epoch: 222 \t Loss: -0.007954532901446024\nVALD: \t Epoch: 222 \t Loss: -0.008927027229219675\nVALD: \t Epoch: 222 \t Loss: -0.008969011226138055\n******************************\nEpoch: social-tag : 222\ntrain_loss -0.013996073796644335\nval_loss -0.008969011226138055\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 223 \t Loss: -0.014738032594323158\nTRAIN: \t Epoch: 223 \t Loss: -0.014142515137791634\nTRAIN: \t Epoch: 223 \t Loss: -0.013787256553769112\nTRAIN: \t Epoch: 223 \t Loss: -0.01360529474914074\nTRAIN: \t Epoch: 223 \t Loss: -0.013634125515818597\nTRAIN: \t Epoch: 223 \t Loss: -0.013660273204247156\nTRAIN: \t Epoch: 223 \t Loss: -0.013750402256846428\nTRAIN: \t Epoch: 223 \t Loss: -0.013818397419527173\nTRAIN: \t Epoch: 223 \t Loss: -0.013813974749710824\nTRAIN: \t Epoch: 223 \t Loss: -0.013867677561938763\nTRAIN: \t Epoch: 223 \t Loss: -0.013859345662322912\nTRAIN: \t Epoch: 223 \t Loss: -0.013906342598299185\nTRAIN: \t Epoch: 223 \t Loss: -0.01391512592538045\nTRAIN: \t Epoch: 223 \t Loss: -0.013927450270525046\nTRAIN: \t Epoch: 223 \t Loss: -0.013954222078124683\nTRAIN: \t Epoch: 223 \t Loss: -0.013914786686655134\nTRAIN: \t Epoch: 223 \t Loss: -0.013891195933170179\nTRAIN: \t Epoch: 223 \t Loss: -0.013937526382505894\nTRAIN: \t Epoch: 223 \t Loss: -0.01396425651680482\nTRAIN: \t Epoch: 223 \t Loss: -0.013978325808420778\nTRAIN: \t Epoch: 223 \t Loss: -0.013990323108806919\nVALD: \t Epoch: 223 \t Loss: -0.00802606251090765\nVALD: \t Epoch: 223 \t Loss: -0.009794233366847038\nVALD: \t Epoch: 223 \t Loss: -0.0076474818245818215\nVALD: \t Epoch: 223 \t Loss: -0.008573402010370046\nVALD: \t Epoch: 223 \t Loss: -0.008732164203640728\n******************************\nEpoch: social-tag : 223\ntrain_loss -0.013990323108806919\nval_loss -0.008732164203640728\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 224 \t Loss: -0.01324502844363451\nTRAIN: \t Epoch: 224 \t Loss: -0.013573801144957542\nTRAIN: \t Epoch: 224 \t Loss: -0.013381758704781532\nTRAIN: \t Epoch: 224 \t Loss: -0.013388403225690126\nTRAIN: \t Epoch: 224 \t Loss: -0.013623460568487645\nTRAIN: \t Epoch: 224 \t Loss: -0.013725882396101952\nTRAIN: \t Epoch: 224 \t Loss: -0.01383882321949516\nTRAIN: \t Epoch: 224 \t Loss: -0.013751647900789976\nTRAIN: \t Epoch: 224 \t Loss: -0.013743919113443958\nTRAIN: \t Epoch: 224 \t Loss: -0.013696821685880423\nTRAIN: \t Epoch: 224 \t Loss: -0.01366767803715034\nTRAIN: \t Epoch: 224 \t Loss: -0.013712000800296664\nTRAIN: \t Epoch: 224 \t Loss: -0.013816940669830028\nTRAIN: \t Epoch: 224 \t Loss: -0.013813473284244537\nTRAIN: \t Epoch: 224 \t Loss: -0.01387340681006511\nTRAIN: \t Epoch: 224 \t Loss: -0.01396559685235843\nTRAIN: \t Epoch: 224 \t Loss: -0.01397712156176567\nTRAIN: \t Epoch: 224 \t Loss: -0.013993154486848248\nTRAIN: \t Epoch: 224 \t Loss: -0.013993498428087486\nTRAIN: \t Epoch: 224 \t Loss: -0.01398729025386274\nTRAIN: \t Epoch: 224 \t Loss: -0.01397975679535818\nVALD: \t Epoch: 224 \t Loss: -0.010293952189385891\nVALD: \t Epoch: 224 \t Loss: -0.01118589611724019\nVALD: \t Epoch: 224 \t Loss: -0.009587282625337442\nVALD: \t Epoch: 224 \t Loss: -0.01025878032669425\nVALD: \t Epoch: 224 \t Loss: -0.010070857794388481\n******************************\nEpoch: social-tag : 224\ntrain_loss -0.01397975679535818\nval_loss -0.010070857794388481\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 225 \t Loss: -0.014488800428807735\nTRAIN: \t Epoch: 225 \t Loss: -0.014152946881949902\nTRAIN: \t Epoch: 225 \t Loss: -0.014528368910153707\nTRAIN: \t Epoch: 225 \t Loss: -0.014347481075674295\nTRAIN: \t Epoch: 225 \t Loss: -0.013953807391226291\nTRAIN: \t Epoch: 225 \t Loss: -0.013986346311867237\nTRAIN: \t Epoch: 225 \t Loss: -0.013913387033556188\nTRAIN: \t Epoch: 225 \t Loss: -0.013957594521343708\nTRAIN: \t Epoch: 225 \t Loss: -0.0140003545416726\nTRAIN: \t Epoch: 225 \t Loss: -0.014001171104609966\nTRAIN: \t Epoch: 225 \t Loss: -0.013988987712020224\nTRAIN: \t Epoch: 225 \t Loss: -0.01384076988324523\nTRAIN: \t Epoch: 225 \t Loss: -0.013896925947987117\nTRAIN: \t Epoch: 225 \t Loss: -0.013951770029962063\nTRAIN: \t Epoch: 225 \t Loss: -0.013967510995765526\nTRAIN: \t Epoch: 225 \t Loss: -0.014072727353777736\nTRAIN: \t Epoch: 225 \t Loss: -0.014099007855881663\nTRAIN: \t Epoch: 225 \t Loss: -0.014036647871964507\nTRAIN: \t Epoch: 225 \t Loss: -0.014043993757743584\nTRAIN: \t Epoch: 225 \t Loss: -0.014059884939342737\nTRAIN: \t Epoch: 225 \t Loss: -0.014067065674429594\nVALD: \t Epoch: 225 \t Loss: -0.012891571968793869\nVALD: \t Epoch: 225 \t Loss: -0.012951938435435295\nVALD: \t Epoch: 225 \t Loss: -0.012488395596543947\nVALD: \t Epoch: 225 \t Loss: -0.012480869423598051\nVALD: \t Epoch: 225 \t Loss: -0.011984424890527402\n******************************\nEpoch: social-tag : 225\ntrain_loss -0.014067065674429594\nval_loss -0.011984424890527402\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 226 \t Loss: -0.01405615545809269\nTRAIN: \t Epoch: 226 \t Loss: -0.014416030142456293\nTRAIN: \t Epoch: 226 \t Loss: -0.014336376761396727\nTRAIN: \t Epoch: 226 \t Loss: -0.014397396007552743\nTRAIN: \t Epoch: 226 \t Loss: -0.014267880655825137\nTRAIN: \t Epoch: 226 \t Loss: -0.014119471578548351\nTRAIN: \t Epoch: 226 \t Loss: -0.01416787106011595\nTRAIN: \t Epoch: 226 \t Loss: -0.014068026444874704\nTRAIN: \t Epoch: 226 \t Loss: -0.014033010125988059\nTRAIN: \t Epoch: 226 \t Loss: -0.013966035377234221\nTRAIN: \t Epoch: 226 \t Loss: -0.013960811140185053\nTRAIN: \t Epoch: 226 \t Loss: -0.014015702608351907\nTRAIN: \t Epoch: 226 \t Loss: -0.014087836544674177\nTRAIN: \t Epoch: 226 \t Loss: -0.014067398251167365\nTRAIN: \t Epoch: 226 \t Loss: -0.014051982946693898\nTRAIN: \t Epoch: 226 \t Loss: -0.014120062463916838\nTRAIN: \t Epoch: 226 \t Loss: -0.014086829991463353\nTRAIN: \t Epoch: 226 \t Loss: -0.01411261155994402\nTRAIN: \t Epoch: 226 \t Loss: -0.01407381720644863\nTRAIN: \t Epoch: 226 \t Loss: -0.014081197790801525\nTRAIN: \t Epoch: 226 \t Loss: -0.014071341331131934\nVALD: \t Epoch: 226 \t Loss: -0.012689431197941303\nVALD: \t Epoch: 226 \t Loss: -0.012976603582501411\nVALD: \t Epoch: 226 \t Loss: -0.012267711261908213\nVALD: \t Epoch: 226 \t Loss: -0.01257779891602695\nVALD: \t Epoch: 226 \t Loss: -0.01207687639767805\n******************************\nEpoch: social-tag : 226\ntrain_loss -0.014071341331131934\nval_loss -0.01207687639767805\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 227 \t Loss: -0.014390566386282444\nTRAIN: \t Epoch: 227 \t Loss: -0.014112456236034632\nTRAIN: \t Epoch: 227 \t Loss: -0.014123628847301006\nTRAIN: \t Epoch: 227 \t Loss: -0.013913398375734687\nTRAIN: \t Epoch: 227 \t Loss: -0.013934329710900783\nTRAIN: \t Epoch: 227 \t Loss: -0.013996566335360209\nTRAIN: \t Epoch: 227 \t Loss: -0.013964695989021234\nTRAIN: \t Epoch: 227 \t Loss: -0.014010176761075854\nTRAIN: \t Epoch: 227 \t Loss: -0.01403485538644923\nTRAIN: \t Epoch: 227 \t Loss: -0.013841235172003507\nTRAIN: \t Epoch: 227 \t Loss: -0.013804883526807482\nTRAIN: \t Epoch: 227 \t Loss: -0.013759297085925937\nTRAIN: \t Epoch: 227 \t Loss: -0.013728336287805667\nTRAIN: \t Epoch: 227 \t Loss: -0.013804833338196789\nTRAIN: \t Epoch: 227 \t Loss: -0.0138295229524374\nTRAIN: \t Epoch: 227 \t Loss: -0.013794802885968238\nTRAIN: \t Epoch: 227 \t Loss: -0.01382674222045085\nTRAIN: \t Epoch: 227 \t Loss: -0.01387735078525212\nTRAIN: \t Epoch: 227 \t Loss: -0.013916004890281903\nTRAIN: \t Epoch: 227 \t Loss: -0.013908719178289175\nTRAIN: \t Epoch: 227 \t Loss: -0.013905287834434392\nVALD: \t Epoch: 227 \t Loss: -0.011747277341783047\nVALD: \t Epoch: 227 \t Loss: -0.012203404679894447\nVALD: \t Epoch: 227 \t Loss: -0.010979507118463516\nVALD: \t Epoch: 227 \t Loss: -0.011413544649258256\nVALD: \t Epoch: 227 \t Loss: -0.011109708971831342\n******************************\nEpoch: social-tag : 227\ntrain_loss -0.013905287834434392\nval_loss -0.011109708971831342\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 228 \t Loss: -0.01506164949387312\nTRAIN: \t Epoch: 228 \t Loss: -0.014636360108852386\nTRAIN: \t Epoch: 228 \t Loss: -0.014815135238071283\nTRAIN: \t Epoch: 228 \t Loss: -0.014608035329729319\nTRAIN: \t Epoch: 228 \t Loss: -0.01435304582118988\nTRAIN: \t Epoch: 228 \t Loss: -0.014238480478525162\nTRAIN: \t Epoch: 228 \t Loss: -0.014190160403294223\nTRAIN: \t Epoch: 228 \t Loss: -0.014115229598246515\nTRAIN: \t Epoch: 228 \t Loss: -0.01403356809169054\nTRAIN: \t Epoch: 228 \t Loss: -0.014039557985961437\nTRAIN: \t Epoch: 228 \t Loss: -0.013948744501579891\nTRAIN: \t Epoch: 228 \t Loss: -0.013978560455143452\nTRAIN: \t Epoch: 228 \t Loss: -0.014057362093948401\nTRAIN: \t Epoch: 228 \t Loss: -0.01408018677362374\nTRAIN: \t Epoch: 228 \t Loss: -0.014118560962378979\nTRAIN: \t Epoch: 228 \t Loss: -0.014124592184089124\nTRAIN: \t Epoch: 228 \t Loss: -0.014102677540744053\nTRAIN: \t Epoch: 228 \t Loss: -0.014074072603964143\nTRAIN: \t Epoch: 228 \t Loss: -0.014051449779225024\nTRAIN: \t Epoch: 228 \t Loss: -0.014065745053812861\nTRAIN: \t Epoch: 228 \t Loss: -0.01408281069860701\nVALD: \t Epoch: 228 \t Loss: -0.008005074225366116\nVALD: \t Epoch: 228 \t Loss: -0.009979338385164738\nVALD: \t Epoch: 228 \t Loss: -0.0072857397608459\nVALD: \t Epoch: 228 \t Loss: -0.008451422792859375\nVALD: \t Epoch: 228 \t Loss: -0.00860663456617346\n******************************\nEpoch: social-tag : 228\ntrain_loss -0.01408281069860701\nval_loss -0.00860663456617346\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 229 \t Loss: -0.01360042579472065\nTRAIN: \t Epoch: 229 \t Loss: -0.014070882927626371\nTRAIN: \t Epoch: 229 \t Loss: -0.014251006456712881\nTRAIN: \t Epoch: 229 \t Loss: -0.014260886004194617\nTRAIN: \t Epoch: 229 \t Loss: -0.014161759614944458\nTRAIN: \t Epoch: 229 \t Loss: -0.014191545235613981\nTRAIN: \t Epoch: 229 \t Loss: -0.014158366114965506\nTRAIN: \t Epoch: 229 \t Loss: -0.014229077496565878\nTRAIN: \t Epoch: 229 \t Loss: -0.014130174906717407\nTRAIN: \t Epoch: 229 \t Loss: -0.014200458023697137\nTRAIN: \t Epoch: 229 \t Loss: -0.014201935296031561\nTRAIN: \t Epoch: 229 \t Loss: -0.014235622948035598\nTRAIN: \t Epoch: 229 \t Loss: -0.014206409382705506\nTRAIN: \t Epoch: 229 \t Loss: -0.014151417118098055\nTRAIN: \t Epoch: 229 \t Loss: -0.014098926012714704\nTRAIN: \t Epoch: 229 \t Loss: -0.014108338160440326\nTRAIN: \t Epoch: 229 \t Loss: -0.014076146680642576\nTRAIN: \t Epoch: 229 \t Loss: -0.014031690441899829\nTRAIN: \t Epoch: 229 \t Loss: -0.014039264648760619\nTRAIN: \t Epoch: 229 \t Loss: -0.014032200537621975\nTRAIN: \t Epoch: 229 \t Loss: -0.014026427622841429\nVALD: \t Epoch: 229 \t Loss: -0.01371324434876442\nVALD: \t Epoch: 229 \t Loss: -0.013728972058743238\nVALD: \t Epoch: 229 \t Loss: -0.013738305307924747\nVALD: \t Epoch: 229 \t Loss: -0.013595449272543192\nVALD: \t Epoch: 229 \t Loss: -0.012940901105146669\n******************************\nEpoch: social-tag : 229\ntrain_loss -0.014026427622841429\nval_loss -0.012940901105146669\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 230 \t Loss: -0.013602769933640957\nTRAIN: \t Epoch: 230 \t Loss: -0.013765515759587288\nTRAIN: \t Epoch: 230 \t Loss: -0.013684990194936594\nTRAIN: \t Epoch: 230 \t Loss: -0.013514922000467777\nTRAIN: \t Epoch: 230 \t Loss: -0.013422513380646706\nTRAIN: \t Epoch: 230 \t Loss: -0.013635283025602499\nTRAIN: \t Epoch: 230 \t Loss: -0.013611997078572\nTRAIN: \t Epoch: 230 \t Loss: -0.013727308134548366\nTRAIN: \t Epoch: 230 \t Loss: -0.013716960843238566\nTRAIN: \t Epoch: 230 \t Loss: -0.013716467004269362\nTRAIN: \t Epoch: 230 \t Loss: -0.01379388469186696\nTRAIN: \t Epoch: 230 \t Loss: -0.013783629518002272\nTRAIN: \t Epoch: 230 \t Loss: -0.013754616778057355\nTRAIN: \t Epoch: 230 \t Loss: -0.01376815200118082\nTRAIN: \t Epoch: 230 \t Loss: -0.013840878692766031\nTRAIN: \t Epoch: 230 \t Loss: -0.013849261100403965\nTRAIN: \t Epoch: 230 \t Loss: -0.013875864391379496\nTRAIN: \t Epoch: 230 \t Loss: -0.013903516189505657\nTRAIN: \t Epoch: 230 \t Loss: -0.013960388489067554\nTRAIN: \t Epoch: 230 \t Loss: -0.013971503358334303\nTRAIN: \t Epoch: 230 \t Loss: -0.013992258168774195\nVALD: \t Epoch: 230 \t Loss: -0.010584449395537376\nVALD: \t Epoch: 230 \t Loss: -0.01153454091399908\nVALD: \t Epoch: 230 \t Loss: -0.009852275718003511\nVALD: \t Epoch: 230 \t Loss: -0.010364249465055764\nVALD: \t Epoch: 230 \t Loss: -0.010216208759713288\n******************************\nEpoch: social-tag : 230\ntrain_loss -0.013992258168774195\nval_loss -0.010216208759713288\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 231 \t Loss: -0.014491993002593517\nTRAIN: \t Epoch: 231 \t Loss: -0.014665739610791206\nTRAIN: \t Epoch: 231 \t Loss: -0.014850991467634836\nTRAIN: \t Epoch: 231 \t Loss: -0.014934501377865672\nTRAIN: \t Epoch: 231 \t Loss: -0.014941528253257275\nTRAIN: \t Epoch: 231 \t Loss: -0.014646533410996199\nTRAIN: \t Epoch: 231 \t Loss: -0.014735617408795016\nTRAIN: \t Epoch: 231 \t Loss: -0.01456915179733187\nTRAIN: \t Epoch: 231 \t Loss: -0.014466978713042207\nTRAIN: \t Epoch: 231 \t Loss: -0.014446577336639167\nTRAIN: \t Epoch: 231 \t Loss: -0.014331935142928904\nTRAIN: \t Epoch: 231 \t Loss: -0.014304633407543102\nTRAIN: \t Epoch: 231 \t Loss: -0.014249670319259167\nTRAIN: \t Epoch: 231 \t Loss: -0.01418743488777961\nTRAIN: \t Epoch: 231 \t Loss: -0.014125370855132739\nTRAIN: \t Epoch: 231 \t Loss: -0.014077121508307755\nTRAIN: \t Epoch: 231 \t Loss: -0.014011706116006654\nTRAIN: \t Epoch: 231 \t Loss: -0.014062902165783776\nTRAIN: \t Epoch: 231 \t Loss: -0.014075555005355886\nTRAIN: \t Epoch: 231 \t Loss: -0.014078791625797749\nTRAIN: \t Epoch: 231 \t Loss: -0.014077892511186916\nVALD: \t Epoch: 231 \t Loss: -0.012404898181557655\nVALD: \t Epoch: 231 \t Loss: -0.0124093908816576\nVALD: \t Epoch: 231 \t Loss: -0.011689082719385624\nVALD: \t Epoch: 231 \t Loss: -0.011822618544101715\nVALD: \t Epoch: 231 \t Loss: -0.011373347130374632\n******************************\nEpoch: social-tag : 231\ntrain_loss -0.014077892511186916\nval_loss -0.011373347130374632\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 232 \t Loss: -0.0135352099314332\nTRAIN: \t Epoch: 232 \t Loss: -0.013752214144915342\nTRAIN: \t Epoch: 232 \t Loss: -0.013865883151690165\nTRAIN: \t Epoch: 232 \t Loss: -0.01399606792256236\nTRAIN: \t Epoch: 232 \t Loss: -0.013845506496727467\nTRAIN: \t Epoch: 232 \t Loss: -0.013991656092305979\nTRAIN: \t Epoch: 232 \t Loss: -0.01404470724186727\nTRAIN: \t Epoch: 232 \t Loss: -0.014120269916020334\nTRAIN: \t Epoch: 232 \t Loss: -0.01406838982883427\nTRAIN: \t Epoch: 232 \t Loss: -0.014080303441733121\nTRAIN: \t Epoch: 232 \t Loss: -0.014044776728207415\nTRAIN: \t Epoch: 232 \t Loss: -0.014028840232640505\nTRAIN: \t Epoch: 232 \t Loss: -0.014046462969138073\nTRAIN: \t Epoch: 232 \t Loss: -0.014050879582230533\nTRAIN: \t Epoch: 232 \t Loss: -0.014067283707360427\nTRAIN: \t Epoch: 232 \t Loss: -0.01404123124666512\nTRAIN: \t Epoch: 232 \t Loss: -0.014075678170603864\nTRAIN: \t Epoch: 232 \t Loss: -0.014014910906553268\nTRAIN: \t Epoch: 232 \t Loss: -0.014021901217730422\nTRAIN: \t Epoch: 232 \t Loss: -0.014043315127491952\nTRAIN: \t Epoch: 232 \t Loss: -0.014014281728861419\nVALD: \t Epoch: 232 \t Loss: -0.012545553036034107\nVALD: \t Epoch: 232 \t Loss: -0.012693570926785469\nVALD: \t Epoch: 232 \t Loss: -0.011991563253104687\nVALD: \t Epoch: 232 \t Loss: -0.012226332444697618\nVALD: \t Epoch: 232 \t Loss: -0.011770405631134476\n******************************\nEpoch: social-tag : 232\ntrain_loss -0.014014281728861419\nval_loss -0.011770405631134476\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 233 \t Loss: -0.01396172121167183\nTRAIN: \t Epoch: 233 \t Loss: -0.013920069206506014\nTRAIN: \t Epoch: 233 \t Loss: -0.01397576400389274\nTRAIN: \t Epoch: 233 \t Loss: -0.013649628264829516\nTRAIN: \t Epoch: 233 \t Loss: -0.013827755860984326\nTRAIN: \t Epoch: 233 \t Loss: -0.013911776555081209\nTRAIN: \t Epoch: 233 \t Loss: -0.013982051850429602\nTRAIN: \t Epoch: 233 \t Loss: -0.013868247740902007\nTRAIN: \t Epoch: 233 \t Loss: -0.01386173659314712\nTRAIN: \t Epoch: 233 \t Loss: -0.013842987641692162\nTRAIN: \t Epoch: 233 \t Loss: -0.013881218280981888\nTRAIN: \t Epoch: 233 \t Loss: -0.013889564899727702\nTRAIN: \t Epoch: 233 \t Loss: -0.013928434883172695\nTRAIN: \t Epoch: 233 \t Loss: -0.01388812537438103\nTRAIN: \t Epoch: 233 \t Loss: -0.013962149247527122\nTRAIN: \t Epoch: 233 \t Loss: -0.013989589526318014\nTRAIN: \t Epoch: 233 \t Loss: -0.014011769391158047\nTRAIN: \t Epoch: 233 \t Loss: -0.014060098626133468\nTRAIN: \t Epoch: 233 \t Loss: -0.01404323129865684\nTRAIN: \t Epoch: 233 \t Loss: -0.014082209672778845\nTRAIN: \t Epoch: 233 \t Loss: -0.014082745659691422\nVALD: \t Epoch: 233 \t Loss: -0.010654465295374393\nVALD: \t Epoch: 233 \t Loss: -0.01124818017706275\nVALD: \t Epoch: 233 \t Loss: -0.009362449869513512\nVALD: \t Epoch: 233 \t Loss: -0.010072427801787853\nVALD: \t Epoch: 233 \t Loss: -0.009859097560630713\n******************************\nEpoch: social-tag : 233\ntrain_loss -0.014082745659691422\nval_loss -0.009859097560630713\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 234 \t Loss: -0.014538144692778587\nTRAIN: \t Epoch: 234 \t Loss: -0.014040810987353325\nTRAIN: \t Epoch: 234 \t Loss: -0.013766594852010408\nTRAIN: \t Epoch: 234 \t Loss: -0.013549395371228456\nTRAIN: \t Epoch: 234 \t Loss: -0.013735772669315338\nTRAIN: \t Epoch: 234 \t Loss: -0.013786807966729006\nTRAIN: \t Epoch: 234 \t Loss: -0.014030805389795984\nTRAIN: \t Epoch: 234 \t Loss: -0.014014616142958403\nTRAIN: \t Epoch: 234 \t Loss: -0.013930702064600255\nTRAIN: \t Epoch: 234 \t Loss: -0.01398147102445364\nTRAIN: \t Epoch: 234 \t Loss: -0.01407915565439246\nTRAIN: \t Epoch: 234 \t Loss: -0.014051373271892468\nTRAIN: \t Epoch: 234 \t Loss: -0.014003442886930246\nTRAIN: \t Epoch: 234 \t Loss: -0.013932424397873027\nTRAIN: \t Epoch: 234 \t Loss: -0.013969570398330688\nTRAIN: \t Epoch: 234 \t Loss: -0.013926643936429173\nTRAIN: \t Epoch: 234 \t Loss: -0.013958996819222675\nTRAIN: \t Epoch: 234 \t Loss: -0.0139814635945691\nTRAIN: \t Epoch: 234 \t Loss: -0.014022695635886569\nTRAIN: \t Epoch: 234 \t Loss: -0.014013179251924157\nTRAIN: \t Epoch: 234 \t Loss: -0.014027066316251674\nVALD: \t Epoch: 234 \t Loss: -0.010050260461866856\nVALD: \t Epoch: 234 \t Loss: -0.01109273498877883\nVALD: \t Epoch: 234 \t Loss: -0.008881778611491123\nVALD: \t Epoch: 234 \t Loss: -0.00936959299724549\nVALD: \t Epoch: 234 \t Loss: -0.009389172141678667\n******************************\nEpoch: social-tag : 234\ntrain_loss -0.014027066316251674\nval_loss -0.009389172141678667\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 235 \t Loss: -0.014050876721739769\nTRAIN: \t Epoch: 235 \t Loss: -0.014184539206326008\nTRAIN: \t Epoch: 235 \t Loss: -0.014392839434246222\nTRAIN: \t Epoch: 235 \t Loss: -0.014071940910071135\nTRAIN: \t Epoch: 235 \t Loss: -0.014108198508620263\nTRAIN: \t Epoch: 235 \t Loss: -0.014146978811671337\nTRAIN: \t Epoch: 235 \t Loss: -0.01426460181495973\nTRAIN: \t Epoch: 235 \t Loss: -0.014212753041647375\nTRAIN: \t Epoch: 235 \t Loss: -0.014188688144915633\nTRAIN: \t Epoch: 235 \t Loss: -0.014194944314658642\nTRAIN: \t Epoch: 235 \t Loss: -0.014217875728552992\nTRAIN: \t Epoch: 235 \t Loss: -0.014213914594923457\nTRAIN: \t Epoch: 235 \t Loss: -0.014215906007358661\nTRAIN: \t Epoch: 235 \t Loss: -0.014170966842877013\nTRAIN: \t Epoch: 235 \t Loss: -0.01417883603523175\nTRAIN: \t Epoch: 235 \t Loss: -0.014064696966670454\nTRAIN: \t Epoch: 235 \t Loss: -0.014094974352594684\nTRAIN: \t Epoch: 235 \t Loss: -0.014106229051119752\nTRAIN: \t Epoch: 235 \t Loss: -0.014083344293268104\nTRAIN: \t Epoch: 235 \t Loss: -0.014072492672130465\nTRAIN: \t Epoch: 235 \t Loss: -0.014068703044352388\nVALD: \t Epoch: 235 \t Loss: -0.01242659892886877\nVALD: \t Epoch: 235 \t Loss: -0.012777746189385653\nVALD: \t Epoch: 235 \t Loss: -0.012261702989538511\nVALD: \t Epoch: 235 \t Loss: -0.012344603892415762\nVALD: \t Epoch: 235 \t Loss: -0.011890777835139522\n******************************\nEpoch: social-tag : 235\ntrain_loss -0.014068703044352388\nval_loss -0.011890777835139522\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 236 \t Loss: -0.013718539848923683\nTRAIN: \t Epoch: 236 \t Loss: -0.013789556454867125\nTRAIN: \t Epoch: 236 \t Loss: -0.013499563870330652\nTRAIN: \t Epoch: 236 \t Loss: -0.013779139844700694\nTRAIN: \t Epoch: 236 \t Loss: -0.01381207387894392\nTRAIN: \t Epoch: 236 \t Loss: -0.013813477164755264\nTRAIN: \t Epoch: 236 \t Loss: -0.013771658364151205\nTRAIN: \t Epoch: 236 \t Loss: -0.013924969942308962\nTRAIN: \t Epoch: 236 \t Loss: -0.013911323932309946\nTRAIN: \t Epoch: 236 \t Loss: -0.01386785265058279\nTRAIN: \t Epoch: 236 \t Loss: -0.013881749304180796\nTRAIN: \t Epoch: 236 \t Loss: -0.013924457831308246\nTRAIN: \t Epoch: 236 \t Loss: -0.013948462903499603\nTRAIN: \t Epoch: 236 \t Loss: -0.014017772993871145\nTRAIN: \t Epoch: 236 \t Loss: -0.013997160643339158\nTRAIN: \t Epoch: 236 \t Loss: -0.014052062353584915\nTRAIN: \t Epoch: 236 \t Loss: -0.014038273407255901\nTRAIN: \t Epoch: 236 \t Loss: -0.014017763651079602\nTRAIN: \t Epoch: 236 \t Loss: -0.014048713700551736\nTRAIN: \t Epoch: 236 \t Loss: -0.014042078470811247\nTRAIN: \t Epoch: 236 \t Loss: -0.014064090864972694\nVALD: \t Epoch: 236 \t Loss: -0.011660585179924965\nVALD: \t Epoch: 236 \t Loss: -0.011665419675409794\nVALD: \t Epoch: 236 \t Loss: -0.009542145455876986\nVALD: \t Epoch: 236 \t Loss: -0.010103891137987375\nVALD: \t Epoch: 236 \t Loss: -0.00985898766156748\n******************************\nEpoch: social-tag : 236\ntrain_loss -0.014064090864972694\nval_loss -0.00985898766156748\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 237 \t Loss: -0.014136874116957188\nTRAIN: \t Epoch: 237 \t Loss: -0.014164211228489876\nTRAIN: \t Epoch: 237 \t Loss: -0.014179730166991552\nTRAIN: \t Epoch: 237 \t Loss: -0.014314950909465551\nTRAIN: \t Epoch: 237 \t Loss: -0.01433052383363247\nTRAIN: \t Epoch: 237 \t Loss: -0.014241010726739963\nTRAIN: \t Epoch: 237 \t Loss: -0.01424882813755955\nTRAIN: \t Epoch: 237 \t Loss: -0.014300281181931496\nTRAIN: \t Epoch: 237 \t Loss: -0.014326135731405683\nTRAIN: \t Epoch: 237 \t Loss: -0.014274845831096172\nTRAIN: \t Epoch: 237 \t Loss: -0.014253030869771133\nTRAIN: \t Epoch: 237 \t Loss: -0.014187310822308064\nTRAIN: \t Epoch: 237 \t Loss: -0.014109553912511239\nTRAIN: \t Epoch: 237 \t Loss: -0.014137421268969774\nTRAIN: \t Epoch: 237 \t Loss: -0.014123220617572466\nTRAIN: \t Epoch: 237 \t Loss: -0.014078360516577959\nTRAIN: \t Epoch: 237 \t Loss: -0.014047457781784674\nTRAIN: \t Epoch: 237 \t Loss: -0.014084750320762396\nTRAIN: \t Epoch: 237 \t Loss: -0.014113186564492551\nTRAIN: \t Epoch: 237 \t Loss: -0.014083086932078003\nTRAIN: \t Epoch: 237 \t Loss: -0.014070802637495806\nVALD: \t Epoch: 237 \t Loss: -0.013881106860935688\nVALD: \t Epoch: 237 \t Loss: -0.013506305869668722\nVALD: \t Epoch: 237 \t Loss: -0.013125790593524774\nVALD: \t Epoch: 237 \t Loss: -0.013108956394717097\nVALD: \t Epoch: 237 \t Loss: -0.012525063973885996\n******************************\nEpoch: social-tag : 237\ntrain_loss -0.014070802637495806\nval_loss -0.012525063973885996\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 238 \t Loss: -0.01350326370447874\nTRAIN: \t Epoch: 238 \t Loss: -0.014226139057427645\nTRAIN: \t Epoch: 238 \t Loss: -0.014132498453060785\nTRAIN: \t Epoch: 238 \t Loss: -0.01406234735623002\nTRAIN: \t Epoch: 238 \t Loss: -0.01401804219931364\nTRAIN: \t Epoch: 238 \t Loss: -0.013958324367801348\nTRAIN: \t Epoch: 238 \t Loss: -0.01417660087879215\nTRAIN: \t Epoch: 238 \t Loss: -0.014230341301299632\nTRAIN: \t Epoch: 238 \t Loss: -0.014182899250752397\nTRAIN: \t Epoch: 238 \t Loss: -0.014363931212574243\nTRAIN: \t Epoch: 238 \t Loss: -0.014336025257679548\nTRAIN: \t Epoch: 238 \t Loss: -0.014305520802736282\nTRAIN: \t Epoch: 238 \t Loss: -0.014240706554399086\nTRAIN: \t Epoch: 238 \t Loss: -0.01421123736404947\nTRAIN: \t Epoch: 238 \t Loss: -0.014168357414503892\nTRAIN: \t Epoch: 238 \t Loss: -0.01417239906731993\nTRAIN: \t Epoch: 238 \t Loss: -0.01416169632883633\nTRAIN: \t Epoch: 238 \t Loss: -0.014041669408066405\nTRAIN: \t Epoch: 238 \t Loss: -0.014043084100673073\nTRAIN: \t Epoch: 238 \t Loss: -0.014035648107528687\nTRAIN: \t Epoch: 238 \t Loss: -0.014029602236534508\nVALD: \t Epoch: 238 \t Loss: -0.01129940990358591\nVALD: \t Epoch: 238 \t Loss: -0.010931190568953753\nVALD: \t Epoch: 238 \t Loss: -0.009694715651373068\nVALD: \t Epoch: 238 \t Loss: -0.009950398933142424\nVALD: \t Epoch: 238 \t Loss: -0.009757137912866958\n******************************\nEpoch: social-tag : 238\ntrain_loss -0.014029602236534508\nval_loss -0.009757137912866958\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 239 \t Loss: -0.01398397795855999\nTRAIN: \t Epoch: 239 \t Loss: -0.013977527618408203\nTRAIN: \t Epoch: 239 \t Loss: -0.014112835749983788\nTRAIN: \t Epoch: 239 \t Loss: -0.014156226301565766\nTRAIN: \t Epoch: 239 \t Loss: -0.01413768008351326\nTRAIN: \t Epoch: 239 \t Loss: -0.014150904336323341\nTRAIN: \t Epoch: 239 \t Loss: -0.014243849154029573\nTRAIN: \t Epoch: 239 \t Loss: -0.014229290885850787\nTRAIN: \t Epoch: 239 \t Loss: -0.014209046235515011\nTRAIN: \t Epoch: 239 \t Loss: -0.01414120029658079\nTRAIN: \t Epoch: 239 \t Loss: -0.01404509244656021\nTRAIN: \t Epoch: 239 \t Loss: -0.01413894227395455\nTRAIN: \t Epoch: 239 \t Loss: -0.014192716218531132\nTRAIN: \t Epoch: 239 \t Loss: -0.014245250195797001\nTRAIN: \t Epoch: 239 \t Loss: -0.014226666341225306\nTRAIN: \t Epoch: 239 \t Loss: -0.014240279153455049\nTRAIN: \t Epoch: 239 \t Loss: -0.014190215233932523\nTRAIN: \t Epoch: 239 \t Loss: -0.014173760596248839\nTRAIN: \t Epoch: 239 \t Loss: -0.014135837113778842\nTRAIN: \t Epoch: 239 \t Loss: -0.014148151455447077\nTRAIN: \t Epoch: 239 \t Loss: -0.014140016807899534\nVALD: \t Epoch: 239 \t Loss: -0.0035659538116306067\nVALD: \t Epoch: 239 \t Loss: -0.007482405169866979\nVALD: \t Epoch: 239 \t Loss: -0.0030023473470161357\nVALD: \t Epoch: 239 \t Loss: -0.005085849843453616\nVALD: \t Epoch: 239 \t Loss: -0.005794182347790631\n******************************\nEpoch: social-tag : 239\ntrain_loss -0.014140016807899534\nval_loss -0.005794182347790631\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 240 \t Loss: -0.014239133335649967\nTRAIN: \t Epoch: 240 \t Loss: -0.01398267038166523\nTRAIN: \t Epoch: 240 \t Loss: -0.014121554481486479\nTRAIN: \t Epoch: 240 \t Loss: -0.014047995442524552\nTRAIN: \t Epoch: 240 \t Loss: -0.014241364412009716\nTRAIN: \t Epoch: 240 \t Loss: -0.014306015179802975\nTRAIN: \t Epoch: 240 \t Loss: -0.01434299030474254\nTRAIN: \t Epoch: 240 \t Loss: -0.014217338291928172\nTRAIN: \t Epoch: 240 \t Loss: -0.014142233981854387\nTRAIN: \t Epoch: 240 \t Loss: -0.014181062020361424\nTRAIN: \t Epoch: 240 \t Loss: -0.014079219492321665\nTRAIN: \t Epoch: 240 \t Loss: -0.014050979710494479\nTRAIN: \t Epoch: 240 \t Loss: -0.014017515887434665\nTRAIN: \t Epoch: 240 \t Loss: -0.013988573037620102\nTRAIN: \t Epoch: 240 \t Loss: -0.014112784651418527\nTRAIN: \t Epoch: 240 \t Loss: -0.014087260817177594\nTRAIN: \t Epoch: 240 \t Loss: -0.014101764187216759\nTRAIN: \t Epoch: 240 \t Loss: -0.01407526040242778\nTRAIN: \t Epoch: 240 \t Loss: -0.01408778341781152\nTRAIN: \t Epoch: 240 \t Loss: -0.014070127438753844\nTRAIN: \t Epoch: 240 \t Loss: -0.014071444869501001\nVALD: \t Epoch: 240 \t Loss: -0.00819255318492651\nVALD: \t Epoch: 240 \t Loss: -0.009796271100640297\nVALD: \t Epoch: 240 \t Loss: -0.0066530066639340175\nVALD: \t Epoch: 240 \t Loss: -0.007768765681248624\nVALD: \t Epoch: 240 \t Loss: -0.008001563443701816\n******************************\nEpoch: social-tag : 240\ntrain_loss -0.014071444869501001\nval_loss -0.008001563443701816\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 241 \t Loss: -0.013891029171645641\nTRAIN: \t Epoch: 241 \t Loss: -0.014138287864625454\nTRAIN: \t Epoch: 241 \t Loss: -0.014498758129775524\nTRAIN: \t Epoch: 241 \t Loss: -0.014050341909751296\nTRAIN: \t Epoch: 241 \t Loss: -0.014149254746735096\nTRAIN: \t Epoch: 241 \t Loss: -0.014074598128596941\nTRAIN: \t Epoch: 241 \t Loss: -0.014049149384456021\nTRAIN: \t Epoch: 241 \t Loss: -0.014030347228981555\nTRAIN: \t Epoch: 241 \t Loss: -0.014017484978669219\nTRAIN: \t Epoch: 241 \t Loss: -0.013978357147425412\nTRAIN: \t Epoch: 241 \t Loss: -0.014081224629824812\nTRAIN: \t Epoch: 241 \t Loss: -0.014081079590444764\nTRAIN: \t Epoch: 241 \t Loss: -0.014003041773461379\nTRAIN: \t Epoch: 241 \t Loss: -0.014037881778287036\nTRAIN: \t Epoch: 241 \t Loss: -0.01405275637904803\nTRAIN: \t Epoch: 241 \t Loss: -0.014039799978490919\nTRAIN: \t Epoch: 241 \t Loss: -0.01409357156166259\nTRAIN: \t Epoch: 241 \t Loss: -0.014111460962643227\nTRAIN: \t Epoch: 241 \t Loss: -0.014064219976334195\nTRAIN: \t Epoch: 241 \t Loss: -0.01403542486950755\nTRAIN: \t Epoch: 241 \t Loss: -0.014046171167398657\nVALD: \t Epoch: 241 \t Loss: -0.010424365289509296\nVALD: \t Epoch: 241 \t Loss: -0.011440997943282127\nVALD: \t Epoch: 241 \t Loss: -0.009534767518440882\nVALD: \t Epoch: 241 \t Loss: -0.010262603405863047\nVALD: \t Epoch: 241 \t Loss: -0.010077633143623093\n******************************\nEpoch: social-tag : 241\ntrain_loss -0.014046171167398657\nval_loss -0.010077633143623093\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 242 \t Loss: -0.013634389266371727\nTRAIN: \t Epoch: 242 \t Loss: -0.014329082798212767\nTRAIN: \t Epoch: 242 \t Loss: -0.014278942408661047\nTRAIN: \t Epoch: 242 \t Loss: -0.013997571542859077\nTRAIN: \t Epoch: 242 \t Loss: -0.01402658149600029\nTRAIN: \t Epoch: 242 \t Loss: -0.01415532873943448\nTRAIN: \t Epoch: 242 \t Loss: -0.014224159930433546\nTRAIN: \t Epoch: 242 \t Loss: -0.014191689202561975\nTRAIN: \t Epoch: 242 \t Loss: -0.014172244817018509\nTRAIN: \t Epoch: 242 \t Loss: -0.014221428520977497\nTRAIN: \t Epoch: 242 \t Loss: -0.014256349764764309\nTRAIN: \t Epoch: 242 \t Loss: -0.014222284856562814\nTRAIN: \t Epoch: 242 \t Loss: -0.014246602041216997\nTRAIN: \t Epoch: 242 \t Loss: -0.014215302121426378\nTRAIN: \t Epoch: 242 \t Loss: -0.014265815913677215\nTRAIN: \t Epoch: 242 \t Loss: -0.01423739327583462\nTRAIN: \t Epoch: 242 \t Loss: -0.01421934158048209\nTRAIN: \t Epoch: 242 \t Loss: -0.014196021171907583\nTRAIN: \t Epoch: 242 \t Loss: -0.01412684004753828\nTRAIN: \t Epoch: 242 \t Loss: -0.014089407585561275\nTRAIN: \t Epoch: 242 \t Loss: -0.014099037063608927\nVALD: \t Epoch: 242 \t Loss: -0.012361454777419567\nVALD: \t Epoch: 242 \t Loss: -0.012809647247195244\nVALD: \t Epoch: 242 \t Loss: -0.012012407494088015\nVALD: \t Epoch: 242 \t Loss: -0.012199270771816373\nVALD: \t Epoch: 242 \t Loss: -0.011719858204876935\n******************************\nEpoch: social-tag : 242\ntrain_loss -0.014099037063608927\nval_loss -0.011719858204876935\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 243 \t Loss: -0.013529732823371887\nTRAIN: \t Epoch: 243 \t Loss: -0.013853379059582949\nTRAIN: \t Epoch: 243 \t Loss: -0.014136424288153648\nTRAIN: \t Epoch: 243 \t Loss: -0.014265866950154305\nTRAIN: \t Epoch: 243 \t Loss: -0.0143094127997756\nTRAIN: \t Epoch: 243 \t Loss: -0.014225806264827648\nTRAIN: \t Epoch: 243 \t Loss: -0.0144387296001826\nTRAIN: \t Epoch: 243 \t Loss: -0.014387606992386281\nTRAIN: \t Epoch: 243 \t Loss: -0.01430125834627284\nTRAIN: \t Epoch: 243 \t Loss: -0.014200009033083915\nTRAIN: \t Epoch: 243 \t Loss: -0.01420340466905724\nTRAIN: \t Epoch: 243 \t Loss: -0.014231911860406399\nTRAIN: \t Epoch: 243 \t Loss: -0.01416632504417346\nTRAIN: \t Epoch: 243 \t Loss: -0.014119933492371015\nTRAIN: \t Epoch: 243 \t Loss: -0.014029600036640963\nTRAIN: \t Epoch: 243 \t Loss: -0.014077364525292069\nTRAIN: \t Epoch: 243 \t Loss: -0.014137088211581987\nTRAIN: \t Epoch: 243 \t Loss: -0.014123259991821315\nTRAIN: \t Epoch: 243 \t Loss: -0.014066709695678008\nTRAIN: \t Epoch: 243 \t Loss: -0.014083648659288883\nTRAIN: \t Epoch: 243 \t Loss: -0.014066311907749866\nVALD: \t Epoch: 243 \t Loss: -0.012812472879886627\nVALD: \t Epoch: 243 \t Loss: -0.013029145076870918\nVALD: \t Epoch: 243 \t Loss: -0.012912068826456865\nVALD: \t Epoch: 243 \t Loss: -0.01282812375575304\nVALD: \t Epoch: 243 \t Loss: -0.012237301195301295\n******************************\nEpoch: social-tag : 243\ntrain_loss -0.014066311907749866\nval_loss -0.012237301195301295\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 244 \t Loss: -0.01313759759068489\nTRAIN: \t Epoch: 244 \t Loss: -0.013733126688748598\nTRAIN: \t Epoch: 244 \t Loss: -0.013878064540525278\nTRAIN: \t Epoch: 244 \t Loss: -0.014210122404620051\nTRAIN: \t Epoch: 244 \t Loss: -0.01439079325646162\nTRAIN: \t Epoch: 244 \t Loss: -0.014298886526376009\nTRAIN: \t Epoch: 244 \t Loss: -0.014289823094648975\nTRAIN: \t Epoch: 244 \t Loss: -0.014310591737739742\nTRAIN: \t Epoch: 244 \t Loss: -0.014292342898746332\nTRAIN: \t Epoch: 244 \t Loss: -0.014180054608732461\nTRAIN: \t Epoch: 244 \t Loss: -0.014210768721320412\nTRAIN: \t Epoch: 244 \t Loss: -0.014204450805361072\nTRAIN: \t Epoch: 244 \t Loss: -0.01418435344329247\nTRAIN: \t Epoch: 244 \t Loss: -0.014219149082366909\nTRAIN: \t Epoch: 244 \t Loss: -0.014183071814477444\nTRAIN: \t Epoch: 244 \t Loss: -0.014151394832879305\nTRAIN: \t Epoch: 244 \t Loss: -0.01414459987598307\nTRAIN: \t Epoch: 244 \t Loss: -0.014155621381683482\nTRAIN: \t Epoch: 244 \t Loss: -0.014127179686176149\nTRAIN: \t Epoch: 244 \t Loss: -0.014128620084375144\nTRAIN: \t Epoch: 244 \t Loss: -0.014117215503705127\nVALD: \t Epoch: 244 \t Loss: -0.009067561477422714\nVALD: \t Epoch: 244 \t Loss: -0.010503801051527262\nVALD: \t Epoch: 244 \t Loss: -0.00801136166167756\nVALD: \t Epoch: 244 \t Loss: -0.008933331642765552\nVALD: \t Epoch: 244 \t Loss: -0.009004315748306865\n******************************\nEpoch: social-tag : 244\ntrain_loss -0.014117215503705127\nval_loss -0.009004315748306865\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 245 \t Loss: -0.013870662078261375\nTRAIN: \t Epoch: 245 \t Loss: -0.013781748246401548\nTRAIN: \t Epoch: 245 \t Loss: -0.013983687075475851\nTRAIN: \t Epoch: 245 \t Loss: -0.013931410387158394\nTRAIN: \t Epoch: 245 \t Loss: -0.014084616675972939\nTRAIN: \t Epoch: 245 \t Loss: -0.014077067064742247\nTRAIN: \t Epoch: 245 \t Loss: -0.01410598454198667\nTRAIN: \t Epoch: 245 \t Loss: -0.014130300958640873\nTRAIN: \t Epoch: 245 \t Loss: -0.014089230448007584\nTRAIN: \t Epoch: 245 \t Loss: -0.014119041338562965\nTRAIN: \t Epoch: 245 \t Loss: -0.014140714129263704\nTRAIN: \t Epoch: 245 \t Loss: -0.014188764539236823\nTRAIN: \t Epoch: 245 \t Loss: -0.014114282738703948\nTRAIN: \t Epoch: 245 \t Loss: -0.014132320348705565\nTRAIN: \t Epoch: 245 \t Loss: -0.014103513211011887\nTRAIN: \t Epoch: 245 \t Loss: -0.014088028285186738\nTRAIN: \t Epoch: 245 \t Loss: -0.014098863829584682\nTRAIN: \t Epoch: 245 \t Loss: -0.014096838215159046\nTRAIN: \t Epoch: 245 \t Loss: -0.014126799461480818\nTRAIN: \t Epoch: 245 \t Loss: -0.014115841453894972\nTRAIN: \t Epoch: 245 \t Loss: -0.014086358852357064\nVALD: \t Epoch: 245 \t Loss: -0.01193287968635559\nVALD: \t Epoch: 245 \t Loss: -0.012232331093400717\nVALD: \t Epoch: 245 \t Loss: -0.011410458013415337\nVALD: \t Epoch: 245 \t Loss: -0.01173975900746882\nVALD: \t Epoch: 245 \t Loss: -0.011323023341512143\n******************************\nEpoch: social-tag : 245\ntrain_loss -0.014086358852357064\nval_loss -0.011323023341512143\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 246 \t Loss: -0.014857128262519836\nTRAIN: \t Epoch: 246 \t Loss: -0.014710674993693829\nTRAIN: \t Epoch: 246 \t Loss: -0.014592309792836508\nTRAIN: \t Epoch: 246 \t Loss: -0.014511534478515387\nTRAIN: \t Epoch: 246 \t Loss: -0.014375772140920163\nTRAIN: \t Epoch: 246 \t Loss: -0.014411834999918938\nTRAIN: \t Epoch: 246 \t Loss: -0.014414659568241664\nTRAIN: \t Epoch: 246 \t Loss: -0.01442569715436548\nTRAIN: \t Epoch: 246 \t Loss: -0.014293174155884318\nTRAIN: \t Epoch: 246 \t Loss: -0.01423760298639536\nTRAIN: \t Epoch: 246 \t Loss: -0.01415665845640681\nTRAIN: \t Epoch: 246 \t Loss: -0.014108581934124231\nTRAIN: \t Epoch: 246 \t Loss: -0.014111586559850436\nTRAIN: \t Epoch: 246 \t Loss: -0.014075092771755797\nTRAIN: \t Epoch: 246 \t Loss: -0.014031239661077659\nTRAIN: \t Epoch: 246 \t Loss: -0.01410218293312937\nTRAIN: \t Epoch: 246 \t Loss: -0.014108522028169212\nTRAIN: \t Epoch: 246 \t Loss: -0.014113466669287946\nTRAIN: \t Epoch: 246 \t Loss: -0.014130852136172746\nTRAIN: \t Epoch: 246 \t Loss: -0.01412527752108872\nTRAIN: \t Epoch: 246 \t Loss: -0.014119266383723286\nVALD: \t Epoch: 246 \t Loss: -0.009868496097624302\nVALD: \t Epoch: 246 \t Loss: -0.011070303153246641\nVALD: \t Epoch: 246 \t Loss: -0.008963915364195904\nVALD: \t Epoch: 246 \t Loss: -0.009763835812918842\nVALD: \t Epoch: 246 \t Loss: -0.009711169678231945\n******************************\nEpoch: social-tag : 246\ntrain_loss -0.014119266383723286\nval_loss -0.009711169678231945\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 247 \t Loss: -0.013766041025519371\nTRAIN: \t Epoch: 247 \t Loss: -0.013851051218807697\nTRAIN: \t Epoch: 247 \t Loss: -0.01375164557248354\nTRAIN: \t Epoch: 247 \t Loss: -0.013835916062816978\nTRAIN: \t Epoch: 247 \t Loss: -0.01377867180854082\nTRAIN: \t Epoch: 247 \t Loss: -0.013880497155090174\nTRAIN: \t Epoch: 247 \t Loss: -0.013937169419867652\nTRAIN: \t Epoch: 247 \t Loss: -0.0138425687327981\nTRAIN: \t Epoch: 247 \t Loss: -0.01392383293973075\nTRAIN: \t Epoch: 247 \t Loss: -0.013961294386535882\nTRAIN: \t Epoch: 247 \t Loss: -0.013999348844994198\nTRAIN: \t Epoch: 247 \t Loss: -0.014002601926525434\nTRAIN: \t Epoch: 247 \t Loss: -0.014029978416286983\nTRAIN: \t Epoch: 247 \t Loss: -0.014067717295672213\nTRAIN: \t Epoch: 247 \t Loss: -0.014067451904217402\nTRAIN: \t Epoch: 247 \t Loss: -0.014105259499046952\nTRAIN: \t Epoch: 247 \t Loss: -0.014073645641260287\nTRAIN: \t Epoch: 247 \t Loss: -0.014072553306404088\nTRAIN: \t Epoch: 247 \t Loss: -0.014089159814542845\nTRAIN: \t Epoch: 247 \t Loss: -0.014111432200297713\nTRAIN: \t Epoch: 247 \t Loss: -0.014127012919534053\nVALD: \t Epoch: 247 \t Loss: -0.014125723391771317\nVALD: \t Epoch: 247 \t Loss: -0.013667029794305563\nVALD: \t Epoch: 247 \t Loss: -0.013253621136148771\nVALD: \t Epoch: 247 \t Loss: -0.013003805885091424\nVALD: \t Epoch: 247 \t Loss: -0.012433441748749614\n******************************\nEpoch: social-tag : 247\ntrain_loss -0.014127012919534053\nval_loss -0.012433441748749614\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 248 \t Loss: -0.014853740110993385\nTRAIN: \t Epoch: 248 \t Loss: -0.014724884647876024\nTRAIN: \t Epoch: 248 \t Loss: -0.014249509821335474\nTRAIN: \t Epoch: 248 \t Loss: -0.014000864932313561\nTRAIN: \t Epoch: 248 \t Loss: -0.014094853401184082\nTRAIN: \t Epoch: 248 \t Loss: -0.014119458540032307\nTRAIN: \t Epoch: 248 \t Loss: -0.014155248712216104\nTRAIN: \t Epoch: 248 \t Loss: -0.014227128820493817\nTRAIN: \t Epoch: 248 \t Loss: -0.014241385273635387\nTRAIN: \t Epoch: 248 \t Loss: -0.014228615164756774\nTRAIN: \t Epoch: 248 \t Loss: -0.0142714013768868\nTRAIN: \t Epoch: 248 \t Loss: -0.014298742093766728\nTRAIN: \t Epoch: 248 \t Loss: -0.014268193752146684\nTRAIN: \t Epoch: 248 \t Loss: -0.014299981695200716\nTRAIN: \t Epoch: 248 \t Loss: -0.014213051833212375\nTRAIN: \t Epoch: 248 \t Loss: -0.014256956812459975\nTRAIN: \t Epoch: 248 \t Loss: -0.014234051327495015\nTRAIN: \t Epoch: 248 \t Loss: -0.014204605068597529\nTRAIN: \t Epoch: 248 \t Loss: -0.014096516772712531\nTRAIN: \t Epoch: 248 \t Loss: -0.014074226794764399\nTRAIN: \t Epoch: 248 \t Loss: -0.014071465124510395\nVALD: \t Epoch: 248 \t Loss: -0.00961935706436634\nVALD: \t Epoch: 248 \t Loss: -0.01091139717027545\nVALD: \t Epoch: 248 \t Loss: -0.009215866370747486\nVALD: \t Epoch: 248 \t Loss: -0.009991478291340172\nVALD: \t Epoch: 248 \t Loss: -0.009890237004284697\n******************************\nEpoch: social-tag : 248\ntrain_loss -0.014071465124510395\nval_loss -0.009890237004284697\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\nTRAIN: \t Epoch: 249 \t Loss: -0.013842332176864147\nTRAIN: \t Epoch: 249 \t Loss: -0.01376730715855956\nTRAIN: \t Epoch: 249 \t Loss: -0.013756039241949717\nTRAIN: \t Epoch: 249 \t Loss: -0.013804279500618577\nTRAIN: \t Epoch: 249 \t Loss: -0.013869357481598853\nTRAIN: \t Epoch: 249 \t Loss: -0.013951883961757025\nTRAIN: \t Epoch: 249 \t Loss: -0.014013945391135556\nTRAIN: \t Epoch: 249 \t Loss: -0.014109523966908455\nTRAIN: \t Epoch: 249 \t Loss: -0.014185559004545212\nTRAIN: \t Epoch: 249 \t Loss: -0.014039548859000207\nTRAIN: \t Epoch: 249 \t Loss: -0.01407906963405284\nTRAIN: \t Epoch: 249 \t Loss: -0.014147726818919182\nTRAIN: \t Epoch: 249 \t Loss: -0.014170136589270372\nTRAIN: \t Epoch: 249 \t Loss: -0.014186686902706112\nTRAIN: \t Epoch: 249 \t Loss: -0.014145154754320781\nTRAIN: \t Epoch: 249 \t Loss: -0.014212817302905023\nTRAIN: \t Epoch: 249 \t Loss: -0.014238101046751528\nTRAIN: \t Epoch: 249 \t Loss: -0.014200331125822332\nTRAIN: \t Epoch: 249 \t Loss: -0.014163109964053882\nTRAIN: \t Epoch: 249 \t Loss: -0.01412910856306553\nTRAIN: \t Epoch: 249 \t Loss: -0.014127550349386269\nVALD: \t Epoch: 249 \t Loss: -0.01027597300708294\nVALD: \t Epoch: 249 \t Loss: -0.011046560481190681\nVALD: \t Epoch: 249 \t Loss: -0.009068657333652178\nVALD: \t Epoch: 249 \t Loss: -0.009657913818955421\nVALD: \t Epoch: 249 \t Loss: -0.009512546653716845\n******************************\nEpoch: social-tag : 249\ntrain_loss -0.014127550349386269\nval_loss -0.009512546653716845\n{'min_val_epoch': 202, 'min_val_loss': -0.013223659589094817}\n******************************\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import pickle\n",
        "import argparse\n",
        "import glob\n",
        "import torch.distributions.multivariate_normal as torchdist\n",
        "import copy\n",
        "\n",
        "def test(model,loader_test, KSTEPS=20):\n",
        "    model.eval()\n",
        "    ade_bigls = []\n",
        "    fde_bigls = []\n",
        "    raw_data_dict = {}\n",
        "    step =0\n",
        "    for batch in loader_test:\n",
        "        step+=1\n",
        "        #Get data\n",
        "        batch = [tensor for tensor in batch]\n",
        "        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, non_linear_ped,\\\n",
        "         loss_mask,V_obs,A_obs,A_dir_obs,V_tr,A_tr,A_dir_tr = batch\n",
        "\n",
        "\n",
        "        num_of_objs = obs_traj_rel.shape[1]\n",
        "\n",
        "        #Forward\n",
        "        #V_obs = batch,seq,node,feat\n",
        "        #V_obs_tmp = batch,feat,seq,node\n",
        "        V_obs_tmp =V_obs.permute(0,3,1,2)\n",
        "\n",
        "        V_pred,_,_ = model(V_obs_tmp,A_obs.squeeze(),A_dir_obs.squeeze())\n",
        "        # print(V_pred.shape)\n",
        "        # torch.Size([1, 5, 12, 2])\n",
        "        # torch.Size([12, 2, 5])\n",
        "        V_pred = V_pred.permute(0,2,3,1)\n",
        "        # torch.Size([1, 12, 2, 5])>>seq,node,feat\n",
        "        # V_pred= torch.rand_like(V_tr).cuda()\n",
        "\n",
        "\n",
        "        V_tr = V_tr.squeeze()\n",
        "        A_tr = A_tr.squeeze()\n",
        "        A_dir_tr = A_dir_tr.squeeze()\n",
        "        V_pred = V_pred.squeeze()\n",
        "        num_of_objs = obs_traj_rel.shape[1]\n",
        "        V_pred,V_tr =  V_pred[:,:num_of_objs,:],V_tr[:,:num_of_objs,:]\n",
        "        #print(V_pred.shape)\n",
        "\n",
        "        #For now I have my bi-variate parameters\n",
        "        #normx =  V_pred[:,:,0:1]\n",
        "        #normy =  V_pred[:,:,1:2]\n",
        "        sx = torch.exp(V_pred[:,:,2]) #sx\n",
        "        sy = torch.exp(V_pred[:,:,3]) #sy\n",
        "        corr = torch.tanh(V_pred[:,:,4]) #corr\n",
        "\n",
        "        cov = torch.zeros(V_pred.shape[0],V_pred.shape[1],2,2)\n",
        "        cov[:,:,0,0]= sx*sx\n",
        "        cov[:,:,0,1]= corr*sx*sy\n",
        "        cov[:,:,1,0]= corr*sx*sy\n",
        "        cov[:,:,1,1]= sy*sy\n",
        "        mean = V_pred[:,:,0:2]\n",
        "\n",
        "        mvnormal = torchdist.MultivariateNormal(mean,cov)\n",
        "\n",
        "\n",
        "        ### Rel to abs\n",
        "        ##obs_traj.shape = torch.Size([1, 6, 2, 8]) Batch, Ped ID, x|y, Seq Len\n",
        "\n",
        "        #Now sample 20 samples\n",
        "        ade_ls = {}\n",
        "        fde_ls = {}\n",
        "        V_x = seq_to_nodes(obs_traj.data.cpu().numpy().copy())\n",
        "        V_x_rel_to_abs = nodes_rel_to_nodes_abs(V_obs.data.cpu().numpy().squeeze().copy(),\n",
        "                                                 V_x[0,:,:].copy())\n",
        "\n",
        "        V_y = seq_to_nodes(pred_traj_gt.data.cpu().numpy().copy())\n",
        "        V_y_rel_to_abs = nodes_rel_to_nodes_abs(V_tr.data.cpu().numpy().squeeze().copy(),\n",
        "                                                 V_x[-1,:,:].copy())\n",
        "\n",
        "        raw_data_dict[step] = {}\n",
        "        raw_data_dict[step]['obs'] = copy.deepcopy(V_x_rel_to_abs)\n",
        "        raw_data_dict[step]['trgt'] = copy.deepcopy(V_y_rel_to_abs)\n",
        "        raw_data_dict[step]['pred'] = []\n",
        "\n",
        "        for n in range(num_of_objs):\n",
        "            ade_ls[n]=[]\n",
        "            fde_ls[n]=[]\n",
        "\n",
        "        for k in range(KSTEPS):\n",
        "\n",
        "            V_pred = mvnormal.sample()\n",
        "\n",
        "\n",
        "\n",
        "            #V_pred = seq_to_nodes(pred_traj_gt.data.numpy().copy())\n",
        "            V_pred_rel_to_abs = nodes_rel_to_nodes_abs(V_pred.data.cpu().numpy().squeeze().copy(),\n",
        "                                                     V_x[-1,:,:].copy())\n",
        "            raw_data_dict[step]['pred'].append(copy.deepcopy(V_pred_rel_to_abs))\n",
        "\n",
        "           # print(V_pred_rel_to_abs.shape) #(12, 3, 2) = seq, ped, location\n",
        "            for n in range(num_of_objs):\n",
        "                pred = []\n",
        "                target = []\n",
        "                obsrvs = []\n",
        "                number_of = []\n",
        "                pred.append(V_pred_rel_to_abs[:,n:n+1,:])\n",
        "                target.append(V_y_rel_to_abs[:,n:n+1,:])\n",
        "                obsrvs.append(V_x_rel_to_abs[:,n:n+1,:])\n",
        "                number_of.append(1)\n",
        "\n",
        "                ade_ls[n].append(ade(pred,target,number_of))\n",
        "                fde_ls[n].append(fde(pred,target,number_of))\n",
        "\n",
        "        for n in range(num_of_objs):\n",
        "            ade_bigls.append(min(ade_ls[n]))\n",
        "            fde_bigls.append(min(fde_ls[n]))\n",
        "\n",
        "    ade_ = sum(ade_bigls)/len(ade_bigls)\n",
        "    fde_ = sum(fde_bigls)/len(fde_bigls)\n",
        "    return ade_,fde_,raw_data_dict\n",
        "#check how our new best saved model performs on test dataset\n",
        "######################################################################################################\n",
        "paths = ['/kaggle/working/checkpoint/social-tag']\n",
        "KSTEPS=20\n",
        "\n",
        "print(\"*\"*50)\n",
        "print('Number of samples:',KSTEPS)\n",
        "print(\"*\"*50)\n",
        "\n",
        "for feta in range(len(paths)):\n",
        "    ade_ls = []\n",
        "    fde_ls = []\n",
        "    path = paths[feta]\n",
        "    exps = glob.glob(path)\n",
        "    print('Model being tested are:',exps)\n",
        "\n",
        "    for exp_path in exps:\n",
        "        print(\"*\"*50)\n",
        "        print(\"Evaluating model:\",exp_path)\n",
        "\n",
        "        model_path = exp_path+'/val_best.pth'\n",
        "        args_path = exp_path+'/args.pkl'\n",
        "        with open(args_path,'rb') as f:\n",
        "            args = pickle.load(f)\n",
        "\n",
        "        #Data prep\n",
        "        obs_seq_len = args.obs_seq_len\n",
        "        pred_seq_len = args.pred_seq_len\n",
        "        data_set = '/kaggle/input/dataset/datasets/'+args.dataset+'/'\n",
        "\n",
        "        dset_test = TrajectoryDataset(\n",
        "                data_set+'test/',\n",
        "                obs_len=obs_seq_len,\n",
        "                pred_len=pred_seq_len,\n",
        "                skip=1,norm_lap_matr=True)\n",
        "\n",
        "        loader_test = DataLoader(\n",
        "                dset_test,\n",
        "                batch_size=1,#This is irrelative to the args batch size parameter\n",
        "                shuffle =False,\n",
        "                num_workers=1)\n",
        "\n",
        "\n",
        "\n",
        "        #Defining the model\n",
        "        modelka = social_stgcnn(n_stgcnn =args.n_stgcnn,n_txpcnn=args.n_txpcnn,\n",
        "        output_feat=args.output_size,seq_len=args.obs_seq_len,\n",
        "        kernel_size=args.kernel_size,pred_seq_len=args.pred_seq_len)\n",
        "        modelka.load_state_dict(torch.load(model_path))\n",
        "\n",
        "\n",
        "        ade_ =999999\n",
        "        fde_ =999999\n",
        "        print(\"Testing ....\")\n",
        "        ad,fd,raw_data_dic_= test(modelka,loader_test)\n",
        "        ade_= min(ade_,ad)\n",
        "        fde_ =min(fde_,fd)\n",
        "        ade_ls.append(ade_)\n",
        "        fde_ls.append(fde_)\n",
        "        print(\"ADE:\",ade_,\" FDE:\",fde_)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(\"*\"*50)\n",
        "############################################################################################################################"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T20:10:37.099296Z",
          "iopub.execute_input": "2024-06-03T20:10:37.099844Z",
          "iopub.status.idle": "2024-06-03T20:11:12.623482Z",
          "shell.execute_reply.started": "2024-06-03T20:10:37.099816Z",
          "shell.execute_reply": "2024-06-03T20:11:12.622411Z"
        },
        "trusted": true,
        "id": "U2xqS1cRbTbc",
        "outputId": "09a84c9e-da21-4646-da24-bca831abc744"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 301/301 [00:23<00:00, 12.55it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.3588602304791608  FDE: 0.53796490825129\n**************************************************\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}