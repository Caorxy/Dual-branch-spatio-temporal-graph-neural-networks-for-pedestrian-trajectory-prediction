{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8585402,
          "sourceType": "datasetVersion",
          "datasetId": 5134904
        }
      ],
      "dockerImageVersionId": 30716,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Defining the model\n",
        "\n",
        "model = social_stgcnn(n_stgcnn =args.n_stgcnn,n_txpcnn=args.n_txpcnn,\n",
        "output_feat=args.output_size,seq_len=args.obs_seq_len,\n",
        "kernel_size=args.kernel_size,pred_seq_len=args.pred_seq_len)\n",
        "\n",
        "\n",
        "#Training settings\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(),lr=args.lr)\n",
        "\n",
        "if args.use_lrschd:\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_sh_rate, gamma=0.1)\n",
        "\n",
        "\n",
        "\n",
        "checkpoint_dir = './checkpoint/'+args.tag+'/'\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "with open(checkpoint_dir+'args.pkl', 'wb') as fp:\n",
        "    pickle.dump(args, fp)\n",
        "\n",
        "\n",
        "\n",
        "print('Data and model loaded')\n",
        "print('Checkpoint dir:', checkpoint_dir)\n",
        "\n",
        "#Training\n",
        "metrics = {'train_loss':[],  'val_loss':[]}\n",
        "constant_metrics = {'min_val_epoch':-1, 'min_val_loss':9999999999999999}\n",
        "\n",
        "def train(epoch):\n",
        "    global metrics,loader_train\n",
        "    model.train()\n",
        "    loss_batch = 0\n",
        "    batch_count = 0\n",
        "    is_fst_loss = True\n",
        "    loader_len = len(loader_train)\n",
        "    turn_point =int(loader_len/args.batch_size)*args.batch_size+ loader_len%args.batch_size -1\n",
        "\n",
        "\n",
        "    for cnt,batch in enumerate(loader_train):\n",
        "        batch_count+=1\n",
        "\n",
        "        #Get data\n",
        "        batch = [tensor for tensor in batch]\n",
        "        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, non_linear_ped,\\\n",
        "         loss_mask,V_obs,A_obs,A_dir_obs, V_tr,A_tr,A_dir_tr = batch\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        #Forward\n",
        "        #V_obs = batch,seq,node,feat\n",
        "        #V_obs_tmp = batch,feat,seq,node\n",
        "        V_obs_tmp =V_obs.permute(0,3,1,2)\n",
        "\n",
        "        V_pred,_,_ = model(V_obs_tmp,A_obs.squeeze(), A_dir_obs.squeeze())\n",
        "\n",
        "        V_pred = V_pred.permute(0,2,3,1)\n",
        "\n",
        "\n",
        "\n",
        "        V_tr = V_tr.squeeze()\n",
        "        A_tr = A_tr.squeeze()\n",
        "        A_dir_tr = A_dir_tr.squeeze()\n",
        "        V_pred = V_pred.squeeze()\n",
        "\n",
        "        if batch_count%args.batch_size !=0 and cnt != turn_point :\n",
        "            l = graph_loss(V_pred,V_tr)\n",
        "            if is_fst_loss :\n",
        "                loss = l\n",
        "                is_fst_loss = False\n",
        "            else:\n",
        "                loss += l\n",
        "\n",
        "        else:\n",
        "            loss = loss/args.batch_size\n",
        "            is_fst_loss = True\n",
        "            loss.backward()\n",
        "\n",
        "            if args.clip_grad is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(),args.clip_grad)\n",
        "\n",
        "\n",
        "            optimizer.step()\n",
        "            #Metrics\n",
        "            loss_batch += loss.item()\n",
        "            print('TRAIN:','\\t Epoch:', epoch,'\\t Loss:',loss_batch/batch_count)\n",
        "\n",
        "    metrics['train_loss'].append(loss_batch/batch_count)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def vald(epoch):\n",
        "    global metrics,loader_val,constant_metrics\n",
        "    model.eval()\n",
        "    loss_batch = 0\n",
        "    batch_count = 0\n",
        "    is_fst_loss = True\n",
        "    loader_len = len(loader_val)\n",
        "    turn_point =int(loader_len/args.batch_size)*args.batch_size+ loader_len%args.batch_size -1\n",
        "\n",
        "    for cnt,batch in enumerate(loader_val):\n",
        "        batch_count+=1\n",
        "\n",
        "        #Get data\n",
        "        batch = [tensor for tensor in batch]\n",
        "        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, non_linear_ped,\\\n",
        "         loss_mask,V_obs,A_obs,A_dir_obs, V_tr,A_tr,A_dir_tr = batch\n",
        "\n",
        "\n",
        "        V_obs_tmp =V_obs.permute(0,3,1,2)\n",
        "\n",
        "        V_pred,_,_ = model(V_obs_tmp,A_obs.squeeze(), A_dir_obs.squeeze())\n",
        "\n",
        "        V_pred = V_pred.permute(0,2,3,1)\n",
        "\n",
        "        V_tr = V_tr.squeeze()\n",
        "        A_tr = A_tr.squeeze()\n",
        "        A_dir_tr = A_dir_tr.squeeze()\n",
        "        V_pred = V_pred.squeeze()\n",
        "\n",
        "        if batch_count%args.batch_size !=0 and cnt != turn_point :\n",
        "            l = graph_loss(V_pred,V_tr)\n",
        "            if is_fst_loss :\n",
        "                loss = l\n",
        "                is_fst_loss = False\n",
        "            else:\n",
        "                loss += l\n",
        "\n",
        "        else:\n",
        "            loss = loss/args.batch_size\n",
        "            is_fst_loss = True\n",
        "            #Metrics\n",
        "            loss_batch += loss.item()\n",
        "            print('VALD:','\\t Epoch:', epoch,'\\t Loss:',loss_batch/batch_count)\n",
        "\n",
        "    metrics['val_loss'].append(loss_batch/batch_count)\n",
        "\n",
        "    if  metrics['val_loss'][-1]< constant_metrics['min_val_loss']:\n",
        "        constant_metrics['min_val_loss'] =  metrics['val_loss'][-1]\n",
        "        constant_metrics['min_val_epoch'] = epoch\n",
        "        torch.save(model.state_dict(),checkpoint_dir+'val_best.pth')  # OK\n",
        "        check_test_performance()\n",
        "\n",
        "print('Training started ...')\n",
        "for epoch in range(args.num_epochs):\n",
        "    train(epoch)\n",
        "    vald(epoch)\n",
        "    if args.use_lrschd:\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "    print('*'*30)\n",
        "    print('Epoch:',args.tag,\":\", epoch)\n",
        "    for k,v in metrics.items():\n",
        "        if len(v)>0:\n",
        "            print(k,v[-1])\n",
        "\n",
        "\n",
        "    print(constant_metrics)\n",
        "    print('*'*30)\n",
        "\n",
        "    with open(checkpoint_dir+'metrics.pkl', 'wb') as fp:\n",
        "        pickle.dump(metrics, fp)\n",
        "\n",
        "    with open(checkpoint_dir+'constant_metrics.pkl', 'wb') as fp:\n",
        "        pickle.dump(constant_metrics, fp)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T02:19:30.810331Z",
          "iopub.execute_input": "2024-06-04T02:19:30.811229Z",
          "iopub.status.idle": "2024-06-04T05:15:19.718079Z",
          "shell.execute_reply.started": "2024-06-04T02:19:30.811195Z",
          "shell.execute_reply": "2024-06-04T05:15:19.716893Z"
        },
        "trusted": true,
        "id": "2waHVW7Jdk-G",
        "outputId": "cffc98cb-2243-4379-9324-89e1a3242ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Data and model loaded\nCheckpoint dir: ./checkpoint/social-tag/\nTraining started ...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "TRAIN: \t Epoch: 0 \t Loss: 0.01595938391983509\nTRAIN: \t Epoch: 0 \t Loss: 0.01579183293506503\nTRAIN: \t Epoch: 0 \t Loss: 0.01562640815973282\nTRAIN: \t Epoch: 0 \t Loss: 0.01544566941447556\nTRAIN: \t Epoch: 0 \t Loss: 0.01526978500187397\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": " 26%|██▌       | 238/921 [58:45<2:48:37, 14.81s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "TRAIN: \t Epoch: 0 \t Loss: 0.015112685815741619\nTRAIN: \t Epoch: 0 \t Loss: 0.014960332374487604\nTRAIN: \t Epoch: 0 \t Loss: 0.014812218374572694\nTRAIN: \t Epoch: 0 \t Loss: 0.01466385368257761\nTRAIN: \t Epoch: 0 \t Loss: 0.014520969148725272\nTRAIN: \t Epoch: 0 \t Loss: 0.014382676136764612\nTRAIN: \t Epoch: 0 \t Loss: 0.014241795986890793\nTRAIN: \t Epoch: 0 \t Loss: 0.014097469572264414\nTRAIN: \t Epoch: 0 \t Loss: 0.013951535802334547\nTRAIN: \t Epoch: 0 \t Loss: 0.013794143497943879\nTRAIN: \t Epoch: 0 \t Loss: 0.013640973484143615\nTRAIN: \t Epoch: 0 \t Loss: 0.013556004642988697\nVALD: \t Epoch: 0 \t Loss: 0.01103119645267725\nVALD: \t Epoch: 0 \t Loss: 0.010940011125057936\nVALD: \t Epoch: 0 \t Loss: 0.013402903142074743\nVALD: \t Epoch: 0 \t Loss: 0.012757524282870416\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.73it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 1.3414280162137824  FDE: 1.4272217310285966\n**************************************************\n******************************\nEpoch: social-tag : 0\ntrain_loss 0.013556004642988697\nval_loss 0.012757524282870416\n{'min_val_epoch': 0, 'min_val_loss': 0.012757524282870416}\n******************************\nTRAIN: \t Epoch: 1 \t Loss: 0.010820485651493073\nTRAIN: \t Epoch: 1 \t Loss: 0.010691329836845398\nTRAIN: \t Epoch: 1 \t Loss: 0.0104485380773743\nTRAIN: \t Epoch: 1 \t Loss: 0.010199002455919981\nTRAIN: \t Epoch: 1 \t Loss: 0.00996267907321453\nTRAIN: \t Epoch: 1 \t Loss: 0.009734799930204948\nTRAIN: \t Epoch: 1 \t Loss: 0.009512458927929401\nTRAIN: \t Epoch: 1 \t Loss: 0.00929327349876985\nTRAIN: \t Epoch: 1 \t Loss: 0.009066290118628077\nTRAIN: \t Epoch: 1 \t Loss: 0.008873005444183946\nTRAIN: \t Epoch: 1 \t Loss: 0.008673099165951664\nTRAIN: \t Epoch: 1 \t Loss: 0.008476898927862445\nTRAIN: \t Epoch: 1 \t Loss: 0.008321821653785614\nTRAIN: \t Epoch: 1 \t Loss: 0.008136895219130176\nTRAIN: \t Epoch: 1 \t Loss: 0.007950467771540086\nTRAIN: \t Epoch: 1 \t Loss: 0.007813099218765274\nTRAIN: \t Epoch: 1 \t Loss: 0.007723643585587993\nVALD: \t Epoch: 1 \t Loss: 0.004405259620398283\nVALD: \t Epoch: 1 \t Loss: 0.004376247292384505\nVALD: \t Epoch: 1 \t Loss: 0.008544515042255322\nVALD: \t Epoch: 1 \t Loss: 0.007702262577658404\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.71it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 1.0001944951887496  FDE: 1.2539989541931287\n**************************************************\n******************************\nEpoch: social-tag : 1\ntrain_loss 0.007723643585587993\nval_loss 0.007702262577658404\n{'min_val_epoch': 1, 'min_val_loss': 0.007702262577658404}\n******************************\nTRAIN: \t Epoch: 2 \t Loss: 0.005165229085832834\nTRAIN: \t Epoch: 2 \t Loss: 0.0049329528119415045\nTRAIN: \t Epoch: 2 \t Loss: 0.004770656892408927\nTRAIN: \t Epoch: 2 \t Loss: 0.004713781527243555\nTRAIN: \t Epoch: 2 \t Loss: 0.004617553483694792\nTRAIN: \t Epoch: 2 \t Loss: 0.004605561960488558\nTRAIN: \t Epoch: 2 \t Loss: 0.004697724112442562\nTRAIN: \t Epoch: 2 \t Loss: 0.0047530499286949635\nTRAIN: \t Epoch: 2 \t Loss: 0.004662769691397746\nTRAIN: \t Epoch: 2 \t Loss: 0.004578635608777404\nTRAIN: \t Epoch: 2 \t Loss: 0.004475536311722614\nTRAIN: \t Epoch: 2 \t Loss: 0.004381864545090745\nTRAIN: \t Epoch: 2 \t Loss: 0.004348027179590785\nTRAIN: \t Epoch: 2 \t Loss: 0.004431097352478121\nTRAIN: \t Epoch: 2 \t Loss: 0.00441520749591291\nTRAIN: \t Epoch: 2 \t Loss: 0.004312991673941724\nTRAIN: \t Epoch: 2 \t Loss: 0.004248858859875437\nVALD: \t Epoch: 2 \t Loss: 0.0011181615991517901\nVALD: \t Epoch: 2 \t Loss: 0.0011692621046677232\nVALD: \t Epoch: 2 \t Loss: 0.005562650427843134\nVALD: \t Epoch: 2 \t Loss: 0.004754386738150896\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.76it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.8293199370290395  FDE: 1.0695845035594977\n**************************************************\n******************************\nEpoch: social-tag : 2\ntrain_loss 0.004248858859875437\nval_loss 0.004754386738150896\n{'min_val_epoch': 2, 'min_val_loss': 0.004754386738150896}\n******************************\nTRAIN: \t Epoch: 3 \t Loss: 0.0026882828678935766\nTRAIN: \t Epoch: 3 \t Loss: 0.0023023626999929547\nTRAIN: \t Epoch: 3 \t Loss: 0.0024755278136581182\nTRAIN: \t Epoch: 3 \t Loss: 0.002437769144307822\nTRAIN: \t Epoch: 3 \t Loss: 0.00273942775093019\nTRAIN: \t Epoch: 3 \t Loss: 0.0029084349128728113\nTRAIN: \t Epoch: 3 \t Loss: 0.0028692053497901986\nTRAIN: \t Epoch: 3 \t Loss: 0.002721979864872992\nTRAIN: \t Epoch: 3 \t Loss: 0.002705320483073592\nTRAIN: \t Epoch: 3 \t Loss: 0.0027230761479586363\nTRAIN: \t Epoch: 3 \t Loss: 0.0027472212360325184\nTRAIN: \t Epoch: 3 \t Loss: 0.002733825570127616\nTRAIN: \t Epoch: 3 \t Loss: 0.002649979988256326\nTRAIN: \t Epoch: 3 \t Loss: 0.0025991476140916348\nTRAIN: \t Epoch: 3 \t Loss: 0.002515723331210514\nTRAIN: \t Epoch: 3 \t Loss: 0.0024319274380104616\nTRAIN: \t Epoch: 3 \t Loss: 0.002418727211823518\nVALD: \t Epoch: 3 \t Loss: 0.00011926613660762087\nVALD: \t Epoch: 3 \t Loss: 0.0013285194472700823\nVALD: \t Epoch: 3 \t Loss: 0.008394640494467845\nVALD: \t Epoch: 3 \t Loss: 0.007123484056881683\n******************************\nEpoch: social-tag : 3\ntrain_loss 0.002418727211823518\nval_loss 0.007123484056881683\n{'min_val_epoch': 2, 'min_val_loss': 0.004754386738150896}\n******************************\nTRAIN: \t Epoch: 4 \t Loss: 0.0016426508082076907\nTRAIN: \t Epoch: 4 \t Loss: 0.0013453167048282921\nTRAIN: \t Epoch: 4 \t Loss: 0.0019196180704360206\nTRAIN: \t Epoch: 4 \t Loss: 0.0023606378526892513\nTRAIN: \t Epoch: 4 \t Loss: 0.0023299440043047072\nTRAIN: \t Epoch: 4 \t Loss: 0.002101675529653827\nTRAIN: \t Epoch: 4 \t Loss: 0.00197902592896883\nTRAIN: \t Epoch: 4 \t Loss: 0.0018119042797479779\nTRAIN: \t Epoch: 4 \t Loss: 0.001772403484210372\nTRAIN: \t Epoch: 4 \t Loss: 0.002012821636162698\nTRAIN: \t Epoch: 4 \t Loss: 0.0020279146476902747\nTRAIN: \t Epoch: 4 \t Loss: 0.0019141977148440976\nTRAIN: \t Epoch: 4 \t Loss: 0.0017755210193662117\nTRAIN: \t Epoch: 4 \t Loss: 0.0017056096777586\nTRAIN: \t Epoch: 4 \t Loss: 0.001601157456752844\nTRAIN: \t Epoch: 4 \t Loss: 0.0015677390865675989\nTRAIN: \t Epoch: 4 \t Loss: 0.0015754765419952685\nVALD: \t Epoch: 4 \t Loss: -0.0012316358042880893\nVALD: \t Epoch: 4 \t Loss: -0.001066399912815541\nVALD: \t Epoch: 4 \t Loss: 0.002997265080921352\nVALD: \t Epoch: 4 \t Loss: 0.0022668336757910466\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.73it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.7177428027387728  FDE: 0.8870875558071932\n**************************************************\n******************************\nEpoch: social-tag : 4\ntrain_loss 0.0015754765419952685\nval_loss 0.0022668336757910466\n{'min_val_epoch': 4, 'min_val_loss': 0.0022668336757910466}\n******************************\nTRAIN: \t Epoch: 5 \t Loss: 0.0007435440784320235\nTRAIN: \t Epoch: 5 \t Loss: 0.0003174316661898047\nTRAIN: \t Epoch: 5 \t Loss: 0.0003410880162846297\nTRAIN: \t Epoch: 5 \t Loss: 0.00028192703211971093\nTRAIN: \t Epoch: 5 \t Loss: 0.0004379431498819031\nTRAIN: \t Epoch: 5 \t Loss: 0.0009466942295451494\nTRAIN: \t Epoch: 5 \t Loss: 0.0011952500967059418\nTRAIN: \t Epoch: 5 \t Loss: 0.0011911282208529883\nTRAIN: \t Epoch: 5 \t Loss: 0.0010012759408305606\nTRAIN: \t Epoch: 5 \t Loss: 0.0008531257517461199\nTRAIN: \t Epoch: 5 \t Loss: 0.0007812467460594648\nTRAIN: \t Epoch: 5 \t Loss: 0.0010104448974743718\nTRAIN: \t Epoch: 5 \t Loss: 0.0010968022884648794\nTRAIN: \t Epoch: 5 \t Loss: 0.0010665578005760576\nTRAIN: \t Epoch: 5 \t Loss: 0.0009813691465145288\nTRAIN: \t Epoch: 5 \t Loss: 0.0008696658383087197\nTRAIN: \t Epoch: 5 \t Loss: 0.0008144234867138563\nVALD: \t Epoch: 5 \t Loss: -0.0012146382359787822\nVALD: \t Epoch: 5 \t Loss: -0.0006270649955695262\nVALD: \t Epoch: 5 \t Loss: 0.004544451887340983\nVALD: \t Epoch: 5 \t Loss: 0.0036728668506229353\n******************************\nEpoch: social-tag : 5\ntrain_loss 0.0008144234867138563\nval_loss 0.0036728668506229353\n{'min_val_epoch': 4, 'min_val_loss': 0.0022668336757910466}\n******************************\nTRAIN: \t Epoch: 6 \t Loss: 0.0006400083657354116\nTRAIN: \t Epoch: 6 \t Loss: 0.0013004762586206198\nTRAIN: \t Epoch: 6 \t Loss: 0.001481848070397973\nTRAIN: \t Epoch: 6 \t Loss: 0.0011447067554399837\nTRAIN: \t Epoch: 6 \t Loss: 0.0007887161715188995\nTRAIN: \t Epoch: 6 \t Loss: 0.0004725703741617811\nTRAIN: \t Epoch: 6 \t Loss: 0.00034398325910193047\nTRAIN: \t Epoch: 6 \t Loss: 0.00018923445713880938\nTRAIN: \t Epoch: 6 \t Loss: 6.489467018076943e-06\nTRAIN: \t Epoch: 6 \t Loss: -5.3115627088118346e-05\nTRAIN: \t Epoch: 6 \t Loss: 0.0003639427688374946\nTRAIN: \t Epoch: 6 \t Loss: 0.0006347027992887888\nTRAIN: \t Epoch: 6 \t Loss: 0.0007540407051027825\nTRAIN: \t Epoch: 6 \t Loss: 0.0007447895508708566\nTRAIN: \t Epoch: 6 \t Loss: 0.0006647452139683689\nTRAIN: \t Epoch: 6 \t Loss: 0.000545712505299889\nTRAIN: \t Epoch: 6 \t Loss: 0.0005203449179658271\nVALD: \t Epoch: 6 \t Loss: -0.0027937826234847307\nVALD: \t Epoch: 6 \t Loss: -0.0027203632052987814\nVALD: \t Epoch: 6 \t Loss: 0.0017187407550712426\nVALD: \t Epoch: 6 \t Loss: 0.0009723963613757592\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.78it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.6368387456991504  FDE: 0.818422482240438\n**************************************************\n******************************\nEpoch: social-tag : 6\ntrain_loss 0.0005203449179658271\nval_loss 0.0009723963613757592\n{'min_val_epoch': 6, 'min_val_loss': 0.0009723963613757592}\n******************************\nTRAIN: \t Epoch: 7 \t Loss: -0.0016139834187924862\nTRAIN: \t Epoch: 7 \t Loss: -0.0009344103746116161\nTRAIN: \t Epoch: 7 \t Loss: 0.00019964746509989104\nTRAIN: \t Epoch: 7 \t Loss: 0.0007632747292518616\nTRAIN: \t Epoch: 7 \t Loss: 0.0007719792192801833\nTRAIN: \t Epoch: 7 \t Loss: 0.000548372307093814\nTRAIN: \t Epoch: 7 \t Loss: 0.00037879897614142725\nTRAIN: \t Epoch: 7 \t Loss: 0.00029333610655157827\nTRAIN: \t Epoch: 7 \t Loss: 0.00010972468428210252\nTRAIN: \t Epoch: 7 \t Loss: 8.282887138193473e-05\nTRAIN: \t Epoch: 7 \t Loss: 8.38987603360279e-05\nTRAIN: \t Epoch: 7 \t Loss: 0.00011722330721871306\nTRAIN: \t Epoch: 7 \t Loss: 1.4495897071006206e-05\nTRAIN: \t Epoch: 7 \t Loss: -0.00010409932500416679\nTRAIN: \t Epoch: 7 \t Loss: -0.00018910291061426202\nTRAIN: \t Epoch: 7 \t Loss: -0.00025765117243281566\nTRAIN: \t Epoch: 7 \t Loss: -0.00023867084581235594\nVALD: \t Epoch: 7 \t Loss: -0.002646109787747264\nVALD: \t Epoch: 7 \t Loss: -0.0026176415849477053\nVALD: \t Epoch: 7 \t Loss: 0.0010410222845772903\nVALD: \t Epoch: 7 \t Loss: 0.0004828381621671056\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.78it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.6344373536188547  FDE: 0.7859000252572655\n**************************************************\n******************************\nEpoch: social-tag : 7\ntrain_loss -0.00023867084581235594\nval_loss 0.0004828381621671056\n{'min_val_epoch': 7, 'min_val_loss': 0.0004828381621671056}\n******************************\nTRAIN: \t Epoch: 8 \t Loss: -0.0013115534093230963\nTRAIN: \t Epoch: 8 \t Loss: -0.0014291786937974393\nTRAIN: \t Epoch: 8 \t Loss: -0.00161708010515819\nTRAIN: \t Epoch: 8 \t Loss: -0.0011903435461135814\nTRAIN: \t Epoch: 8 \t Loss: -0.00020611991203622892\nTRAIN: \t Epoch: 8 \t Loss: 2.110581044689752e-05\nTRAIN: \t Epoch: 8 \t Loss: -7.739581321532439e-05\nTRAIN: \t Epoch: 8 \t Loss: -0.00041096577206189977\nTRAIN: \t Epoch: 8 \t Loss: -0.0006652371545593875\nTRAIN: \t Epoch: 8 \t Loss: -0.000772243212122703\nTRAIN: \t Epoch: 8 \t Loss: -0.0008080715441875244\nTRAIN: \t Epoch: 8 \t Loss: -0.0004438562397505545\nTRAIN: \t Epoch: 8 \t Loss: -0.0003383204574884775\nTRAIN: \t Epoch: 8 \t Loss: -0.0002961323921876361\nTRAIN: \t Epoch: 8 \t Loss: -0.00035035175048202895\nTRAIN: \t Epoch: 8 \t Loss: -0.00046518556564478786\nTRAIN: \t Epoch: 8 \t Loss: -0.0005096162437411929\nVALD: \t Epoch: 8 \t Loss: -0.0040460312739014626\nVALD: \t Epoch: 8 \t Loss: -0.003513337462209165\nVALD: \t Epoch: 8 \t Loss: 0.0021858824572215476\nVALD: \t Epoch: 8 \t Loss: 0.0011387868317777286\n******************************\nEpoch: social-tag : 8\ntrain_loss -0.0005096162437411929\nval_loss 0.0011387868317777286\n{'min_val_epoch': 7, 'min_val_loss': 0.0004828381621671056}\n******************************\nTRAIN: \t Epoch: 9 \t Loss: -0.002265957649797201\nTRAIN: \t Epoch: 9 \t Loss: -0.001710674143396318\nTRAIN: \t Epoch: 9 \t Loss: -0.0009950673944937687\nTRAIN: \t Epoch: 9 \t Loss: -0.0011704521166393533\nTRAIN: \t Epoch: 9 \t Loss: -0.0013820148189552129\nTRAIN: \t Epoch: 9 \t Loss: -0.0013978021549216162\nTRAIN: \t Epoch: 9 \t Loss: -0.0010090427988741016\nTRAIN: \t Epoch: 9 \t Loss: -0.0009221236105076969\nTRAIN: \t Epoch: 9 \t Loss: -0.0011030736431065532\nTRAIN: \t Epoch: 9 \t Loss: -0.0012972252210602165\nTRAIN: \t Epoch: 9 \t Loss: -0.001460381889377128\nTRAIN: \t Epoch: 9 \t Loss: -0.0014527270783825468\nTRAIN: \t Epoch: 9 \t Loss: -0.0011592374380248098\nTRAIN: \t Epoch: 9 \t Loss: -0.0010383712734827505\nTRAIN: \t Epoch: 9 \t Loss: -0.0010782111474933723\nTRAIN: \t Epoch: 9 \t Loss: -0.0011417317182349507\nTRAIN: \t Epoch: 9 \t Loss: -0.0012215541456967142\nVALD: \t Epoch: 9 \t Loss: -0.005215171724557877\nVALD: \t Epoch: 9 \t Loss: -0.004877863684669137\nVALD: \t Epoch: 9 \t Loss: -4.9064711978038154e-05\nVALD: \t Epoch: 9 \t Loss: -0.0008187993557867176\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.76it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.5702675678548101  FDE: 0.7216751618094195\n**************************************************\n******************************\nEpoch: social-tag : 9\ntrain_loss -0.0012215541456967142\nval_loss -0.0008187993557867176\n{'min_val_epoch': 9, 'min_val_loss': -0.0008187993557867176}\n******************************\nTRAIN: \t Epoch: 10 \t Loss: -0.003269252134487033\nTRAIN: \t Epoch: 10 \t Loss: -0.0030228913528844714\nTRAIN: \t Epoch: 10 \t Loss: -0.002385149554659923\nTRAIN: \t Epoch: 10 \t Loss: -0.0010289682541042566\nTRAIN: \t Epoch: 10 \t Loss: -0.0006506187026388943\nTRAIN: \t Epoch: 10 \t Loss: -0.0007393325698406746\nTRAIN: \t Epoch: 10 \t Loss: -0.000983855828443276\nTRAIN: \t Epoch: 10 \t Loss: -0.001247253043402452\nTRAIN: \t Epoch: 10 \t Loss: -0.0014269580715335906\nTRAIN: \t Epoch: 10 \t Loss: -0.0014564404671546071\nTRAIN: \t Epoch: 10 \t Loss: -0.0013447031206768852\nTRAIN: \t Epoch: 10 \t Loss: -0.001284043153039723\nTRAIN: \t Epoch: 10 \t Loss: -0.0013237045036825852\nTRAIN: \t Epoch: 10 \t Loss: -0.0014285941506386735\nTRAIN: \t Epoch: 10 \t Loss: -0.001545409022946842\nTRAIN: \t Epoch: 10 \t Loss: -0.0016157359086719225\nTRAIN: \t Epoch: 10 \t Loss: -0.001639500107778462\nVALD: \t Epoch: 10 \t Loss: -0.004387229215353727\nVALD: \t Epoch: 10 \t Loss: -0.004510321421548724\nVALD: \t Epoch: 10 \t Loss: -0.0012885245184103649\nVALD: \t Epoch: 10 \t Loss: -0.0017362656945477941\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.77it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.5513490264279397  FDE: 0.6817346625104336\n**************************************************\n******************************\nEpoch: social-tag : 10\ntrain_loss -0.001639500107778462\nval_loss -0.0017362656945477941\n{'min_val_epoch': 10, 'min_val_loss': -0.0017362656945477941}\n******************************\nTRAIN: \t Epoch: 11 \t Loss: -0.0031788635533303022\nTRAIN: \t Epoch: 11 \t Loss: -0.002907094662077725\nTRAIN: \t Epoch: 11 \t Loss: -0.0026131668904175362\nTRAIN: \t Epoch: 11 \t Loss: -0.002112394053256139\nTRAIN: \t Epoch: 11 \t Loss: -0.0018781233695335687\nTRAIN: \t Epoch: 11 \t Loss: -0.0019755896743542203\nTRAIN: \t Epoch: 11 \t Loss: -0.002213979314547032\nTRAIN: \t Epoch: 11 \t Loss: -0.002316066653293092\nTRAIN: \t Epoch: 11 \t Loss: -0.0023552629281766713\nTRAIN: \t Epoch: 11 \t Loss: -0.0022388354351278394\nTRAIN: \t Epoch: 11 \t Loss: -0.002091695993757722\nTRAIN: \t Epoch: 11 \t Loss: -0.0021290968240161114\nTRAIN: \t Epoch: 11 \t Loss: -0.00217585112953272\nTRAIN: \t Epoch: 11 \t Loss: -0.0022534716699738055\nTRAIN: \t Epoch: 11 \t Loss: -0.002347858308348805\nTRAIN: \t Epoch: 11 \t Loss: -0.0023913650184113067\nTRAIN: \t Epoch: 11 \t Loss: -0.002344879487557619\nVALD: \t Epoch: 11 \t Loss: -0.0037405395414680243\nVALD: \t Epoch: 11 \t Loss: -0.00379298091866076\nVALD: \t Epoch: 11 \t Loss: -0.001042976975440979\nVALD: \t Epoch: 11 \t Loss: -0.0014416555206694765\n******************************\nEpoch: social-tag : 11\ntrain_loss -0.002344879487557619\nval_loss -0.0014416555206694765\n{'min_val_epoch': 10, 'min_val_loss': -0.0017362656945477941}\n******************************\nTRAIN: \t Epoch: 12 \t Loss: -0.0022748815827071667\nTRAIN: \t Epoch: 12 \t Loss: -0.0031987421680241823\nTRAIN: \t Epoch: 12 \t Loss: -0.0029280579959352813\nTRAIN: \t Epoch: 12 \t Loss: -0.0020182858424959704\nTRAIN: \t Epoch: 12 \t Loss: -0.0015150163322687148\nTRAIN: \t Epoch: 12 \t Loss: -0.001518245223754396\nTRAIN: \t Epoch: 12 \t Loss: -0.0017189108600307787\nTRAIN: \t Epoch: 12 \t Loss: -0.0019873381970683113\nTRAIN: \t Epoch: 12 \t Loss: -0.0020744832258464563\nTRAIN: \t Epoch: 12 \t Loss: -0.0020609116763807833\nTRAIN: \t Epoch: 12 \t Loss: -0.0017965567319399931\nTRAIN: \t Epoch: 12 \t Loss: -0.0018817783857230097\nTRAIN: \t Epoch: 12 \t Loss: -0.002022067398334352\nTRAIN: \t Epoch: 12 \t Loss: -0.0021080847779688027\nTRAIN: \t Epoch: 12 \t Loss: -0.002217290496143202\nTRAIN: \t Epoch: 12 \t Loss: -0.0022422443798859604\nTRAIN: \t Epoch: 12 \t Loss: -0.0021634140231815927\nVALD: \t Epoch: 12 \t Loss: -0.003704921342432499\nVALD: \t Epoch: 12 \t Loss: -0.00378894351888448\nVALD: \t Epoch: 12 \t Loss: -0.0009977863325426977\nVALD: \t Epoch: 12 \t Loss: -0.0014381284366348785\n******************************\nEpoch: social-tag : 12\ntrain_loss -0.0021634140231815927\nval_loss -0.0014381284366348785\n{'min_val_epoch': 10, 'min_val_loss': -0.0017362656945477941}\n******************************\nTRAIN: \t Epoch: 13 \t Loss: -0.0028553521260619164\nTRAIN: \t Epoch: 13 \t Loss: -0.0035234868992120028\nTRAIN: \t Epoch: 13 \t Loss: -0.0037889255521198115\nTRAIN: \t Epoch: 13 \t Loss: -0.0033046886092051864\nTRAIN: \t Epoch: 13 \t Loss: -0.002221555495634675\nTRAIN: \t Epoch: 13 \t Loss: -0.0017869457854734112\nTRAIN: \t Epoch: 13 \t Loss: -0.0017636516547229672\nTRAIN: \t Epoch: 13 \t Loss: -0.0019462476557237096\nTRAIN: \t Epoch: 13 \t Loss: -0.002061120784168856\nTRAIN: \t Epoch: 13 \t Loss: -0.002252200321527198\nTRAIN: \t Epoch: 13 \t Loss: -0.002404002202886411\nTRAIN: \t Epoch: 13 \t Loss: -0.002486639855002674\nTRAIN: \t Epoch: 13 \t Loss: -0.002527761474574128\nTRAIN: \t Epoch: 13 \t Loss: -0.0024807172553015073\nTRAIN: \t Epoch: 13 \t Loss: -0.0024461215478368105\nTRAIN: \t Epoch: 13 \t Loss: -0.0024963668220152613\nTRAIN: \t Epoch: 13 \t Loss: -0.0025189238720375933\nVALD: \t Epoch: 13 \t Loss: -0.004865691065788269\nVALD: \t Epoch: 13 \t Loss: -0.004756438313052058\nVALD: \t Epoch: 13 \t Loss: -0.0011750850826501846\nVALD: \t Epoch: 13 \t Loss: -0.0016524349738975723\n******************************\nEpoch: social-tag : 13\ntrain_loss -0.0025189238720375933\nval_loss -0.0016524349738975723\n{'min_val_epoch': 10, 'min_val_loss': -0.0017362656945477941}\n******************************\nTRAIN: \t Epoch: 14 \t Loss: -0.003600310068577528\nTRAIN: \t Epoch: 14 \t Loss: -0.004419976379722357\nTRAIN: \t Epoch: 14 \t Loss: -0.0038762474432587624\nTRAIN: \t Epoch: 14 \t Loss: -0.0034766510943882167\nTRAIN: \t Epoch: 14 \t Loss: -0.002827688277466223\nTRAIN: \t Epoch: 14 \t Loss: -0.0026944871594120436\nTRAIN: \t Epoch: 14 \t Loss: -0.0028388046983829035\nTRAIN: \t Epoch: 14 \t Loss: -0.002964906521810917\nTRAIN: \t Epoch: 14 \t Loss: -0.0030052786686509433\nTRAIN: \t Epoch: 14 \t Loss: -0.0028776320250472054\nTRAIN: \t Epoch: 14 \t Loss: -0.002635855229031718\nTRAIN: \t Epoch: 14 \t Loss: -0.00259420508640081\nTRAIN: \t Epoch: 14 \t Loss: -0.00267608481786178\nTRAIN: \t Epoch: 14 \t Loss: -0.0027812437963023384\nTRAIN: \t Epoch: 14 \t Loss: -0.00287234606318331\nTRAIN: \t Epoch: 14 \t Loss: -0.00292651323343307\nTRAIN: \t Epoch: 14 \t Loss: -0.002906531950099055\nVALD: \t Epoch: 14 \t Loss: -0.004755500704050064\nVALD: \t Epoch: 14 \t Loss: -0.0049580729100853205\nVALD: \t Epoch: 14 \t Loss: -0.0022175380339225135\nVALD: \t Epoch: 14 \t Loss: -0.002560455285146565\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.76it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.5210283842870312  FDE: 0.6156516518015326\n**************************************************\n******************************\nEpoch: social-tag : 14\ntrain_loss -0.002906531950099055\nval_loss -0.002560455285146565\n{'min_val_epoch': 14, 'min_val_loss': -0.002560455285146565}\n******************************\nTRAIN: \t Epoch: 15 \t Loss: -0.004124920815229416\nTRAIN: \t Epoch: 15 \t Loss: -0.004330050898715854\nTRAIN: \t Epoch: 15 \t Loss: -0.0045648204783598585\nTRAIN: \t Epoch: 15 \t Loss: -0.0040983593207784\nTRAIN: \t Epoch: 15 \t Loss: -0.0030079377349466084\nTRAIN: \t Epoch: 15 \t Loss: -0.0025524712740055597\nTRAIN: \t Epoch: 15 \t Loss: -0.0024768551104768577\nTRAIN: \t Epoch: 15 \t Loss: -0.002563193698733812\nTRAIN: \t Epoch: 15 \t Loss: -0.002782453546792062\nTRAIN: \t Epoch: 15 \t Loss: -0.002940376868355088\nTRAIN: \t Epoch: 15 \t Loss: -0.003080256598133763\nTRAIN: \t Epoch: 15 \t Loss: -0.0029887598526935713\nTRAIN: \t Epoch: 15 \t Loss: -0.002599238093423012\nTRAIN: \t Epoch: 15 \t Loss: -0.002516488076902793\nTRAIN: \t Epoch: 15 \t Loss: -0.0025175372740098585\nTRAIN: \t Epoch: 15 \t Loss: -0.0025654092878539814\nTRAIN: \t Epoch: 15 \t Loss: -0.002615449426230043\nVALD: \t Epoch: 15 \t Loss: -0.005887654609978199\nVALD: \t Epoch: 15 \t Loss: -0.005771634168922901\nVALD: \t Epoch: 15 \t Loss: -0.0024427082389593124\nVALD: \t Epoch: 15 \t Loss: -0.0028761991721665313\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.77it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.5240394750065437  FDE: 0.6556324321601666\n**************************************************\n******************************\nEpoch: social-tag : 15\ntrain_loss -0.002615449426230043\nval_loss -0.0028761991721665313\n{'min_val_epoch': 15, 'min_val_loss': -0.0028761991721665313}\n******************************\nTRAIN: \t Epoch: 16 \t Loss: -0.004005820956081152\nTRAIN: \t Epoch: 16 \t Loss: -0.004138601711019874\nTRAIN: \t Epoch: 16 \t Loss: -0.004176403861492872\nTRAIN: \t Epoch: 16 \t Loss: -0.0037205973640084267\nTRAIN: \t Epoch: 16 \t Loss: -0.00325464247725904\nTRAIN: \t Epoch: 16 \t Loss: -0.0033083674497902393\nTRAIN: \t Epoch: 16 \t Loss: -0.0035410719657582895\nTRAIN: \t Epoch: 16 \t Loss: -0.0037577596376650035\nTRAIN: \t Epoch: 16 \t Loss: -0.0038012217523323167\nTRAIN: \t Epoch: 16 \t Loss: -0.0038843108806759117\nTRAIN: \t Epoch: 16 \t Loss: -0.003971409983932972\nTRAIN: \t Epoch: 16 \t Loss: -0.0040074215503409505\nTRAIN: \t Epoch: 16 \t Loss: -0.0037337378665912324\nTRAIN: \t Epoch: 16 \t Loss: -0.0035287526407046244\nTRAIN: \t Epoch: 16 \t Loss: -0.0034760172575867424\nTRAIN: \t Epoch: 16 \t Loss: -0.003510923745125183\nTRAIN: \t Epoch: 16 \t Loss: -0.003548161784596177\nVALD: \t Epoch: 16 \t Loss: -0.006851155310869217\nVALD: \t Epoch: 16 \t Loss: -0.006606382085010409\nVALD: \t Epoch: 16 \t Loss: -0.0025636102072894573\nVALD: \t Epoch: 16 \t Loss: -0.0030095068042625688\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.80it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.499266821437544  FDE: 0.5974720768273474\n**************************************************\n******************************\nEpoch: social-tag : 16\ntrain_loss -0.003548161784596177\nval_loss -0.0030095068042625688\n{'min_val_epoch': 16, 'min_val_loss': -0.0030095068042625688}\n******************************\nTRAIN: \t Epoch: 17 \t Loss: -0.005492310971021652\nTRAIN: \t Epoch: 17 \t Loss: -0.005012857960537076\nTRAIN: \t Epoch: 17 \t Loss: -0.004454666981473565\nTRAIN: \t Epoch: 17 \t Loss: -0.0032281608364428394\nTRAIN: \t Epoch: 17 \t Loss: -0.0030162054987158625\nTRAIN: \t Epoch: 17 \t Loss: -0.0031185118665841096\nTRAIN: \t Epoch: 17 \t Loss: -0.0033283351679399076\nTRAIN: \t Epoch: 17 \t Loss: -0.0035205337007937487\nTRAIN: \t Epoch: 17 \t Loss: -0.0036865742068686006\nTRAIN: \t Epoch: 17 \t Loss: -0.0037718251551268622\nTRAIN: \t Epoch: 17 \t Loss: -0.0034534581857521766\nTRAIN: \t Epoch: 17 \t Loss: -0.00317439206992276\nTRAIN: \t Epoch: 17 \t Loss: -0.0031358239043933842\nTRAIN: \t Epoch: 17 \t Loss: -0.003240527626725712\nTRAIN: \t Epoch: 17 \t Loss: -0.0033240870339795946\nTRAIN: \t Epoch: 17 \t Loss: -0.0033781827441998757\nTRAIN: \t Epoch: 17 \t Loss: -0.0034663299253831306\nVALD: \t Epoch: 17 \t Loss: -0.006694077514111996\nVALD: \t Epoch: 17 \t Loss: -0.005758289713412523\nVALD: \t Epoch: 17 \t Loss: -0.0014343980389336746\nVALD: \t Epoch: 17 \t Loss: -0.002058540097253765\n******************************\nEpoch: social-tag : 17\ntrain_loss -0.0034663299253831306\nval_loss -0.002058540097253765\n{'min_val_epoch': 16, 'min_val_loss': -0.0030095068042625688}\n******************************\nTRAIN: \t Epoch: 18 \t Loss: -0.005141876637935638\nTRAIN: \t Epoch: 18 \t Loss: -0.004411133471876383\nTRAIN: \t Epoch: 18 \t Loss: -0.0034117926843464375\nTRAIN: \t Epoch: 18 \t Loss: -0.003274412069004029\nTRAIN: \t Epoch: 18 \t Loss: -0.003373038023710251\nTRAIN: \t Epoch: 18 \t Loss: -0.0036133380296329656\nTRAIN: \t Epoch: 18 \t Loss: -0.0038434980171067373\nTRAIN: \t Epoch: 18 \t Loss: -0.0039187404327094555\nTRAIN: \t Epoch: 18 \t Loss: -0.004072272477464544\nTRAIN: \t Epoch: 18 \t Loss: -0.004077824158594012\nTRAIN: \t Epoch: 18 \t Loss: -0.0037303200806491077\nTRAIN: \t Epoch: 18 \t Loss: -0.003608386755028429\nTRAIN: \t Epoch: 18 \t Loss: -0.0036077199799295226\nTRAIN: \t Epoch: 18 \t Loss: -0.003699047877619575\nTRAIN: \t Epoch: 18 \t Loss: -0.003873929587037613\nTRAIN: \t Epoch: 18 \t Loss: -0.003928112786525162\nTRAIN: \t Epoch: 18 \t Loss: -0.003947164457658249\nVALD: \t Epoch: 18 \t Loss: -0.006125237327069044\nVALD: \t Epoch: 18 \t Loss: -0.004614481935277581\nVALD: \t Epoch: 18 \t Loss: 0.0008012644636134306\nVALD: \t Epoch: 18 \t Loss: -0.00020977004083568704\n******************************\nEpoch: social-tag : 18\ntrain_loss -0.003947164457658249\nval_loss -0.00020977004083568704\n{'min_val_epoch': 16, 'min_val_loss': -0.0030095068042625688}\n******************************\nTRAIN: \t Epoch: 19 \t Loss: -0.004797576926648617\nTRAIN: \t Epoch: 19 \t Loss: -0.004473594715818763\nTRAIN: \t Epoch: 19 \t Loss: -0.00403483584523201\nTRAIN: \t Epoch: 19 \t Loss: -0.0035053124593105167\nTRAIN: \t Epoch: 19 \t Loss: -0.003393511543981731\nTRAIN: \t Epoch: 19 \t Loss: -0.0036077676146912077\nTRAIN: \t Epoch: 19 \t Loss: -0.0038989332471308963\nTRAIN: \t Epoch: 19 \t Loss: -0.004107635919353925\nTRAIN: \t Epoch: 19 \t Loss: -0.004207009902327425\nTRAIN: \t Epoch: 19 \t Loss: -0.003931638249196112\nTRAIN: \t Epoch: 19 \t Loss: -0.003781113646585833\nTRAIN: \t Epoch: 19 \t Loss: -0.0038139097935830555\nTRAIN: \t Epoch: 19 \t Loss: -0.0038827174844650123\nTRAIN: \t Epoch: 19 \t Loss: -0.004023548648027437\nTRAIN: \t Epoch: 19 \t Loss: -0.004023630451411009\nTRAIN: \t Epoch: 19 \t Loss: -0.004004030124633573\nTRAIN: \t Epoch: 19 \t Loss: -0.004024635943951029\nVALD: \t Epoch: 19 \t Loss: -0.006507711485028267\nVALD: \t Epoch: 19 \t Loss: -0.006328947376459837\nVALD: \t Epoch: 19 \t Loss: -0.0021071828280886016\nVALD: \t Epoch: 19 \t Loss: -0.002641027678034739\n******************************\nEpoch: social-tag : 19\ntrain_loss -0.004024635943951029\nval_loss -0.002641027678034739\n{'min_val_epoch': 16, 'min_val_loss': -0.0030095068042625688}\n******************************\nTRAIN: \t Epoch: 20 \t Loss: -0.004963901825249195\nTRAIN: \t Epoch: 20 \t Loss: -0.004641749896109104\nTRAIN: \t Epoch: 20 \t Loss: -0.0048066433519124985\nTRAIN: \t Epoch: 20 \t Loss: -0.004344023298472166\nTRAIN: \t Epoch: 20 \t Loss: -0.0037529793800786136\nTRAIN: \t Epoch: 20 \t Loss: -0.0037014814054903886\nTRAIN: \t Epoch: 20 \t Loss: -0.003922094707377255\nTRAIN: \t Epoch: 20 \t Loss: -0.004178652408882044\nTRAIN: \t Epoch: 20 \t Loss: -0.004260005002530913\nTRAIN: \t Epoch: 20 \t Loss: -0.004402120632585138\nTRAIN: \t Epoch: 20 \t Loss: -0.004531224501657893\nTRAIN: \t Epoch: 20 \t Loss: -0.004503054146577294\nTRAIN: \t Epoch: 20 \t Loss: -0.004213925768943647\nTRAIN: \t Epoch: 20 \t Loss: -0.003933583947530549\nTRAIN: \t Epoch: 20 \t Loss: -0.0038575728229867916\nTRAIN: \t Epoch: 20 \t Loss: -0.003920260158338351\nTRAIN: \t Epoch: 20 \t Loss: -0.003934412894798725\nVALD: \t Epoch: 20 \t Loss: -0.006701465230435133\nVALD: \t Epoch: 20 \t Loss: -0.006318873725831509\nVALD: \t Epoch: 20 \t Loss: -0.0018571275286376476\nVALD: \t Epoch: 20 \t Loss: -0.0025228601015970377\n******************************\nEpoch: social-tag : 20\ntrain_loss -0.003934412894798725\nval_loss -0.0025228601015970377\n{'min_val_epoch': 16, 'min_val_loss': -0.0030095068042625688}\n******************************\nTRAIN: \t Epoch: 21 \t Loss: -0.006180899683386087\nTRAIN: \t Epoch: 21 \t Loss: -0.00573467742651701\nTRAIN: \t Epoch: 21 \t Loss: -0.005587475219120582\nTRAIN: \t Epoch: 21 \t Loss: -0.0048264674842357635\nTRAIN: \t Epoch: 21 \t Loss: -0.003859423481117119\nTRAIN: \t Epoch: 21 \t Loss: -0.0037115029666286623\nTRAIN: \t Epoch: 21 \t Loss: -0.003828528309376062\nTRAIN: \t Epoch: 21 \t Loss: -0.004039173805040264\nTRAIN: \t Epoch: 21 \t Loss: -0.004154176067459048\nTRAIN: \t Epoch: 21 \t Loss: -0.004394596516249294\nTRAIN: \t Epoch: 21 \t Loss: -0.004543151985093242\nTRAIN: \t Epoch: 21 \t Loss: -0.004491540742340779\nTRAIN: \t Epoch: 21 \t Loss: -0.004334951753872491\nTRAIN: \t Epoch: 21 \t Loss: -0.004235694137735534\nTRAIN: \t Epoch: 21 \t Loss: -0.004270895836937901\nTRAIN: \t Epoch: 21 \t Loss: -0.004356933163421672\nTRAIN: \t Epoch: 21 \t Loss: -0.004385697121478822\nVALD: \t Epoch: 21 \t Loss: -0.007859490811824799\nVALD: \t Epoch: 21 \t Loss: -0.007038813550025225\nVALD: \t Epoch: 21 \t Loss: -0.00447133705408002\nVALD: \t Epoch: 21 \t Loss: -0.004744694216879542\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.74it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.49928696191164573  FDE: 0.646750598869781\n**************************************************\n******************************\nEpoch: social-tag : 21\ntrain_loss -0.004385697121478822\nval_loss -0.004744694216879542\n{'min_val_epoch': 21, 'min_val_loss': -0.004744694216879542}\n******************************\nTRAIN: \t Epoch: 22 \t Loss: -0.005090996157377958\nTRAIN: \t Epoch: 22 \t Loss: -0.005224158754572272\nTRAIN: \t Epoch: 22 \t Loss: -0.005086888714383046\nTRAIN: \t Epoch: 22 \t Loss: -0.004366294539067894\nTRAIN: \t Epoch: 22 \t Loss: -0.004135255888104439\nTRAIN: \t Epoch: 22 \t Loss: -0.004238048878808816\nTRAIN: \t Epoch: 22 \t Loss: -0.0044596500561705655\nTRAIN: \t Epoch: 22 \t Loss: -0.004645381530281156\nTRAIN: \t Epoch: 22 \t Loss: -0.0047298465441498495\nTRAIN: \t Epoch: 22 \t Loss: -0.004790900880470872\nTRAIN: \t Epoch: 22 \t Loss: -0.004755786789411848\nTRAIN: \t Epoch: 22 \t Loss: -0.004625830954561631\nTRAIN: \t Epoch: 22 \t Loss: -0.004573370759876875\nTRAIN: \t Epoch: 22 \t Loss: -0.004623550589063338\nTRAIN: \t Epoch: 22 \t Loss: -0.00465780021622777\nTRAIN: \t Epoch: 22 \t Loss: -0.004740041884360835\nTRAIN: \t Epoch: 22 \t Loss: -0.0047689641712967195\nVALD: \t Epoch: 22 \t Loss: -0.008189900778234005\nVALD: \t Epoch: 22 \t Loss: -0.007507505360990763\nVALD: \t Epoch: 22 \t Loss: -0.003518084374566873\nVALD: \t Epoch: 22 \t Loss: -0.004009524624266786\n******************************\nEpoch: social-tag : 22\ntrain_loss -0.0047689641712967195\nval_loss -0.004009524624266786\n{'min_val_epoch': 21, 'min_val_loss': -0.004744694216879542}\n******************************\nTRAIN: \t Epoch: 23 \t Loss: -0.006471800617873669\nTRAIN: \t Epoch: 23 \t Loss: -0.006230844184756279\nTRAIN: \t Epoch: 23 \t Loss: -0.006042295911659797\nTRAIN: \t Epoch: 23 \t Loss: -0.005117488035466522\nTRAIN: \t Epoch: 23 \t Loss: -0.0042409114073961975\nTRAIN: \t Epoch: 23 \t Loss: -0.004213849354224901\nTRAIN: \t Epoch: 23 \t Loss: -0.004357524582051805\nTRAIN: \t Epoch: 23 \t Loss: -0.004482563497731462\nTRAIN: \t Epoch: 23 \t Loss: -0.004626772081893351\nTRAIN: \t Epoch: 23 \t Loss: -0.004777190857566893\nTRAIN: \t Epoch: 23 \t Loss: -0.0048880796583200044\nTRAIN: \t Epoch: 23 \t Loss: -0.004713663035848488\nTRAIN: \t Epoch: 23 \t Loss: -0.004370733685881043\nTRAIN: \t Epoch: 23 \t Loss: -0.0043255242739438215\nTRAIN: \t Epoch: 23 \t Loss: -0.0043558275346489\nTRAIN: \t Epoch: 23 \t Loss: -0.004360019056548481\nTRAIN: \t Epoch: 23 \t Loss: -0.004429901330943473\nVALD: \t Epoch: 23 \t Loss: -0.0058630541898310184\nVALD: \t Epoch: 23 \t Loss: -0.0043287447188049555\nVALD: \t Epoch: 23 \t Loss: -0.0002521246982117494\nVALD: \t Epoch: 23 \t Loss: -0.0010843087218240825\n******************************\nEpoch: social-tag : 23\ntrain_loss -0.004429901330943473\nval_loss -0.0010843087218240825\n{'min_val_epoch': 21, 'min_val_loss': -0.004744694216879542}\n******************************\nTRAIN: \t Epoch: 24 \t Loss: -0.006166834384202957\nTRAIN: \t Epoch: 24 \t Loss: -0.006423738785088062\nTRAIN: \t Epoch: 24 \t Loss: -0.00617461387688915\nTRAIN: \t Epoch: 24 \t Loss: -0.00596713658887893\nTRAIN: \t Epoch: 24 \t Loss: -0.005596335884183646\nTRAIN: \t Epoch: 24 \t Loss: -0.005109221596891682\nTRAIN: \t Epoch: 24 \t Loss: -0.004921029122280223\nTRAIN: \t Epoch: 24 \t Loss: -0.00494893454015255\nTRAIN: \t Epoch: 24 \t Loss: -0.005045441910624504\nTRAIN: \t Epoch: 24 \t Loss: -0.005193286528810859\nTRAIN: \t Epoch: 24 \t Loss: -0.00530157924037088\nTRAIN: \t Epoch: 24 \t Loss: -0.005196761999589701\nTRAIN: \t Epoch: 24 \t Loss: -0.0051861131707063085\nTRAIN: \t Epoch: 24 \t Loss: -0.005135235210348453\nTRAIN: \t Epoch: 24 \t Loss: -0.005133534347017606\nTRAIN: \t Epoch: 24 \t Loss: -0.005128766788402572\nTRAIN: \t Epoch: 24 \t Loss: -0.005181507644892642\nVALD: \t Epoch: 24 \t Loss: -0.008622413501143456\nVALD: \t Epoch: 24 \t Loss: -0.007993513019755483\nVALD: \t Epoch: 24 \t Loss: -0.004492701729759574\nVALD: \t Epoch: 24 \t Loss: -0.004861238652360654\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.79it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.4555444688980528  FDE: 0.558271229673751\n**************************************************\n******************************\nEpoch: social-tag : 24\ntrain_loss -0.005181507644892642\nval_loss -0.004861238652360654\n{'min_val_epoch': 24, 'min_val_loss': -0.004861238652360654}\n******************************\nTRAIN: \t Epoch: 25 \t Loss: -0.006395348813384771\nTRAIN: \t Epoch: 25 \t Loss: -0.006469912361353636\nTRAIN: \t Epoch: 25 \t Loss: -0.00614998039479057\nTRAIN: \t Epoch: 25 \t Loss: -0.0050495543400757015\nTRAIN: \t Epoch: 25 \t Loss: -0.004577814880758524\nTRAIN: \t Epoch: 25 \t Loss: -0.004660788225010037\nTRAIN: \t Epoch: 25 \t Loss: -0.0048361556338412425\nTRAIN: \t Epoch: 25 \t Loss: -0.0049844892346300185\nTRAIN: \t Epoch: 25 \t Loss: -0.005062497169193294\nTRAIN: \t Epoch: 25 \t Loss: -0.0050703621935099365\nTRAIN: \t Epoch: 25 \t Loss: -0.005066910970278762\nTRAIN: \t Epoch: 25 \t Loss: -0.004915345188540717\nTRAIN: \t Epoch: 25 \t Loss: -0.004850041766006213\nTRAIN: \t Epoch: 25 \t Loss: -0.004899345537913697\nTRAIN: \t Epoch: 25 \t Loss: -0.004988124004254738\nTRAIN: \t Epoch: 25 \t Loss: -0.005086830817162991\nTRAIN: \t Epoch: 25 \t Loss: -0.005116996679906592\nVALD: \t Epoch: 25 \t Loss: -0.009026392363011837\nVALD: \t Epoch: 25 \t Loss: -0.008421788923442364\nVALD: \t Epoch: 25 \t Loss: -0.004706916864961386\nVALD: \t Epoch: 25 \t Loss: -0.0050622462750432974\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.75it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.4672628749804151  FDE: 0.5902517081219397\n**************************************************\n******************************\nEpoch: social-tag : 25\ntrain_loss -0.005116996679906592\nval_loss -0.0050622462750432974\n{'min_val_epoch': 25, 'min_val_loss': -0.0050622462750432974}\n******************************\nTRAIN: \t Epoch: 26 \t Loss: -0.0066827572882175446\nTRAIN: \t Epoch: 26 \t Loss: -0.006571491481736302\nTRAIN: \t Epoch: 26 \t Loss: -0.005647556468223532\nTRAIN: \t Epoch: 26 \t Loss: -0.004842952825129032\nTRAIN: \t Epoch: 26 \t Loss: -0.004616772104054689\nTRAIN: \t Epoch: 26 \t Loss: -0.004825407639145851\nTRAIN: \t Epoch: 26 \t Loss: -0.00498153202767883\nTRAIN: \t Epoch: 26 \t Loss: -0.005135035084094852\nTRAIN: \t Epoch: 26 \t Loss: -0.005331763376792272\nTRAIN: \t Epoch: 26 \t Loss: -0.00541462660767138\nTRAIN: \t Epoch: 26 \t Loss: -0.0050619426077570424\nTRAIN: \t Epoch: 26 \t Loss: -0.004767244982455547\nTRAIN: \t Epoch: 26 \t Loss: -0.004745924048340664\nTRAIN: \t Epoch: 26 \t Loss: -0.004804811166000685\nTRAIN: \t Epoch: 26 \t Loss: -0.004938028931307296\nTRAIN: \t Epoch: 26 \t Loss: -0.005005122126021888\nTRAIN: \t Epoch: 26 \t Loss: -0.00507623393963458\nVALD: \t Epoch: 26 \t Loss: -0.008742913603782654\nVALD: \t Epoch: 26 \t Loss: -0.008215977111831307\nVALD: \t Epoch: 26 \t Loss: -0.004666742015009125\nVALD: \t Epoch: 26 \t Loss: -0.004992032895782988\n******************************\nEpoch: social-tag : 26\ntrain_loss -0.00507623393963458\nval_loss -0.004992032895782988\n{'min_val_epoch': 25, 'min_val_loss': -0.0050622462750432974}\n******************************\nTRAIN: \t Epoch: 27 \t Loss: -0.006349246948957443\nTRAIN: \t Epoch: 27 \t Loss: -0.006323403911665082\nTRAIN: \t Epoch: 27 \t Loss: -0.006397496598462264\nTRAIN: \t Epoch: 27 \t Loss: -0.006235927576199174\nTRAIN: \t Epoch: 27 \t Loss: -0.005281985783949494\nTRAIN: \t Epoch: 27 \t Loss: -0.005119325088647504\nTRAIN: \t Epoch: 27 \t Loss: -0.005204426756660853\nTRAIN: \t Epoch: 27 \t Loss: -0.005281990597723052\nTRAIN: \t Epoch: 27 \t Loss: -0.005407896332649721\nTRAIN: \t Epoch: 27 \t Loss: -0.0054495625896379355\nTRAIN: \t Epoch: 27 \t Loss: -0.005473082694648342\nTRAIN: \t Epoch: 27 \t Loss: -0.005432550193897138\nTRAIN: \t Epoch: 27 \t Loss: -0.005307046212972357\nTRAIN: \t Epoch: 27 \t Loss: -0.005345832040932562\nTRAIN: \t Epoch: 27 \t Loss: -0.00542685881567498\nTRAIN: \t Epoch: 27 \t Loss: -0.0054350145073840395\nTRAIN: \t Epoch: 27 \t Loss: -0.005498903408420809\nVALD: \t Epoch: 27 \t Loss: -0.008738338947296143\nVALD: \t Epoch: 27 \t Loss: -0.007653144421055913\nVALD: \t Epoch: 27 \t Loss: -0.003035669835905234\nVALD: \t Epoch: 27 \t Loss: -0.0036343588324602018\n******************************\nEpoch: social-tag : 27\ntrain_loss -0.005498903408420809\nval_loss -0.0036343588324602018\n{'min_val_epoch': 25, 'min_val_loss': -0.0050622462750432974}\n******************************\nTRAIN: \t Epoch: 28 \t Loss: -0.006828992627561092\nTRAIN: \t Epoch: 28 \t Loss: -0.006672474090009928\nTRAIN: \t Epoch: 28 \t Loss: -0.006555771144727866\nTRAIN: \t Epoch: 28 \t Loss: -0.005708820302970707\nTRAIN: \t Epoch: 28 \t Loss: -0.004903801390901208\nTRAIN: \t Epoch: 28 \t Loss: -0.004972634177344541\nTRAIN: \t Epoch: 28 \t Loss: -0.005156133024554167\nTRAIN: \t Epoch: 28 \t Loss: -0.005426725692814216\nTRAIN: \t Epoch: 28 \t Loss: -0.0053727169676373405\nTRAIN: \t Epoch: 28 \t Loss: -0.005329433945007622\nTRAIN: \t Epoch: 28 \t Loss: -0.005110755562782288\nTRAIN: \t Epoch: 28 \t Loss: -0.0051069694648807245\nTRAIN: \t Epoch: 28 \t Loss: -0.0051883658609138085\nTRAIN: \t Epoch: 28 \t Loss: -0.005288867113579597\nTRAIN: \t Epoch: 28 \t Loss: -0.00541927320882678\nTRAIN: \t Epoch: 28 \t Loss: -0.0054581497388426214\nTRAIN: \t Epoch: 28 \t Loss: -0.005431556862524964\nVALD: \t Epoch: 28 \t Loss: -0.008702547289431095\nVALD: \t Epoch: 28 \t Loss: -0.00856598699465394\nVALD: \t Epoch: 28 \t Loss: -0.005543547837684552\nVALD: \t Epoch: 28 \t Loss: -0.005784119555574215\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.78it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.44649341024857214  FDE: 0.5526667786534005\n**************************************************\n******************************\nEpoch: social-tag : 28\ntrain_loss -0.005431556862524964\nval_loss -0.005784119555574215\n{'min_val_epoch': 28, 'min_val_loss': -0.005784119555574215}\n******************************\nTRAIN: \t Epoch: 29 \t Loss: -0.007043214049190283\nTRAIN: \t Epoch: 29 \t Loss: -0.0066565528977662325\nTRAIN: \t Epoch: 29 \t Loss: -0.006766690872609615\nTRAIN: \t Epoch: 29 \t Loss: -0.006779551971703768\nTRAIN: \t Epoch: 29 \t Loss: -0.006351316161453724\nTRAIN: \t Epoch: 29 \t Loss: -0.005622694113602241\nTRAIN: \t Epoch: 29 \t Loss: -0.005389844599579062\nTRAIN: \t Epoch: 29 \t Loss: -0.005422923422884196\nTRAIN: \t Epoch: 29 \t Loss: -0.005590322996593184\nTRAIN: \t Epoch: 29 \t Loss: -0.005798190180212259\nTRAIN: \t Epoch: 29 \t Loss: -0.0058622720481997185\nTRAIN: \t Epoch: 29 \t Loss: -0.0059087935369461775\nTRAIN: \t Epoch: 29 \t Loss: -0.0056289007994704526\nTRAIN: \t Epoch: 29 \t Loss: -0.005251827564539521\nTRAIN: \t Epoch: 29 \t Loss: -0.005133997196874892\nTRAIN: \t Epoch: 29 \t Loss: -0.00508471230386931\nTRAIN: \t Epoch: 29 \t Loss: -0.005137933755053602\nVALD: \t Epoch: 29 \t Loss: -0.007894161157310009\nVALD: \t Epoch: 29 \t Loss: -0.007201340049505234\nVALD: \t Epoch: 29 \t Loss: -0.004884522350039333\nVALD: \t Epoch: 29 \t Loss: -0.005117282018571081\n******************************\nEpoch: social-tag : 29\ntrain_loss -0.005137933755053602\nval_loss -0.005117282018571081\n{'min_val_epoch': 28, 'min_val_loss': -0.005784119555574215}\n******************************\nTRAIN: \t Epoch: 30 \t Loss: -0.007488576229661703\nTRAIN: \t Epoch: 30 \t Loss: -0.007139286492019892\nTRAIN: \t Epoch: 30 \t Loss: -0.007078359058747689\nTRAIN: \t Epoch: 30 \t Loss: -0.00694992917124182\nTRAIN: \t Epoch: 30 \t Loss: -0.006724355928599835\nTRAIN: \t Epoch: 30 \t Loss: -0.006427422709142168\nTRAIN: \t Epoch: 30 \t Loss: -0.006023553832034979\nTRAIN: \t Epoch: 30 \t Loss: -0.005772346426965669\nTRAIN: \t Epoch: 30 \t Loss: -0.005813777162176039\nTRAIN: \t Epoch: 30 \t Loss: -0.005836784769780934\nTRAIN: \t Epoch: 30 \t Loss: -0.005931676894595677\nTRAIN: \t Epoch: 30 \t Loss: -0.005993835142968844\nTRAIN: \t Epoch: 30 \t Loss: -0.0060924663244245145\nTRAIN: \t Epoch: 30 \t Loss: -0.006057328727495458\nTRAIN: \t Epoch: 30 \t Loss: -0.005893894579882423\nTRAIN: \t Epoch: 30 \t Loss: -0.005849749883054756\nTRAIN: \t Epoch: 30 \t Loss: -0.0058747102562902555\nVALD: \t Epoch: 30 \t Loss: -0.006918151397258043\nVALD: \t Epoch: 30 \t Loss: -0.004119159770198166\nVALD: \t Epoch: 30 \t Loss: 0.0005036849373330673\nVALD: \t Epoch: 30 \t Loss: -0.0005001172928990956\n******************************\nEpoch: social-tag : 30\ntrain_loss -0.0058747102562902555\nval_loss -0.0005001172928990956\n{'min_val_epoch': 28, 'min_val_loss': -0.005784119555574215}\n******************************\nTRAIN: \t Epoch: 31 \t Loss: -0.007103320676833391\nTRAIN: \t Epoch: 31 \t Loss: -0.0068726863246411085\nTRAIN: \t Epoch: 31 \t Loss: -0.007108181404570739\nTRAIN: \t Epoch: 31 \t Loss: -0.006873277365230024\nTRAIN: \t Epoch: 31 \t Loss: -0.006525562237948179\nTRAIN: \t Epoch: 31 \t Loss: -0.006186118038992087\nTRAIN: \t Epoch: 31 \t Loss: -0.006177170401705163\nTRAIN: \t Epoch: 31 \t Loss: -0.006205656041856855\nTRAIN: \t Epoch: 31 \t Loss: -0.0062782894302573465\nTRAIN: \t Epoch: 31 \t Loss: -0.006284682406112551\nTRAIN: \t Epoch: 31 \t Loss: -0.006169903007420627\nTRAIN: \t Epoch: 31 \t Loss: -0.005949043862832089\nTRAIN: \t Epoch: 31 \t Loss: -0.005896248675596256\nTRAIN: \t Epoch: 31 \t Loss: -0.0059325608890503645\nTRAIN: \t Epoch: 31 \t Loss: -0.006040344294160604\nTRAIN: \t Epoch: 31 \t Loss: -0.006155338807730004\nTRAIN: \t Epoch: 31 \t Loss: -0.0061616987610856695\nVALD: \t Epoch: 31 \t Loss: -0.008175758644938469\nVALD: \t Epoch: 31 \t Loss: -0.00803837226703763\nVALD: \t Epoch: 31 \t Loss: -0.0061227630358189344\nVALD: \t Epoch: 31 \t Loss: -0.006133179643196974\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.78it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.48627378896777007  FDE: 0.6880463909048493\n**************************************************\n******************************\nEpoch: social-tag : 31\ntrain_loss -0.0061616987610856695\nval_loss -0.006133179643196974\n{'min_val_epoch': 31, 'min_val_loss': -0.006133179643196974}\n******************************\nTRAIN: \t Epoch: 32 \t Loss: -0.005847828462719917\nTRAIN: \t Epoch: 32 \t Loss: -0.0058250876609236\nTRAIN: \t Epoch: 32 \t Loss: -0.005343164938191573\nTRAIN: \t Epoch: 32 \t Loss: -0.005354959983378649\nTRAIN: \t Epoch: 32 \t Loss: -0.005636603105813265\nTRAIN: \t Epoch: 32 \t Loss: -0.005789230111986399\nTRAIN: \t Epoch: 32 \t Loss: -0.0058719235738473275\nTRAIN: \t Epoch: 32 \t Loss: -0.006074588454794139\nTRAIN: \t Epoch: 32 \t Loss: -0.006109972018748522\nTRAIN: \t Epoch: 32 \t Loss: -0.00598569456487894\nTRAIN: \t Epoch: 32 \t Loss: -0.005724877267229286\nTRAIN: \t Epoch: 32 \t Loss: -0.005714665186436226\nTRAIN: \t Epoch: 32 \t Loss: -0.005743700455730924\nTRAIN: \t Epoch: 32 \t Loss: -0.0058238798186981255\nTRAIN: \t Epoch: 32 \t Loss: -0.005909252989416321\nTRAIN: \t Epoch: 32 \t Loss: -0.005891124470508657\nTRAIN: \t Epoch: 32 \t Loss: -0.005811800093700488\nVALD: \t Epoch: 32 \t Loss: -0.007083890959620476\nVALD: \t Epoch: 32 \t Loss: -0.007270874222740531\nVALD: \t Epoch: 32 \t Loss: -0.005660174026464422\nVALD: \t Epoch: 32 \t Loss: -0.0057820107289655006\n******************************\nEpoch: social-tag : 32\ntrain_loss -0.005811800093700488\nval_loss -0.0057820107289655006\n{'min_val_epoch': 31, 'min_val_loss': -0.006133179643196974}\n******************************\nTRAIN: \t Epoch: 33 \t Loss: -0.005593490786850452\nTRAIN: \t Epoch: 33 \t Loss: -0.00619249208830297\nTRAIN: \t Epoch: 33 \t Loss: -0.006630260031670332\nTRAIN: \t Epoch: 33 \t Loss: -0.006828336394391954\nTRAIN: \t Epoch: 33 \t Loss: -0.0069777736440300945\nTRAIN: \t Epoch: 33 \t Loss: -0.006687575097506245\nTRAIN: \t Epoch: 33 \t Loss: -0.005996778208230223\nTRAIN: \t Epoch: 33 \t Loss: -0.005860976525582373\nTRAIN: \t Epoch: 33 \t Loss: -0.005862338830613428\nTRAIN: \t Epoch: 33 \t Loss: -0.0059561369940638546\nTRAIN: \t Epoch: 33 \t Loss: -0.006057398690080101\nTRAIN: \t Epoch: 33 \t Loss: -0.0061698097658033175\nTRAIN: \t Epoch: 33 \t Loss: -0.006254696394675053\nTRAIN: \t Epoch: 33 \t Loss: -0.006318976603714483\nTRAIN: \t Epoch: 33 \t Loss: -0.006243225559592247\nTRAIN: \t Epoch: 33 \t Loss: -0.006014086931827478\nTRAIN: \t Epoch: 33 \t Loss: -0.005984116895970973\nVALD: \t Epoch: 33 \t Loss: -0.007290627341717482\nVALD: \t Epoch: 33 \t Loss: -0.006919738603755832\nVALD: \t Epoch: 33 \t Loss: -0.004158103760952751\nVALD: \t Epoch: 33 \t Loss: -0.004458991412868995\n******************************\nEpoch: social-tag : 33\ntrain_loss -0.005984116895970973\nval_loss -0.004458991412868995\n{'min_val_epoch': 31, 'min_val_loss': -0.006133179643196974}\n******************************\nTRAIN: \t Epoch: 34 \t Loss: -0.006434290204197168\nTRAIN: \t Epoch: 34 \t Loss: -0.006592966848984361\nTRAIN: \t Epoch: 34 \t Loss: -0.006885801752408345\nTRAIN: \t Epoch: 34 \t Loss: -0.0068949825363233685\nTRAIN: \t Epoch: 34 \t Loss: -0.006820737197995186\nTRAIN: \t Epoch: 34 \t Loss: -0.006935040932148695\nTRAIN: \t Epoch: 34 \t Loss: -0.006754335481673479\nTRAIN: \t Epoch: 34 \t Loss: -0.00628621390205808\nTRAIN: \t Epoch: 34 \t Loss: -0.006087590552245577\nTRAIN: \t Epoch: 34 \t Loss: -0.0060944687342271205\nTRAIN: \t Epoch: 34 \t Loss: -0.006160487950017507\nTRAIN: \t Epoch: 34 \t Loss: -0.006273099783963214\nTRAIN: \t Epoch: 34 \t Loss: -0.006345889871366895\nTRAIN: \t Epoch: 34 \t Loss: -0.006499416178225407\nTRAIN: \t Epoch: 34 \t Loss: -0.0065276863208661474\nTRAIN: \t Epoch: 34 \t Loss: -0.006263299554120749\nTRAIN: \t Epoch: 34 \t Loss: -0.006134257461367683\nVALD: \t Epoch: 34 \t Loss: -0.00432942109182477\nVALD: \t Epoch: 34 \t Loss: -0.003880362492054701\nVALD: \t Epoch: 34 \t Loss: -0.0018122556308905284\nVALD: \t Epoch: 34 \t Loss: -0.0019955438173221733\n******************************\nEpoch: social-tag : 34\ntrain_loss -0.006134257461367683\nval_loss -0.0019955438173221733\n{'min_val_epoch': 31, 'min_val_loss': -0.006133179643196974}\n******************************\nTRAIN: \t Epoch: 35 \t Loss: -0.0036320816725492477\nTRAIN: \t Epoch: 35 \t Loss: -0.004743150202557445\nTRAIN: \t Epoch: 35 \t Loss: -0.0054292020698388415\nTRAIN: \t Epoch: 35 \t Loss: -0.005970360711216927\nTRAIN: \t Epoch: 35 \t Loss: -0.0062394654378294945\nTRAIN: \t Epoch: 35 \t Loss: -0.006171615018198888\nTRAIN: \t Epoch: 35 \t Loss: -0.006302792512412582\nTRAIN: \t Epoch: 35 \t Loss: -0.006485343794338405\nTRAIN: \t Epoch: 35 \t Loss: -0.006457428137461345\nTRAIN: \t Epoch: 35 \t Loss: -0.006325895478948951\nTRAIN: \t Epoch: 35 \t Loss: -0.005934961965646256\nTRAIN: \t Epoch: 35 \t Loss: -0.005888608381307374\nTRAIN: \t Epoch: 35 \t Loss: -0.00589843586875269\nTRAIN: \t Epoch: 35 \t Loss: -0.005962819542868861\nTRAIN: \t Epoch: 35 \t Loss: -0.006103932748859128\nTRAIN: \t Epoch: 35 \t Loss: -0.0062651130283484235\nTRAIN: \t Epoch: 35 \t Loss: -0.00625345040338509\nVALD: \t Epoch: 35 \t Loss: -0.009678109548985958\nVALD: \t Epoch: 35 \t Loss: -0.009161809459328651\nVALD: \t Epoch: 35 \t Loss: -0.006517166116585334\nVALD: \t Epoch: 35 \t Loss: -0.00667389150627121\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.71it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.4489409252049959  FDE: 0.5839986635009106\n**************************************************\n******************************\nEpoch: social-tag : 35\ntrain_loss -0.00625345040338509\nval_loss -0.00667389150627121\n{'min_val_epoch': 35, 'min_val_loss': -0.00667389150627121}\n******************************\nTRAIN: \t Epoch: 36 \t Loss: -0.007071640342473984\nTRAIN: \t Epoch: 36 \t Loss: -0.007528659887611866\nTRAIN: \t Epoch: 36 \t Loss: -0.007600080997993548\nTRAIN: \t Epoch: 36 \t Loss: -0.007561454316601157\nTRAIN: \t Epoch: 36 \t Loss: -0.007226305268704891\nTRAIN: \t Epoch: 36 \t Loss: -0.006508744088932872\nTRAIN: \t Epoch: 36 \t Loss: -0.0062327418210250994\nTRAIN: \t Epoch: 36 \t Loss: -0.006240648799575865\nTRAIN: \t Epoch: 36 \t Loss: -0.006274417197952668\nTRAIN: \t Epoch: 36 \t Loss: -0.006391503289341927\nTRAIN: \t Epoch: 36 \t Loss: -0.006474565714597702\nTRAIN: \t Epoch: 36 \t Loss: -0.006432513434750338\nTRAIN: \t Epoch: 36 \t Loss: -0.006407226829861219\nTRAIN: \t Epoch: 36 \t Loss: -0.006327377698783364\nTRAIN: \t Epoch: 36 \t Loss: -0.006258740089833736\nTRAIN: \t Epoch: 36 \t Loss: -0.006300689041381702\nTRAIN: \t Epoch: 36 \t Loss: -0.006322948854755272\nVALD: \t Epoch: 36 \t Loss: -0.009525910019874573\nVALD: \t Epoch: 36 \t Loss: -0.008919457904994488\nVALD: \t Epoch: 36 \t Loss: -0.005758045532274991\nVALD: \t Epoch: 36 \t Loss: -0.005928819274117133\n******************************\nEpoch: social-tag : 36\ntrain_loss -0.006322948854755272\nval_loss -0.005928819274117133\n{'min_val_epoch': 35, 'min_val_loss': -0.00667389150627121}\n******************************\nTRAIN: \t Epoch: 37 \t Loss: -0.007547816261649132\nTRAIN: \t Epoch: 37 \t Loss: -0.007817284669727087\nTRAIN: \t Epoch: 37 \t Loss: -0.007603996122876803\nTRAIN: \t Epoch: 37 \t Loss: -0.007355581037700176\nTRAIN: \t Epoch: 37 \t Loss: -0.006779606454074383\nTRAIN: \t Epoch: 37 \t Loss: -0.006499580107629299\nTRAIN: \t Epoch: 37 \t Loss: -0.0065081835990505555\nTRAIN: \t Epoch: 37 \t Loss: -0.006643864151556045\nTRAIN: \t Epoch: 37 \t Loss: -0.006793130861802233\nTRAIN: \t Epoch: 37 \t Loss: -0.0068374656606465575\nTRAIN: \t Epoch: 37 \t Loss: -0.006812508598985997\nTRAIN: \t Epoch: 37 \t Loss: -0.006786609922225277\nTRAIN: \t Epoch: 37 \t Loss: -0.006713465106888459\nTRAIN: \t Epoch: 37 \t Loss: -0.006685972413314241\nTRAIN: \t Epoch: 37 \t Loss: -0.006671499988685051\nTRAIN: \t Epoch: 37 \t Loss: -0.006660445389570668\nTRAIN: \t Epoch: 37 \t Loss: -0.006707092521317078\nVALD: \t Epoch: 37 \t Loss: -0.009728738106787205\nVALD: \t Epoch: 37 \t Loss: -0.009374869056046009\nVALD: \t Epoch: 37 \t Loss: -0.006753699388355017\nVALD: \t Epoch: 37 \t Loss: -0.00686789427450793\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.76it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.4409859549379138  FDE: 0.5750819929034439\n**************************************************\n******************************\nEpoch: social-tag : 37\ntrain_loss -0.006707092521317078\nval_loss -0.00686789427450793\n{'min_val_epoch': 37, 'min_val_loss': -0.00686789427450793}\n******************************\nTRAIN: \t Epoch: 38 \t Loss: -0.007901020348072052\nTRAIN: \t Epoch: 38 \t Loss: -0.008087985683232546\nTRAIN: \t Epoch: 38 \t Loss: -0.007831059861928225\nTRAIN: \t Epoch: 38 \t Loss: -0.0072294085985049605\nTRAIN: \t Epoch: 38 \t Loss: -0.006384799536317587\nTRAIN: \t Epoch: 38 \t Loss: -0.006301938556134701\nTRAIN: \t Epoch: 38 \t Loss: -0.006467498905424561\nTRAIN: \t Epoch: 38 \t Loss: -0.006621425272896886\nTRAIN: \t Epoch: 38 \t Loss: -0.006784119022389253\nTRAIN: \t Epoch: 38 \t Loss: -0.006924698408693075\nTRAIN: \t Epoch: 38 \t Loss: -0.0069494213146919556\nTRAIN: \t Epoch: 38 \t Loss: -0.006881031479376058\nTRAIN: \t Epoch: 38 \t Loss: -0.0067199477925896645\nTRAIN: \t Epoch: 38 \t Loss: -0.006718238083911794\nTRAIN: \t Epoch: 38 \t Loss: -0.006766583056499561\nTRAIN: \t Epoch: 38 \t Loss: -0.0068139476352371275\nTRAIN: \t Epoch: 38 \t Loss: -0.006840691802966775\nVALD: \t Epoch: 38 \t Loss: -0.00990460067987442\nVALD: \t Epoch: 38 \t Loss: -0.009710365906357765\nVALD: \t Epoch: 38 \t Loss: -0.007061771893252929\nVALD: \t Epoch: 38 \t Loss: -0.00719517314743377\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:35<00:00,  9.62it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.4460739696667786  FDE: 0.5935385097012102\n**************************************************\n******************************\nEpoch: social-tag : 38\ntrain_loss -0.006840691802966775\nval_loss -0.00719517314743377\n{'min_val_epoch': 38, 'min_val_loss': -0.00719517314743377}\n******************************\nTRAIN: \t Epoch: 39 \t Loss: -0.008349341340363026\nTRAIN: \t Epoch: 39 \t Loss: -0.0075081028044223785\nTRAIN: \t Epoch: 39 \t Loss: -0.006814829694728057\nTRAIN: \t Epoch: 39 \t Loss: -0.006022523681167513\nTRAIN: \t Epoch: 39 \t Loss: -0.005901132198050618\nTRAIN: \t Epoch: 39 \t Loss: -0.005969570678037901\nTRAIN: \t Epoch: 39 \t Loss: -0.0061929438462747\nTRAIN: \t Epoch: 39 \t Loss: -0.006455571128753945\nTRAIN: \t Epoch: 39 \t Loss: -0.006636204482573602\nTRAIN: \t Epoch: 39 \t Loss: -0.006760941701941192\nTRAIN: \t Epoch: 39 \t Loss: -0.006862479156221856\nTRAIN: \t Epoch: 39 \t Loss: -0.006857666672052194\nTRAIN: \t Epoch: 39 \t Loss: -0.006599450591378487\nTRAIN: \t Epoch: 39 \t Loss: -0.006413890581045832\nTRAIN: \t Epoch: 39 \t Loss: -0.006444784191747506\nTRAIN: \t Epoch: 39 \t Loss: -0.00651567880413495\nTRAIN: \t Epoch: 39 \t Loss: -0.006569065576927228\nVALD: \t Epoch: 39 \t Loss: -0.009694594889879227\nVALD: \t Epoch: 39 \t Loss: -0.008572912774980068\nVALD: \t Epoch: 39 \t Loss: -0.005353450891561806\nVALD: \t Epoch: 39 \t Loss: -0.005678924704026319\n******************************\nEpoch: social-tag : 39\ntrain_loss -0.006569065576927228\nval_loss -0.005678924704026319\n{'min_val_epoch': 38, 'min_val_loss': -0.00719517314743377}\n******************************\nTRAIN: \t Epoch: 40 \t Loss: -0.007894017733633518\nTRAIN: \t Epoch: 40 \t Loss: -0.007509150542318821\nTRAIN: \t Epoch: 40 \t Loss: -0.0072179803003867464\nTRAIN: \t Epoch: 40 \t Loss: -0.0068024658830836415\nTRAIN: \t Epoch: 40 \t Loss: -0.006307696551084518\nTRAIN: \t Epoch: 40 \t Loss: -0.006330038886517286\nTRAIN: \t Epoch: 40 \t Loss: -0.006451422614710671\nTRAIN: \t Epoch: 40 \t Loss: -0.006667955545708537\nTRAIN: \t Epoch: 40 \t Loss: -0.006756440063731538\nTRAIN: \t Epoch: 40 \t Loss: -0.006708118319511414\nTRAIN: \t Epoch: 40 \t Loss: -0.0064132359217513694\nTRAIN: \t Epoch: 40 \t Loss: -0.006345039738031725\nTRAIN: \t Epoch: 40 \t Loss: -0.006386711297986599\nTRAIN: \t Epoch: 40 \t Loss: -0.006503174912982753\nTRAIN: \t Epoch: 40 \t Loss: -0.006558901102592548\nTRAIN: \t Epoch: 40 \t Loss: -0.006605784234125167\nTRAIN: \t Epoch: 40 \t Loss: -0.006613407058246208\nVALD: \t Epoch: 40 \t Loss: -0.009841140359640121\nVALD: \t Epoch: 40 \t Loss: -0.00924675539135933\nVALD: \t Epoch: 40 \t Loss: -0.006708955857902765\nVALD: \t Epoch: 40 \t Loss: -0.006836907473390926\n******************************\nEpoch: social-tag : 40\ntrain_loss -0.006613407058246208\nval_loss -0.006836907473390926\n{'min_val_epoch': 38, 'min_val_loss': -0.00719517314743377}\n******************************\nTRAIN: \t Epoch: 41 \t Loss: -0.008087464608252048\nTRAIN: \t Epoch: 41 \t Loss: -0.007969527505338192\nTRAIN: \t Epoch: 41 \t Loss: -0.007887970966597399\nTRAIN: \t Epoch: 41 \t Loss: -0.0078249970683828\nTRAIN: \t Epoch: 41 \t Loss: -0.00739488722756505\nTRAIN: \t Epoch: 41 \t Loss: -0.006805648988423248\nTRAIN: \t Epoch: 41 \t Loss: -0.00661286901283477\nTRAIN: \t Epoch: 41 \t Loss: -0.006717001524521038\nTRAIN: \t Epoch: 41 \t Loss: -0.00684245797391567\nTRAIN: \t Epoch: 41 \t Loss: -0.007039012177847326\nTRAIN: \t Epoch: 41 \t Loss: -0.007129296131262725\nTRAIN: \t Epoch: 41 \t Loss: -0.007085212098900229\nTRAIN: \t Epoch: 41 \t Loss: -0.007095517071250539\nTRAIN: \t Epoch: 41 \t Loss: -0.007175949452045772\nTRAIN: \t Epoch: 41 \t Loss: -0.007208263548091054\nTRAIN: \t Epoch: 41 \t Loss: -0.007207559086964466\nTRAIN: \t Epoch: 41 \t Loss: -0.007122338213252298\nVALD: \t Epoch: 41 \t Loss: -0.008987290784716606\nVALD: \t Epoch: 41 \t Loss: -0.008479565382003784\nVALD: \t Epoch: 41 \t Loss: -0.005173474627857407\nVALD: \t Epoch: 41 \t Loss: -0.005469133218605361\n******************************\nEpoch: social-tag : 41\ntrain_loss -0.007122338213252298\nval_loss -0.005469133218605361\n{'min_val_epoch': 38, 'min_val_loss': -0.00719517314743377}\n******************************\nTRAIN: \t Epoch: 42 \t Loss: -0.008322471752762794\nTRAIN: \t Epoch: 42 \t Loss: -0.008032999699935317\nTRAIN: \t Epoch: 42 \t Loss: -0.008278076692173878\nTRAIN: \t Epoch: 42 \t Loss: -0.008323857909999788\nTRAIN: \t Epoch: 42 \t Loss: -0.008328735362738371\nTRAIN: \t Epoch: 42 \t Loss: -0.008082084124907851\nTRAIN: \t Epoch: 42 \t Loss: -0.0074682604255420825\nTRAIN: \t Epoch: 42 \t Loss: -0.007060768955852836\nTRAIN: \t Epoch: 42 \t Loss: -0.006980103699283468\nTRAIN: \t Epoch: 42 \t Loss: -0.007058577286079526\nTRAIN: \t Epoch: 42 \t Loss: -0.007112535182386637\nTRAIN: \t Epoch: 42 \t Loss: -0.007167102419771254\nTRAIN: \t Epoch: 42 \t Loss: -0.007232307850454862\nTRAIN: \t Epoch: 42 \t Loss: -0.007297082172174539\nTRAIN: \t Epoch: 42 \t Loss: -0.00731747184569637\nTRAIN: \t Epoch: 42 \t Loss: -0.00726023648167029\nTRAIN: \t Epoch: 42 \t Loss: -0.007288422068637429\nVALD: \t Epoch: 42 \t Loss: -0.009524848312139511\nVALD: \t Epoch: 42 \t Loss: -0.008803209755569696\nVALD: \t Epoch: 42 \t Loss: -0.005331132250527541\nVALD: \t Epoch: 42 \t Loss: -0.005713472228326246\n******************************\nEpoch: social-tag : 42\ntrain_loss -0.007288422068637429\nval_loss -0.005713472228326246\n{'min_val_epoch': 38, 'min_val_loss': -0.00719517314743377}\n******************************\nTRAIN: \t Epoch: 43 \t Loss: -0.00787375494837761\nTRAIN: \t Epoch: 43 \t Loss: -0.00783189432695508\nTRAIN: \t Epoch: 43 \t Loss: -0.007957915465037027\nTRAIN: \t Epoch: 43 \t Loss: -0.007979988353326917\nTRAIN: \t Epoch: 43 \t Loss: -0.008089650422334671\nTRAIN: \t Epoch: 43 \t Loss: -0.007608913428460558\nTRAIN: \t Epoch: 43 \t Loss: -0.007150174717285803\nTRAIN: \t Epoch: 43 \t Loss: -0.007042085926514119\nTRAIN: \t Epoch: 43 \t Loss: -0.0071348863032956915\nTRAIN: \t Epoch: 43 \t Loss: -0.007240444282069802\nTRAIN: \t Epoch: 43 \t Loss: -0.007268583783033219\nTRAIN: \t Epoch: 43 \t Loss: -0.007259903475642204\nTRAIN: \t Epoch: 43 \t Loss: -0.0070318256934674885\nTRAIN: \t Epoch: 43 \t Loss: -0.006867725568424378\nTRAIN: \t Epoch: 43 \t Loss: -0.006910003038744132\nTRAIN: \t Epoch: 43 \t Loss: -0.006979880039580166\nTRAIN: \t Epoch: 43 \t Loss: -0.007053903783812668\nVALD: \t Epoch: 43 \t Loss: -0.010150289162993431\nVALD: \t Epoch: 43 \t Loss: -0.009700571652501822\nVALD: \t Epoch: 43 \t Loss: -0.007699718543638785\nVALD: \t Epoch: 43 \t Loss: -0.00771303888328537\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:36<00:00,  9.53it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.4247907039889409  FDE: 0.5727882491278176\n**************************************************\n******************************\nEpoch: social-tag : 43\ntrain_loss -0.007053903783812668\nval_loss -0.00771303888328537\n{'min_val_epoch': 43, 'min_val_loss': -0.00771303888328537}\n******************************\nTRAIN: \t Epoch: 44 \t Loss: -0.007126794196665287\nTRAIN: \t Epoch: 44 \t Loss: -0.008010624907910824\nTRAIN: \t Epoch: 44 \t Loss: -0.007913795299828053\nTRAIN: \t Epoch: 44 \t Loss: -0.00744857604149729\nTRAIN: \t Epoch: 44 \t Loss: -0.006708717625588179\nTRAIN: \t Epoch: 44 \t Loss: -0.006374482298269868\nTRAIN: \t Epoch: 44 \t Loss: -0.006523526872375182\nTRAIN: \t Epoch: 44 \t Loss: -0.0066931869951076806\nTRAIN: \t Epoch: 44 \t Loss: -0.006844052268813054\nTRAIN: \t Epoch: 44 \t Loss: -0.0070172437001019715\nTRAIN: \t Epoch: 44 \t Loss: -0.007051620886407115\nTRAIN: \t Epoch: 44 \t Loss: -0.007006826771733661\nTRAIN: \t Epoch: 44 \t Loss: -0.006813491694629192\nTRAIN: \t Epoch: 44 \t Loss: -0.006640460375430328\nTRAIN: \t Epoch: 44 \t Loss: -0.006650417577475309\nTRAIN: \t Epoch: 44 \t Loss: -0.006747963227098808\nTRAIN: \t Epoch: 44 \t Loss: -0.006788243708962744\nVALD: \t Epoch: 44 \t Loss: -0.01002707052975893\nVALD: \t Epoch: 44 \t Loss: -0.009917393792420626\nVALD: \t Epoch: 44 \t Loss: -0.007658217179899414\nVALD: \t Epoch: 44 \t Loss: -0.007691282652571292\n******************************\nEpoch: social-tag : 44\ntrain_loss -0.006788243708962744\nval_loss -0.007691282652571292\n{'min_val_epoch': 43, 'min_val_loss': -0.00771303888328537}\n******************************\nTRAIN: \t Epoch: 45 \t Loss: -0.007459966000169516\nTRAIN: \t Epoch: 45 \t Loss: -0.007603101432323456\nTRAIN: \t Epoch: 45 \t Loss: -0.008073707421620687\nTRAIN: \t Epoch: 45 \t Loss: -0.00812764884904027\nTRAIN: \t Epoch: 45 \t Loss: -0.008163724094629288\nTRAIN: \t Epoch: 45 \t Loss: -0.007712785775462787\nTRAIN: \t Epoch: 45 \t Loss: -0.007117108574935368\nTRAIN: \t Epoch: 45 \t Loss: -0.00701657310128212\nTRAIN: \t Epoch: 45 \t Loss: -0.007084985884527366\nTRAIN: \t Epoch: 45 \t Loss: -0.0071312823798507455\nTRAIN: \t Epoch: 45 \t Loss: -0.007274096704680811\nTRAIN: \t Epoch: 45 \t Loss: -0.007342435070313513\nTRAIN: \t Epoch: 45 \t Loss: -0.0074022934915354615\nTRAIN: \t Epoch: 45 \t Loss: -0.007395399867423943\nTRAIN: \t Epoch: 45 \t Loss: -0.007292124629020691\nTRAIN: \t Epoch: 45 \t Loss: -0.007063879500492476\nTRAIN: \t Epoch: 45 \t Loss: -0.007048181568582852\nVALD: \t Epoch: 45 \t Loss: -0.008774494752287865\nVALD: \t Epoch: 45 \t Loss: -0.008484963327646255\nVALD: \t Epoch: 45 \t Loss: -0.0061866755519683165\nVALD: \t Epoch: 45 \t Loss: -0.006345060503411435\n******************************\nEpoch: social-tag : 45\ntrain_loss -0.007048181568582852\nval_loss -0.006345060503411435\n{'min_val_epoch': 43, 'min_val_loss': -0.00771303888328537}\n******************************\nTRAIN: \t Epoch: 46 \t Loss: -0.0074117365293204784\nTRAIN: \t Epoch: 46 \t Loss: -0.007633049739524722\nTRAIN: \t Epoch: 46 \t Loss: -0.007660655304789543\nTRAIN: \t Epoch: 46 \t Loss: -0.0077165968250483274\nTRAIN: \t Epoch: 46 \t Loss: -0.007717923447489739\nTRAIN: \t Epoch: 46 \t Loss: -0.007708224700763822\nTRAIN: \t Epoch: 46 \t Loss: -0.007471603008785418\nTRAIN: \t Epoch: 46 \t Loss: -0.007332934357691556\nTRAIN: \t Epoch: 46 \t Loss: -0.007243531600882609\nTRAIN: \t Epoch: 46 \t Loss: -0.007294950308278203\nTRAIN: \t Epoch: 46 \t Loss: -0.007387687833133069\nTRAIN: \t Epoch: 46 \t Loss: -0.007418977213092148\nTRAIN: \t Epoch: 46 \t Loss: -0.00725447447397388\nTRAIN: \t Epoch: 46 \t Loss: -0.007049474166706204\nTRAIN: \t Epoch: 46 \t Loss: -0.007034596179922422\nTRAIN: \t Epoch: 46 \t Loss: -0.00711990479612723\nTRAIN: \t Epoch: 46 \t Loss: -0.007136599358284112\nVALD: \t Epoch: 46 \t Loss: -0.009898549877107143\nVALD: \t Epoch: 46 \t Loss: -0.009276764933019876\nVALD: \t Epoch: 46 \t Loss: -0.007456593525906404\nVALD: \t Epoch: 46 \t Loss: -0.007489606172976618\n******************************\nEpoch: social-tag : 46\ntrain_loss -0.007136599358284112\nval_loss -0.007489606172976618\n{'min_val_epoch': 43, 'min_val_loss': -0.00771303888328537}\n******************************\nTRAIN: \t Epoch: 47 \t Loss: -0.008226205594837666\nTRAIN: \t Epoch: 47 \t Loss: -0.008341278415173292\nTRAIN: \t Epoch: 47 \t Loss: -0.008261680603027344\nTRAIN: \t Epoch: 47 \t Loss: -0.0078770334366709\nTRAIN: \t Epoch: 47 \t Loss: -0.007599592488259077\nTRAIN: \t Epoch: 47 \t Loss: -0.007153343331689636\nTRAIN: \t Epoch: 47 \t Loss: -0.007047088018485478\nTRAIN: \t Epoch: 47 \t Loss: -0.007182445609942079\nTRAIN: \t Epoch: 47 \t Loss: -0.007365611174868213\nTRAIN: \t Epoch: 47 \t Loss: -0.007519473787397146\nTRAIN: \t Epoch: 47 \t Loss: -0.007483979433097623\nTRAIN: \t Epoch: 47 \t Loss: -0.007172318078422298\nTRAIN: \t Epoch: 47 \t Loss: -0.007037914949111068\nTRAIN: \t Epoch: 47 \t Loss: -0.007063340046443045\nTRAIN: \t Epoch: 47 \t Loss: -0.00712548076796035\nTRAIN: \t Epoch: 47 \t Loss: -0.007179966472904198\nTRAIN: \t Epoch: 47 \t Loss: -0.007208765554947384\nVALD: \t Epoch: 47 \t Loss: -0.010542480275034904\nVALD: \t Epoch: 47 \t Loss: -0.010125693865120411\nVALD: \t Epoch: 47 \t Loss: -0.007155207024576764\nVALD: \t Epoch: 47 \t Loss: -0.007330367337919756\n******************************\nEpoch: social-tag : 47\ntrain_loss -0.007208765554947384\nval_loss -0.007330367337919756\n{'min_val_epoch': 43, 'min_val_loss': -0.00771303888328537}\n******************************\nTRAIN: \t Epoch: 48 \t Loss: -0.008654536679387093\nTRAIN: \t Epoch: 48 \t Loss: -0.00795193505473435\nTRAIN: \t Epoch: 48 \t Loss: -0.007629481765131156\nTRAIN: \t Epoch: 48 \t Loss: -0.0072112082270905375\nTRAIN: \t Epoch: 48 \t Loss: -0.006872659083455801\nTRAIN: \t Epoch: 48 \t Loss: -0.006945175506795446\nTRAIN: \t Epoch: 48 \t Loss: -0.00709540829328554\nTRAIN: \t Epoch: 48 \t Loss: -0.007263929990585893\nTRAIN: \t Epoch: 48 \t Loss: -0.007434668485075235\nTRAIN: \t Epoch: 48 \t Loss: -0.007541769044473767\nTRAIN: \t Epoch: 48 \t Loss: -0.007564042864198034\nTRAIN: \t Epoch: 48 \t Loss: -0.007505292305722833\nTRAIN: \t Epoch: 48 \t Loss: -0.00743383108280026\nTRAIN: \t Epoch: 48 \t Loss: -0.007260096026584506\nTRAIN: \t Epoch: 48 \t Loss: -0.007203227560967207\nTRAIN: \t Epoch: 48 \t Loss: -0.007282465550815687\nTRAIN: \t Epoch: 48 \t Loss: -0.007327802959039356\nVALD: \t Epoch: 48 \t Loss: -0.010915485210716724\nVALD: \t Epoch: 48 \t Loss: -0.010450242087244987\nVALD: \t Epoch: 48 \t Loss: -0.008257112931460142\nVALD: \t Epoch: 48 \t Loss: -0.00825286125708483\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.79it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.42013155077264686  FDE: 0.5730152690964262\n**************************************************\n******************************\nEpoch: social-tag : 48\ntrain_loss -0.007327802959039356\nval_loss -0.00825286125708483\n{'min_val_epoch': 48, 'min_val_loss': -0.00825286125708483}\n******************************\nTRAIN: \t Epoch: 49 \t Loss: -0.008841217495501041\nTRAIN: \t Epoch: 49 \t Loss: -0.008906253147870302\nTRAIN: \t Epoch: 49 \t Loss: -0.00878974391768376\nTRAIN: \t Epoch: 49 \t Loss: -0.008676933590322733\nTRAIN: \t Epoch: 49 \t Loss: -0.008772098273038865\nTRAIN: \t Epoch: 49 \t Loss: -0.008616479579359293\nTRAIN: \t Epoch: 49 \t Loss: -0.008248806132801942\nTRAIN: \t Epoch: 49 \t Loss: -0.007834233227185905\nTRAIN: \t Epoch: 49 \t Loss: -0.00764277960277266\nTRAIN: \t Epoch: 49 \t Loss: -0.007635901356115938\nTRAIN: \t Epoch: 49 \t Loss: -0.007675859780812805\nTRAIN: \t Epoch: 49 \t Loss: -0.007747562408136825\nTRAIN: \t Epoch: 49 \t Loss: -0.007878747016478043\nTRAIN: \t Epoch: 49 \t Loss: -0.007869411326412643\nTRAIN: \t Epoch: 49 \t Loss: -0.007878783096869787\nTRAIN: \t Epoch: 49 \t Loss: -0.007906609622295946\nTRAIN: \t Epoch: 49 \t Loss: -0.00787148977432287\nVALD: \t Epoch: 49 \t Loss: -0.009118106216192245\nVALD: \t Epoch: 49 \t Loss: -0.009153954219073057\nVALD: \t Epoch: 49 \t Loss: -0.007721710950136185\nVALD: \t Epoch: 49 \t Loss: -0.007785093046709925\n******************************\nEpoch: social-tag : 49\ntrain_loss -0.00787148977432287\nval_loss -0.007785093046709925\n{'min_val_epoch': 48, 'min_val_loss': -0.00825286125708483}\n******************************\nTRAIN: \t Epoch: 50 \t Loss: -0.009007302112877369\nTRAIN: \t Epoch: 50 \t Loss: -0.008698651567101479\nTRAIN: \t Epoch: 50 \t Loss: -0.00838639261201024\nTRAIN: \t Epoch: 50 \t Loss: -0.008246011915616691\nTRAIN: \t Epoch: 50 \t Loss: -0.008091308828443288\nTRAIN: \t Epoch: 50 \t Loss: -0.007744200062006712\nTRAIN: \t Epoch: 50 \t Loss: -0.007476991375109979\nTRAIN: \t Epoch: 50 \t Loss: -0.007453113386873156\nTRAIN: \t Epoch: 50 \t Loss: -0.007553418943037589\nTRAIN: \t Epoch: 50 \t Loss: -0.007723379461094737\nTRAIN: \t Epoch: 50 \t Loss: -0.007788216906853698\nTRAIN: \t Epoch: 50 \t Loss: -0.007802755921147764\nTRAIN: \t Epoch: 50 \t Loss: -0.007621603229871163\nTRAIN: \t Epoch: 50 \t Loss: -0.007584453161273684\nTRAIN: \t Epoch: 50 \t Loss: -0.007574094242105881\nTRAIN: \t Epoch: 50 \t Loss: -0.007619946060003713\nTRAIN: \t Epoch: 50 \t Loss: -0.007648180492899634\nVALD: \t Epoch: 50 \t Loss: -0.010593844577670097\nVALD: \t Epoch: 50 \t Loss: -0.00991143798455596\nVALD: \t Epoch: 50 \t Loss: -0.007663056409607331\nVALD: \t Epoch: 50 \t Loss: -0.007727636548573385\n******************************\nEpoch: social-tag : 50\ntrain_loss -0.007648180492899634\nval_loss -0.007727636548573385\n{'min_val_epoch': 48, 'min_val_loss': -0.00825286125708483}\n******************************\nTRAIN: \t Epoch: 51 \t Loss: -0.008546210825443268\nTRAIN: \t Epoch: 51 \t Loss: -0.00909914867952466\nTRAIN: \t Epoch: 51 \t Loss: -0.00887804323186477\nTRAIN: \t Epoch: 51 \t Loss: -0.008718803292140365\nTRAIN: \t Epoch: 51 \t Loss: -0.008278676494956016\nTRAIN: \t Epoch: 51 \t Loss: -0.007547306828200817\nTRAIN: \t Epoch: 51 \t Loss: -0.00738358184961336\nTRAIN: \t Epoch: 51 \t Loss: -0.007463601941708475\nTRAIN: \t Epoch: 51 \t Loss: -0.007597809088312917\nTRAIN: \t Epoch: 51 \t Loss: -0.007573917089030146\nTRAIN: \t Epoch: 51 \t Loss: -0.007590502246536992\nTRAIN: \t Epoch: 51 \t Loss: -0.007675317969794075\nTRAIN: \t Epoch: 51 \t Loss: -0.007803744493195644\nTRAIN: \t Epoch: 51 \t Loss: -0.007791042394403901\nTRAIN: \t Epoch: 51 \t Loss: -0.007775777205824852\nTRAIN: \t Epoch: 51 \t Loss: -0.0076691286521963775\nTRAIN: \t Epoch: 51 \t Loss: -0.007589548812106703\nVALD: \t Epoch: 51 \t Loss: -0.008552310056984425\nVALD: \t Epoch: 51 \t Loss: -0.008669022470712662\nVALD: \t Epoch: 51 \t Loss: -0.007322282996028662\nVALD: \t Epoch: 51 \t Loss: -0.007238555216265772\n******************************\nEpoch: social-tag : 51\ntrain_loss -0.007589548812106703\nval_loss -0.007238555216265772\n{'min_val_epoch': 48, 'min_val_loss': -0.00825286125708483}\n******************************\nTRAIN: \t Epoch: 52 \t Loss: -0.008178542368113995\nTRAIN: \t Epoch: 52 \t Loss: -0.008658434264361858\nTRAIN: \t Epoch: 52 \t Loss: -0.008828135828177134\nTRAIN: \t Epoch: 52 \t Loss: -0.008917179889976978\nTRAIN: \t Epoch: 52 \t Loss: -0.00884084440767765\nTRAIN: \t Epoch: 52 \t Loss: -0.008696096483618021\nTRAIN: \t Epoch: 52 \t Loss: -0.00832249317318201\nTRAIN: \t Epoch: 52 \t Loss: -0.008024423150345683\nTRAIN: \t Epoch: 52 \t Loss: -0.007853126981192164\nTRAIN: \t Epoch: 52 \t Loss: -0.00786142870783806\nTRAIN: \t Epoch: 52 \t Loss: -0.00793183044615117\nTRAIN: \t Epoch: 52 \t Loss: -0.007955917157232761\nTRAIN: \t Epoch: 52 \t Loss: -0.007986193881011926\nTRAIN: \t Epoch: 52 \t Loss: -0.008060461708477564\nTRAIN: \t Epoch: 52 \t Loss: -0.008039242494851351\nTRAIN: \t Epoch: 52 \t Loss: -0.007836636912543327\nTRAIN: \t Epoch: 52 \t Loss: -0.007768840557246497\nVALD: \t Epoch: 52 \t Loss: -0.008709987625479698\nVALD: \t Epoch: 52 \t Loss: -0.008411832619458437\nVALD: \t Epoch: 52 \t Loss: -0.0070256648274759454\nVALD: \t Epoch: 52 \t Loss: -0.007015846803516685\n******************************\nEpoch: social-tag : 52\ntrain_loss -0.007768840557246497\nval_loss -0.007015846803516685\n{'min_val_epoch': 48, 'min_val_loss': -0.00825286125708483}\n******************************\nTRAIN: \t Epoch: 53 \t Loss: -0.00791524164378643\nTRAIN: \t Epoch: 53 \t Loss: -0.007792750373482704\nTRAIN: \t Epoch: 53 \t Loss: -0.008075151592493057\nTRAIN: \t Epoch: 53 \t Loss: -0.0082119086291641\nTRAIN: \t Epoch: 53 \t Loss: -0.008257233723998069\nTRAIN: \t Epoch: 53 \t Loss: -0.00787092954851687\nTRAIN: \t Epoch: 53 \t Loss: -0.007237243931740522\nTRAIN: \t Epoch: 53 \t Loss: -0.0071937995962798595\nTRAIN: \t Epoch: 53 \t Loss: -0.00737072392884228\nTRAIN: \t Epoch: 53 \t Loss: -0.007442103419452906\nTRAIN: \t Epoch: 53 \t Loss: -0.007585468617352572\nTRAIN: \t Epoch: 53 \t Loss: -0.007660590345039964\nTRAIN: \t Epoch: 53 \t Loss: -0.007574779375527914\nTRAIN: \t Epoch: 53 \t Loss: -0.007416209272508111\nTRAIN: \t Epoch: 53 \t Loss: -0.007273582369089126\nTRAIN: \t Epoch: 53 \t Loss: -0.007312595611438155\nTRAIN: \t Epoch: 53 \t Loss: -0.007352029453172829\nVALD: \t Epoch: 53 \t Loss: -0.010185951367020607\nVALD: \t Epoch: 53 \t Loss: -0.009264651220291853\nVALD: \t Epoch: 53 \t Loss: -0.0076507509996493655\nVALD: \t Epoch: 53 \t Loss: -0.0076469002131692426\n******************************\nEpoch: social-tag : 53\ntrain_loss -0.007352029453172829\nval_loss -0.0076469002131692426\n{'min_val_epoch': 48, 'min_val_loss': -0.00825286125708483}\n******************************\nTRAIN: \t Epoch: 54 \t Loss: -0.0077138193883001804\nTRAIN: \t Epoch: 54 \t Loss: -0.00829494115896523\nTRAIN: \t Epoch: 54 \t Loss: -0.008515946722278992\nTRAIN: \t Epoch: 54 \t Loss: -0.008728689164854586\nTRAIN: \t Epoch: 54 \t Loss: -0.008424820192158223\nTRAIN: \t Epoch: 54 \t Loss: -0.00789444373610119\nTRAIN: \t Epoch: 54 \t Loss: -0.007652473569448505\nTRAIN: \t Epoch: 54 \t Loss: -0.00770727515919134\nTRAIN: \t Epoch: 54 \t Loss: -0.007820900270922316\nTRAIN: \t Epoch: 54 \t Loss: -0.007940293895080686\nTRAIN: \t Epoch: 54 \t Loss: -0.007942099251191725\nTRAIN: \t Epoch: 54 \t Loss: -0.007849680609069765\nTRAIN: \t Epoch: 54 \t Loss: -0.007766535398192131\nTRAIN: \t Epoch: 54 \t Loss: -0.007746778262246933\nTRAIN: \t Epoch: 54 \t Loss: -0.0077850242766241235\nTRAIN: \t Epoch: 54 \t Loss: -0.007842910854378715\nTRAIN: \t Epoch: 54 \t Loss: -0.007896706518350225\nVALD: \t Epoch: 54 \t Loss: -0.011307764798402786\nVALD: \t Epoch: 54 \t Loss: -0.010625424329191446\nVALD: \t Epoch: 54 \t Loss: -0.00799455726519227\nVALD: \t Epoch: 54 \t Loss: -0.00807580060349729\n******************************\nEpoch: social-tag : 54\ntrain_loss -0.007896706518350225\nval_loss -0.00807580060349729\n{'min_val_epoch': 48, 'min_val_loss': -0.00825286125708483}\n******************************\nTRAIN: \t Epoch: 55 \t Loss: -0.008754867129027843\nTRAIN: \t Epoch: 55 \t Loss: -0.008776196278631687\nTRAIN: \t Epoch: 55 \t Loss: -0.008694772608578205\nTRAIN: \t Epoch: 55 \t Loss: -0.008527010213583708\nTRAIN: \t Epoch: 55 \t Loss: -0.008028193097561597\nTRAIN: \t Epoch: 55 \t Loss: -0.007678993434334795\nTRAIN: \t Epoch: 55 \t Loss: -0.0077576480938919955\nTRAIN: \t Epoch: 55 \t Loss: -0.00790819205576554\nTRAIN: \t Epoch: 55 \t Loss: -0.007944538723677397\nTRAIN: \t Epoch: 55 \t Loss: -0.007938031712546945\nTRAIN: \t Epoch: 55 \t Loss: -0.00802803627977317\nTRAIN: \t Epoch: 55 \t Loss: -0.007982437149621546\nTRAIN: \t Epoch: 55 \t Loss: -0.007719002341708312\nTRAIN: \t Epoch: 55 \t Loss: -0.007577696382733328\nTRAIN: \t Epoch: 55 \t Loss: -0.007584062094489734\nTRAIN: \t Epoch: 55 \t Loss: -0.0076665644883178174\nTRAIN: \t Epoch: 55 \t Loss: -0.007718515751714056\nVALD: \t Epoch: 55 \t Loss: -0.01157386600971222\nVALD: \t Epoch: 55 \t Loss: -0.011075576767325401\nVALD: \t Epoch: 55 \t Loss: -0.009001414912442366\nVALD: \t Epoch: 55 \t Loss: -0.00886874094218789\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:33<00:00,  9.90it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.3915182016698352  FDE: 0.5304298285471358\n**************************************************\n******************************\nEpoch: social-tag : 55\ntrain_loss -0.007718515751714056\nval_loss -0.00886874094218789\n{'min_val_epoch': 55, 'min_val_loss': -0.00886874094218789}\n******************************\nTRAIN: \t Epoch: 56 \t Loss: -0.008874704129993916\nTRAIN: \t Epoch: 56 \t Loss: -0.009061007760465145\nTRAIN: \t Epoch: 56 \t Loss: -0.008851157190899054\nTRAIN: \t Epoch: 56 \t Loss: -0.008440283010713756\nTRAIN: \t Epoch: 56 \t Loss: -0.008311343099921941\nTRAIN: \t Epoch: 56 \t Loss: -0.008073465277751287\nTRAIN: \t Epoch: 56 \t Loss: -0.008002848630504949\nTRAIN: \t Epoch: 56 \t Loss: -0.0079192771227099\nTRAIN: \t Epoch: 56 \t Loss: -0.007941235115544664\nTRAIN: \t Epoch: 56 \t Loss: -0.00799037297256291\nTRAIN: \t Epoch: 56 \t Loss: -0.007925123780627142\nTRAIN: \t Epoch: 56 \t Loss: -0.007823069502289096\nTRAIN: \t Epoch: 56 \t Loss: -0.007810899808716316\nTRAIN: \t Epoch: 56 \t Loss: -0.007828782611925687\nTRAIN: \t Epoch: 56 \t Loss: -0.007890456511328617\nTRAIN: \t Epoch: 56 \t Loss: -0.007985748030478135\nTRAIN: \t Epoch: 56 \t Loss: -0.007987300188026646\nVALD: \t Epoch: 56 \t Loss: -0.010443905368447304\nVALD: \t Epoch: 56 \t Loss: -0.009901771787554026\nVALD: \t Epoch: 56 \t Loss: -0.008281469034651915\nVALD: \t Epoch: 56 \t Loss: -0.008133957604924124\n******************************\nEpoch: social-tag : 56\ntrain_loss -0.007987300188026646\nval_loss -0.008133957604924124\n{'min_val_epoch': 55, 'min_val_loss': -0.00886874094218789}\n******************************\nTRAIN: \t Epoch: 57 \t Loss: -0.009732991456985474\nTRAIN: \t Epoch: 57 \t Loss: -0.00899120606482029\nTRAIN: \t Epoch: 57 \t Loss: -0.007968492961178223\nTRAIN: \t Epoch: 57 \t Loss: -0.00709327170625329\nTRAIN: \t Epoch: 57 \t Loss: -0.007026291452348232\nTRAIN: \t Epoch: 57 \t Loss: -0.007195123316099246\nTRAIN: \t Epoch: 57 \t Loss: -0.007411892259759563\nTRAIN: \t Epoch: 57 \t Loss: -0.007499718572944403\nTRAIN: \t Epoch: 57 \t Loss: -0.00778217085947593\nTRAIN: \t Epoch: 57 \t Loss: -0.007965603563934564\nTRAIN: \t Epoch: 57 \t Loss: -0.008016716011545875\nTRAIN: \t Epoch: 57 \t Loss: -0.007937188531892994\nTRAIN: \t Epoch: 57 \t Loss: -0.007908438045818072\nTRAIN: \t Epoch: 57 \t Loss: -0.007863451061504228\nTRAIN: \t Epoch: 57 \t Loss: -0.00786957579354445\nTRAIN: \t Epoch: 57 \t Loss: -0.00788854481652379\nTRAIN: \t Epoch: 57 \t Loss: -0.00787928652469859\nVALD: \t Epoch: 57 \t Loss: -0.010485108010470867\nVALD: \t Epoch: 57 \t Loss: -0.01012939028441906\nVALD: \t Epoch: 57 \t Loss: -0.008558999591817459\nVALD: \t Epoch: 57 \t Loss: -0.00851539497128028\n******************************\nEpoch: social-tag : 57\ntrain_loss -0.00787928652469859\nval_loss -0.00851539497128028\n{'min_val_epoch': 55, 'min_val_loss': -0.00886874094218789}\n******************************\nTRAIN: \t Epoch: 58 \t Loss: -0.00876577291637659\nTRAIN: \t Epoch: 58 \t Loss: -0.008966181892901659\nTRAIN: \t Epoch: 58 \t Loss: -0.00897393717120091\nTRAIN: \t Epoch: 58 \t Loss: -0.008756039198487997\nTRAIN: \t Epoch: 58 \t Loss: -0.008011138252913951\nTRAIN: \t Epoch: 58 \t Loss: -0.007717531019200881\nTRAIN: \t Epoch: 58 \t Loss: -0.007818847761622496\nTRAIN: \t Epoch: 58 \t Loss: -0.007852740352973342\nTRAIN: \t Epoch: 58 \t Loss: -0.007892848820322089\nTRAIN: \t Epoch: 58 \t Loss: -0.008081244211643935\nTRAIN: \t Epoch: 58 \t Loss: -0.008219223977489904\nTRAIN: \t Epoch: 58 \t Loss: -0.008227868936955929\nTRAIN: \t Epoch: 58 \t Loss: -0.008113971266608972\nTRAIN: \t Epoch: 58 \t Loss: -0.007976681293387498\nTRAIN: \t Epoch: 58 \t Loss: -0.007911023342361052\nTRAIN: \t Epoch: 58 \t Loss: -0.007932774926302955\nTRAIN: \t Epoch: 58 \t Loss: -0.007962481485623302\nVALD: \t Epoch: 58 \t Loss: -0.011073296889662743\nVALD: \t Epoch: 58 \t Loss: -0.010179100558161736\nVALD: \t Epoch: 58 \t Loss: -0.007949931469435493\nVALD: \t Epoch: 58 \t Loss: -0.008016318558694835\n******************************\nEpoch: social-tag : 58\ntrain_loss -0.007962481485623302\nval_loss -0.008016318558694835\n{'min_val_epoch': 55, 'min_val_loss': -0.00886874094218789}\n******************************\nTRAIN: \t Epoch: 59 \t Loss: -0.009206905029714108\nTRAIN: \t Epoch: 59 \t Loss: -0.008932591881603003\nTRAIN: \t Epoch: 59 \t Loss: -0.008605695950488249\nTRAIN: \t Epoch: 59 \t Loss: -0.00827810587361455\nTRAIN: \t Epoch: 59 \t Loss: -0.008008732087910175\nTRAIN: \t Epoch: 59 \t Loss: -0.008026187773793936\nTRAIN: \t Epoch: 59 \t Loss: -0.008000781094389302\nTRAIN: \t Epoch: 59 \t Loss: -0.008144122315570712\nTRAIN: \t Epoch: 59 \t Loss: -0.008164829264084498\nTRAIN: \t Epoch: 59 \t Loss: -0.008100885199382901\nTRAIN: \t Epoch: 59 \t Loss: -0.007878200405023315\nTRAIN: \t Epoch: 59 \t Loss: -0.007794292954107125\nTRAIN: \t Epoch: 59 \t Loss: -0.007854378495651942\nTRAIN: \t Epoch: 59 \t Loss: -0.00788102245756558\nTRAIN: \t Epoch: 59 \t Loss: -0.007983564275006454\nTRAIN: \t Epoch: 59 \t Loss: -0.008057421538978815\nTRAIN: \t Epoch: 59 \t Loss: -0.008026902930754604\nVALD: \t Epoch: 59 \t Loss: -0.011078597977757454\nVALD: \t Epoch: 59 \t Loss: -0.010682716500014067\nVALD: \t Epoch: 59 \t Loss: -0.008922431152313948\nVALD: \t Epoch: 59 \t Loss: -0.00886212149542011\n******************************\nEpoch: social-tag : 59\ntrain_loss -0.008026902930754604\nval_loss -0.00886212149542011\n{'min_val_epoch': 55, 'min_val_loss': -0.00886874094218789}\n******************************\nTRAIN: \t Epoch: 60 \t Loss: -0.008918603882193565\nTRAIN: \t Epoch: 60 \t Loss: -0.008623882196843624\nTRAIN: \t Epoch: 60 \t Loss: -0.008846413033703962\nTRAIN: \t Epoch: 60 \t Loss: -0.008876846637576818\nTRAIN: \t Epoch: 60 \t Loss: -0.008875646255910396\nTRAIN: \t Epoch: 60 \t Loss: -0.008646396532033881\nTRAIN: \t Epoch: 60 \t Loss: -0.008213876014841455\nTRAIN: \t Epoch: 60 \t Loss: -0.008107835950795561\nTRAIN: \t Epoch: 60 \t Loss: -0.008140983008262184\nTRAIN: \t Epoch: 60 \t Loss: -0.008248878316953779\nTRAIN: \t Epoch: 60 \t Loss: -0.00832708573646166\nTRAIN: \t Epoch: 60 \t Loss: -0.008328951934042076\nTRAIN: \t Epoch: 60 \t Loss: -0.008337220069594108\nTRAIN: \t Epoch: 60 \t Loss: -0.008336299997089165\nTRAIN: \t Epoch: 60 \t Loss: -0.008095574068526427\nTRAIN: \t Epoch: 60 \t Loss: -0.007961233204696327\nTRAIN: \t Epoch: 60 \t Loss: -0.0079731573774056\nVALD: \t Epoch: 60 \t Loss: -0.01072461623698473\nVALD: \t Epoch: 60 \t Loss: -0.010049549397081137\nVALD: \t Epoch: 60 \t Loss: -0.007996206016590198\nVALD: \t Epoch: 60 \t Loss: -0.008002771826799282\n******************************\nEpoch: social-tag : 60\ntrain_loss -0.0079731573774056\nval_loss -0.008002771826799282\n{'min_val_epoch': 55, 'min_val_loss': -0.00886874094218789}\n******************************\nTRAIN: \t Epoch: 61 \t Loss: -0.008626626804471016\nTRAIN: \t Epoch: 61 \t Loss: -0.008737575728446245\nTRAIN: \t Epoch: 61 \t Loss: -0.009049679773549238\nTRAIN: \t Epoch: 61 \t Loss: -0.009114615619182587\nTRAIN: \t Epoch: 61 \t Loss: -0.009030316956341267\nTRAIN: \t Epoch: 61 \t Loss: -0.008497388722995916\nTRAIN: \t Epoch: 61 \t Loss: -0.008015128850404705\nTRAIN: \t Epoch: 61 \t Loss: -0.00802895106608048\nTRAIN: \t Epoch: 61 \t Loss: -0.008126950150148736\nTRAIN: \t Epoch: 61 \t Loss: -0.008179016457870603\nTRAIN: \t Epoch: 61 \t Loss: -0.008256957908584312\nTRAIN: \t Epoch: 61 \t Loss: -0.008362300655183693\nTRAIN: \t Epoch: 61 \t Loss: -0.008394305343524767\nTRAIN: \t Epoch: 61 \t Loss: -0.008371055691636034\nTRAIN: \t Epoch: 61 \t Loss: -0.008351743811120589\nTRAIN: \t Epoch: 61 \t Loss: -0.008282447408419102\nTRAIN: \t Epoch: 61 \t Loss: -0.008280387825586578\nVALD: \t Epoch: 61 \t Loss: -0.010885238647460938\nVALD: \t Epoch: 61 \t Loss: -0.01010383665561676\nVALD: \t Epoch: 61 \t Loss: -0.0073837088809038205\nVALD: \t Epoch: 61 \t Loss: -0.007539557750353556\n******************************\nEpoch: social-tag : 61\ntrain_loss -0.008280387825586578\nval_loss -0.007539557750353556\n{'min_val_epoch': 55, 'min_val_loss': -0.00886874094218789}\n******************************\nTRAIN: \t Epoch: 62 \t Loss: -0.008349151350557804\nTRAIN: \t Epoch: 62 \t Loss: -0.008367447648197412\nTRAIN: \t Epoch: 62 \t Loss: -0.008273712359368801\nTRAIN: \t Epoch: 62 \t Loss: -0.00817229482345283\nTRAIN: \t Epoch: 62 \t Loss: -0.008032093290239573\nTRAIN: \t Epoch: 62 \t Loss: -0.008073941571637988\nTRAIN: \t Epoch: 62 \t Loss: -0.008059253450483084\nTRAIN: \t Epoch: 62 \t Loss: -0.008124477870296687\nTRAIN: \t Epoch: 62 \t Loss: -0.008193272838575972\nTRAIN: \t Epoch: 62 \t Loss: -0.008101786067709327\nTRAIN: \t Epoch: 62 \t Loss: -0.008002513934942808\nTRAIN: \t Epoch: 62 \t Loss: -0.007993591949343681\nTRAIN: \t Epoch: 62 \t Loss: -0.008037683124152513\nTRAIN: \t Epoch: 62 \t Loss: -0.008107341160731656\nTRAIN: \t Epoch: 62 \t Loss: -0.00814076904207468\nTRAIN: \t Epoch: 62 \t Loss: -0.008171143126673996\nTRAIN: \t Epoch: 62 \t Loss: -0.008178020714584625\nVALD: \t Epoch: 62 \t Loss: -0.010895519517362118\nVALD: \t Epoch: 62 \t Loss: -0.010175623465329409\nVALD: \t Epoch: 62 \t Loss: -0.00795433841024836\nVALD: \t Epoch: 62 \t Loss: -0.008060488515271398\n******************************\nEpoch: social-tag : 62\ntrain_loss -0.008178020714584625\nval_loss -0.008060488515271398\n{'min_val_epoch': 55, 'min_val_loss': -0.00886874094218789}\n******************************\nTRAIN: \t Epoch: 63 \t Loss: -0.009022001177072525\nTRAIN: \t Epoch: 63 \t Loss: -0.008849192410707474\nTRAIN: \t Epoch: 63 \t Loss: -0.008792500322063765\nTRAIN: \t Epoch: 63 \t Loss: -0.008513137581758201\nTRAIN: \t Epoch: 63 \t Loss: -0.008363430388271809\nTRAIN: \t Epoch: 63 \t Loss: -0.008323681385566791\nTRAIN: \t Epoch: 63 \t Loss: -0.008265966948653971\nTRAIN: \t Epoch: 63 \t Loss: -0.00830749236047268\nTRAIN: \t Epoch: 63 \t Loss: -0.008382440958586004\nTRAIN: \t Epoch: 63 \t Loss: -0.008363792207092047\nTRAIN: \t Epoch: 63 \t Loss: -0.008234136864881624\nTRAIN: \t Epoch: 63 \t Loss: -0.008276021922938526\nTRAIN: \t Epoch: 63 \t Loss: -0.008358972612768412\nTRAIN: \t Epoch: 63 \t Loss: -0.008324346338797892\nTRAIN: \t Epoch: 63 \t Loss: -0.008277061612655718\nTRAIN: \t Epoch: 63 \t Loss: -0.008244658267358318\nTRAIN: \t Epoch: 63 \t Loss: -0.008270978391396277\nVALD: \t Epoch: 63 \t Loss: -0.011118383146822453\nVALD: \t Epoch: 63 \t Loss: -0.010881844442337751\nVALD: \t Epoch: 63 \t Loss: -0.009224013114968935\nVALD: \t Epoch: 63 \t Loss: -0.009142571818566847\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.71it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.3756898580496378  FDE: 0.5130446441906911\n**************************************************\n******************************\nEpoch: social-tag : 63\ntrain_loss -0.008270978391396277\nval_loss -0.009142571818566847\n{'min_val_epoch': 63, 'min_val_loss': -0.009142571818566847}\n******************************\nTRAIN: \t Epoch: 64 \t Loss: -0.009205538779497147\nTRAIN: \t Epoch: 64 \t Loss: -0.009105111472308636\nTRAIN: \t Epoch: 64 \t Loss: -0.009458554287751516\nTRAIN: \t Epoch: 64 \t Loss: -0.009309939108788967\nTRAIN: \t Epoch: 64 \t Loss: -0.00934507679194212\nTRAIN: \t Epoch: 64 \t Loss: -0.009178197166572014\nTRAIN: \t Epoch: 64 \t Loss: -0.009237907294716154\nTRAIN: \t Epoch: 64 \t Loss: -0.009217289392836392\nTRAIN: \t Epoch: 64 \t Loss: -0.009174760224090682\nTRAIN: \t Epoch: 64 \t Loss: -0.008895367197692395\nTRAIN: \t Epoch: 64 \t Loss: -0.00878652934492989\nTRAIN: \t Epoch: 64 \t Loss: -0.008793228810342649\nTRAIN: \t Epoch: 64 \t Loss: -0.008789542310226422\nTRAIN: \t Epoch: 64 \t Loss: -0.008858220086299948\nTRAIN: \t Epoch: 64 \t Loss: -0.008889149781316519\nTRAIN: \t Epoch: 64 \t Loss: -0.008832971943775192\nTRAIN: \t Epoch: 64 \t Loss: -0.008713062147073673\nVALD: \t Epoch: 64 \t Loss: -0.010034800507128239\nVALD: \t Epoch: 64 \t Loss: -0.009784241206943989\nVALD: \t Epoch: 64 \t Loss: -0.00805249841262897\nVALD: \t Epoch: 64 \t Loss: -0.008099046533931039\n******************************\nEpoch: social-tag : 64\ntrain_loss -0.008713062147073673\nval_loss -0.008099046533931039\n{'min_val_epoch': 63, 'min_val_loss': -0.009142571818566847}\n******************************\nTRAIN: \t Epoch: 65 \t Loss: -0.008063997142016888\nTRAIN: \t Epoch: 65 \t Loss: -0.008880836889147758\nTRAIN: \t Epoch: 65 \t Loss: -0.009169035280744234\nTRAIN: \t Epoch: 65 \t Loss: -0.009433736326172948\nTRAIN: \t Epoch: 65 \t Loss: -0.009492235258221626\nTRAIN: \t Epoch: 65 \t Loss: -0.009423226738969484\nTRAIN: \t Epoch: 65 \t Loss: -0.009201718627342157\nTRAIN: \t Epoch: 65 \t Loss: -0.00892983132507652\nTRAIN: \t Epoch: 65 \t Loss: -0.008617417679892646\nTRAIN: \t Epoch: 65 \t Loss: -0.008575030881911517\nTRAIN: \t Epoch: 65 \t Loss: -0.008687114258381453\nTRAIN: \t Epoch: 65 \t Loss: -0.0086846596095711\nTRAIN: \t Epoch: 65 \t Loss: -0.00875083335603659\nTRAIN: \t Epoch: 65 \t Loss: -0.008786185711090053\nTRAIN: \t Epoch: 65 \t Loss: -0.008759451843798161\nTRAIN: \t Epoch: 65 \t Loss: -0.008603466034401208\nTRAIN: \t Epoch: 65 \t Loss: -0.008512368580947319\nVALD: \t Epoch: 65 \t Loss: -0.01006903313100338\nVALD: \t Epoch: 65 \t Loss: -0.00975531479343772\nVALD: \t Epoch: 65 \t Loss: -0.008185647117594877\nVALD: \t Epoch: 65 \t Loss: -0.008221402853548884\n******************************\nEpoch: social-tag : 65\ntrain_loss -0.008512368580947319\nval_loss -0.008221402853548884\n{'min_val_epoch': 63, 'min_val_loss': -0.009142571818566847}\n******************************\nTRAIN: \t Epoch: 66 \t Loss: -0.0079042324796319\nTRAIN: \t Epoch: 66 \t Loss: -0.008690005633980036\nTRAIN: \t Epoch: 66 \t Loss: -0.009206809413929781\nTRAIN: \t Epoch: 66 \t Loss: -0.00930733815766871\nTRAIN: \t Epoch: 66 \t Loss: -0.008978183846920729\nTRAIN: \t Epoch: 66 \t Loss: -0.008312481843555966\nTRAIN: \t Epoch: 66 \t Loss: -0.008164417863424336\nTRAIN: \t Epoch: 66 \t Loss: -0.008298494794871658\nTRAIN: \t Epoch: 66 \t Loss: -0.008389081443763442\nTRAIN: \t Epoch: 66 \t Loss: -0.008477215515449643\nTRAIN: \t Epoch: 66 \t Loss: -0.008597189179536972\nTRAIN: \t Epoch: 66 \t Loss: -0.008548363150718311\nTRAIN: \t Epoch: 66 \t Loss: -0.008444690253012456\nTRAIN: \t Epoch: 66 \t Loss: -0.008218044548162393\nTRAIN: \t Epoch: 66 \t Loss: -0.00819182141373555\nTRAIN: \t Epoch: 66 \t Loss: -0.0082472805515863\nTRAIN: \t Epoch: 66 \t Loss: -0.008266549275228472\nVALD: \t Epoch: 66 \t Loss: -0.01088007166981697\nVALD: \t Epoch: 66 \t Loss: -0.010137889068573713\nVALD: \t Epoch: 66 \t Loss: -0.008531945602347454\nVALD: \t Epoch: 66 \t Loss: -0.008603328359341193\n******************************\nEpoch: social-tag : 66\ntrain_loss -0.008266549275228472\nval_loss -0.008603328359341193\n{'min_val_epoch': 63, 'min_val_loss': -0.009142571818566847}\n******************************\nTRAIN: \t Epoch: 67 \t Loss: -0.009279207326471806\nTRAIN: \t Epoch: 67 \t Loss: -0.00948662357404828\nTRAIN: \t Epoch: 67 \t Loss: -0.009694245333472887\nTRAIN: \t Epoch: 67 \t Loss: -0.00956585118547082\nTRAIN: \t Epoch: 67 \t Loss: -0.009304886683821678\nTRAIN: \t Epoch: 67 \t Loss: -0.009064381631712118\nTRAIN: \t Epoch: 67 \t Loss: -0.008857559213148696\nTRAIN: \t Epoch: 67 \t Loss: -0.008766117098275572\nTRAIN: \t Epoch: 67 \t Loss: -0.008783118126706945\nTRAIN: \t Epoch: 67 \t Loss: -0.00881827580742538\nTRAIN: \t Epoch: 67 \t Loss: -0.008852275638756428\nTRAIN: \t Epoch: 67 \t Loss: -0.008767925862533351\nTRAIN: \t Epoch: 67 \t Loss: -0.008585967302608948\nTRAIN: \t Epoch: 67 \t Loss: -0.008454184313969952\nTRAIN: \t Epoch: 67 \t Loss: -0.008445586947103342\nTRAIN: \t Epoch: 67 \t Loss: -0.008487383427564055\nTRAIN: \t Epoch: 67 \t Loss: -0.008518464609303257\nVALD: \t Epoch: 67 \t Loss: -0.011399980634450912\nVALD: \t Epoch: 67 \t Loss: -0.01110343774780631\nVALD: \t Epoch: 67 \t Loss: -0.008930014291157326\nVALD: \t Epoch: 67 \t Loss: -0.008969951413586705\n******************************\nEpoch: social-tag : 67\ntrain_loss -0.008518464609303257\nval_loss -0.008969951413586705\n{'min_val_epoch': 63, 'min_val_loss': -0.009142571818566847}\n******************************\nTRAIN: \t Epoch: 68 \t Loss: -0.009703883901238441\nTRAIN: \t Epoch: 68 \t Loss: -0.009602345991879702\nTRAIN: \t Epoch: 68 \t Loss: -0.009284203872084618\nTRAIN: \t Epoch: 68 \t Loss: -0.009271458489820361\nTRAIN: \t Epoch: 68 \t Loss: -0.009184612706303597\nTRAIN: \t Epoch: 68 \t Loss: -0.008895493112504482\nTRAIN: \t Epoch: 68 \t Loss: -0.008440548554062843\nTRAIN: \t Epoch: 68 \t Loss: -0.008254400338046253\nTRAIN: \t Epoch: 68 \t Loss: -0.008307969715032313\nTRAIN: \t Epoch: 68 \t Loss: -0.008414563536643983\nTRAIN: \t Epoch: 68 \t Loss: -0.008529212918471207\nTRAIN: \t Epoch: 68 \t Loss: -0.008656576042994857\nTRAIN: \t Epoch: 68 \t Loss: -0.008552740757855086\nTRAIN: \t Epoch: 68 \t Loss: -0.008482439509992088\nTRAIN: \t Epoch: 68 \t Loss: -0.008395521435886621\nTRAIN: \t Epoch: 68 \t Loss: -0.008408309338847175\nTRAIN: \t Epoch: 68 \t Loss: -0.008411895455510326\nVALD: \t Epoch: 68 \t Loss: -0.01099396776407957\nVALD: \t Epoch: 68 \t Loss: -0.010577213484793901\nVALD: \t Epoch: 68 \t Loss: -0.00899422417084376\nVALD: \t Epoch: 68 \t Loss: -0.008978592183537588\n******************************\nEpoch: social-tag : 68\ntrain_loss -0.008411895455510326\nval_loss -0.008978592183537588\n{'min_val_epoch': 63, 'min_val_loss': -0.009142571818566847}\n******************************\nTRAIN: \t Epoch: 69 \t Loss: -0.009544354863464832\nTRAIN: \t Epoch: 69 \t Loss: -0.009096820373088121\nTRAIN: \t Epoch: 69 \t Loss: -0.008959093752006689\nTRAIN: \t Epoch: 69 \t Loss: -0.009136792970821261\nTRAIN: \t Epoch: 69 \t Loss: -0.008672366663813592\nTRAIN: \t Epoch: 69 \t Loss: -0.008263542782515287\nTRAIN: \t Epoch: 69 \t Loss: -0.008047451597771474\nTRAIN: \t Epoch: 69 \t Loss: -0.008177988755051047\nTRAIN: \t Epoch: 69 \t Loss: -0.008348585365133153\nTRAIN: \t Epoch: 69 \t Loss: -0.008426530426368118\nTRAIN: \t Epoch: 69 \t Loss: -0.008467264685102484\nTRAIN: \t Epoch: 69 \t Loss: -0.008433538838289678\nTRAIN: \t Epoch: 69 \t Loss: -0.008354101365861984\nTRAIN: \t Epoch: 69 \t Loss: -0.008272345643490553\nTRAIN: \t Epoch: 69 \t Loss: -0.008348711828390757\nTRAIN: \t Epoch: 69 \t Loss: -0.008449307119008154\nTRAIN: \t Epoch: 69 \t Loss: -0.008475003848699007\nVALD: \t Epoch: 69 \t Loss: -0.011316641233861446\nVALD: \t Epoch: 69 \t Loss: -0.011102074291557074\nVALD: \t Epoch: 69 \t Loss: -0.009481214918196201\nVALD: \t Epoch: 69 \t Loss: -0.009441235822117972\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:33<00:00,  9.87it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.3999648031024932  FDE: 0.5835195638872654\n**************************************************\n******************************\nEpoch: social-tag : 69\ntrain_loss -0.008475003848699007\nval_loss -0.009441235822117972\n{'min_val_epoch': 69, 'min_val_loss': -0.009441235822117972}\n******************************\nTRAIN: \t Epoch: 70 \t Loss: -0.009933863766491413\nTRAIN: \t Epoch: 70 \t Loss: -0.009973991196602583\nTRAIN: \t Epoch: 70 \t Loss: -0.010133279177049795\nTRAIN: \t Epoch: 70 \t Loss: -0.009347387938760221\nTRAIN: \t Epoch: 70 \t Loss: -0.008120818715542554\nTRAIN: \t Epoch: 70 \t Loss: -0.0077560915766904754\nTRAIN: \t Epoch: 70 \t Loss: -0.007790513935365847\nTRAIN: \t Epoch: 70 \t Loss: -0.007859621953684837\nTRAIN: \t Epoch: 70 \t Loss: -0.008136417354560561\nTRAIN: \t Epoch: 70 \t Loss: -0.008324653329327702\nTRAIN: \t Epoch: 70 \t Loss: -0.008476328739727085\nTRAIN: \t Epoch: 70 \t Loss: -0.008484959873991707\nTRAIN: \t Epoch: 70 \t Loss: -0.008562397605811175\nTRAIN: \t Epoch: 70 \t Loss: -0.008501579486099737\nTRAIN: \t Epoch: 70 \t Loss: -0.008396748453378677\nTRAIN: \t Epoch: 70 \t Loss: -0.008279159927042201\nTRAIN: \t Epoch: 70 \t Loss: -0.00830486018887975\nVALD: \t Epoch: 70 \t Loss: -0.0108162397518754\nVALD: \t Epoch: 70 \t Loss: -0.01013102987781167\nVALD: \t Epoch: 70 \t Loss: -0.007940806836510697\nVALD: \t Epoch: 70 \t Loss: -0.008052367590620607\n******************************\nEpoch: social-tag : 70\ntrain_loss -0.00830486018887975\nval_loss -0.008052367590620607\n{'min_val_epoch': 69, 'min_val_loss': -0.009441235822117972}\n******************************\nTRAIN: \t Epoch: 71 \t Loss: -0.010013514198362827\nTRAIN: \t Epoch: 71 \t Loss: -0.010088975541293621\nTRAIN: \t Epoch: 71 \t Loss: -0.009969700438280901\nTRAIN: \t Epoch: 71 \t Loss: -0.009606981417164207\nTRAIN: \t Epoch: 71 \t Loss: -0.009307130612432957\nTRAIN: \t Epoch: 71 \t Loss: -0.009226358806093534\nTRAIN: \t Epoch: 71 \t Loss: -0.009076023873473917\nTRAIN: \t Epoch: 71 \t Loss: -0.009021669393405318\nTRAIN: \t Epoch: 71 \t Loss: -0.00905006668633885\nTRAIN: \t Epoch: 71 \t Loss: -0.008990126103162766\nTRAIN: \t Epoch: 71 \t Loss: -0.008909357373010029\nTRAIN: \t Epoch: 71 \t Loss: -0.008904300164431334\nTRAIN: \t Epoch: 71 \t Loss: -0.008869001283668555\nTRAIN: \t Epoch: 71 \t Loss: -0.008802524501723903\nTRAIN: \t Epoch: 71 \t Loss: -0.008790567455192407\nTRAIN: \t Epoch: 71 \t Loss: -0.008811211737338454\nTRAIN: \t Epoch: 71 \t Loss: -0.008777138692411509\nVALD: \t Epoch: 71 \t Loss: -0.01157137006521225\nVALD: \t Epoch: 71 \t Loss: -0.01092904293909669\nVALD: \t Epoch: 71 \t Loss: -0.00848511311536034\nVALD: \t Epoch: 71 \t Loss: -0.00855763955506498\n******************************\nEpoch: social-tag : 71\ntrain_loss -0.008777138692411509\nval_loss -0.00855763955506498\n{'min_val_epoch': 69, 'min_val_loss': -0.009441235822117972}\n******************************\nTRAIN: \t Epoch: 72 \t Loss: -0.009470110759139061\nTRAIN: \t Epoch: 72 \t Loss: -0.009563084691762924\nTRAIN: \t Epoch: 72 \t Loss: -0.009737297582129637\nTRAIN: \t Epoch: 72 \t Loss: -0.0098763273563236\nTRAIN: \t Epoch: 72 \t Loss: -0.009731238894164562\nTRAIN: \t Epoch: 72 \t Loss: -0.009432634804397821\nTRAIN: \t Epoch: 72 \t Loss: -0.009142948314547539\nTRAIN: \t Epoch: 72 \t Loss: -0.009010646142996848\nTRAIN: \t Epoch: 72 \t Loss: -0.00905986906339725\nTRAIN: \t Epoch: 72 \t Loss: -0.009077130630612374\nTRAIN: \t Epoch: 72 \t Loss: -0.009119405157186768\nTRAIN: \t Epoch: 72 \t Loss: -0.008980501249122122\nTRAIN: \t Epoch: 72 \t Loss: -0.008893792839864125\nTRAIN: \t Epoch: 72 \t Loss: -0.008826973016506858\nTRAIN: \t Epoch: 72 \t Loss: -0.008765875454992056\nTRAIN: \t Epoch: 72 \t Loss: -0.008781136915786192\nTRAIN: \t Epoch: 72 \t Loss: -0.00880307729609988\nVALD: \t Epoch: 72 \t Loss: -0.011321124620735645\nVALD: \t Epoch: 72 \t Loss: -0.01082835253328085\nVALD: \t Epoch: 72 \t Loss: -0.008585820440202951\nVALD: \t Epoch: 72 \t Loss: -0.008699262332535552\n******************************\nEpoch: social-tag : 72\ntrain_loss -0.00880307729609988\nval_loss -0.008699262332535552\n{'min_val_epoch': 69, 'min_val_loss': -0.009441235822117972}\n******************************\nTRAIN: \t Epoch: 73 \t Loss: -0.010575419291853905\nTRAIN: \t Epoch: 73 \t Loss: -0.00990275014191866\nTRAIN: \t Epoch: 73 \t Loss: -0.00990010891109705\nTRAIN: \t Epoch: 73 \t Loss: -0.009630553191527724\nTRAIN: \t Epoch: 73 \t Loss: -0.009293385222554207\nTRAIN: \t Epoch: 73 \t Loss: -0.009020524409910044\nTRAIN: \t Epoch: 73 \t Loss: -0.008909674361348152\nTRAIN: \t Epoch: 73 \t Loss: -0.008989196387119591\nTRAIN: \t Epoch: 73 \t Loss: -0.009111897192067571\nTRAIN: \t Epoch: 73 \t Loss: -0.009259866084903479\nTRAIN: \t Epoch: 73 \t Loss: -0.009255620481615717\nTRAIN: \t Epoch: 73 \t Loss: -0.009081914322450757\nTRAIN: \t Epoch: 73 \t Loss: -0.008860242517235188\nTRAIN: \t Epoch: 73 \t Loss: -0.008673478682924594\nTRAIN: \t Epoch: 73 \t Loss: -0.008687283688535294\nTRAIN: \t Epoch: 73 \t Loss: -0.00872463037376292\nTRAIN: \t Epoch: 73 \t Loss: -0.008773053996264935\nVALD: \t Epoch: 73 \t Loss: -0.012446120381355286\nVALD: \t Epoch: 73 \t Loss: -0.011739925015717745\nVALD: \t Epoch: 73 \t Loss: -0.009785254330684742\nVALD: \t Epoch: 73 \t Loss: -0.009664174920308613\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:33<00:00,  9.83it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.3698631067319051  FDE: 0.5102573593465509\n**************************************************\n******************************\nEpoch: social-tag : 73\ntrain_loss -0.008773053996264935\nval_loss -0.009664174920308613\n{'min_val_epoch': 73, 'min_val_loss': -0.009664174920308613}\n******************************\nTRAIN: \t Epoch: 74 \t Loss: -0.009084353223443031\nTRAIN: \t Epoch: 74 \t Loss: -0.009177450556308031\nTRAIN: \t Epoch: 74 \t Loss: -0.009537793385485807\nTRAIN: \t Epoch: 74 \t Loss: -0.009450097102671862\nTRAIN: \t Epoch: 74 \t Loss: -0.00919442791491747\nTRAIN: \t Epoch: 74 \t Loss: -0.008776610173905889\nTRAIN: \t Epoch: 74 \t Loss: -0.008687132237745183\nTRAIN: \t Epoch: 74 \t Loss: -0.008857273205649108\nTRAIN: \t Epoch: 74 \t Loss: -0.008927724686347775\nTRAIN: \t Epoch: 74 \t Loss: -0.008997022220864893\nTRAIN: \t Epoch: 74 \t Loss: -0.008978796149180695\nTRAIN: \t Epoch: 74 \t Loss: -0.008908062203166386\nTRAIN: \t Epoch: 74 \t Loss: -0.008753748646435829\nTRAIN: \t Epoch: 74 \t Loss: -0.00862924914274897\nTRAIN: \t Epoch: 74 \t Loss: -0.00870982992152373\nTRAIN: \t Epoch: 74 \t Loss: -0.008783693076111376\nTRAIN: \t Epoch: 74 \t Loss: -0.008759135747271957\nVALD: \t Epoch: 74 \t Loss: -0.011729649268090725\nVALD: \t Epoch: 74 \t Loss: -0.011473938822746277\nVALD: \t Epoch: 74 \t Loss: -0.009592673896501461\nVALD: \t Epoch: 74 \t Loss: -0.00955605685353993\n******************************\nEpoch: social-tag : 74\ntrain_loss -0.008759135747271957\nval_loss -0.00955605685353993\n{'min_val_epoch': 73, 'min_val_loss': -0.009664174920308613}\n******************************\nTRAIN: \t Epoch: 75 \t Loss: -0.010221509262919426\nTRAIN: \t Epoch: 75 \t Loss: -0.010392054915428162\nTRAIN: \t Epoch: 75 \t Loss: -0.010570265663166841\nTRAIN: \t Epoch: 75 \t Loss: -0.010437821270897985\nTRAIN: \t Epoch: 75 \t Loss: -0.00984226232394576\nTRAIN: \t Epoch: 75 \t Loss: -0.009076576757555207\nTRAIN: \t Epoch: 75 \t Loss: -0.008755034700568234\nTRAIN: \t Epoch: 75 \t Loss: -0.008641460735816509\nTRAIN: \t Epoch: 75 \t Loss: -0.008627049573179748\nTRAIN: \t Epoch: 75 \t Loss: -0.008796229353174567\nTRAIN: \t Epoch: 75 \t Loss: -0.008890669919889082\nTRAIN: \t Epoch: 75 \t Loss: -0.008956607469978431\nTRAIN: \t Epoch: 75 \t Loss: -0.00889481851257957\nTRAIN: \t Epoch: 75 \t Loss: -0.008806162713361638\nTRAIN: \t Epoch: 75 \t Loss: -0.008791881799697875\nTRAIN: \t Epoch: 75 \t Loss: -0.008859372406732291\nTRAIN: \t Epoch: 75 \t Loss: -0.008882021390353188\nVALD: \t Epoch: 75 \t Loss: -0.01153971254825592\nVALD: \t Epoch: 75 \t Loss: -0.010982434265315533\nVALD: \t Epoch: 75 \t Loss: -0.008885056246072054\nVALD: \t Epoch: 75 \t Loss: -0.008950667585917338\n******************************\nEpoch: social-tag : 75\ntrain_loss -0.008882021390353188\nval_loss -0.008950667585917338\n{'min_val_epoch': 73, 'min_val_loss': -0.009664174920308613}\n******************************\nTRAIN: \t Epoch: 76 \t Loss: -0.009575922973453999\nTRAIN: \t Epoch: 76 \t Loss: -0.010219634510576725\nTRAIN: \t Epoch: 76 \t Loss: -0.00956883126248916\nTRAIN: \t Epoch: 76 \t Loss: -0.009215560741722584\nTRAIN: \t Epoch: 76 \t Loss: -0.009039337188005448\nTRAIN: \t Epoch: 76 \t Loss: -0.009164807697137197\nTRAIN: \t Epoch: 76 \t Loss: -0.009209358798606055\nTRAIN: \t Epoch: 76 \t Loss: -0.00914389779791236\nTRAIN: \t Epoch: 76 \t Loss: -0.009038279971314801\nTRAIN: \t Epoch: 76 \t Loss: -0.008816290693357588\nTRAIN: \t Epoch: 76 \t Loss: -0.008747400588948618\nTRAIN: \t Epoch: 76 \t Loss: -0.008810679467084507\nTRAIN: \t Epoch: 76 \t Loss: -0.00891074613453104\nTRAIN: \t Epoch: 76 \t Loss: -0.008923265317987119\nTRAIN: \t Epoch: 76 \t Loss: -0.008879485943665107\nTRAIN: \t Epoch: 76 \t Loss: -0.008834718057187274\nTRAIN: \t Epoch: 76 \t Loss: -0.008833069777624174\nVALD: \t Epoch: 76 \t Loss: -0.011379782110452652\nVALD: \t Epoch: 76 \t Loss: -0.011119619477540255\nVALD: \t Epoch: 76 \t Loss: -0.009519360959529877\nVALD: \t Epoch: 76 \t Loss: -0.009463382338335415\n******************************\nEpoch: social-tag : 76\ntrain_loss -0.008833069777624174\nval_loss -0.009463382338335415\n{'min_val_epoch': 73, 'min_val_loss': -0.009664174920308613}\n******************************\nTRAIN: \t Epoch: 77 \t Loss: -0.010020765475928783\nTRAIN: \t Epoch: 77 \t Loss: -0.010250020306557417\nTRAIN: \t Epoch: 77 \t Loss: -0.01003815078486999\nTRAIN: \t Epoch: 77 \t Loss: -0.009806426940485835\nTRAIN: \t Epoch: 77 \t Loss: -0.009849874675273896\nTRAIN: \t Epoch: 77 \t Loss: -0.009629842514793078\nTRAIN: \t Epoch: 77 \t Loss: -0.009314622730016708\nTRAIN: \t Epoch: 77 \t Loss: -0.009094160457607359\nTRAIN: \t Epoch: 77 \t Loss: -0.009028010742945803\nTRAIN: \t Epoch: 77 \t Loss: -0.009042895259335637\nTRAIN: \t Epoch: 77 \t Loss: -0.009097484321418133\nTRAIN: \t Epoch: 77 \t Loss: -0.009155524312518537\nTRAIN: \t Epoch: 77 \t Loss: -0.009199793557994641\nTRAIN: \t Epoch: 77 \t Loss: -0.009105098932715399\nTRAIN: \t Epoch: 77 \t Loss: -0.009013498326142629\nTRAIN: \t Epoch: 77 \t Loss: -0.008913154277252033\nTRAIN: \t Epoch: 77 \t Loss: -0.008922737402220568\nVALD: \t Epoch: 77 \t Loss: -0.011208363808691502\nVALD: \t Epoch: 77 \t Loss: -0.010668964590877295\nVALD: \t Epoch: 77 \t Loss: -0.008697155863046646\nVALD: \t Epoch: 77 \t Loss: -0.008781948727286028\n******************************\nEpoch: social-tag : 77\ntrain_loss -0.008922737402220568\nval_loss -0.008781948727286028\n{'min_val_epoch': 73, 'min_val_loss': -0.009664174920308613}\n******************************\nTRAIN: \t Epoch: 78 \t Loss: -0.009872904047369957\nTRAIN: \t Epoch: 78 \t Loss: -0.009651631116867065\nTRAIN: \t Epoch: 78 \t Loss: -0.009696765492359797\nTRAIN: \t Epoch: 78 \t Loss: -0.009697217727079988\nTRAIN: \t Epoch: 78 \t Loss: -0.009696995466947555\nTRAIN: \t Epoch: 78 \t Loss: -0.00956707370157043\nTRAIN: \t Epoch: 78 \t Loss: -0.009411807038954325\nTRAIN: \t Epoch: 78 \t Loss: -0.00926577195059508\nTRAIN: \t Epoch: 78 \t Loss: -0.009265063227050833\nTRAIN: \t Epoch: 78 \t Loss: -0.009266562759876251\nTRAIN: \t Epoch: 78 \t Loss: -0.009299901737408205\nTRAIN: \t Epoch: 78 \t Loss: -0.009213068910563985\nTRAIN: \t Epoch: 78 \t Loss: -0.00907561111335571\nTRAIN: \t Epoch: 78 \t Loss: -0.009029788031641926\nTRAIN: \t Epoch: 78 \t Loss: -0.009052464428047339\nTRAIN: \t Epoch: 78 \t Loss: -0.009095263201743364\nTRAIN: \t Epoch: 78 \t Loss: -0.009101568382572044\nVALD: \t Epoch: 78 \t Loss: -0.01131236832588911\nVALD: \t Epoch: 78 \t Loss: -0.011304749641567469\nVALD: \t Epoch: 78 \t Loss: -0.009187377678851286\nVALD: \t Epoch: 78 \t Loss: -0.009208707038513914\n******************************\nEpoch: social-tag : 78\ntrain_loss -0.009101568382572044\nval_loss -0.009208707038513914\n{'min_val_epoch': 73, 'min_val_loss': -0.009664174920308613}\n******************************\nTRAIN: \t Epoch: 79 \t Loss: -0.009572336450219154\nTRAIN: \t Epoch: 79 \t Loss: -0.010165447369217873\nTRAIN: \t Epoch: 79 \t Loss: -0.010440004368623098\nTRAIN: \t Epoch: 79 \t Loss: -0.010292387334629893\nTRAIN: \t Epoch: 79 \t Loss: -0.010352925769984722\nTRAIN: \t Epoch: 79 \t Loss: -0.010057507858922085\nTRAIN: \t Epoch: 79 \t Loss: -0.009851275544081415\nTRAIN: \t Epoch: 79 \t Loss: -0.009567041939590126\nTRAIN: \t Epoch: 79 \t Loss: -0.009292557421657775\nTRAIN: \t Epoch: 79 \t Loss: -0.009204194694757462\nTRAIN: \t Epoch: 79 \t Loss: -0.009236973625692453\nTRAIN: \t Epoch: 79 \t Loss: -0.009238239843398333\nTRAIN: \t Epoch: 79 \t Loss: -0.009283018871568717\nTRAIN: \t Epoch: 79 \t Loss: -0.009338434452989272\nTRAIN: \t Epoch: 79 \t Loss: -0.009326724894344806\nTRAIN: \t Epoch: 79 \t Loss: -0.009367075108457357\nTRAIN: \t Epoch: 79 \t Loss: -0.00935295300388878\nVALD: \t Epoch: 79 \t Loss: -0.012147752568125725\nVALD: \t Epoch: 79 \t Loss: -0.011826756410300732\nVALD: \t Epoch: 79 \t Loss: -0.010514486891527971\nVALD: \t Epoch: 79 \t Loss: -0.010355321471086757\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:33<00:00,  9.80it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.36337614447612354  FDE: 0.5154776409109715\n**************************************************\n******************************\nEpoch: social-tag : 79\ntrain_loss -0.00935295300388878\nval_loss -0.010355321471086757\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 80 \t Loss: -0.010410909540951252\nTRAIN: \t Epoch: 80 \t Loss: -0.010350246913731098\nTRAIN: \t Epoch: 80 \t Loss: -0.010291887757678827\nTRAIN: \t Epoch: 80 \t Loss: -0.010044487426057458\nTRAIN: \t Epoch: 80 \t Loss: -0.0098157849162817\nTRAIN: \t Epoch: 80 \t Loss: -0.00947556815420588\nTRAIN: \t Epoch: 80 \t Loss: -0.009325389484209674\nTRAIN: \t Epoch: 80 \t Loss: -0.009308096370659769\nTRAIN: \t Epoch: 80 \t Loss: -0.009245725245111518\nTRAIN: \t Epoch: 80 \t Loss: -0.009180235303938388\nTRAIN: \t Epoch: 80 \t Loss: -0.009135614741932262\nTRAIN: \t Epoch: 80 \t Loss: -0.009057336331655582\nTRAIN: \t Epoch: 80 \t Loss: -0.009095493847360978\nTRAIN: \t Epoch: 80 \t Loss: -0.009068604558706284\nTRAIN: \t Epoch: 80 \t Loss: -0.009096324195464451\nTRAIN: \t Epoch: 80 \t Loss: -0.009126524324528873\nTRAIN: \t Epoch: 80 \t Loss: -0.009156322332494186\nVALD: \t Epoch: 80 \t Loss: -0.0118303457275033\nVALD: \t Epoch: 80 \t Loss: -0.011454800143837929\nVALD: \t Epoch: 80 \t Loss: -0.009908808084825674\nVALD: \t Epoch: 80 \t Loss: -0.009830807735344132\n******************************\nEpoch: social-tag : 80\ntrain_loss -0.009156322332494186\nval_loss -0.009830807735344132\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 81 \t Loss: -0.01102475170046091\nTRAIN: \t Epoch: 81 \t Loss: -0.010711182840168476\nTRAIN: \t Epoch: 81 \t Loss: -0.010307202115654945\nTRAIN: \t Epoch: 81 \t Loss: -0.009831659030169249\nTRAIN: \t Epoch: 81 \t Loss: -0.009513817541301251\nTRAIN: \t Epoch: 81 \t Loss: -0.00930355271945397\nTRAIN: \t Epoch: 81 \t Loss: -0.009366804334734167\nTRAIN: \t Epoch: 81 \t Loss: -0.009376231231726706\nTRAIN: \t Epoch: 81 \t Loss: -0.009315136095715893\nTRAIN: \t Epoch: 81 \t Loss: -0.00930049056187272\nTRAIN: \t Epoch: 81 \t Loss: -0.009293817830356684\nTRAIN: \t Epoch: 81 \t Loss: -0.009132939235617718\nTRAIN: \t Epoch: 81 \t Loss: -0.009026528551028324\nTRAIN: \t Epoch: 81 \t Loss: -0.009060183140848364\nTRAIN: \t Epoch: 81 \t Loss: -0.009162035460273424\nTRAIN: \t Epoch: 81 \t Loss: -0.009135582717135549\nTRAIN: \t Epoch: 81 \t Loss: -0.009149673620634007\nVALD: \t Epoch: 81 \t Loss: -0.011376903392374516\nVALD: \t Epoch: 81 \t Loss: -0.010807363782078028\nVALD: \t Epoch: 81 \t Loss: -0.008660041727125645\nVALD: \t Epoch: 81 \t Loss: -0.008754335715623196\n******************************\nEpoch: social-tag : 81\ntrain_loss -0.009149673620634007\nval_loss -0.008754335715623196\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 82 \t Loss: -0.009589777328073978\nTRAIN: \t Epoch: 82 \t Loss: -0.00967321265488863\nTRAIN: \t Epoch: 82 \t Loss: -0.009938225460549196\nTRAIN: \t Epoch: 82 \t Loss: -0.009717954322695732\nTRAIN: \t Epoch: 82 \t Loss: -0.009145461302250624\nTRAIN: \t Epoch: 82 \t Loss: -0.009144142347698411\nTRAIN: \t Epoch: 82 \t Loss: -0.009208793951464551\nTRAIN: \t Epoch: 82 \t Loss: -0.00922059646109119\nTRAIN: \t Epoch: 82 \t Loss: -0.009320432992859019\nTRAIN: \t Epoch: 82 \t Loss: -0.009485651599243283\nTRAIN: \t Epoch: 82 \t Loss: -0.00953404829752716\nTRAIN: \t Epoch: 82 \t Loss: -0.009541902807541192\nTRAIN: \t Epoch: 82 \t Loss: -0.00947387434112338\nTRAIN: \t Epoch: 82 \t Loss: -0.009486421111172863\nTRAIN: \t Epoch: 82 \t Loss: -0.009355876035988331\nTRAIN: \t Epoch: 82 \t Loss: -0.009327118576038629\nTRAIN: \t Epoch: 82 \t Loss: -0.009327626730682272\nVALD: \t Epoch: 82 \t Loss: -0.011471381410956383\nVALD: \t Epoch: 82 \t Loss: -0.010670827236026525\nVALD: \t Epoch: 82 \t Loss: -0.00892302424957355\nVALD: \t Epoch: 82 \t Loss: -0.008986643926350181\n******************************\nEpoch: social-tag : 82\ntrain_loss -0.009327626730682272\nval_loss -0.008986643926350181\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 83 \t Loss: -0.010853033512830734\nTRAIN: \t Epoch: 83 \t Loss: -0.010526713915169239\nTRAIN: \t Epoch: 83 \t Loss: -0.010395371044675509\nTRAIN: \t Epoch: 83 \t Loss: -0.010044629452750087\nTRAIN: \t Epoch: 83 \t Loss: -0.009710575453937054\nTRAIN: \t Epoch: 83 \t Loss: -0.009404119104146957\nTRAIN: \t Epoch: 83 \t Loss: -0.009103788062930107\nTRAIN: \t Epoch: 83 \t Loss: -0.0091064830776304\nTRAIN: \t Epoch: 83 \t Loss: -0.009243622318738036\nTRAIN: \t Epoch: 83 \t Loss: -0.009299932140856981\nTRAIN: \t Epoch: 83 \t Loss: -0.009374649433249777\nTRAIN: \t Epoch: 83 \t Loss: -0.009304982066775361\nTRAIN: \t Epoch: 83 \t Loss: -0.009224747021037798\nTRAIN: \t Epoch: 83 \t Loss: -0.009020530984603934\nTRAIN: \t Epoch: 83 \t Loss: -0.009009510868539412\nTRAIN: \t Epoch: 83 \t Loss: -0.009030550223542377\nTRAIN: \t Epoch: 83 \t Loss: -0.009073728215739582\nVALD: \t Epoch: 83 \t Loss: -0.012759784236550331\nVALD: \t Epoch: 83 \t Loss: -0.012184417340904474\nVALD: \t Epoch: 83 \t Loss: -0.01037278197084864\nVALD: \t Epoch: 83 \t Loss: -0.010217846391681664\n******************************\nEpoch: social-tag : 83\ntrain_loss -0.009073728215739582\nval_loss -0.010217846391681664\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 84 \t Loss: -0.009520485065877438\nTRAIN: \t Epoch: 84 \t Loss: -0.009569941088557243\nTRAIN: \t Epoch: 84 \t Loss: -0.009730975143611431\nTRAIN: \t Epoch: 84 \t Loss: -0.009734205901622772\nTRAIN: \t Epoch: 84 \t Loss: -0.009896256402134896\nTRAIN: \t Epoch: 84 \t Loss: -0.010097270831465721\nTRAIN: \t Epoch: 84 \t Loss: -0.009942445105740003\nTRAIN: \t Epoch: 84 \t Loss: -0.009630322107113898\nTRAIN: \t Epoch: 84 \t Loss: -0.009438629158669047\nTRAIN: \t Epoch: 84 \t Loss: -0.009419030509889126\nTRAIN: \t Epoch: 84 \t Loss: -0.00943965862759135\nTRAIN: \t Epoch: 84 \t Loss: -0.0094040147960186\nTRAIN: \t Epoch: 84 \t Loss: -0.009471534536435055\nTRAIN: \t Epoch: 84 \t Loss: -0.009537435003689356\nTRAIN: \t Epoch: 84 \t Loss: -0.009416477320094903\nTRAIN: \t Epoch: 84 \t Loss: -0.009187818708596751\nTRAIN: \t Epoch: 84 \t Loss: -0.009141430682079359\nVALD: \t Epoch: 84 \t Loss: -0.009370083920657635\nVALD: \t Epoch: 84 \t Loss: -0.009429597295820713\nVALD: \t Epoch: 84 \t Loss: -0.008328422127912441\nVALD: \t Epoch: 84 \t Loss: -0.008403036527766914\n******************************\nEpoch: social-tag : 84\ntrain_loss -0.009141430682079359\nval_loss -0.008403036527766914\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 85 \t Loss: -0.00849590077996254\nTRAIN: \t Epoch: 85 \t Loss: -0.009278393816202879\nTRAIN: \t Epoch: 85 \t Loss: -0.009477056562900543\nTRAIN: \t Epoch: 85 \t Loss: -0.009891369845718145\nTRAIN: \t Epoch: 85 \t Loss: -0.009835985489189625\nTRAIN: \t Epoch: 85 \t Loss: -0.009551805599282185\nTRAIN: \t Epoch: 85 \t Loss: -0.009352490305900574\nTRAIN: \t Epoch: 85 \t Loss: -0.009181969217024744\nTRAIN: \t Epoch: 85 \t Loss: -0.009253207801116837\nTRAIN: \t Epoch: 85 \t Loss: -0.009364472422748803\nTRAIN: \t Epoch: 85 \t Loss: -0.009358833797953346\nTRAIN: \t Epoch: 85 \t Loss: -0.009233295839900771\nTRAIN: \t Epoch: 85 \t Loss: -0.009087387866412219\nTRAIN: \t Epoch: 85 \t Loss: -0.009022570780611463\nTRAIN: \t Epoch: 85 \t Loss: -0.009095768599460522\nTRAIN: \t Epoch: 85 \t Loss: -0.009150459402007982\nTRAIN: \t Epoch: 85 \t Loss: -0.009182256053794514\nVALD: \t Epoch: 85 \t Loss: -0.011734663508832455\nVALD: \t Epoch: 85 \t Loss: -0.010390227194875479\nVALD: \t Epoch: 85 \t Loss: -0.007604498493795593\nVALD: \t Epoch: 85 \t Loss: -0.007847536645249693\n******************************\nEpoch: social-tag : 85\ntrain_loss -0.009182256053794514\nval_loss -0.007847536645249693\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 86 \t Loss: -0.010668107308447361\nTRAIN: \t Epoch: 86 \t Loss: -0.01026346767321229\nTRAIN: \t Epoch: 86 \t Loss: -0.010080967408915361\nTRAIN: \t Epoch: 86 \t Loss: -0.010118880774825811\nTRAIN: \t Epoch: 86 \t Loss: -0.010001804679632187\nTRAIN: \t Epoch: 86 \t Loss: -0.00939194291519622\nTRAIN: \t Epoch: 86 \t Loss: -0.008973569010517426\nTRAIN: \t Epoch: 86 \t Loss: -0.008975346630904824\nTRAIN: \t Epoch: 86 \t Loss: -0.009028369198656745\nTRAIN: \t Epoch: 86 \t Loss: -0.009110973076894879\nTRAIN: \t Epoch: 86 \t Loss: -0.009237245783548464\nTRAIN: \t Epoch: 86 \t Loss: -0.009280393249355257\nTRAIN: \t Epoch: 86 \t Loss: -0.009343311011504669\nTRAIN: \t Epoch: 86 \t Loss: -0.0093266094835209\nTRAIN: \t Epoch: 86 \t Loss: -0.00920458221808076\nTRAIN: \t Epoch: 86 \t Loss: -0.009106685465667397\nTRAIN: \t Epoch: 86 \t Loss: -0.009106674598473492\nVALD: \t Epoch: 86 \t Loss: -0.011537686921656132\nVALD: \t Epoch: 86 \t Loss: -0.010966852772980928\nVALD: \t Epoch: 86 \t Loss: -0.009402619364360968\nVALD: \t Epoch: 86 \t Loss: -0.009328658947211778\n******************************\nEpoch: social-tag : 86\ntrain_loss -0.009106674598473492\nval_loss -0.009328658947211778\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 87 \t Loss: -0.009237553924322128\nTRAIN: \t Epoch: 87 \t Loss: -0.00973937101662159\nTRAIN: \t Epoch: 87 \t Loss: -0.010120848814646402\nTRAIN: \t Epoch: 87 \t Loss: -0.01006732857786119\nTRAIN: \t Epoch: 87 \t Loss: -0.010070516914129257\nTRAIN: \t Epoch: 87 \t Loss: -0.009837926675875982\nTRAIN: \t Epoch: 87 \t Loss: -0.009444314454283034\nTRAIN: \t Epoch: 87 \t Loss: -0.009369139093905687\nTRAIN: \t Epoch: 87 \t Loss: -0.009349157619807456\nTRAIN: \t Epoch: 87 \t Loss: -0.009402532503008843\nTRAIN: \t Epoch: 87 \t Loss: -0.009420320306989279\nTRAIN: \t Epoch: 87 \t Loss: -0.009377170276517669\nTRAIN: \t Epoch: 87 \t Loss: -0.009336753103595514\nTRAIN: \t Epoch: 87 \t Loss: -0.009369409975728818\nTRAIN: \t Epoch: 87 \t Loss: -0.009349350879589716\nTRAIN: \t Epoch: 87 \t Loss: -0.00933077494846657\nTRAIN: \t Epoch: 87 \t Loss: -0.009340329829490545\nVALD: \t Epoch: 87 \t Loss: -0.012355430983006954\nVALD: \t Epoch: 87 \t Loss: -0.01178231555968523\nVALD: \t Epoch: 87 \t Loss: -0.009737719626476368\nVALD: \t Epoch: 87 \t Loss: -0.009698283291624454\n******************************\nEpoch: social-tag : 87\ntrain_loss -0.009340329829490545\nval_loss -0.009698283291624454\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 88 \t Loss: -0.010508444160223007\nTRAIN: \t Epoch: 88 \t Loss: -0.010550785344094038\nTRAIN: \t Epoch: 88 \t Loss: -0.010484520656367144\nTRAIN: \t Epoch: 88 \t Loss: -0.010114653734490275\nTRAIN: \t Epoch: 88 \t Loss: -0.009413826372474432\nTRAIN: \t Epoch: 88 \t Loss: -0.009222058656935891\nTRAIN: \t Epoch: 88 \t Loss: -0.009128318500838109\nTRAIN: \t Epoch: 88 \t Loss: -0.00920905329985544\nTRAIN: \t Epoch: 88 \t Loss: -0.009334972904374203\nTRAIN: \t Epoch: 88 \t Loss: -0.00941860261373222\nTRAIN: \t Epoch: 88 \t Loss: -0.009421273092315956\nTRAIN: \t Epoch: 88 \t Loss: -0.00930096406955272\nTRAIN: \t Epoch: 88 \t Loss: -0.009178934869571375\nTRAIN: \t Epoch: 88 \t Loss: -0.009222706547006965\nTRAIN: \t Epoch: 88 \t Loss: -0.009249752977242072\nTRAIN: \t Epoch: 88 \t Loss: -0.009293114038882777\nTRAIN: \t Epoch: 88 \t Loss: -0.009312531233511188\nVALD: \t Epoch: 88 \t Loss: -0.012093554250895977\nVALD: \t Epoch: 88 \t Loss: -0.010860914830118418\nVALD: \t Epoch: 88 \t Loss: -0.009194324258714914\nVALD: \t Epoch: 88 \t Loss: -0.009161194046576343\n******************************\nEpoch: social-tag : 88\ntrain_loss -0.009312531233511188\nval_loss -0.009161194046576343\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 89 \t Loss: -0.010136698372662067\nTRAIN: \t Epoch: 89 \t Loss: -0.010444499552249908\nTRAIN: \t Epoch: 89 \t Loss: -0.009851781651377678\nTRAIN: \t Epoch: 89 \t Loss: -0.009036244475282729\nTRAIN: \t Epoch: 89 \t Loss: -0.008960052859038115\nTRAIN: \t Epoch: 89 \t Loss: -0.009148308774456382\nTRAIN: \t Epoch: 89 \t Loss: -0.009176638309976884\nTRAIN: \t Epoch: 89 \t Loss: -0.009329864697065204\nTRAIN: \t Epoch: 89 \t Loss: -0.00945110913986961\nTRAIN: \t Epoch: 89 \t Loss: -0.009508641203865409\nTRAIN: \t Epoch: 89 \t Loss: -0.00955241567201235\nTRAIN: \t Epoch: 89 \t Loss: -0.00951961261065056\nTRAIN: \t Epoch: 89 \t Loss: -0.009406781647927485\nTRAIN: \t Epoch: 89 \t Loss: -0.009229034950424517\nTRAIN: \t Epoch: 89 \t Loss: -0.009220908985783656\nTRAIN: \t Epoch: 89 \t Loss: -0.009272329712985083\nTRAIN: \t Epoch: 89 \t Loss: -0.009304909007341572\nVALD: \t Epoch: 89 \t Loss: -0.011180693283677101\nVALD: \t Epoch: 89 \t Loss: -0.010056388098746538\nVALD: \t Epoch: 89 \t Loss: -0.00819047773256898\nVALD: \t Epoch: 89 \t Loss: -0.008269323917206176\n******************************\nEpoch: social-tag : 89\ntrain_loss -0.009304909007341572\nval_loss -0.008269323917206176\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 90 \t Loss: -0.009603805840015411\nTRAIN: \t Epoch: 90 \t Loss: -0.009833529591560364\nTRAIN: \t Epoch: 90 \t Loss: -0.009597071446478367\nTRAIN: \t Epoch: 90 \t Loss: -0.00969498185440898\nTRAIN: \t Epoch: 90 \t Loss: -0.00984232798218727\nTRAIN: \t Epoch: 90 \t Loss: -0.009989280408869186\nTRAIN: \t Epoch: 90 \t Loss: -0.009839282903288091\nTRAIN: \t Epoch: 90 \t Loss: -0.009941947646439075\nTRAIN: \t Epoch: 90 \t Loss: -0.00996051294108232\nTRAIN: \t Epoch: 90 \t Loss: -0.00971268923021853\nTRAIN: \t Epoch: 90 \t Loss: -0.00936101194979115\nTRAIN: \t Epoch: 90 \t Loss: -0.009339707787148654\nTRAIN: \t Epoch: 90 \t Loss: -0.009415435139089823\nTRAIN: \t Epoch: 90 \t Loss: -0.009468951162749104\nTRAIN: \t Epoch: 90 \t Loss: -0.009547856108595928\nTRAIN: \t Epoch: 90 \t Loss: -0.009661832445999607\nTRAIN: \t Epoch: 90 \t Loss: -0.009640713984316046\nVALD: \t Epoch: 90 \t Loss: -0.012044907547533512\nVALD: \t Epoch: 90 \t Loss: -0.010687864851206541\nVALD: \t Epoch: 90 \t Loss: -0.008018764511992535\nVALD: \t Epoch: 90 \t Loss: -0.008213888980195433\n******************************\nEpoch: social-tag : 90\ntrain_loss -0.009640713984316046\nval_loss -0.008213888980195433\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 91 \t Loss: -0.010919886641204357\nTRAIN: \t Epoch: 91 \t Loss: -0.010510636493563652\nTRAIN: \t Epoch: 91 \t Loss: -0.010178672770659128\nTRAIN: \t Epoch: 91 \t Loss: -0.009262408479116857\nTRAIN: \t Epoch: 91 \t Loss: -0.008843134716153144\nTRAIN: \t Epoch: 91 \t Loss: -0.008858582625786463\nTRAIN: \t Epoch: 91 \t Loss: -0.008981176119829928\nTRAIN: \t Epoch: 91 \t Loss: -0.009098205482587218\nTRAIN: \t Epoch: 91 \t Loss: -0.009272438577479787\nTRAIN: \t Epoch: 91 \t Loss: -0.009373061638325453\nTRAIN: \t Epoch: 91 \t Loss: -0.009416984224861319\nTRAIN: \t Epoch: 91 \t Loss: -0.009556421156351766\nTRAIN: \t Epoch: 91 \t Loss: -0.00949612632393837\nTRAIN: \t Epoch: 91 \t Loss: -0.009242473635822535\nTRAIN: \t Epoch: 91 \t Loss: -0.009099783127506573\nTRAIN: \t Epoch: 91 \t Loss: -0.00909719173796475\nTRAIN: \t Epoch: 91 \t Loss: -0.009149393631201801\nVALD: \t Epoch: 91 \t Loss: -0.012031683698296547\nVALD: \t Epoch: 91 \t Loss: -0.011224071495234966\nVALD: \t Epoch: 91 \t Loss: -0.00907416424403588\nVALD: \t Epoch: 91 \t Loss: -0.009204831665861392\n******************************\nEpoch: social-tag : 91\ntrain_loss -0.009149393631201801\nval_loss -0.009204831665861392\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 92 \t Loss: -0.010394894517958164\nTRAIN: \t Epoch: 92 \t Loss: -0.010616360697895288\nTRAIN: \t Epoch: 92 \t Loss: -0.010277468711137772\nTRAIN: \t Epoch: 92 \t Loss: -0.01008914178237319\nTRAIN: \t Epoch: 92 \t Loss: -0.010268716886639596\nTRAIN: \t Epoch: 92 \t Loss: -0.010107895359396935\nTRAIN: \t Epoch: 92 \t Loss: -0.00955029424013836\nTRAIN: \t Epoch: 92 \t Loss: -0.009216215461492538\nTRAIN: \t Epoch: 92 \t Loss: -0.009277440917988619\nTRAIN: \t Epoch: 92 \t Loss: -0.009293400496244431\nTRAIN: \t Epoch: 92 \t Loss: -0.00939685885201801\nTRAIN: \t Epoch: 92 \t Loss: -0.009527849421525994\nTRAIN: \t Epoch: 92 \t Loss: -0.009594737122265192\nTRAIN: \t Epoch: 92 \t Loss: -0.009603692218661308\nTRAIN: \t Epoch: 92 \t Loss: -0.009490817288557689\nTRAIN: \t Epoch: 92 \t Loss: -0.00942575791850686\nTRAIN: \t Epoch: 92 \t Loss: -0.009385391477156769\nVALD: \t Epoch: 92 \t Loss: -0.011683768592774868\nVALD: \t Epoch: 92 \t Loss: -0.011440013535320759\nVALD: \t Epoch: 92 \t Loss: -0.010073466381678978\nVALD: \t Epoch: 92 \t Loss: -0.00994393556655762\n******************************\nEpoch: social-tag : 92\ntrain_loss -0.009385391477156769\nval_loss -0.00994393556655762\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 93 \t Loss: -0.01029345765709877\nTRAIN: \t Epoch: 93 \t Loss: -0.010591112077236176\nTRAIN: \t Epoch: 93 \t Loss: -0.010425867202381292\nTRAIN: \t Epoch: 93 \t Loss: -0.010283615672960877\nTRAIN: \t Epoch: 93 \t Loss: -0.009938117489218712\nTRAIN: \t Epoch: 93 \t Loss: -0.009662469383329153\nTRAIN: \t Epoch: 93 \t Loss: -0.009555085961307799\nTRAIN: \t Epoch: 93 \t Loss: -0.009579330682754517\nTRAIN: \t Epoch: 93 \t Loss: -0.009687754325568676\nTRAIN: \t Epoch: 93 \t Loss: -0.009711639303714038\nTRAIN: \t Epoch: 93 \t Loss: -0.009622473354366693\nTRAIN: \t Epoch: 93 \t Loss: -0.009438854719822606\nTRAIN: \t Epoch: 93 \t Loss: -0.009321878377634745\nTRAIN: \t Epoch: 93 \t Loss: -0.009365763648280076\nTRAIN: \t Epoch: 93 \t Loss: -0.009484823917349179\nTRAIN: \t Epoch: 93 \t Loss: -0.009533138771075755\nTRAIN: \t Epoch: 93 \t Loss: -0.009576234176303402\nVALD: \t Epoch: 93 \t Loss: -0.012753487564623356\nVALD: \t Epoch: 93 \t Loss: -0.011575612239539623\nVALD: \t Epoch: 93 \t Loss: -0.008828982400397459\nVALD: \t Epoch: 93 \t Loss: -0.008927059506703756\n******************************\nEpoch: social-tag : 93\ntrain_loss -0.009576234176303402\nval_loss -0.008927059506703756\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 94 \t Loss: -0.011568276211619377\nTRAIN: \t Epoch: 94 \t Loss: -0.011152459774166346\nTRAIN: \t Epoch: 94 \t Loss: -0.010472735079626242\nTRAIN: \t Epoch: 94 \t Loss: -0.010118343401700258\nTRAIN: \t Epoch: 94 \t Loss: -0.009827526472508907\nTRAIN: \t Epoch: 94 \t Loss: -0.009580306087930998\nTRAIN: \t Epoch: 94 \t Loss: -0.009534266644290515\nTRAIN: \t Epoch: 94 \t Loss: -0.00956558738835156\nTRAIN: \t Epoch: 94 \t Loss: -0.009606356422106424\nTRAIN: \t Epoch: 94 \t Loss: -0.009514189045876264\nTRAIN: \t Epoch: 94 \t Loss: -0.00954201470383189\nTRAIN: \t Epoch: 94 \t Loss: -0.009462545004983744\nTRAIN: \t Epoch: 94 \t Loss: -0.009437489609878797\nTRAIN: \t Epoch: 94 \t Loss: -0.009493305547428983\nTRAIN: \t Epoch: 94 \t Loss: -0.009519681396583716\nTRAIN: \t Epoch: 94 \t Loss: -0.009557845303788781\nTRAIN: \t Epoch: 94 \t Loss: -0.009545093035381851\nVALD: \t Epoch: 94 \t Loss: -0.01243661530315876\nVALD: \t Epoch: 94 \t Loss: -0.011614476796239614\nVALD: \t Epoch: 94 \t Loss: -0.00994382860759894\nVALD: \t Epoch: 94 \t Loss: -0.009912487989414237\n******************************\nEpoch: social-tag : 94\ntrain_loss -0.009545093035381851\nval_loss -0.009912487989414237\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 95 \t Loss: -0.009764597751200199\nTRAIN: \t Epoch: 95 \t Loss: -0.009442899841815233\nTRAIN: \t Epoch: 95 \t Loss: -0.009716195985674858\nTRAIN: \t Epoch: 95 \t Loss: -0.009779836283996701\nTRAIN: \t Epoch: 95 \t Loss: -0.009858650341629981\nTRAIN: \t Epoch: 95 \t Loss: -0.009921427040050427\nTRAIN: \t Epoch: 95 \t Loss: -0.009833793421941144\nTRAIN: \t Epoch: 95 \t Loss: -0.009783224551938474\nTRAIN: \t Epoch: 95 \t Loss: -0.009742382706867324\nTRAIN: \t Epoch: 95 \t Loss: -0.009749726485460996\nTRAIN: \t Epoch: 95 \t Loss: -0.009712676517665386\nTRAIN: \t Epoch: 95 \t Loss: -0.009752901659036676\nTRAIN: \t Epoch: 95 \t Loss: -0.009706204112332601\nTRAIN: \t Epoch: 95 \t Loss: -0.009584184935582536\nTRAIN: \t Epoch: 95 \t Loss: -0.009616268922885259\nTRAIN: \t Epoch: 95 \t Loss: -0.009608011110685766\nTRAIN: \t Epoch: 95 \t Loss: -0.009635157685613993\nVALD: \t Epoch: 95 \t Loss: -0.012678406201303005\nVALD: \t Epoch: 95 \t Loss: -0.011778146494179964\nVALD: \t Epoch: 95 \t Loss: -0.010073167582352957\nVALD: \t Epoch: 95 \t Loss: -0.009977475849692217\n******************************\nEpoch: social-tag : 95\ntrain_loss -0.009635157685613993\nval_loss -0.009977475849692217\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 96 \t Loss: -0.010344987735152245\nTRAIN: \t Epoch: 96 \t Loss: -0.01037546619772911\nTRAIN: \t Epoch: 96 \t Loss: -0.010669494358201822\nTRAIN: \t Epoch: 96 \t Loss: -0.010046556359156966\nTRAIN: \t Epoch: 96 \t Loss: -0.008856778405606747\nTRAIN: \t Epoch: 96 \t Loss: -0.008625610498711467\nTRAIN: \t Epoch: 96 \t Loss: -0.008726826935474361\nTRAIN: \t Epoch: 96 \t Loss: -0.008970754744950682\nTRAIN: \t Epoch: 96 \t Loss: -0.00908056620715393\nTRAIN: \t Epoch: 96 \t Loss: -0.009234247216954827\nTRAIN: \t Epoch: 96 \t Loss: -0.009140452412380413\nTRAIN: \t Epoch: 96 \t Loss: -0.009212252257081369\nTRAIN: \t Epoch: 96 \t Loss: -0.009327320202898521\nTRAIN: \t Epoch: 96 \t Loss: -0.0092817214650235\nTRAIN: \t Epoch: 96 \t Loss: -0.00920072194809715\nTRAIN: \t Epoch: 96 \t Loss: -0.009179050015518442\nTRAIN: \t Epoch: 96 \t Loss: -0.009218320363398754\nVALD: \t Epoch: 96 \t Loss: -0.011576606892049313\nVALD: \t Epoch: 96 \t Loss: -0.011368332896381617\nVALD: \t Epoch: 96 \t Loss: -0.009354222100228071\nVALD: \t Epoch: 96 \t Loss: -0.009458020418703913\n******************************\nEpoch: social-tag : 96\ntrain_loss -0.009218320363398754\nval_loss -0.009458020418703913\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 97 \t Loss: -0.01021001860499382\nTRAIN: \t Epoch: 97 \t Loss: -0.010733846109360456\nTRAIN: \t Epoch: 97 \t Loss: -0.010559428793688616\nTRAIN: \t Epoch: 97 \t Loss: -0.010277022840455174\nTRAIN: \t Epoch: 97 \t Loss: -0.009833934716880322\nTRAIN: \t Epoch: 97 \t Loss: -0.009308011814331016\nTRAIN: \t Epoch: 97 \t Loss: -0.009174210485070944\nTRAIN: \t Epoch: 97 \t Loss: -0.009296435804571956\nTRAIN: \t Epoch: 97 \t Loss: -0.009388347828967704\nTRAIN: \t Epoch: 97 \t Loss: -0.00954467081464827\nTRAIN: \t Epoch: 97 \t Loss: -0.009703714104199951\nTRAIN: \t Epoch: 97 \t Loss: -0.009837896058646342\nTRAIN: \t Epoch: 97 \t Loss: -0.009908118535979437\nTRAIN: \t Epoch: 97 \t Loss: -0.009914727382627981\nTRAIN: \t Epoch: 97 \t Loss: -0.009773485828191042\nTRAIN: \t Epoch: 97 \t Loss: -0.009701300092274323\nTRAIN: \t Epoch: 97 \t Loss: -0.009678199746166214\nVALD: \t Epoch: 97 \t Loss: -0.012058819644153118\nVALD: \t Epoch: 97 \t Loss: -0.011657870840281248\nVALD: \t Epoch: 97 \t Loss: -0.010151813582827648\nVALD: \t Epoch: 97 \t Loss: -0.010046453414087048\n******************************\nEpoch: social-tag : 97\ntrain_loss -0.009678199746166214\nval_loss -0.010046453414087048\n{'min_val_epoch': 79, 'min_val_loss': -0.010355321471086757}\n******************************\nTRAIN: \t Epoch: 98 \t Loss: -0.010379464365541935\nTRAIN: \t Epoch: 98 \t Loss: -0.01063264487311244\nTRAIN: \t Epoch: 98 \t Loss: -0.010885574854910374\nTRAIN: \t Epoch: 98 \t Loss: -0.010721206432208419\nTRAIN: \t Epoch: 98 \t Loss: -0.010259630158543587\nTRAIN: \t Epoch: 98 \t Loss: -0.009940023999661207\nTRAIN: \t Epoch: 98 \t Loss: -0.009705272104058946\nTRAIN: \t Epoch: 98 \t Loss: -0.009661101503297687\nTRAIN: \t Epoch: 98 \t Loss: -0.009756042725510068\nTRAIN: \t Epoch: 98 \t Loss: -0.00981808863580227\nTRAIN: \t Epoch: 98 \t Loss: -0.009853843176229433\nTRAIN: \t Epoch: 98 \t Loss: -0.009832113593195876\nTRAIN: \t Epoch: 98 \t Loss: -0.00975870849707952\nTRAIN: \t Epoch: 98 \t Loss: -0.009685757370399577\nTRAIN: \t Epoch: 98 \t Loss: -0.009656670813759169\nTRAIN: \t Epoch: 98 \t Loss: -0.009734803577885032\nTRAIN: \t Epoch: 98 \t Loss: -0.009756889391803381\nVALD: \t Epoch: 98 \t Loss: -0.013037972152233124\nVALD: \t Epoch: 98 \t Loss: -0.012486571446061134\nVALD: \t Epoch: 98 \t Loss: -0.010618031490594149\nVALD: \t Epoch: 98 \t Loss: -0.010519706679437451\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:33<00:00,  9.81it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.359930380515479  FDE: 0.5352021027830878\n**************************************************\n******************************\nEpoch: social-tag : 98\ntrain_loss -0.009756889391803381\nval_loss -0.010519706679437451\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 99 \t Loss: -0.011303303763270378\nTRAIN: \t Epoch: 99 \t Loss: -0.011194224935024977\nTRAIN: \t Epoch: 99 \t Loss: -0.010383141847948233\nTRAIN: \t Epoch: 99 \t Loss: -0.009909773012623191\nTRAIN: \t Epoch: 99 \t Loss: -0.009527717344462871\nTRAIN: \t Epoch: 99 \t Loss: -0.00946610669294993\nTRAIN: \t Epoch: 99 \t Loss: -0.009487217558281762\nTRAIN: \t Epoch: 99 \t Loss: -0.009530080948024988\nTRAIN: \t Epoch: 99 \t Loss: -0.009562189069887003\nTRAIN: \t Epoch: 99 \t Loss: -0.009549298044294118\nTRAIN: \t Epoch: 99 \t Loss: -0.00955235246907581\nTRAIN: \t Epoch: 99 \t Loss: -0.009529243068148693\nTRAIN: \t Epoch: 99 \t Loss: -0.009500541795904819\nTRAIN: \t Epoch: 99 \t Loss: -0.00954557755695922\nTRAIN: \t Epoch: 99 \t Loss: -0.009508316156764824\nTRAIN: \t Epoch: 99 \t Loss: -0.00942245137412101\nTRAIN: \t Epoch: 99 \t Loss: -0.009399043029230652\nVALD: \t Epoch: 99 \t Loss: -0.01230933703482151\nVALD: \t Epoch: 99 \t Loss: -0.011558480560779572\nVALD: \t Epoch: 99 \t Loss: -0.009938701832046112\nVALD: \t Epoch: 99 \t Loss: -0.009873877385419286\n******************************\nEpoch: social-tag : 99\ntrain_loss -0.009399043029230652\nval_loss -0.009873877385419286\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 100 \t Loss: -0.009953984059393406\nTRAIN: \t Epoch: 100 \t Loss: -0.01048210496082902\nTRAIN: \t Epoch: 100 \t Loss: -0.010610154829919338\nTRAIN: \t Epoch: 100 \t Loss: -0.010464268969371915\nTRAIN: \t Epoch: 100 \t Loss: -0.010300137102603912\nTRAIN: \t Epoch: 100 \t Loss: -0.010310186228404442\nTRAIN: \t Epoch: 100 \t Loss: -0.010250101265098368\nTRAIN: \t Epoch: 100 \t Loss: -0.010235930676572025\nTRAIN: \t Epoch: 100 \t Loss: -0.010195202194154263\nTRAIN: \t Epoch: 100 \t Loss: -0.010125451907515525\nTRAIN: \t Epoch: 100 \t Loss: -0.01005822132256898\nTRAIN: \t Epoch: 100 \t Loss: -0.009945039714997014\nTRAIN: \t Epoch: 100 \t Loss: -0.009915303080700912\nTRAIN: \t Epoch: 100 \t Loss: -0.010011726458157812\nTRAIN: \t Epoch: 100 \t Loss: -0.009987045576175054\nTRAIN: \t Epoch: 100 \t Loss: -0.009893596870824695\nTRAIN: \t Epoch: 100 \t Loss: -0.009884628664815065\nVALD: \t Epoch: 100 \t Loss: -0.012393655255436897\nVALD: \t Epoch: 100 \t Loss: -0.011905435007065535\nVALD: \t Epoch: 100 \t Loss: -0.010228949909408888\nVALD: \t Epoch: 100 \t Loss: -0.01013077114394563\n******************************\nEpoch: social-tag : 100\ntrain_loss -0.009884628664815065\nval_loss -0.01013077114394563\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 101 \t Loss: -0.009971794672310352\nTRAIN: \t Epoch: 101 \t Loss: -0.010167818982154131\nTRAIN: \t Epoch: 101 \t Loss: -0.010375745284060637\nTRAIN: \t Epoch: 101 \t Loss: -0.010428884997963905\nTRAIN: \t Epoch: 101 \t Loss: -0.010468783974647521\nTRAIN: \t Epoch: 101 \t Loss: -0.010578759635488192\nTRAIN: \t Epoch: 101 \t Loss: -0.010667972532766206\nTRAIN: \t Epoch: 101 \t Loss: -0.010523573379032314\nTRAIN: \t Epoch: 101 \t Loss: -0.010161992628127337\nTRAIN: \t Epoch: 101 \t Loss: -0.009920559264719487\nTRAIN: \t Epoch: 101 \t Loss: -0.009905733083459463\nTRAIN: \t Epoch: 101 \t Loss: -0.010004085178176561\nTRAIN: \t Epoch: 101 \t Loss: -0.010077406150790362\nTRAIN: \t Epoch: 101 \t Loss: -0.01012238028592297\nTRAIN: \t Epoch: 101 \t Loss: -0.010137047121922175\nTRAIN: \t Epoch: 101 \t Loss: -0.010122874635271728\nTRAIN: \t Epoch: 101 \t Loss: -0.010049308551418962\nVALD: \t Epoch: 101 \t Loss: -0.012229295447468758\nVALD: \t Epoch: 101 \t Loss: -0.011181342415511608\nVALD: \t Epoch: 101 \t Loss: -0.008411356247961521\nVALD: \t Epoch: 101 \t Loss: -0.008652363946575842\n******************************\nEpoch: social-tag : 101\ntrain_loss -0.010049308551418962\nval_loss -0.008652363946575842\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 102 \t Loss: -0.01036421861499548\nTRAIN: \t Epoch: 102 \t Loss: -0.010509565006941557\nTRAIN: \t Epoch: 102 \t Loss: -0.010744693999489149\nTRAIN: \t Epoch: 102 \t Loss: -0.010675481520593166\nTRAIN: \t Epoch: 102 \t Loss: -0.010510324127972126\nTRAIN: \t Epoch: 102 \t Loss: -0.010485894201944271\nTRAIN: \t Epoch: 102 \t Loss: -0.010371669993868895\nTRAIN: \t Epoch: 102 \t Loss: -0.010004650626797229\nTRAIN: \t Epoch: 102 \t Loss: -0.009838519462694725\nTRAIN: \t Epoch: 102 \t Loss: -0.009812685986980796\nTRAIN: \t Epoch: 102 \t Loss: -0.009910886048931967\nTRAIN: \t Epoch: 102 \t Loss: -0.009867463532524804\nTRAIN: \t Epoch: 102 \t Loss: -0.009940496036926141\nTRAIN: \t Epoch: 102 \t Loss: -0.009926397918856569\nTRAIN: \t Epoch: 102 \t Loss: -0.009858446661382914\nTRAIN: \t Epoch: 102 \t Loss: -0.009749272285262123\nTRAIN: \t Epoch: 102 \t Loss: -0.009730813065261551\nVALD: \t Epoch: 102 \t Loss: -0.01162285078316927\nVALD: \t Epoch: 102 \t Loss: -0.011085318867117167\nVALD: \t Epoch: 102 \t Loss: -0.008873022937526306\nVALD: \t Epoch: 102 \t Loss: -0.00903632529005557\n******************************\nEpoch: social-tag : 102\ntrain_loss -0.009730813065261551\nval_loss -0.00903632529005557\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 103 \t Loss: -0.010103624314069748\nTRAIN: \t Epoch: 103 \t Loss: -0.0104638347402215\nTRAIN: \t Epoch: 103 \t Loss: -0.010662472806870937\nTRAIN: \t Epoch: 103 \t Loss: -0.010655923513695598\nTRAIN: \t Epoch: 103 \t Loss: -0.010668843053281307\nTRAIN: \t Epoch: 103 \t Loss: -0.010229829388360182\nTRAIN: \t Epoch: 103 \t Loss: -0.009806044532784395\nTRAIN: \t Epoch: 103 \t Loss: -0.009657186805270612\nTRAIN: \t Epoch: 103 \t Loss: -0.009754822382496463\nTRAIN: \t Epoch: 103 \t Loss: -0.009860469866544009\nTRAIN: \t Epoch: 103 \t Loss: -0.009916359558701515\nTRAIN: \t Epoch: 103 \t Loss: -0.009868007696544131\nTRAIN: \t Epoch: 103 \t Loss: -0.009954113083390089\nTRAIN: \t Epoch: 103 \t Loss: -0.009957877320370503\nTRAIN: \t Epoch: 103 \t Loss: -0.009931699807445208\nTRAIN: \t Epoch: 103 \t Loss: -0.009814787947107106\nTRAIN: \t Epoch: 103 \t Loss: -0.009756569157947193\nVALD: \t Epoch: 103 \t Loss: -0.010978656820952892\nVALD: \t Epoch: 103 \t Loss: -0.010382880922406912\nVALD: \t Epoch: 103 \t Loss: -0.008667723586161932\nVALD: \t Epoch: 103 \t Loss: -0.008766725629627586\n******************************\nEpoch: social-tag : 103\ntrain_loss -0.009756569157947193\nval_loss -0.008766725629627586\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 104 \t Loss: -0.009715614840388298\nTRAIN: \t Epoch: 104 \t Loss: -0.010743087157607079\nTRAIN: \t Epoch: 104 \t Loss: -0.010964399203658104\nTRAIN: \t Epoch: 104 \t Loss: -0.010622321162372828\nTRAIN: \t Epoch: 104 \t Loss: -0.010542904399335385\nTRAIN: \t Epoch: 104 \t Loss: -0.01019364238406221\nTRAIN: \t Epoch: 104 \t Loss: -0.00989213479416711\nTRAIN: \t Epoch: 104 \t Loss: -0.00976692687254399\nTRAIN: \t Epoch: 104 \t Loss: -0.009756138651735254\nTRAIN: \t Epoch: 104 \t Loss: -0.009800542425364257\nTRAIN: \t Epoch: 104 \t Loss: -0.009924852119928057\nTRAIN: \t Epoch: 104 \t Loss: -0.009895651368424296\nTRAIN: \t Epoch: 104 \t Loss: -0.009910983678240042\nTRAIN: \t Epoch: 104 \t Loss: -0.00990938228954162\nTRAIN: \t Epoch: 104 \t Loss: -0.009812312511106333\nTRAIN: \t Epoch: 104 \t Loss: -0.009785665606614202\nTRAIN: \t Epoch: 104 \t Loss: -0.009780928605433666\nVALD: \t Epoch: 104 \t Loss: -0.012213917449116707\nVALD: \t Epoch: 104 \t Loss: -0.011486423201858997\nVALD: \t Epoch: 104 \t Loss: -0.009948042687028646\nVALD: \t Epoch: 104 \t Loss: -0.009885677201543264\n******************************\nEpoch: social-tag : 104\ntrain_loss -0.009780928605433666\nval_loss -0.009885677201543264\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 105 \t Loss: -0.011182380840182304\nTRAIN: \t Epoch: 105 \t Loss: -0.011325093917548656\nTRAIN: \t Epoch: 105 \t Loss: -0.01115113589912653\nTRAIN: \t Epoch: 105 \t Loss: -0.01125858398154378\nTRAIN: \t Epoch: 105 \t Loss: -0.010711424797773362\nTRAIN: \t Epoch: 105 \t Loss: -0.01005735364742577\nTRAIN: \t Epoch: 105 \t Loss: -0.009550416975149087\nTRAIN: \t Epoch: 105 \t Loss: -0.009482903056778014\nTRAIN: \t Epoch: 105 \t Loss: -0.009458491164777014\nTRAIN: \t Epoch: 105 \t Loss: -0.009638382773846387\nTRAIN: \t Epoch: 105 \t Loss: -0.00964558387005871\nTRAIN: \t Epoch: 105 \t Loss: -0.009631634301816424\nTRAIN: \t Epoch: 105 \t Loss: -0.009642493051405136\nTRAIN: \t Epoch: 105 \t Loss: -0.009634164228503193\nTRAIN: \t Epoch: 105 \t Loss: -0.009658869169652463\nTRAIN: \t Epoch: 105 \t Loss: -0.009621896198950708\nTRAIN: \t Epoch: 105 \t Loss: -0.009634822522374716\nVALD: \t Epoch: 105 \t Loss: -0.012706749141216278\nVALD: \t Epoch: 105 \t Loss: -0.012190673500299454\nVALD: \t Epoch: 105 \t Loss: -0.010604439458499352\nVALD: \t Epoch: 105 \t Loss: -0.010516530382419061\n******************************\nEpoch: social-tag : 105\ntrain_loss -0.009634822522374716\nval_loss -0.010516530382419061\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 106 \t Loss: -0.01122331153601408\nTRAIN: \t Epoch: 106 \t Loss: -0.011404084973037243\nTRAIN: \t Epoch: 106 \t Loss: -0.011502856078247229\nTRAIN: \t Epoch: 106 \t Loss: -0.01124688284471631\nTRAIN: \t Epoch: 106 \t Loss: -0.010935423336923123\nTRAIN: \t Epoch: 106 \t Loss: -0.010534127398083607\nTRAIN: \t Epoch: 106 \t Loss: -0.01008909075920071\nTRAIN: \t Epoch: 106 \t Loss: -0.009940888150595129\nTRAIN: \t Epoch: 106 \t Loss: -0.010074701470633348\nTRAIN: \t Epoch: 106 \t Loss: -0.010164146218448878\nTRAIN: \t Epoch: 106 \t Loss: -0.010177253152836453\nTRAIN: \t Epoch: 106 \t Loss: -0.01017306869228681\nTRAIN: \t Epoch: 106 \t Loss: -0.010078653334998168\nTRAIN: \t Epoch: 106 \t Loss: -0.00999427866190672\nTRAIN: \t Epoch: 106 \t Loss: -0.009923627910514673\nTRAIN: \t Epoch: 106 \t Loss: -0.009948691353201866\nTRAIN: \t Epoch: 106 \t Loss: -0.009992219959244583\nVALD: \t Epoch: 106 \t Loss: -0.012500178068876266\nVALD: \t Epoch: 106 \t Loss: -0.011858080513775349\nVALD: \t Epoch: 106 \t Loss: -0.009532273902247349\nVALD: \t Epoch: 106 \t Loss: -0.00962215376471331\n******************************\nEpoch: social-tag : 106\ntrain_loss -0.009992219959244583\nval_loss -0.00962215376471331\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 107 \t Loss: -0.011450368911027908\nTRAIN: \t Epoch: 107 \t Loss: -0.0111806346103549\nTRAIN: \t Epoch: 107 \t Loss: -0.010702565312385559\nTRAIN: \t Epoch: 107 \t Loss: -0.010239510796964169\nTRAIN: \t Epoch: 107 \t Loss: -0.00962929604575038\nTRAIN: \t Epoch: 107 \t Loss: -0.00942132257235547\nTRAIN: \t Epoch: 107 \t Loss: -0.009521964338741131\nTRAIN: \t Epoch: 107 \t Loss: -0.009713154227938503\nTRAIN: \t Epoch: 107 \t Loss: -0.00992888000069393\nTRAIN: \t Epoch: 107 \t Loss: -0.010057861590757966\nTRAIN: \t Epoch: 107 \t Loss: -0.01009259419515729\nTRAIN: \t Epoch: 107 \t Loss: -0.010148113088992735\nTRAIN: \t Epoch: 107 \t Loss: -0.010130455609984122\nTRAIN: \t Epoch: 107 \t Loss: -0.010114199847781233\nTRAIN: \t Epoch: 107 \t Loss: -0.009848236261556545\nTRAIN: \t Epoch: 107 \t Loss: -0.009734501334605739\nTRAIN: \t Epoch: 107 \t Loss: -0.009712035739512154\nVALD: \t Epoch: 107 \t Loss: -0.01217223796993494\nVALD: \t Epoch: 107 \t Loss: -0.011610216461122036\nVALD: \t Epoch: 107 \t Loss: -0.00966487918049097\nVALD: \t Epoch: 107 \t Loss: -0.00969546902441455\n******************************\nEpoch: social-tag : 107\ntrain_loss -0.009712035739512154\nval_loss -0.00969546902441455\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 108 \t Loss: -0.009856631979346275\nTRAIN: \t Epoch: 108 \t Loss: -0.010288939345628023\nTRAIN: \t Epoch: 108 \t Loss: -0.010710951251288256\nTRAIN: \t Epoch: 108 \t Loss: -0.010811121435835958\nTRAIN: \t Epoch: 108 \t Loss: -0.010860399529337882\nTRAIN: \t Epoch: 108 \t Loss: -0.01085121650248766\nTRAIN: \t Epoch: 108 \t Loss: -0.010792021905737264\nTRAIN: \t Epoch: 108 \t Loss: -0.010618483647704124\nTRAIN: \t Epoch: 108 \t Loss: -0.010148402717378404\nTRAIN: \t Epoch: 108 \t Loss: -0.009911831002682447\nTRAIN: \t Epoch: 108 \t Loss: -0.009902430782941255\nTRAIN: \t Epoch: 108 \t Loss: -0.00990334937038521\nTRAIN: \t Epoch: 108 \t Loss: -0.010014155879616737\nTRAIN: \t Epoch: 108 \t Loss: -0.010106379964521952\nTRAIN: \t Epoch: 108 \t Loss: -0.010139506620665392\nTRAIN: \t Epoch: 108 \t Loss: -0.010112552263308316\nTRAIN: \t Epoch: 108 \t Loss: -0.010052950403681307\nVALD: \t Epoch: 108 \t Loss: -0.011712523177266121\nVALD: \t Epoch: 108 \t Loss: -0.011401277501136065\nVALD: \t Epoch: 108 \t Loss: -0.009689660121997198\nVALD: \t Epoch: 108 \t Loss: -0.009728159257275854\n******************************\nEpoch: social-tag : 108\ntrain_loss -0.010052950403681307\nval_loss -0.009728159257275854\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 109 \t Loss: -0.010508473962545395\nTRAIN: \t Epoch: 109 \t Loss: -0.010624836664646864\nTRAIN: \t Epoch: 109 \t Loss: -0.010679761879146099\nTRAIN: \t Epoch: 109 \t Loss: -0.01056974334642291\nTRAIN: \t Epoch: 109 \t Loss: -0.01028999239206314\nTRAIN: \t Epoch: 109 \t Loss: -0.00995354587212205\nTRAIN: \t Epoch: 109 \t Loss: -0.009731699446482318\nTRAIN: \t Epoch: 109 \t Loss: -0.009829147602431476\nTRAIN: \t Epoch: 109 \t Loss: -0.009915103825430075\nTRAIN: \t Epoch: 109 \t Loss: -0.010037865862250328\nTRAIN: \t Epoch: 109 \t Loss: -0.010077255100689152\nTRAIN: \t Epoch: 109 \t Loss: -0.010107621472949782\nTRAIN: \t Epoch: 109 \t Loss: -0.01016088890341612\nTRAIN: \t Epoch: 109 \t Loss: -0.01003550472004073\nTRAIN: \t Epoch: 109 \t Loss: -0.009866922628134489\nTRAIN: \t Epoch: 109 \t Loss: -0.009853193507296965\nTRAIN: \t Epoch: 109 \t Loss: -0.009857701278771416\nVALD: \t Epoch: 109 \t Loss: -0.012515407986938953\nVALD: \t Epoch: 109 \t Loss: -0.011476028244942427\nVALD: \t Epoch: 109 \t Loss: -0.009119092797239622\nVALD: \t Epoch: 109 \t Loss: -0.009278108735760291\n******************************\nEpoch: social-tag : 109\ntrain_loss -0.009857701278771416\nval_loss -0.009278108735760291\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 110 \t Loss: -0.011334395036101341\nTRAIN: \t Epoch: 110 \t Loss: -0.011476489715278149\nTRAIN: \t Epoch: 110 \t Loss: -0.010909990717967352\nTRAIN: \t Epoch: 110 \t Loss: -0.010872309794649482\nTRAIN: \t Epoch: 110 \t Loss: -0.010938490368425846\nTRAIN: \t Epoch: 110 \t Loss: -0.010701688006520271\nTRAIN: \t Epoch: 110 \t Loss: -0.010521004774740763\nTRAIN: \t Epoch: 110 \t Loss: -0.010311790043488145\nTRAIN: \t Epoch: 110 \t Loss: -0.010305443054272069\nTRAIN: \t Epoch: 110 \t Loss: -0.010291441064327955\nTRAIN: \t Epoch: 110 \t Loss: -0.010252037932249632\nTRAIN: \t Epoch: 110 \t Loss: -0.010268536706765493\nTRAIN: \t Epoch: 110 \t Loss: -0.010201188807304088\nTRAIN: \t Epoch: 110 \t Loss: -0.01021003523575408\nTRAIN: \t Epoch: 110 \t Loss: -0.010149531128505866\nTRAIN: \t Epoch: 110 \t Loss: -0.010142254119273275\nTRAIN: \t Epoch: 110 \t Loss: -0.010083926079625433\nVALD: \t Epoch: 110 \t Loss: -0.011683721095323563\nVALD: \t Epoch: 110 \t Loss: -0.01114244619384408\nVALD: \t Epoch: 110 \t Loss: -0.009565592433015505\nVALD: \t Epoch: 110 \t Loss: -0.009562155681693864\n******************************\nEpoch: social-tag : 110\ntrain_loss -0.010083926079625433\nval_loss -0.009562155681693864\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 111 \t Loss: -0.010781972669064999\nTRAIN: \t Epoch: 111 \t Loss: -0.010784178506582975\nTRAIN: \t Epoch: 111 \t Loss: -0.010520998698969683\nTRAIN: \t Epoch: 111 \t Loss: -0.010450871661305428\nTRAIN: \t Epoch: 111 \t Loss: -0.010356342419981956\nTRAIN: \t Epoch: 111 \t Loss: -0.010280658801396688\nTRAIN: \t Epoch: 111 \t Loss: -0.010225403787834304\nTRAIN: \t Epoch: 111 \t Loss: -0.010275155887939036\nTRAIN: \t Epoch: 111 \t Loss: -0.010260603597594632\nTRAIN: \t Epoch: 111 \t Loss: -0.010153953079134225\nTRAIN: \t Epoch: 111 \t Loss: -0.01014160161668604\nTRAIN: \t Epoch: 111 \t Loss: -0.010167043811331192\nTRAIN: \t Epoch: 111 \t Loss: -0.010237180126401095\nTRAIN: \t Epoch: 111 \t Loss: -0.010266538177217756\nTRAIN: \t Epoch: 111 \t Loss: -0.010234433288375536\nTRAIN: \t Epoch: 111 \t Loss: -0.010151264141313732\nTRAIN: \t Epoch: 111 \t Loss: -0.010110219545436628\nVALD: \t Epoch: 111 \t Loss: -0.012206612154841423\nVALD: \t Epoch: 111 \t Loss: -0.011899199802428484\nVALD: \t Epoch: 111 \t Loss: -0.01036533092459043\nVALD: \t Epoch: 111 \t Loss: -0.010283667408301682\n******************************\nEpoch: social-tag : 111\ntrain_loss -0.010110219545436628\nval_loss -0.010283667408301682\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 112 \t Loss: -0.010804060846567154\nTRAIN: \t Epoch: 112 \t Loss: -0.011048707645386457\nTRAIN: \t Epoch: 112 \t Loss: -0.010917102607587973\nTRAIN: \t Epoch: 112 \t Loss: -0.010804587975144386\nTRAIN: \t Epoch: 112 \t Loss: -0.010620918683707714\nTRAIN: \t Epoch: 112 \t Loss: -0.010390358977019787\nTRAIN: \t Epoch: 112 \t Loss: -0.010012153802173478\nTRAIN: \t Epoch: 112 \t Loss: -0.009807840688154101\nTRAIN: \t Epoch: 112 \t Loss: -0.009807308204472065\nTRAIN: \t Epoch: 112 \t Loss: -0.009943823795765638\nTRAIN: \t Epoch: 112 \t Loss: -0.010026839477094736\nTRAIN: \t Epoch: 112 \t Loss: -0.010111836871753136\nTRAIN: \t Epoch: 112 \t Loss: -0.010150064929173542\nTRAIN: \t Epoch: 112 \t Loss: -0.010150926453726632\nTRAIN: \t Epoch: 112 \t Loss: -0.010038239633043606\nTRAIN: \t Epoch: 112 \t Loss: -0.010031415149569511\nTRAIN: \t Epoch: 112 \t Loss: -0.010030972348018126\nVALD: \t Epoch: 112 \t Loss: -0.01276618242263794\nVALD: \t Epoch: 112 \t Loss: -0.011952064465731382\nVALD: \t Epoch: 112 \t Loss: -0.010093019188692173\nVALD: \t Epoch: 112 \t Loss: -0.01010432464633873\n******************************\nEpoch: social-tag : 112\ntrain_loss -0.010030972348018126\nval_loss -0.01010432464633873\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 113 \t Loss: -0.011014887131750584\nTRAIN: \t Epoch: 113 \t Loss: -0.011140648275613785\nTRAIN: \t Epoch: 113 \t Loss: -0.011023935241003832\nTRAIN: \t Epoch: 113 \t Loss: -0.01081602810882032\nTRAIN: \t Epoch: 113 \t Loss: -0.010709827579557896\nTRAIN: \t Epoch: 113 \t Loss: -0.010097288293763995\nTRAIN: \t Epoch: 113 \t Loss: -0.009762967537556375\nTRAIN: \t Epoch: 113 \t Loss: -0.009814157849177718\nTRAIN: \t Epoch: 113 \t Loss: -0.009971321870883306\nTRAIN: \t Epoch: 113 \t Loss: -0.010090973041951657\nTRAIN: \t Epoch: 113 \t Loss: -0.010255624116821722\nTRAIN: \t Epoch: 113 \t Loss: -0.010279545715699593\nTRAIN: \t Epoch: 113 \t Loss: -0.010236312563602742\nTRAIN: \t Epoch: 113 \t Loss: -0.010158326676381486\nTRAIN: \t Epoch: 113 \t Loss: -0.010190377819041412\nTRAIN: \t Epoch: 113 \t Loss: -0.010192545072641224\nTRAIN: \t Epoch: 113 \t Loss: -0.010203095237639818\nVALD: \t Epoch: 113 \t Loss: -0.01295493170619011\nVALD: \t Epoch: 113 \t Loss: -0.012018419336527586\nVALD: \t Epoch: 113 \t Loss: -0.009846654410163561\nVALD: \t Epoch: 113 \t Loss: -0.009912058740794778\n******************************\nEpoch: social-tag : 113\ntrain_loss -0.010203095237639818\nval_loss -0.009912058740794778\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 114 \t Loss: -0.010573375970125198\nTRAIN: \t Epoch: 114 \t Loss: -0.010770685505121946\nTRAIN: \t Epoch: 114 \t Loss: -0.010997498718400797\nTRAIN: \t Epoch: 114 \t Loss: -0.011136126471683383\nTRAIN: \t Epoch: 114 \t Loss: -0.0109760619699955\nTRAIN: \t Epoch: 114 \t Loss: -0.010750639562805494\nTRAIN: \t Epoch: 114 \t Loss: -0.010420780096735274\nTRAIN: \t Epoch: 114 \t Loss: -0.010365911526605487\nTRAIN: \t Epoch: 114 \t Loss: -0.010430383392506175\nTRAIN: \t Epoch: 114 \t Loss: -0.010455890186131\nTRAIN: \t Epoch: 114 \t Loss: -0.010490217415446585\nTRAIN: \t Epoch: 114 \t Loss: -0.010550864273682237\nTRAIN: \t Epoch: 114 \t Loss: -0.010472789908257814\nTRAIN: \t Epoch: 114 \t Loss: -0.01043435392369117\nTRAIN: \t Epoch: 114 \t Loss: -0.010414819791913032\nTRAIN: \t Epoch: 114 \t Loss: -0.010321190522518009\nTRAIN: \t Epoch: 114 \t Loss: -0.010269065020662365\nVALD: \t Epoch: 114 \t Loss: -0.012064538896083832\nVALD: \t Epoch: 114 \t Loss: -0.011231184471398592\nVALD: \t Epoch: 114 \t Loss: -0.009574461417893568\nVALD: \t Epoch: 114 \t Loss: -0.009589298280651222\n******************************\nEpoch: social-tag : 114\ntrain_loss -0.010269065020662365\nval_loss -0.009589298280651222\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 115 \t Loss: -0.01075298897922039\nTRAIN: \t Epoch: 115 \t Loss: -0.010784758254885674\nTRAIN: \t Epoch: 115 \t Loss: -0.010872216274340948\nTRAIN: \t Epoch: 115 \t Loss: -0.010776735143736005\nTRAIN: \t Epoch: 115 \t Loss: -0.010770242847502231\nTRAIN: \t Epoch: 115 \t Loss: -0.010772380201766888\nTRAIN: \t Epoch: 115 \t Loss: -0.010511485593659537\nTRAIN: \t Epoch: 115 \t Loss: -0.010226379614323378\nTRAIN: \t Epoch: 115 \t Loss: -0.01015604618522856\nTRAIN: \t Epoch: 115 \t Loss: -0.010139851365238428\nTRAIN: \t Epoch: 115 \t Loss: -0.010195492089472034\nTRAIN: \t Epoch: 115 \t Loss: -0.010121736287449798\nTRAIN: \t Epoch: 115 \t Loss: -0.010094499860245448\nTRAIN: \t Epoch: 115 \t Loss: -0.010078143062336105\nTRAIN: \t Epoch: 115 \t Loss: -0.010097660993536314\nTRAIN: \t Epoch: 115 \t Loss: -0.01008429640205577\nTRAIN: \t Epoch: 115 \t Loss: -0.01008643994502949\nVALD: \t Epoch: 115 \t Loss: -0.012767042964696884\nVALD: \t Epoch: 115 \t Loss: -0.011887295171618462\nVALD: \t Epoch: 115 \t Loss: -0.009517283954968056\nVALD: \t Epoch: 115 \t Loss: -0.009663097277848783\n******************************\nEpoch: social-tag : 115\ntrain_loss -0.01008643994502949\nval_loss -0.009663097277848783\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 116 \t Loss: -0.011022256687283516\nTRAIN: \t Epoch: 116 \t Loss: -0.010908263269811869\nTRAIN: \t Epoch: 116 \t Loss: -0.011244933120906353\nTRAIN: \t Epoch: 116 \t Loss: -0.011136329267174006\nTRAIN: \t Epoch: 116 \t Loss: -0.0109547333791852\nTRAIN: \t Epoch: 116 \t Loss: -0.010681218778093656\nTRAIN: \t Epoch: 116 \t Loss: -0.010396069580955165\nTRAIN: \t Epoch: 116 \t Loss: -0.010268173180520535\nTRAIN: \t Epoch: 116 \t Loss: -0.01025977140913407\nTRAIN: \t Epoch: 116 \t Loss: -0.010268949158489703\nTRAIN: \t Epoch: 116 \t Loss: -0.010349405844780531\nTRAIN: \t Epoch: 116 \t Loss: -0.010330465234195193\nTRAIN: \t Epoch: 116 \t Loss: -0.010304716200782703\nTRAIN: \t Epoch: 116 \t Loss: -0.010310026351362467\nTRAIN: \t Epoch: 116 \t Loss: -0.01030952421327432\nTRAIN: \t Epoch: 116 \t Loss: -0.010285195778124034\nTRAIN: \t Epoch: 116 \t Loss: -0.010237993277383573\nVALD: \t Epoch: 116 \t Loss: -0.01247318647801876\nVALD: \t Epoch: 116 \t Loss: -0.011744329705834389\nVALD: \t Epoch: 116 \t Loss: -0.009768828749656677\nVALD: \t Epoch: 116 \t Loss: -0.00984313554630546\n******************************\nEpoch: social-tag : 116\ntrain_loss -0.010237993277383573\nval_loss -0.00984313554630546\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 117 \t Loss: -0.01139495987445116\nTRAIN: \t Epoch: 117 \t Loss: -0.011701879557222128\nTRAIN: \t Epoch: 117 \t Loss: -0.011107349147399267\nTRAIN: \t Epoch: 117 \t Loss: -0.010515747126191854\nTRAIN: \t Epoch: 117 \t Loss: -0.010010921023786068\nTRAIN: \t Epoch: 117 \t Loss: -0.009895632664362589\nTRAIN: \t Epoch: 117 \t Loss: -0.0099170110853655\nTRAIN: \t Epoch: 117 \t Loss: -0.009984343661926687\nTRAIN: \t Epoch: 117 \t Loss: -0.010009839613404539\nTRAIN: \t Epoch: 117 \t Loss: -0.010095373541116715\nTRAIN: \t Epoch: 117 \t Loss: -0.010076426900923252\nTRAIN: \t Epoch: 117 \t Loss: -0.009942846683164438\nTRAIN: \t Epoch: 117 \t Loss: -0.009894585021986412\nTRAIN: \t Epoch: 117 \t Loss: -0.009937739332339593\nTRAIN: \t Epoch: 117 \t Loss: -0.00998893715441227\nTRAIN: \t Epoch: 117 \t Loss: -0.010019976296462119\nTRAIN: \t Epoch: 117 \t Loss: -0.010059353641488335\nVALD: \t Epoch: 117 \t Loss: -0.01308040227741003\nVALD: \t Epoch: 117 \t Loss: -0.01204919395968318\nVALD: \t Epoch: 117 \t Loss: -0.01011418846125404\nVALD: \t Epoch: 117 \t Loss: -0.0101003057942419\n******************************\nEpoch: social-tag : 117\ntrain_loss -0.010059353641488335\nval_loss -0.0101003057942419\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 118 \t Loss: -0.010185222141444683\nTRAIN: \t Epoch: 118 \t Loss: -0.010675657074898481\nTRAIN: \t Epoch: 118 \t Loss: -0.010799470357596874\nTRAIN: \t Epoch: 118 \t Loss: -0.010913736186921597\nTRAIN: \t Epoch: 118 \t Loss: -0.010913151130080224\nTRAIN: \t Epoch: 118 \t Loss: -0.010878001029292742\nTRAIN: \t Epoch: 118 \t Loss: -0.010882142399038588\nTRAIN: \t Epoch: 118 \t Loss: -0.010779855190776289\nTRAIN: \t Epoch: 118 \t Loss: -0.010657919984724786\nTRAIN: \t Epoch: 118 \t Loss: -0.010506767313927412\nTRAIN: \t Epoch: 118 \t Loss: -0.010522530922158197\nTRAIN: \t Epoch: 118 \t Loss: -0.010501259782661995\nTRAIN: \t Epoch: 118 \t Loss: -0.010523807257413864\nTRAIN: \t Epoch: 118 \t Loss: -0.010505422750221831\nTRAIN: \t Epoch: 118 \t Loss: -0.010413747777541478\nTRAIN: \t Epoch: 118 \t Loss: -0.010219191841315478\nTRAIN: \t Epoch: 118 \t Loss: -0.010210630794366201\nVALD: \t Epoch: 118 \t Loss: -0.012047079391777515\nVALD: \t Epoch: 118 \t Loss: -0.011302631348371506\nVALD: \t Epoch: 118 \t Loss: -0.008944326390822729\nVALD: \t Epoch: 118 \t Loss: -0.009136016497355022\n******************************\nEpoch: social-tag : 118\ntrain_loss -0.010210630794366201\nval_loss -0.009136016497355022\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 119 \t Loss: -0.010271322913467884\nTRAIN: \t Epoch: 119 \t Loss: -0.010815187357366085\nTRAIN: \t Epoch: 119 \t Loss: -0.010963428455094496\nTRAIN: \t Epoch: 119 \t Loss: -0.010955382604151964\nTRAIN: \t Epoch: 119 \t Loss: -0.010998735576868058\nTRAIN: \t Epoch: 119 \t Loss: -0.010670853313058615\nTRAIN: \t Epoch: 119 \t Loss: -0.010256938968918152\nTRAIN: \t Epoch: 119 \t Loss: -0.010110000672284514\nTRAIN: \t Epoch: 119 \t Loss: -0.010162590951141384\nTRAIN: \t Epoch: 119 \t Loss: -0.010204758262261748\nTRAIN: \t Epoch: 119 \t Loss: -0.010296923235397448\nTRAIN: \t Epoch: 119 \t Loss: -0.010324063051181534\nTRAIN: \t Epoch: 119 \t Loss: -0.010288561157022532\nTRAIN: \t Epoch: 119 \t Loss: -0.010203417956030794\nTRAIN: \t Epoch: 119 \t Loss: -0.01010889799023668\nTRAIN: \t Epoch: 119 \t Loss: -0.010045610455563292\nTRAIN: \t Epoch: 119 \t Loss: -0.010073647665029222\nVALD: \t Epoch: 119 \t Loss: -0.012771433219313622\nVALD: \t Epoch: 119 \t Loss: -0.011905111838132143\nVALD: \t Epoch: 119 \t Loss: -0.009734317970772585\nVALD: \t Epoch: 119 \t Loss: -0.009838563477445742\n******************************\nEpoch: social-tag : 119\ntrain_loss -0.010073647665029222\nval_loss -0.009838563477445742\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 120 \t Loss: -0.011079315096139908\nTRAIN: \t Epoch: 120 \t Loss: -0.011092417407780886\nTRAIN: \t Epoch: 120 \t Loss: -0.010786397072176138\nTRAIN: \t Epoch: 120 \t Loss: -0.010891216341406107\nTRAIN: \t Epoch: 120 \t Loss: -0.01075981967151165\nTRAIN: \t Epoch: 120 \t Loss: -0.010668218446274599\nTRAIN: \t Epoch: 120 \t Loss: -0.010244623824421848\nTRAIN: \t Epoch: 120 \t Loss: -0.010087713075336069\nTRAIN: \t Epoch: 120 \t Loss: -0.01005785410395927\nTRAIN: \t Epoch: 120 \t Loss: -0.010124150244519115\nTRAIN: \t Epoch: 120 \t Loss: -0.010191618591885675\nTRAIN: \t Epoch: 120 \t Loss: -0.010294473070340851\nTRAIN: \t Epoch: 120 \t Loss: -0.010339113059811868\nTRAIN: \t Epoch: 120 \t Loss: -0.01037153332228107\nTRAIN: \t Epoch: 120 \t Loss: -0.010316440369933844\nTRAIN: \t Epoch: 120 \t Loss: -0.010130290291272104\nTRAIN: \t Epoch: 120 \t Loss: -0.010087168577945593\nVALD: \t Epoch: 120 \t Loss: -0.011941871605813503\nVALD: \t Epoch: 120 \t Loss: -0.011329928413033485\nVALD: \t Epoch: 120 \t Loss: -0.00968002574518323\nVALD: \t Epoch: 120 \t Loss: -0.009641896821781546\n******************************\nEpoch: social-tag : 120\ntrain_loss -0.010087168577945593\nval_loss -0.009641896821781546\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 121 \t Loss: -0.010571951977908611\nTRAIN: \t Epoch: 121 \t Loss: -0.010916972532868385\nTRAIN: \t Epoch: 121 \t Loss: -0.011397884227335453\nTRAIN: \t Epoch: 121 \t Loss: -0.011165725532919168\nTRAIN: \t Epoch: 121 \t Loss: -0.0109274385496974\nTRAIN: \t Epoch: 121 \t Loss: -0.010601652165253958\nTRAIN: \t Epoch: 121 \t Loss: -0.010293426657361644\nTRAIN: \t Epoch: 121 \t Loss: -0.010178520227782428\nTRAIN: \t Epoch: 121 \t Loss: -0.0103004469225804\nTRAIN: \t Epoch: 121 \t Loss: -0.010331457201391458\nTRAIN: \t Epoch: 121 \t Loss: -0.01037616722963073\nTRAIN: \t Epoch: 121 \t Loss: -0.010424337349832058\nTRAIN: \t Epoch: 121 \t Loss: -0.010488126570215592\nTRAIN: \t Epoch: 121 \t Loss: -0.010544395406863518\nTRAIN: \t Epoch: 121 \t Loss: -0.010481383465230466\nTRAIN: \t Epoch: 121 \t Loss: -0.010315187799278647\nTRAIN: \t Epoch: 121 \t Loss: -0.010227423034269701\nVALD: \t Epoch: 121 \t Loss: -0.011785125359892845\nVALD: \t Epoch: 121 \t Loss: -0.011287981178611517\nVALD: \t Epoch: 121 \t Loss: -0.009654366411268711\nVALD: \t Epoch: 121 \t Loss: -0.009728128086782977\n******************************\nEpoch: social-tag : 121\ntrain_loss -0.010227423034269701\nval_loss -0.009728128086782977\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 122 \t Loss: -0.011201152577996254\nTRAIN: \t Epoch: 122 \t Loss: -0.011087642051279545\nTRAIN: \t Epoch: 122 \t Loss: -0.01110890849183003\nTRAIN: \t Epoch: 122 \t Loss: -0.010704746469855309\nTRAIN: \t Epoch: 122 \t Loss: -0.010619639419019222\nTRAIN: \t Epoch: 122 \t Loss: -0.01080237670491139\nTRAIN: \t Epoch: 122 \t Loss: -0.010714038940412658\nTRAIN: \t Epoch: 122 \t Loss: -0.010506412596441805\nTRAIN: \t Epoch: 122 \t Loss: -0.010281528553201092\nTRAIN: \t Epoch: 122 \t Loss: -0.01016623955219984\nTRAIN: \t Epoch: 122 \t Loss: -0.010193418457426807\nTRAIN: \t Epoch: 122 \t Loss: -0.010259159995863834\nTRAIN: \t Epoch: 122 \t Loss: -0.010251910569002995\nTRAIN: \t Epoch: 122 \t Loss: -0.01029752048530749\nTRAIN: \t Epoch: 122 \t Loss: -0.010286002606153487\nTRAIN: \t Epoch: 122 \t Loss: -0.010128523106686771\nTRAIN: \t Epoch: 122 \t Loss: -0.010072185516808971\nVALD: \t Epoch: 122 \t Loss: -0.011881308630108833\nVALD: \t Epoch: 122 \t Loss: -0.011118389666080475\nVALD: \t Epoch: 122 \t Loss: -0.009499648896356424\nVALD: \t Epoch: 122 \t Loss: -0.009421543446843495\n******************************\nEpoch: social-tag : 122\ntrain_loss -0.010072185516808971\nval_loss -0.009421543446843495\n{'min_val_epoch': 98, 'min_val_loss': -0.010519706679437451}\n******************************\nTRAIN: \t Epoch: 123 \t Loss: -0.010484511032700539\nTRAIN: \t Epoch: 123 \t Loss: -0.010555766522884369\nTRAIN: \t Epoch: 123 \t Loss: -0.010945410778125128\nTRAIN: \t Epoch: 123 \t Loss: -0.010994681855663657\nTRAIN: \t Epoch: 123 \t Loss: -0.01093966867774725\nTRAIN: \t Epoch: 123 \t Loss: -0.010904008212188879\nTRAIN: \t Epoch: 123 \t Loss: -0.010705189912446908\nTRAIN: \t Epoch: 123 \t Loss: -0.010439622565172613\nTRAIN: \t Epoch: 123 \t Loss: -0.01031792898558908\nTRAIN: \t Epoch: 123 \t Loss: -0.01030868636444211\nTRAIN: \t Epoch: 123 \t Loss: -0.010375905104658821\nTRAIN: \t Epoch: 123 \t Loss: -0.010362301953136921\nTRAIN: \t Epoch: 123 \t Loss: -0.010449463358292213\nTRAIN: \t Epoch: 123 \t Loss: -0.01055599789002112\nTRAIN: \t Epoch: 123 \t Loss: -0.010632319500048955\nTRAIN: \t Epoch: 123 \t Loss: -0.010644318186677992\nTRAIN: \t Epoch: 123 \t Loss: -0.0105969983745705\nVALD: \t Epoch: 123 \t Loss: -0.013257304206490517\nVALD: \t Epoch: 123 \t Loss: -0.012467840686440468\nVALD: \t Epoch: 123 \t Loss: -0.010622274285803238\nVALD: \t Epoch: 123 \t Loss: -0.010585334605561521\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:35<00:00,  9.61it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.3653677020687803  FDE: 0.5287774432593\n**************************************************\n******************************\nEpoch: social-tag : 123\ntrain_loss -0.0105969983745705\nval_loss -0.010585334605561521\n{'min_val_epoch': 123, 'min_val_loss': -0.010585334605561521}\n******************************\nTRAIN: \t Epoch: 124 \t Loss: -0.010651127435266972\nTRAIN: \t Epoch: 124 \t Loss: -0.010792853310704231\nTRAIN: \t Epoch: 124 \t Loss: -0.010988619178533554\nTRAIN: \t Epoch: 124 \t Loss: -0.010622985661029816\nTRAIN: \t Epoch: 124 \t Loss: -0.01050469521433115\nTRAIN: \t Epoch: 124 \t Loss: -0.0103111588396132\nTRAIN: \t Epoch: 124 \t Loss: -0.010292659380606242\nTRAIN: \t Epoch: 124 \t Loss: -0.010414621559903026\nTRAIN: \t Epoch: 124 \t Loss: -0.010430374907122718\nTRAIN: \t Epoch: 124 \t Loss: -0.010443575866520404\nTRAIN: \t Epoch: 124 \t Loss: -0.010465009899979288\nTRAIN: \t Epoch: 124 \t Loss: -0.010346642229706049\nTRAIN: \t Epoch: 124 \t Loss: -0.01023848419292615\nTRAIN: \t Epoch: 124 \t Loss: -0.010168723362897123\nTRAIN: \t Epoch: 124 \t Loss: -0.010237668144206206\nTRAIN: \t Epoch: 124 \t Loss: -0.010320916946511716\nTRAIN: \t Epoch: 124 \t Loss: -0.01038393939873486\nVALD: \t Epoch: 124 \t Loss: -0.012770778499543667\nVALD: \t Epoch: 124 \t Loss: -0.01204295177012682\nVALD: \t Epoch: 124 \t Loss: -0.009874194394797087\nVALD: \t Epoch: 124 \t Loss: -0.00985832735449968\n******************************\nEpoch: social-tag : 124\ntrain_loss -0.01038393939873486\nval_loss -0.00985832735449968\n{'min_val_epoch': 123, 'min_val_loss': -0.010585334605561521}\n******************************\nTRAIN: \t Epoch: 125 \t Loss: -0.010200504213571548\nTRAIN: \t Epoch: 125 \t Loss: -0.010370817501097918\nTRAIN: \t Epoch: 125 \t Loss: -0.0107680040722092\nTRAIN: \t Epoch: 125 \t Loss: -0.010516647482290864\nTRAIN: \t Epoch: 125 \t Loss: -0.010349201411008835\nTRAIN: \t Epoch: 125 \t Loss: -0.010308404763539633\nTRAIN: \t Epoch: 125 \t Loss: -0.01040541912828173\nTRAIN: \t Epoch: 125 \t Loss: -0.010412882664240897\nTRAIN: \t Epoch: 125 \t Loss: -0.01045557235678037\nTRAIN: \t Epoch: 125 \t Loss: -0.010443396028131246\nTRAIN: \t Epoch: 125 \t Loss: -0.010404290123419329\nTRAIN: \t Epoch: 125 \t Loss: -0.010409200874467691\nTRAIN: \t Epoch: 125 \t Loss: -0.010423597020025436\nTRAIN: \t Epoch: 125 \t Loss: -0.010466918282743012\nTRAIN: \t Epoch: 125 \t Loss: -0.010452961424986522\nTRAIN: \t Epoch: 125 \t Loss: -0.0104275171761401\nTRAIN: \t Epoch: 125 \t Loss: -0.01035781441764398\nVALD: \t Epoch: 125 \t Loss: -0.012356108985841274\nVALD: \t Epoch: 125 \t Loss: -0.012076358310878277\nVALD: \t Epoch: 125 \t Loss: -0.010465809454520544\nVALD: \t Epoch: 125 \t Loss: -0.010460053851266584\n******************************\nEpoch: social-tag : 125\ntrain_loss -0.01035781441764398\nval_loss -0.010460053851266584\n{'min_val_epoch': 123, 'min_val_loss': -0.010585334605561521}\n******************************\nTRAIN: \t Epoch: 126 \t Loss: -0.010194363072514534\nTRAIN: \t Epoch: 126 \t Loss: -0.010601964313536882\nTRAIN: \t Epoch: 126 \t Loss: -0.01075434157003959\nTRAIN: \t Epoch: 126 \t Loss: -0.011029987363144755\nTRAIN: \t Epoch: 126 \t Loss: -0.01113582830876112\nTRAIN: \t Epoch: 126 \t Loss: -0.011275064665824175\nTRAIN: \t Epoch: 126 \t Loss: -0.011305315286985465\nTRAIN: \t Epoch: 126 \t Loss: -0.011162936221808195\nTRAIN: \t Epoch: 126 \t Loss: -0.01087040195448531\nTRAIN: \t Epoch: 126 \t Loss: -0.010666241403669119\nTRAIN: \t Epoch: 126 \t Loss: -0.010575196875089949\nTRAIN: \t Epoch: 126 \t Loss: -0.010576114368935427\nTRAIN: \t Epoch: 126 \t Loss: -0.010609191197615404\nTRAIN: \t Epoch: 126 \t Loss: -0.010686647213463272\nTRAIN: \t Epoch: 126 \t Loss: -0.010760742301742236\nTRAIN: \t Epoch: 126 \t Loss: -0.010690524999517947\nTRAIN: \t Epoch: 126 \t Loss: -0.010616926391693678\nVALD: \t Epoch: 126 \t Loss: -0.012629546225070953\nVALD: \t Epoch: 126 \t Loss: -0.011837103869765997\nVALD: \t Epoch: 126 \t Loss: -0.00987153872847557\nVALD: \t Epoch: 126 \t Loss: -0.009969136672105618\n******************************\nEpoch: social-tag : 126\ntrain_loss -0.010616926391693678\nval_loss -0.009969136672105618\n{'min_val_epoch': 123, 'min_val_loss': -0.010585334605561521}\n******************************\nTRAIN: \t Epoch: 127 \t Loss: -0.010485566221177578\nTRAIN: \t Epoch: 127 \t Loss: -0.010759846772998571\nTRAIN: \t Epoch: 127 \t Loss: -0.010854324325919151\nTRAIN: \t Epoch: 127 \t Loss: -0.010878481669351459\nTRAIN: \t Epoch: 127 \t Loss: -0.01079783458262682\nTRAIN: \t Epoch: 127 \t Loss: -0.01028746529482305\nTRAIN: \t Epoch: 127 \t Loss: -0.009969265798905067\nTRAIN: \t Epoch: 127 \t Loss: -0.009971845371183008\nTRAIN: \t Epoch: 127 \t Loss: -0.010032754546652237\nTRAIN: \t Epoch: 127 \t Loss: -0.01018050299026072\nTRAIN: \t Epoch: 127 \t Loss: -0.010322461963038553\nTRAIN: \t Epoch: 127 \t Loss: -0.010411037947051227\nTRAIN: \t Epoch: 127 \t Loss: -0.010493991789049827\nTRAIN: \t Epoch: 127 \t Loss: -0.010533998287948114\nTRAIN: \t Epoch: 127 \t Loss: -0.010523889741549889\nTRAIN: \t Epoch: 127 \t Loss: -0.010402038024039939\nTRAIN: \t Epoch: 127 \t Loss: -0.010365086043197098\nVALD: \t Epoch: 127 \t Loss: -0.011348417960107327\nVALD: \t Epoch: 127 \t Loss: -0.009748175274580717\nVALD: \t Epoch: 127 \t Loss: -0.00714399778128912\nVALD: \t Epoch: 127 \t Loss: -0.007581922495198583\n******************************\nEpoch: social-tag : 127\ntrain_loss -0.010365086043197098\nval_loss -0.007581922495198583\n{'min_val_epoch': 123, 'min_val_loss': -0.010585334605561521}\n******************************\nTRAIN: \t Epoch: 128 \t Loss: -0.010910645127296448\nTRAIN: \t Epoch: 128 \t Loss: -0.011412940919399261\nTRAIN: \t Epoch: 128 \t Loss: -0.011110289332767328\nTRAIN: \t Epoch: 128 \t Loss: -0.011222055414691567\nTRAIN: \t Epoch: 128 \t Loss: -0.011124854907393456\nTRAIN: \t Epoch: 128 \t Loss: -0.010981478573133549\nTRAIN: \t Epoch: 128 \t Loss: -0.010949865648789065\nTRAIN: \t Epoch: 128 \t Loss: -0.010791304521262646\nTRAIN: \t Epoch: 128 \t Loss: -0.01073054327732987\nTRAIN: \t Epoch: 128 \t Loss: -0.010740049090236426\nTRAIN: \t Epoch: 128 \t Loss: -0.010622613385997036\nTRAIN: \t Epoch: 128 \t Loss: -0.010560387435058752\nTRAIN: \t Epoch: 128 \t Loss: -0.010539445596245619\nTRAIN: \t Epoch: 128 \t Loss: -0.010540549377245563\nTRAIN: \t Epoch: 128 \t Loss: -0.010512432580192884\nTRAIN: \t Epoch: 128 \t Loss: -0.010540729272179306\nTRAIN: \t Epoch: 128 \t Loss: -0.010491054166447033\nVALD: \t Epoch: 128 \t Loss: -0.01275652926415205\nVALD: \t Epoch: 128 \t Loss: -0.012174451723694801\nVALD: \t Epoch: 128 \t Loss: -0.010108369092146555\nVALD: \t Epoch: 128 \t Loss: -0.010166190817446527\n******************************\nEpoch: social-tag : 128\ntrain_loss -0.010491054166447033\nval_loss -0.010166190817446527\n{'min_val_epoch': 123, 'min_val_loss': -0.010585334605561521}\n******************************\nTRAIN: \t Epoch: 129 \t Loss: -0.010716691613197327\nTRAIN: \t Epoch: 129 \t Loss: -0.011101713869720697\nTRAIN: \t Epoch: 129 \t Loss: -0.01117626049866279\nTRAIN: \t Epoch: 129 \t Loss: -0.01108019519597292\nTRAIN: \t Epoch: 129 \t Loss: -0.010723194293677806\nTRAIN: \t Epoch: 129 \t Loss: -0.010331865244855484\nTRAIN: \t Epoch: 129 \t Loss: -0.010098913684487343\nTRAIN: \t Epoch: 129 \t Loss: -0.010139398858882487\nTRAIN: \t Epoch: 129 \t Loss: -0.010263596454428302\nTRAIN: \t Epoch: 129 \t Loss: -0.010449084267020226\nTRAIN: \t Epoch: 129 \t Loss: -0.010507076978683472\nTRAIN: \t Epoch: 129 \t Loss: -0.01047947498348852\nTRAIN: \t Epoch: 129 \t Loss: -0.010492382714381585\nTRAIN: \t Epoch: 129 \t Loss: -0.010355512678091015\nTRAIN: \t Epoch: 129 \t Loss: -0.010274034490187963\nTRAIN: \t Epoch: 129 \t Loss: -0.010249894752632827\nTRAIN: \t Epoch: 129 \t Loss: -0.01025415144183419\nVALD: \t Epoch: 129 \t Loss: -0.013030155561864376\nVALD: \t Epoch: 129 \t Loss: -0.012227301020175219\nVALD: \t Epoch: 129 \t Loss: -0.010196411206076542\nVALD: \t Epoch: 129 \t Loss: -0.010181768568689952\n******************************\nEpoch: social-tag : 129\ntrain_loss -0.01025415144183419\nval_loss -0.010181768568689952\n{'min_val_epoch': 123, 'min_val_loss': -0.010585334605561521}\n******************************\nTRAIN: \t Epoch: 130 \t Loss: -0.011606351472437382\nTRAIN: \t Epoch: 130 \t Loss: -0.011428991332650185\nTRAIN: \t Epoch: 130 \t Loss: -0.011137403237322966\nTRAIN: \t Epoch: 130 \t Loss: -0.011158269131556153\nTRAIN: \t Epoch: 130 \t Loss: -0.011059080436825752\nTRAIN: \t Epoch: 130 \t Loss: -0.010772186797112226\nTRAIN: \t Epoch: 130 \t Loss: -0.010614739464862006\nTRAIN: \t Epoch: 130 \t Loss: -0.010442676488310099\nTRAIN: \t Epoch: 130 \t Loss: -0.0105557380658057\nTRAIN: \t Epoch: 130 \t Loss: -0.010685079731047153\nTRAIN: \t Epoch: 130 \t Loss: -0.01073662843555212\nTRAIN: \t Epoch: 130 \t Loss: -0.01076442104143401\nTRAIN: \t Epoch: 130 \t Loss: -0.010741026928791633\nTRAIN: \t Epoch: 130 \t Loss: -0.010674247983843088\nTRAIN: \t Epoch: 130 \t Loss: -0.010571628933151563\nTRAIN: \t Epoch: 130 \t Loss: -0.010484411555808038\nTRAIN: \t Epoch: 130 \t Loss: -0.010471749712120403\nVALD: \t Epoch: 130 \t Loss: -0.012338110245764256\nVALD: \t Epoch: 130 \t Loss: -0.01144072087481618\nVALD: \t Epoch: 130 \t Loss: -0.009804043297966322\nVALD: \t Epoch: 130 \t Loss: -0.00986077614173204\n******************************\nEpoch: social-tag : 130\ntrain_loss -0.010471749712120403\nval_loss -0.00986077614173204\n{'min_val_epoch': 123, 'min_val_loss': -0.010585334605561521}\n******************************\nTRAIN: \t Epoch: 131 \t Loss: -0.011106234043836594\nTRAIN: \t Epoch: 131 \t Loss: -0.011450261808931828\nTRAIN: \t Epoch: 131 \t Loss: -0.011405400310953459\nTRAIN: \t Epoch: 131 \t Loss: -0.011397255584597588\nTRAIN: \t Epoch: 131 \t Loss: -0.011152341961860657\nTRAIN: \t Epoch: 131 \t Loss: -0.010824901207039753\nTRAIN: \t Epoch: 131 \t Loss: -0.010638335320566381\nTRAIN: \t Epoch: 131 \t Loss: -0.010597851709462702\nTRAIN: \t Epoch: 131 \t Loss: -0.010635223964022266\nTRAIN: \t Epoch: 131 \t Loss: -0.010680262185633182\nTRAIN: \t Epoch: 131 \t Loss: -0.010757171633568678\nTRAIN: \t Epoch: 131 \t Loss: -0.010804172682886323\nTRAIN: \t Epoch: 131 \t Loss: -0.010817185402489625\nTRAIN: \t Epoch: 131 \t Loss: -0.010663991660944052\nTRAIN: \t Epoch: 131 \t Loss: -0.010646156966686249\nTRAIN: \t Epoch: 131 \t Loss: -0.010694731667172164\nTRAIN: \t Epoch: 131 \t Loss: -0.01070334068076177\nVALD: \t Epoch: 131 \t Loss: -0.013492587953805923\nVALD: \t Epoch: 131 \t Loss: -0.012648381292819977\nVALD: \t Epoch: 131 \t Loss: -0.010553468639651934\nVALD: \t Epoch: 131 \t Loss: -0.010501543918769517\n******************************\nEpoch: social-tag : 131\ntrain_loss -0.01070334068076177\nval_loss -0.010501543918769517\n{'min_val_epoch': 123, 'min_val_loss': -0.010585334605561521}\n******************************\nTRAIN: \t Epoch: 132 \t Loss: -0.011739355511963367\nTRAIN: \t Epoch: 132 \t Loss: -0.01185321668162942\nTRAIN: \t Epoch: 132 \t Loss: -0.011674284934997559\nTRAIN: \t Epoch: 132 \t Loss: -0.011252403259277344\nTRAIN: \t Epoch: 132 \t Loss: -0.010797486454248429\nTRAIN: \t Epoch: 132 \t Loss: -0.010475062609960636\nTRAIN: \t Epoch: 132 \t Loss: -0.010356841076697623\nTRAIN: \t Epoch: 132 \t Loss: -0.01040661649312824\nTRAIN: \t Epoch: 132 \t Loss: -0.010553390925957097\nTRAIN: \t Epoch: 132 \t Loss: -0.010623902268707752\nTRAIN: \t Epoch: 132 \t Loss: -0.010667176747863943\nTRAIN: \t Epoch: 132 \t Loss: -0.010541245574131608\nTRAIN: \t Epoch: 132 \t Loss: -0.010400191164360596\nTRAIN: \t Epoch: 132 \t Loss: -0.010382535135639566\nTRAIN: \t Epoch: 132 \t Loss: -0.010402395079533258\nTRAIN: \t Epoch: 132 \t Loss: -0.010450186615344137\nTRAIN: \t Epoch: 132 \t Loss: -0.010492001095730247\nVALD: \t Epoch: 132 \t Loss: -0.013097720220685005\nVALD: \t Epoch: 132 \t Loss: -0.012719477992504835\nVALD: \t Epoch: 132 \t Loss: -0.010524035276224216\nVALD: \t Epoch: 132 \t Loss: -0.010586160028766015\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:38<00:00,  9.32it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.3772365519398051  FDE: 0.5651950428842403\n**************************************************\n******************************\nEpoch: social-tag : 132\ntrain_loss -0.010492001095730247\nval_loss -0.010586160028766015\n{'min_val_epoch': 132, 'min_val_loss': -0.010586160028766015}\n******************************\nTRAIN: \t Epoch: 133 \t Loss: -0.011537628248333931\nTRAIN: \t Epoch: 133 \t Loss: -0.011178351938724518\nTRAIN: \t Epoch: 133 \t Loss: -0.011442112115522226\nTRAIN: \t Epoch: 133 \t Loss: -0.010997683508321643\nTRAIN: \t Epoch: 133 \t Loss: -0.0107333118095994\nTRAIN: \t Epoch: 133 \t Loss: -0.01050987932831049\nTRAIN: \t Epoch: 133 \t Loss: -0.010602248034306936\nTRAIN: \t Epoch: 133 \t Loss: -0.01063421810977161\nTRAIN: \t Epoch: 133 \t Loss: -0.010749997778071297\nTRAIN: \t Epoch: 133 \t Loss: -0.010789244901388884\nTRAIN: \t Epoch: 133 \t Loss: -0.010780604640868578\nTRAIN: \t Epoch: 133 \t Loss: -0.01074074332912763\nTRAIN: \t Epoch: 133 \t Loss: -0.010697651367921095\nTRAIN: \t Epoch: 133 \t Loss: -0.010597581309931619\nTRAIN: \t Epoch: 133 \t Loss: -0.010609786212444305\nTRAIN: \t Epoch: 133 \t Loss: -0.010651006596162915\nTRAIN: \t Epoch: 133 \t Loss: -0.010623412488987951\nVALD: \t Epoch: 133 \t Loss: -0.013411330059170723\nVALD: \t Epoch: 133 \t Loss: -0.012765113729983568\nVALD: \t Epoch: 133 \t Loss: -0.010845236014574766\nVALD: \t Epoch: 133 \t Loss: -0.010756680708445475\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:33<00:00,  9.84it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.34545418480284207  FDE: 0.5125674239959317\n**************************************************\n******************************\nEpoch: social-tag : 133\ntrain_loss -0.010623412488987951\nval_loss -0.010756680708445475\n{'min_val_epoch': 133, 'min_val_loss': -0.010756680708445475}\n******************************\nTRAIN: \t Epoch: 134 \t Loss: -0.01139060314744711\nTRAIN: \t Epoch: 134 \t Loss: -0.01103996392339468\nTRAIN: \t Epoch: 134 \t Loss: -0.01135147176682949\nTRAIN: \t Epoch: 134 \t Loss: -0.01126494281925261\nTRAIN: \t Epoch: 134 \t Loss: -0.01123074758797884\nTRAIN: \t Epoch: 134 \t Loss: -0.011238091625273228\nTRAIN: \t Epoch: 134 \t Loss: -0.011238311284354754\nTRAIN: \t Epoch: 134 \t Loss: -0.011256263358518481\nTRAIN: \t Epoch: 134 \t Loss: -0.011091023890508546\nTRAIN: \t Epoch: 134 \t Loss: -0.010852263495326041\nTRAIN: \t Epoch: 134 \t Loss: -0.010776795362207022\nTRAIN: \t Epoch: 134 \t Loss: -0.010812900650004545\nTRAIN: \t Epoch: 134 \t Loss: -0.010860782474852525\nTRAIN: \t Epoch: 134 \t Loss: -0.010931862584714378\nTRAIN: \t Epoch: 134 \t Loss: -0.010959864656130473\nTRAIN: \t Epoch: 134 \t Loss: -0.010960460698697716\nTRAIN: \t Epoch: 134 \t Loss: -0.010954089236981941\nVALD: \t Epoch: 134 \t Loss: -0.013163131661713123\nVALD: \t Epoch: 134 \t Loss: -0.0126710613258183\nVALD: \t Epoch: 134 \t Loss: -0.010616945568472147\nVALD: \t Epoch: 134 \t Loss: -0.010673829062494214\n******************************\nEpoch: social-tag : 134\ntrain_loss -0.010954089236981941\nval_loss -0.010673829062494214\n{'min_val_epoch': 133, 'min_val_loss': -0.010756680708445475}\n******************************\nTRAIN: \t Epoch: 135 \t Loss: -0.01125707384198904\nTRAIN: \t Epoch: 135 \t Loss: -0.011169642675668001\nTRAIN: \t Epoch: 135 \t Loss: -0.011233217703799406\nTRAIN: \t Epoch: 135 \t Loss: -0.01125210989266634\nTRAIN: \t Epoch: 135 \t Loss: -0.01107929702848196\nTRAIN: \t Epoch: 135 \t Loss: -0.010954152482251326\nTRAIN: \t Epoch: 135 \t Loss: -0.010804752819240093\nTRAIN: \t Epoch: 135 \t Loss: -0.010728920809924603\nTRAIN: \t Epoch: 135 \t Loss: -0.010806338240702948\nTRAIN: \t Epoch: 135 \t Loss: -0.010809984151273965\nTRAIN: \t Epoch: 135 \t Loss: -0.010811456394466486\nTRAIN: \t Epoch: 135 \t Loss: -0.010764224377150336\nTRAIN: \t Epoch: 135 \t Loss: -0.01050498429685831\nTRAIN: \t Epoch: 135 \t Loss: -0.010301160426544291\nTRAIN: \t Epoch: 135 \t Loss: -0.010275165239969889\nTRAIN: \t Epoch: 135 \t Loss: -0.010328469274099916\nTRAIN: \t Epoch: 135 \t Loss: -0.010339191437444904\nVALD: \t Epoch: 135 \t Loss: -0.013303764164447784\nVALD: \t Epoch: 135 \t Loss: -0.012278053909540176\nVALD: \t Epoch: 135 \t Loss: -0.010203908508022627\nVALD: \t Epoch: 135 \t Loss: -0.010225394290840316\n******************************\nEpoch: social-tag : 135\ntrain_loss -0.010339191437444904\nval_loss -0.010225394290840316\n{'min_val_epoch': 133, 'min_val_loss': -0.010756680708445475}\n******************************\nTRAIN: \t Epoch: 136 \t Loss: -0.011572576127946377\nTRAIN: \t Epoch: 136 \t Loss: -0.011407441459596157\nTRAIN: \t Epoch: 136 \t Loss: -0.011292569649716219\nTRAIN: \t Epoch: 136 \t Loss: -0.011220674961805344\nTRAIN: \t Epoch: 136 \t Loss: -0.011139019578695297\nTRAIN: \t Epoch: 136 \t Loss: -0.010788577143102884\nTRAIN: \t Epoch: 136 \t Loss: -0.010475012872900282\nTRAIN: \t Epoch: 136 \t Loss: -0.010327496216632426\nTRAIN: \t Epoch: 136 \t Loss: -0.010321718433664905\nTRAIN: \t Epoch: 136 \t Loss: -0.010450490191578865\nTRAIN: \t Epoch: 136 \t Loss: -0.010568906553089619\nTRAIN: \t Epoch: 136 \t Loss: -0.010675730416551232\nTRAIN: \t Epoch: 136 \t Loss: -0.01067837535475309\nTRAIN: \t Epoch: 136 \t Loss: -0.010478920502854245\nTRAIN: \t Epoch: 136 \t Loss: -0.010351717968781789\nTRAIN: \t Epoch: 136 \t Loss: -0.010358872532378882\nTRAIN: \t Epoch: 136 \t Loss: -0.010381056362706604\nVALD: \t Epoch: 136 \t Loss: -0.01274134498089552\nVALD: \t Epoch: 136 \t Loss: -0.011777366045862436\nVALD: \t Epoch: 136 \t Loss: -0.010021346310774485\nVALD: \t Epoch: 136 \t Loss: -0.009997005472164193\n******************************\nEpoch: social-tag : 136\ntrain_loss -0.010381056362706604\nval_loss -0.009997005472164193\n{'min_val_epoch': 133, 'min_val_loss': -0.010756680708445475}\n******************************\nTRAIN: \t Epoch: 137 \t Loss: -0.012197130359709263\nTRAIN: \t Epoch: 137 \t Loss: -0.012242475058883429\nTRAIN: \t Epoch: 137 \t Loss: -0.011792637097338835\nTRAIN: \t Epoch: 137 \t Loss: -0.011803124332800508\nTRAIN: \t Epoch: 137 \t Loss: -0.011413653194904328\nTRAIN: \t Epoch: 137 \t Loss: -0.010885626543313265\nTRAIN: \t Epoch: 137 \t Loss: -0.010687594967229026\nTRAIN: \t Epoch: 137 \t Loss: -0.010611632489599288\nTRAIN: \t Epoch: 137 \t Loss: -0.010565125279956393\nTRAIN: \t Epoch: 137 \t Loss: -0.010592708829790354\nTRAIN: \t Epoch: 137 \t Loss: -0.010647757063535128\nTRAIN: \t Epoch: 137 \t Loss: -0.01071606227196753\nTRAIN: \t Epoch: 137 \t Loss: -0.010787609248207165\nTRAIN: \t Epoch: 137 \t Loss: -0.010626417917332478\nTRAIN: \t Epoch: 137 \t Loss: -0.010557852312922477\nTRAIN: \t Epoch: 137 \t Loss: -0.01056076871464029\nTRAIN: \t Epoch: 137 \t Loss: -0.010572816104148373\nVALD: \t Epoch: 137 \t Loss: -0.013269324786961079\nVALD: \t Epoch: 137 \t Loss: -0.012454546988010406\nVALD: \t Epoch: 137 \t Loss: -0.010571019258350134\nVALD: \t Epoch: 137 \t Loss: -0.010580339950477767\n******************************\nEpoch: social-tag : 137\ntrain_loss -0.010572816104148373\nval_loss -0.010580339950477767\n{'min_val_epoch': 133, 'min_val_loss': -0.010756680708445475}\n******************************\nTRAIN: \t Epoch: 138 \t Loss: -0.010439175181090832\nTRAIN: \t Epoch: 138 \t Loss: -0.01092354441061616\nTRAIN: \t Epoch: 138 \t Loss: -0.010872772273917993\nTRAIN: \t Epoch: 138 \t Loss: -0.0111688117031008\nTRAIN: \t Epoch: 138 \t Loss: -0.011133827082812787\nTRAIN: \t Epoch: 138 \t Loss: -0.010856617552538713\nTRAIN: \t Epoch: 138 \t Loss: -0.010742430708238057\nTRAIN: \t Epoch: 138 \t Loss: -0.010632280725985765\nTRAIN: \t Epoch: 138 \t Loss: -0.01073783367044396\nTRAIN: \t Epoch: 138 \t Loss: -0.010848421603441238\nTRAIN: \t Epoch: 138 \t Loss: -0.010916689241474325\nTRAIN: \t Epoch: 138 \t Loss: -0.010987553279846907\nTRAIN: \t Epoch: 138 \t Loss: -0.010877725381690722\nTRAIN: \t Epoch: 138 \t Loss: -0.010748517499970538\nTRAIN: \t Epoch: 138 \t Loss: -0.01065488886088133\nTRAIN: \t Epoch: 138 \t Loss: -0.010654623853042722\nTRAIN: \t Epoch: 138 \t Loss: -0.01064362470060587\nVALD: \t Epoch: 138 \t Loss: -0.01284028496593237\nVALD: \t Epoch: 138 \t Loss: -0.011767058167606592\nVALD: \t Epoch: 138 \t Loss: -0.009092201013118029\nVALD: \t Epoch: 138 \t Loss: -0.009384597251991074\n******************************\nEpoch: social-tag : 138\ntrain_loss -0.01064362470060587\nval_loss -0.009384597251991074\n{'min_val_epoch': 133, 'min_val_loss': -0.010756680708445475}\n******************************\nTRAIN: \t Epoch: 139 \t Loss: -0.01083388365805149\nTRAIN: \t Epoch: 139 \t Loss: -0.011184782721102238\nTRAIN: \t Epoch: 139 \t Loss: -0.01140560768544674\nTRAIN: \t Epoch: 139 \t Loss: -0.011431457940489054\nTRAIN: \t Epoch: 139 \t Loss: -0.011091307364404201\nTRAIN: \t Epoch: 139 \t Loss: -0.010653762612491846\nTRAIN: \t Epoch: 139 \t Loss: -0.010682396590709686\nTRAIN: \t Epoch: 139 \t Loss: -0.010766571154817939\nTRAIN: \t Epoch: 139 \t Loss: -0.010844535815219084\nTRAIN: \t Epoch: 139 \t Loss: -0.010865046549588442\nTRAIN: \t Epoch: 139 \t Loss: -0.0108965454115109\nTRAIN: \t Epoch: 139 \t Loss: -0.010928334978719553\nTRAIN: \t Epoch: 139 \t Loss: -0.010909441858530045\nTRAIN: \t Epoch: 139 \t Loss: -0.010764377125139748\nTRAIN: \t Epoch: 139 \t Loss: -0.010666705730060736\nTRAIN: \t Epoch: 139 \t Loss: -0.010676781355869025\nTRAIN: \t Epoch: 139 \t Loss: -0.01065418039533225\nVALD: \t Epoch: 139 \t Loss: -0.013163159601390362\nVALD: \t Epoch: 139 \t Loss: -0.01249536033719778\nVALD: \t Epoch: 139 \t Loss: -0.010809265853216251\nVALD: \t Epoch: 139 \t Loss: -0.010758711073450937\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:36<00:00,  9.50it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.360759999037365  FDE: 0.5316548700304576\n**************************************************\n******************************\nEpoch: social-tag : 139\ntrain_loss -0.01065418039533225\nval_loss -0.010758711073450937\n{'min_val_epoch': 139, 'min_val_loss': -0.010758711073450937}\n******************************\nTRAIN: \t Epoch: 140 \t Loss: -0.011566037312150002\nTRAIN: \t Epoch: 140 \t Loss: -0.01160138240084052\nTRAIN: \t Epoch: 140 \t Loss: -0.011730083885292212\nTRAIN: \t Epoch: 140 \t Loss: -0.0118550646584481\nTRAIN: \t Epoch: 140 \t Loss: -0.01173242349177599\nTRAIN: \t Epoch: 140 \t Loss: -0.011470050240556398\nTRAIN: \t Epoch: 140 \t Loss: -0.011270369270018168\nTRAIN: \t Epoch: 140 \t Loss: -0.011046529514715075\nTRAIN: \t Epoch: 140 \t Loss: -0.010936240562134318\nTRAIN: \t Epoch: 140 \t Loss: -0.010934978537261487\nTRAIN: \t Epoch: 140 \t Loss: -0.010958143425258722\nTRAIN: \t Epoch: 140 \t Loss: -0.010999886241431037\nTRAIN: \t Epoch: 140 \t Loss: -0.011028195946262432\nTRAIN: \t Epoch: 140 \t Loss: -0.011006273262734925\nTRAIN: \t Epoch: 140 \t Loss: -0.010910200203458469\nTRAIN: \t Epoch: 140 \t Loss: -0.010855756176169962\nTRAIN: \t Epoch: 140 \t Loss: -0.010851821495276508\nVALD: \t Epoch: 140 \t Loss: -0.012946235947310925\nVALD: \t Epoch: 140 \t Loss: -0.012092063203454018\nVALD: \t Epoch: 140 \t Loss: -0.010227954636017481\nVALD: \t Epoch: 140 \t Loss: -0.010247764949075238\n******************************\nEpoch: social-tag : 140\ntrain_loss -0.010851821495276508\nval_loss -0.010247764949075238\n{'min_val_epoch': 139, 'min_val_loss': -0.010758711073450937}\n******************************\nTRAIN: \t Epoch: 141 \t Loss: -0.011953727342188358\nTRAIN: \t Epoch: 141 \t Loss: -0.01202818425372243\nTRAIN: \t Epoch: 141 \t Loss: -0.011898019351065159\nTRAIN: \t Epoch: 141 \t Loss: -0.011729315156117082\nTRAIN: \t Epoch: 141 \t Loss: -0.011556765623390674\nTRAIN: \t Epoch: 141 \t Loss: -0.011401835829019547\nTRAIN: \t Epoch: 141 \t Loss: -0.011200005853814738\nTRAIN: \t Epoch: 141 \t Loss: -0.011070828652009368\nTRAIN: \t Epoch: 141 \t Loss: -0.01097081984496779\nTRAIN: \t Epoch: 141 \t Loss: -0.01091847661882639\nTRAIN: \t Epoch: 141 \t Loss: -0.010701726783405651\nTRAIN: \t Epoch: 141 \t Loss: -0.010675481675813595\nTRAIN: \t Epoch: 141 \t Loss: -0.010711223603441166\nTRAIN: \t Epoch: 141 \t Loss: -0.010701030625828676\nTRAIN: \t Epoch: 141 \t Loss: -0.01074585219224294\nTRAIN: \t Epoch: 141 \t Loss: -0.010766491293907166\nTRAIN: \t Epoch: 141 \t Loss: -0.01071229689952099\nVALD: \t Epoch: 141 \t Loss: -0.012912621721625328\nVALD: \t Epoch: 141 \t Loss: -0.01209160266444087\nVALD: \t Epoch: 141 \t Loss: -0.010098955749223629\nVALD: \t Epoch: 141 \t Loss: -0.010152025494032992\n******************************\nEpoch: social-tag : 141\ntrain_loss -0.01071229689952099\nval_loss -0.010152025494032992\n{'min_val_epoch': 139, 'min_val_loss': -0.010758711073450937}\n******************************\nTRAIN: \t Epoch: 142 \t Loss: -0.011971745640039444\nTRAIN: \t Epoch: 142 \t Loss: -0.011872939299792051\nTRAIN: \t Epoch: 142 \t Loss: -0.011930204927921295\nTRAIN: \t Epoch: 142 \t Loss: -0.011848940514028072\nTRAIN: \t Epoch: 142 \t Loss: -0.011568151414394379\nTRAIN: \t Epoch: 142 \t Loss: -0.011379318001369635\nTRAIN: \t Epoch: 142 \t Loss: -0.010972050816885062\nTRAIN: \t Epoch: 142 \t Loss: -0.01086312043480575\nTRAIN: \t Epoch: 142 \t Loss: -0.01072055504967769\nTRAIN: \t Epoch: 142 \t Loss: -0.010774681717157364\nTRAIN: \t Epoch: 142 \t Loss: -0.01079723975536498\nTRAIN: \t Epoch: 142 \t Loss: -0.010885953282316526\nTRAIN: \t Epoch: 142 \t Loss: -0.01093898369715764\nTRAIN: \t Epoch: 142 \t Loss: -0.010834729032857078\nTRAIN: \t Epoch: 142 \t Loss: -0.010706496673325698\nTRAIN: \t Epoch: 142 \t Loss: -0.010720254271291196\nTRAIN: \t Epoch: 142 \t Loss: -0.010709618782681046\nVALD: \t Epoch: 142 \t Loss: -0.013280399143695831\nVALD: \t Epoch: 142 \t Loss: -0.012345322873443365\nVALD: \t Epoch: 142 \t Loss: -0.010344458743929863\nVALD: \t Epoch: 142 \t Loss: -0.010364872490812442\n******************************\nEpoch: social-tag : 142\ntrain_loss -0.010709618782681046\nval_loss -0.010364872490812442\n{'min_val_epoch': 139, 'min_val_loss': -0.010758711073450937}\n******************************\nTRAIN: \t Epoch: 143 \t Loss: -0.011742614209651947\nTRAIN: \t Epoch: 143 \t Loss: -0.011933229863643646\nTRAIN: \t Epoch: 143 \t Loss: -0.011796291607121626\nTRAIN: \t Epoch: 143 \t Loss: -0.01125119929201901\nTRAIN: \t Epoch: 143 \t Loss: -0.010834994912147521\nTRAIN: \t Epoch: 143 \t Loss: -0.010584873612970114\nTRAIN: \t Epoch: 143 \t Loss: -0.01060991215386561\nTRAIN: \t Epoch: 143 \t Loss: -0.010803461540490389\nTRAIN: \t Epoch: 143 \t Loss: -0.010893758800294664\nTRAIN: \t Epoch: 143 \t Loss: -0.010931399278342723\nTRAIN: \t Epoch: 143 \t Loss: -0.010940598747269674\nTRAIN: \t Epoch: 143 \t Loss: -0.0110302174774309\nTRAIN: \t Epoch: 143 \t Loss: -0.01096760775320805\nTRAIN: \t Epoch: 143 \t Loss: -0.01089335944769638\nTRAIN: \t Epoch: 143 \t Loss: -0.010844821358720462\nTRAIN: \t Epoch: 143 \t Loss: -0.010813252709340304\nTRAIN: \t Epoch: 143 \t Loss: -0.010791106121332356\nVALD: \t Epoch: 143 \t Loss: -0.013297738507390022\nVALD: \t Epoch: 143 \t Loss: -0.012586415745317936\nVALD: \t Epoch: 143 \t Loss: -0.01080588453138868\nVALD: \t Epoch: 143 \t Loss: -0.01078663864535486\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:32<00:00,  9.91it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.37558828787210247  FDE: 0.5601887021402512\n**************************************************\n******************************\nEpoch: social-tag : 143\ntrain_loss -0.010791106121332356\nval_loss -0.01078663864535486\n{'min_val_epoch': 143, 'min_val_loss': -0.01078663864535486}\n******************************\nTRAIN: \t Epoch: 144 \t Loss: -0.011446867138147354\nTRAIN: \t Epoch: 144 \t Loss: -0.011255053337663412\nTRAIN: \t Epoch: 144 \t Loss: -0.011286536231637001\nTRAIN: \t Epoch: 144 \t Loss: -0.011225313181057572\nTRAIN: \t Epoch: 144 \t Loss: -0.01142372377216816\nTRAIN: \t Epoch: 144 \t Loss: -0.011315927219887575\nTRAIN: \t Epoch: 144 \t Loss: -0.011397494801453181\nTRAIN: \t Epoch: 144 \t Loss: -0.011151601327583194\nTRAIN: \t Epoch: 144 \t Loss: -0.010900546900100179\nTRAIN: \t Epoch: 144 \t Loss: -0.010827207192778587\nTRAIN: \t Epoch: 144 \t Loss: -0.010900781574574385\nTRAIN: \t Epoch: 144 \t Loss: -0.010901285180201134\nTRAIN: \t Epoch: 144 \t Loss: -0.010837300394016963\nTRAIN: \t Epoch: 144 \t Loss: -0.010801092788044895\nTRAIN: \t Epoch: 144 \t Loss: -0.010738298793633779\nTRAIN: \t Epoch: 144 \t Loss: -0.010711893846746534\nTRAIN: \t Epoch: 144 \t Loss: -0.010722936497944775\nVALD: \t Epoch: 144 \t Loss: -0.013283935375511646\nVALD: \t Epoch: 144 \t Loss: -0.012890512123703957\nVALD: \t Epoch: 144 \t Loss: -0.010863733943551779\nVALD: \t Epoch: 144 \t Loss: -0.010836608038690989\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:32<00:00,  9.94it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.35280165861982715  FDE: 0.5304120204469636\n**************************************************\n******************************\nEpoch: social-tag : 144\ntrain_loss -0.010722936497944775\nval_loss -0.010836608038690989\n{'min_val_epoch': 144, 'min_val_loss': -0.010836608038690989}\n******************************\nTRAIN: \t Epoch: 145 \t Loss: -0.010821458883583546\nTRAIN: \t Epoch: 145 \t Loss: -0.011191089637577534\nTRAIN: \t Epoch: 145 \t Loss: -0.011264673744638761\nTRAIN: \t Epoch: 145 \t Loss: -0.011440004920586944\nTRAIN: \t Epoch: 145 \t Loss: -0.011440778337419032\nTRAIN: \t Epoch: 145 \t Loss: -0.01165191705028216\nTRAIN: \t Epoch: 145 \t Loss: -0.011512930771069867\nTRAIN: \t Epoch: 145 \t Loss: -0.011293398682028055\nTRAIN: \t Epoch: 145 \t Loss: -0.011061268548170725\nTRAIN: \t Epoch: 145 \t Loss: -0.010901154391467572\nTRAIN: \t Epoch: 145 \t Loss: -0.010953523387963121\nTRAIN: \t Epoch: 145 \t Loss: -0.010988179283837477\nTRAIN: \t Epoch: 145 \t Loss: -0.011055141615752991\nTRAIN: \t Epoch: 145 \t Loss: -0.011124331504106522\nTRAIN: \t Epoch: 145 \t Loss: -0.01115341695646445\nTRAIN: \t Epoch: 145 \t Loss: -0.011148084537126124\nTRAIN: \t Epoch: 145 \t Loss: -0.011140451117446928\nVALD: \t Epoch: 145 \t Loss: -0.012903198599815369\nVALD: \t Epoch: 145 \t Loss: -0.012728792615234852\nVALD: \t Epoch: 145 \t Loss: -0.010767131888618072\nVALD: \t Epoch: 145 \t Loss: -0.01083258871070877\n******************************\nEpoch: social-tag : 145\ntrain_loss -0.011140451117446928\nval_loss -0.01083258871070877\n{'min_val_epoch': 144, 'min_val_loss': -0.010836608038690989}\n******************************\nTRAIN: \t Epoch: 146 \t Loss: -0.012201184406876564\nTRAIN: \t Epoch: 146 \t Loss: -0.01197447907179594\nTRAIN: \t Epoch: 146 \t Loss: -0.01144698386391004\nTRAIN: \t Epoch: 146 \t Loss: -0.010618499014526606\nTRAIN: \t Epoch: 146 \t Loss: -0.010349839366972447\nTRAIN: \t Epoch: 146 \t Loss: -0.01030928393205007\nTRAIN: \t Epoch: 146 \t Loss: -0.010485448209302766\nTRAIN: \t Epoch: 146 \t Loss: -0.01059781201183796\nTRAIN: \t Epoch: 146 \t Loss: -0.010613540394438637\nTRAIN: \t Epoch: 146 \t Loss: -0.010751030500978231\nTRAIN: \t Epoch: 146 \t Loss: -0.010672552934424444\nTRAIN: \t Epoch: 146 \t Loss: -0.010627450111011663\nTRAIN: \t Epoch: 146 \t Loss: -0.010622831276403023\nTRAIN: \t Epoch: 146 \t Loss: -0.010592287605894464\nTRAIN: \t Epoch: 146 \t Loss: -0.010606859996914863\nTRAIN: \t Epoch: 146 \t Loss: -0.010647006158251315\nTRAIN: \t Epoch: 146 \t Loss: -0.010688482811956695\nVALD: \t Epoch: 146 \t Loss: -0.013510048389434814\nVALD: \t Epoch: 146 \t Loss: -0.012962088454514742\nVALD: \t Epoch: 146 \t Loss: -0.01143569095681111\nVALD: \t Epoch: 146 \t Loss: -0.011253198225817043\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:32<00:00,  9.98it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.3371639766362564  FDE: 0.5089386925457643\n**************************************************\n******************************\nEpoch: social-tag : 146\ntrain_loss -0.010688482811956695\nval_loss -0.011253198225817043\n{'min_val_epoch': 146, 'min_val_loss': -0.011253198225817043}\n******************************\nTRAIN: \t Epoch: 147 \t Loss: -0.011536849662661552\nTRAIN: \t Epoch: 147 \t Loss: -0.011886742431670427\nTRAIN: \t Epoch: 147 \t Loss: -0.0120589475457867\nTRAIN: \t Epoch: 147 \t Loss: -0.011836104094982147\nTRAIN: \t Epoch: 147 \t Loss: -0.011853997223079204\nTRAIN: \t Epoch: 147 \t Loss: -0.011613231307516495\nTRAIN: \t Epoch: 147 \t Loss: -0.011474782467952796\nTRAIN: \t Epoch: 147 \t Loss: -0.01141952385660261\nTRAIN: \t Epoch: 147 \t Loss: -0.011376372228066126\nTRAIN: \t Epoch: 147 \t Loss: -0.011392825469374656\nTRAIN: \t Epoch: 147 \t Loss: -0.011366927318952301\nTRAIN: \t Epoch: 147 \t Loss: -0.011224672275905808\nTRAIN: \t Epoch: 147 \t Loss: -0.011016295649684392\nTRAIN: \t Epoch: 147 \t Loss: -0.010911350338054555\nTRAIN: \t Epoch: 147 \t Loss: -0.010876247783501942\nTRAIN: \t Epoch: 147 \t Loss: -0.010905382281634957\nTRAIN: \t Epoch: 147 \t Loss: -0.01090013560359225\nVALD: \t Epoch: 147 \t Loss: -0.013576088473200798\nVALD: \t Epoch: 147 \t Loss: -0.012596298474818468\nVALD: \t Epoch: 147 \t Loss: -0.010408747786035141\nVALD: \t Epoch: 147 \t Loss: -0.010466567175592967\n******************************\nEpoch: social-tag : 147\ntrain_loss -0.01090013560359225\nval_loss -0.010466567175592967\n{'min_val_epoch': 146, 'min_val_loss': -0.011253198225817043}\n******************************\nTRAIN: \t Epoch: 148 \t Loss: -0.011783934198319912\nTRAIN: \t Epoch: 148 \t Loss: -0.0118692796677351\nTRAIN: \t Epoch: 148 \t Loss: -0.011960450870295366\nTRAIN: \t Epoch: 148 \t Loss: -0.01172281475737691\nTRAIN: \t Epoch: 148 \t Loss: -0.011417420022189618\nTRAIN: \t Epoch: 148 \t Loss: -0.010829338027785221\nTRAIN: \t Epoch: 148 \t Loss: -0.010793793414320265\nTRAIN: \t Epoch: 148 \t Loss: -0.010785398888401687\nTRAIN: \t Epoch: 148 \t Loss: -0.010861506168213155\nTRAIN: \t Epoch: 148 \t Loss: -0.010942092537879944\nTRAIN: \t Epoch: 148 \t Loss: -0.010970185172151436\nTRAIN: \t Epoch: 148 \t Loss: -0.011004823880891005\nTRAIN: \t Epoch: 148 \t Loss: -0.010905329424601335\nTRAIN: \t Epoch: 148 \t Loss: -0.01079687669074961\nTRAIN: \t Epoch: 148 \t Loss: -0.010825986228883266\nTRAIN: \t Epoch: 148 \t Loss: -0.01077918749069795\nTRAIN: \t Epoch: 148 \t Loss: -0.010777178615557425\nVALD: \t Epoch: 148 \t Loss: -0.013368608430027962\nVALD: \t Epoch: 148 \t Loss: -0.012912148144096136\nVALD: \t Epoch: 148 \t Loss: -0.011244820120433966\nVALD: \t Epoch: 148 \t Loss: -0.011174389940060066\n******************************\nEpoch: social-tag : 148\ntrain_loss -0.010777178615557425\nval_loss -0.011174389940060066\n{'min_val_epoch': 146, 'min_val_loss': -0.011253198225817043}\n******************************\nTRAIN: \t Epoch: 149 \t Loss: -0.01248269435018301\nTRAIN: \t Epoch: 149 \t Loss: -0.01190232951194048\nTRAIN: \t Epoch: 149 \t Loss: -0.011912425669531027\nTRAIN: \t Epoch: 149 \t Loss: -0.011964787496253848\nTRAIN: \t Epoch: 149 \t Loss: -0.011932565085589885\nTRAIN: \t Epoch: 149 \t Loss: -0.011555323222031197\nTRAIN: \t Epoch: 149 \t Loss: -0.011221290699073247\nTRAIN: \t Epoch: 149 \t Loss: -0.01110209443140775\nTRAIN: \t Epoch: 149 \t Loss: -0.011113659685684575\nTRAIN: \t Epoch: 149 \t Loss: -0.011008946876972913\nTRAIN: \t Epoch: 149 \t Loss: -0.011088076301596382\nTRAIN: \t Epoch: 149 \t Loss: -0.011129985563457012\nTRAIN: \t Epoch: 149 \t Loss: -0.011060938382377991\nTRAIN: \t Epoch: 149 \t Loss: -0.010998559650033712\nTRAIN: \t Epoch: 149 \t Loss: -0.01083104300002257\nTRAIN: \t Epoch: 149 \t Loss: -0.010814062785357237\nTRAIN: \t Epoch: 149 \t Loss: -0.010836224353900461\nVALD: \t Epoch: 149 \t Loss: -0.01274560485035181\nVALD: \t Epoch: 149 \t Loss: -0.012015410233289003\nVALD: \t Epoch: 149 \t Loss: -0.009612777115156254\nVALD: \t Epoch: 149 \t Loss: -0.009750065094458605\n******************************\nEpoch: social-tag : 149\ntrain_loss -0.010836224353900461\nval_loss -0.009750065094458605\n{'min_val_epoch': 146, 'min_val_loss': -0.011253198225817043}\n******************************\nTRAIN: \t Epoch: 150 \t Loss: -0.012187386862933636\nTRAIN: \t Epoch: 150 \t Loss: -0.011556652840226889\nTRAIN: \t Epoch: 150 \t Loss: -0.01173131943990787\nTRAIN: \t Epoch: 150 \t Loss: -0.011881652753800154\nTRAIN: \t Epoch: 150 \t Loss: -0.011775272153317928\nTRAIN: \t Epoch: 150 \t Loss: -0.011831719583521286\nTRAIN: \t Epoch: 150 \t Loss: -0.01181848839457546\nTRAIN: \t Epoch: 150 \t Loss: -0.011893258546479046\nTRAIN: \t Epoch: 150 \t Loss: -0.011837695518301593\nTRAIN: \t Epoch: 150 \t Loss: -0.011801030300557614\nTRAIN: \t Epoch: 150 \t Loss: -0.011852029880339449\nTRAIN: \t Epoch: 150 \t Loss: -0.011918529014413556\nTRAIN: \t Epoch: 150 \t Loss: -0.011923455418302463\nTRAIN: \t Epoch: 150 \t Loss: -0.011938540863671474\nTRAIN: \t Epoch: 150 \t Loss: -0.01184385574112336\nTRAIN: \t Epoch: 150 \t Loss: -0.011762444861233234\nTRAIN: \t Epoch: 150 \t Loss: -0.011782415352310196\nVALD: \t Epoch: 150 \t Loss: -0.013229507021605968\nVALD: \t Epoch: 150 \t Loss: -0.01217965129762888\nVALD: \t Epoch: 150 \t Loss: -0.010051791400959095\nVALD: \t Epoch: 150 \t Loss: -0.010127966632386168\n******************************\nEpoch: social-tag : 150\ntrain_loss -0.011782415352310196\nval_loss -0.010127966632386168\n{'min_val_epoch': 146, 'min_val_loss': -0.011253198225817043}\n******************************\nTRAIN: \t Epoch: 151 \t Loss: -0.012116489000618458\nTRAIN: \t Epoch: 151 \t Loss: -0.012056588660925627\nTRAIN: \t Epoch: 151 \t Loss: -0.012008549955983957\nTRAIN: \t Epoch: 151 \t Loss: -0.012139209313318133\nTRAIN: \t Epoch: 151 \t Loss: -0.012165575474500655\nTRAIN: \t Epoch: 151 \t Loss: -0.012162407860159874\nTRAIN: \t Epoch: 151 \t Loss: -0.012099775618740491\nTRAIN: \t Epoch: 151 \t Loss: -0.012177120079286397\nTRAIN: \t Epoch: 151 \t Loss: -0.012143922969698906\nTRAIN: \t Epoch: 151 \t Loss: -0.012152802292257547\nTRAIN: \t Epoch: 151 \t Loss: -0.012049816887487064\nTRAIN: \t Epoch: 151 \t Loss: -0.012048269078756372\nTRAIN: \t Epoch: 151 \t Loss: -0.01200457733984177\nTRAIN: \t Epoch: 151 \t Loss: -0.01193275728396007\nTRAIN: \t Epoch: 151 \t Loss: -0.011895670183002948\nTRAIN: \t Epoch: 151 \t Loss: -0.011923187295906246\nTRAIN: \t Epoch: 151 \t Loss: -0.011899447830563242\nVALD: \t Epoch: 151 \t Loss: -0.013925537467002869\nVALD: \t Epoch: 151 \t Loss: -0.012970268726348877\nVALD: \t Epoch: 151 \t Loss: -0.010387066596498093\nVALD: \t Epoch: 151 \t Loss: -0.01046967970396944\n******************************\nEpoch: social-tag : 151\ntrain_loss -0.011899447830563242\nval_loss -0.01046967970396944\n{'min_val_epoch': 146, 'min_val_loss': -0.011253198225817043}\n******************************\nTRAIN: \t Epoch: 152 \t Loss: -0.01208560448139906\nTRAIN: \t Epoch: 152 \t Loss: -0.011807970702648163\nTRAIN: \t Epoch: 152 \t Loss: -0.012115946039557457\nTRAIN: \t Epoch: 152 \t Loss: -0.012052032630890608\nTRAIN: \t Epoch: 152 \t Loss: -0.012026935629546643\nTRAIN: \t Epoch: 152 \t Loss: -0.012029964321603378\nTRAIN: \t Epoch: 152 \t Loss: -0.011913832144013472\nTRAIN: \t Epoch: 152 \t Loss: -0.011875643394887447\nTRAIN: \t Epoch: 152 \t Loss: -0.011945782643225458\nTRAIN: \t Epoch: 152 \t Loss: -0.011903603374958039\nTRAIN: \t Epoch: 152 \t Loss: -0.011911672794006088\nTRAIN: \t Epoch: 152 \t Loss: -0.011956193406755725\nTRAIN: \t Epoch: 152 \t Loss: -0.011944109072478918\nTRAIN: \t Epoch: 152 \t Loss: -0.011984984323914562\nTRAIN: \t Epoch: 152 \t Loss: -0.011946142154435317\nTRAIN: \t Epoch: 152 \t Loss: -0.011921452009119093\nTRAIN: \t Epoch: 152 \t Loss: -0.011925205447231278\nVALD: \t Epoch: 152 \t Loss: -0.0141246123239398\nVALD: \t Epoch: 152 \t Loss: -0.013359623495489359\nVALD: \t Epoch: 152 \t Loss: -0.01119413428629438\nVALD: \t Epoch: 152 \t Loss: -0.011090652790374148\n******************************\nEpoch: social-tag : 152\ntrain_loss -0.011925205447231278\nval_loss -0.011090652790374148\n{'min_val_epoch': 146, 'min_val_loss': -0.011253198225817043}\n******************************\nTRAIN: \t Epoch: 153 \t Loss: -0.012523443438112736\nTRAIN: \t Epoch: 153 \t Loss: -0.0124714570119977\nTRAIN: \t Epoch: 153 \t Loss: -0.012441374671955904\nTRAIN: \t Epoch: 153 \t Loss: -0.012407997390255332\nTRAIN: \t Epoch: 153 \t Loss: -0.012302850559353828\nTRAIN: \t Epoch: 153 \t Loss: -0.01228034837792317\nTRAIN: \t Epoch: 153 \t Loss: -0.011991735813873155\nTRAIN: \t Epoch: 153 \t Loss: -0.01210117049049586\nTRAIN: \t Epoch: 153 \t Loss: -0.012048550467524264\nTRAIN: \t Epoch: 153 \t Loss: -0.011934219114482402\nTRAIN: \t Epoch: 153 \t Loss: -0.011972329917956482\nTRAIN: \t Epoch: 153 \t Loss: -0.011944240580002466\nTRAIN: \t Epoch: 153 \t Loss: -0.011958293760052094\nTRAIN: \t Epoch: 153 \t Loss: -0.011988725579742874\nTRAIN: \t Epoch: 153 \t Loss: -0.011938678411146006\nTRAIN: \t Epoch: 153 \t Loss: -0.011971480736974627\nTRAIN: \t Epoch: 153 \t Loss: -0.011954654741919401\nVALD: \t Epoch: 153 \t Loss: -0.013976141810417175\nVALD: \t Epoch: 153 \t Loss: -0.01332610473036766\nVALD: \t Epoch: 153 \t Loss: -0.011482045675317446\nVALD: \t Epoch: 153 \t Loss: -0.011299880202896819\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:32<00:00,  9.93it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.3453811903617901  FDE: 0.5335610558910834\n**************************************************\n******************************\nEpoch: social-tag : 153\ntrain_loss -0.011954654741919401\nval_loss -0.011299880202896819\n{'min_val_epoch': 153, 'min_val_loss': -0.011299880202896819}\n******************************\nTRAIN: \t Epoch: 154 \t Loss: -0.012275638058781624\nTRAIN: \t Epoch: 154 \t Loss: -0.011857236735522747\nTRAIN: \t Epoch: 154 \t Loss: -0.011715567049880823\nTRAIN: \t Epoch: 154 \t Loss: -0.011471289908513427\nTRAIN: \t Epoch: 154 \t Loss: -0.011372689343988895\nTRAIN: \t Epoch: 154 \t Loss: -0.011434624437242746\nTRAIN: \t Epoch: 154 \t Loss: -0.011596304630594594\nTRAIN: \t Epoch: 154 \t Loss: -0.01170072762761265\nTRAIN: \t Epoch: 154 \t Loss: -0.011746433563530445\nTRAIN: \t Epoch: 154 \t Loss: -0.01179378954693675\nTRAIN: \t Epoch: 154 \t Loss: -0.011862680823965506\nTRAIN: \t Epoch: 154 \t Loss: -0.011885828338563442\nTRAIN: \t Epoch: 154 \t Loss: -0.011973427035487615\nTRAIN: \t Epoch: 154 \t Loss: -0.012022611086389847\nTRAIN: \t Epoch: 154 \t Loss: -0.01202395608027776\nTRAIN: \t Epoch: 154 \t Loss: -0.012005769007373601\nTRAIN: \t Epoch: 154 \t Loss: -0.011970609849826857\nVALD: \t Epoch: 154 \t Loss: -0.014056837186217308\nVALD: \t Epoch: 154 \t Loss: -0.013510267715901136\nVALD: \t Epoch: 154 \t Loss: -0.01165561160693566\nVALD: \t Epoch: 154 \t Loss: -0.01143932866003223\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:32<00:00,  9.97it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.33342175177738576  FDE: 0.520677364512073\n**************************************************\n******************************\nEpoch: social-tag : 154\ntrain_loss -0.011970609849826857\nval_loss -0.01143932866003223\n{'min_val_epoch': 154, 'min_val_loss': -0.01143932866003223}\n******************************\nTRAIN: \t Epoch: 155 \t Loss: -0.011287621222436428\nTRAIN: \t Epoch: 155 \t Loss: -0.011898234952241182\nTRAIN: \t Epoch: 155 \t Loss: -0.011758979099492231\nTRAIN: \t Epoch: 155 \t Loss: -0.011980140581727028\nTRAIN: \t Epoch: 155 \t Loss: -0.011903367191553115\nTRAIN: \t Epoch: 155 \t Loss: -0.012057649282117685\nTRAIN: \t Epoch: 155 \t Loss: -0.01209843744124685\nTRAIN: \t Epoch: 155 \t Loss: -0.011957861133851111\nTRAIN: \t Epoch: 155 \t Loss: -0.011977761776910888\nTRAIN: \t Epoch: 155 \t Loss: -0.011906022019684315\nTRAIN: \t Epoch: 155 \t Loss: -0.012009484439410946\nTRAIN: \t Epoch: 155 \t Loss: -0.012021317922820648\nTRAIN: \t Epoch: 155 \t Loss: -0.012048257801395196\nTRAIN: \t Epoch: 155 \t Loss: -0.012026234995573759\nTRAIN: \t Epoch: 155 \t Loss: -0.012036181551714738\nTRAIN: \t Epoch: 155 \t Loss: -0.01195928012020886\nTRAIN: \t Epoch: 155 \t Loss: -0.01199065318161791\nVALD: \t Epoch: 155 \t Loss: -0.013871965929865837\nVALD: \t Epoch: 155 \t Loss: -0.013384426012635231\nVALD: \t Epoch: 155 \t Loss: -0.011572750906149546\nVALD: \t Epoch: 155 \t Loss: -0.011456328237841942\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:32<00:00,  9.94it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.36068577118878287  FDE: 0.561445185668614\n**************************************************\n******************************\nEpoch: social-tag : 155\ntrain_loss -0.01199065318161791\nval_loss -0.011456328237841942\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 156 \t Loss: -0.012114848010241985\nTRAIN: \t Epoch: 156 \t Loss: -0.012275583110749722\nTRAIN: \t Epoch: 156 \t Loss: -0.011991746413211027\nTRAIN: \t Epoch: 156 \t Loss: -0.011918435338884592\nTRAIN: \t Epoch: 156 \t Loss: -0.011872122436761856\nTRAIN: \t Epoch: 156 \t Loss: -0.011739937899013361\nTRAIN: \t Epoch: 156 \t Loss: -0.011853623603071486\nTRAIN: \t Epoch: 156 \t Loss: -0.011774281971156597\nTRAIN: \t Epoch: 156 \t Loss: -0.01189630375140243\nTRAIN: \t Epoch: 156 \t Loss: -0.011942301876842976\nTRAIN: \t Epoch: 156 \t Loss: -0.012000454166396097\nTRAIN: \t Epoch: 156 \t Loss: -0.01200486874828736\nTRAIN: \t Epoch: 156 \t Loss: -0.011976382265297266\nTRAIN: \t Epoch: 156 \t Loss: -0.01205721530797226\nTRAIN: \t Epoch: 156 \t Loss: -0.012090175847212474\nTRAIN: \t Epoch: 156 \t Loss: -0.012015793414320797\nTRAIN: \t Epoch: 156 \t Loss: -0.01198177287975947\nVALD: \t Epoch: 156 \t Loss: -0.013586965389549732\nVALD: \t Epoch: 156 \t Loss: -0.013110088184475899\nVALD: \t Epoch: 156 \t Loss: -0.011073706982036432\nVALD: \t Epoch: 156 \t Loss: -0.011083733059926899\n******************************\nEpoch: social-tag : 156\ntrain_loss -0.01198177287975947\nval_loss -0.011083733059926899\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 157 \t Loss: -0.011964036151766777\nTRAIN: \t Epoch: 157 \t Loss: -0.012334098108112812\nTRAIN: \t Epoch: 157 \t Loss: -0.012398764801522097\nTRAIN: \t Epoch: 157 \t Loss: -0.012507818639278412\nTRAIN: \t Epoch: 157 \t Loss: -0.01215925719588995\nTRAIN: \t Epoch: 157 \t Loss: -0.012166004162281752\nTRAIN: \t Epoch: 157 \t Loss: -0.012171535619667597\nTRAIN: \t Epoch: 157 \t Loss: -0.012131096213124692\nTRAIN: \t Epoch: 157 \t Loss: -0.012204843676752515\nTRAIN: \t Epoch: 157 \t Loss: -0.01216622730717063\nTRAIN: \t Epoch: 157 \t Loss: -0.012116322039880535\nTRAIN: \t Epoch: 157 \t Loss: -0.01205654201718668\nTRAIN: \t Epoch: 157 \t Loss: -0.01208242609237249\nTRAIN: \t Epoch: 157 \t Loss: -0.01201925240457058\nTRAIN: \t Epoch: 157 \t Loss: -0.011984364998837312\nTRAIN: \t Epoch: 157 \t Loss: -0.011998049158137292\nTRAIN: \t Epoch: 157 \t Loss: -0.011996417673248234\nVALD: \t Epoch: 157 \t Loss: -0.013550499454140663\nVALD: \t Epoch: 157 \t Loss: -0.012425974011421204\nVALD: \t Epoch: 157 \t Loss: -0.010160291567444801\nVALD: \t Epoch: 157 \t Loss: -0.010193274882501232\n******************************\nEpoch: social-tag : 157\ntrain_loss -0.011996417673248234\nval_loss -0.010193274882501232\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 158 \t Loss: -0.012313422746956348\nTRAIN: \t Epoch: 158 \t Loss: -0.012412454932928085\nTRAIN: \t Epoch: 158 \t Loss: -0.01222116562227408\nTRAIN: \t Epoch: 158 \t Loss: -0.012119450606405735\nTRAIN: \t Epoch: 158 \t Loss: -0.011983183026313782\nTRAIN: \t Epoch: 158 \t Loss: -0.01191293215379119\nTRAIN: \t Epoch: 158 \t Loss: -0.011800157837569714\nTRAIN: \t Epoch: 158 \t Loss: -0.011894331662915647\nTRAIN: \t Epoch: 158 \t Loss: -0.011878625179330507\nTRAIN: \t Epoch: 158 \t Loss: -0.011818539816886187\nTRAIN: \t Epoch: 158 \t Loss: -0.011931047520854256\nTRAIN: \t Epoch: 158 \t Loss: -0.011959193429599205\nTRAIN: \t Epoch: 158 \t Loss: -0.011951068989359416\nTRAIN: \t Epoch: 158 \t Loss: -0.011953858858240502\nTRAIN: \t Epoch: 158 \t Loss: -0.01196500218162934\nTRAIN: \t Epoch: 158 \t Loss: -0.011966381047386676\nTRAIN: \t Epoch: 158 \t Loss: -0.011995364601413408\nVALD: \t Epoch: 158 \t Loss: -0.013898074626922607\nVALD: \t Epoch: 158 \t Loss: -0.013037411961704493\nVALD: \t Epoch: 158 \t Loss: -0.010588775699337324\nVALD: \t Epoch: 158 \t Loss: -0.01058851625629052\n******************************\nEpoch: social-tag : 158\ntrain_loss -0.011995364601413408\nval_loss -0.01058851625629052\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 159 \t Loss: -0.01086403988301754\nTRAIN: \t Epoch: 159 \t Loss: -0.011141571216285229\nTRAIN: \t Epoch: 159 \t Loss: -0.011681270164748033\nTRAIN: \t Epoch: 159 \t Loss: -0.011914180591702461\nTRAIN: \t Epoch: 159 \t Loss: -0.011899889260530472\nTRAIN: \t Epoch: 159 \t Loss: -0.012006338530530533\nTRAIN: \t Epoch: 159 \t Loss: -0.012073281620229994\nTRAIN: \t Epoch: 159 \t Loss: -0.011988674872554839\nTRAIN: \t Epoch: 159 \t Loss: -0.01206273813214567\nTRAIN: \t Epoch: 159 \t Loss: -0.012135702185332775\nTRAIN: \t Epoch: 159 \t Loss: -0.0121147424321283\nTRAIN: \t Epoch: 159 \t Loss: -0.012116897307957212\nTRAIN: \t Epoch: 159 \t Loss: -0.012085141184238287\nTRAIN: \t Epoch: 159 \t Loss: -0.01206144956605775\nTRAIN: \t Epoch: 159 \t Loss: -0.012044984847307205\nTRAIN: \t Epoch: 159 \t Loss: -0.01202332612592727\nTRAIN: \t Epoch: 159 \t Loss: -0.012013691251702381\nVALD: \t Epoch: 159 \t Loss: -0.013713059946894646\nVALD: \t Epoch: 159 \t Loss: -0.012632953468710184\nVALD: \t Epoch: 159 \t Loss: -0.010062997850279013\nVALD: \t Epoch: 159 \t Loss: -0.010219570881354357\n******************************\nEpoch: social-tag : 159\ntrain_loss -0.012013691251702381\nval_loss -0.010219570881354357\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 160 \t Loss: -0.01164177805185318\nTRAIN: \t Epoch: 160 \t Loss: -0.011776991654187441\nTRAIN: \t Epoch: 160 \t Loss: -0.011903136347730955\nTRAIN: \t Epoch: 160 \t Loss: -0.011792086530476809\nTRAIN: \t Epoch: 160 \t Loss: -0.01210419312119484\nTRAIN: \t Epoch: 160 \t Loss: -0.012014057642469803\nTRAIN: \t Epoch: 160 \t Loss: -0.012118313860680376\nTRAIN: \t Epoch: 160 \t Loss: -0.01211365102790296\nTRAIN: \t Epoch: 160 \t Loss: -0.012204294610354636\nTRAIN: \t Epoch: 160 \t Loss: -0.012218944169580937\nTRAIN: \t Epoch: 160 \t Loss: -0.012137499129907652\nTRAIN: \t Epoch: 160 \t Loss: -0.012093698838725686\nTRAIN: \t Epoch: 160 \t Loss: -0.012039297260344028\nTRAIN: \t Epoch: 160 \t Loss: -0.012067729806793588\nTRAIN: \t Epoch: 160 \t Loss: -0.012083273008465767\nTRAIN: \t Epoch: 160 \t Loss: -0.012015711341518909\nTRAIN: \t Epoch: 160 \t Loss: -0.012019449873178295\nVALD: \t Epoch: 160 \t Loss: -0.014170033857226372\nVALD: \t Epoch: 160 \t Loss: -0.013485617935657501\nVALD: \t Epoch: 160 \t Loss: -0.01124178757891059\nVALD: \t Epoch: 160 \t Loss: -0.011187768149042796\n******************************\nEpoch: social-tag : 160\ntrain_loss -0.012019449873178295\nval_loss -0.011187768149042796\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 161 \t Loss: -0.011707320809364319\nTRAIN: \t Epoch: 161 \t Loss: -0.011809143703430891\nTRAIN: \t Epoch: 161 \t Loss: -0.012121083214879036\nTRAIN: \t Epoch: 161 \t Loss: -0.012303782161325216\nTRAIN: \t Epoch: 161 \t Loss: -0.012166079506278038\nTRAIN: \t Epoch: 161 \t Loss: -0.011971308073649803\nTRAIN: \t Epoch: 161 \t Loss: -0.012025753435279642\nTRAIN: \t Epoch: 161 \t Loss: -0.012046910240314901\nTRAIN: \t Epoch: 161 \t Loss: -0.011955350120034482\nTRAIN: \t Epoch: 161 \t Loss: -0.011935922782868147\nTRAIN: \t Epoch: 161 \t Loss: -0.012012305500155146\nTRAIN: \t Epoch: 161 \t Loss: -0.012022947116444508\nTRAIN: \t Epoch: 161 \t Loss: -0.011998142832173752\nTRAIN: \t Epoch: 161 \t Loss: -0.011980230587401561\nTRAIN: \t Epoch: 161 \t Loss: -0.011995247565209866\nTRAIN: \t Epoch: 161 \t Loss: -0.011992748128250241\nTRAIN: \t Epoch: 161 \t Loss: -0.012022238394076174\nVALD: \t Epoch: 161 \t Loss: -0.013660231605172157\nVALD: \t Epoch: 161 \t Loss: -0.012455703690648079\nVALD: \t Epoch: 161 \t Loss: -0.00943664344958961\nVALD: \t Epoch: 161 \t Loss: -0.009633657758583328\n******************************\nEpoch: social-tag : 161\ntrain_loss -0.012022238394076174\nval_loss -0.009633657758583328\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 162 \t Loss: -0.01217549666762352\nTRAIN: \t Epoch: 162 \t Loss: -0.012290838174521923\nTRAIN: \t Epoch: 162 \t Loss: -0.012295758041242758\nTRAIN: \t Epoch: 162 \t Loss: -0.012271013110876083\nTRAIN: \t Epoch: 162 \t Loss: -0.012331541627645493\nTRAIN: \t Epoch: 162 \t Loss: -0.012087797125180563\nTRAIN: \t Epoch: 162 \t Loss: -0.012057816343648093\nTRAIN: \t Epoch: 162 \t Loss: -0.012125836685299873\nTRAIN: \t Epoch: 162 \t Loss: -0.012036992030011283\nTRAIN: \t Epoch: 162 \t Loss: -0.012053839117288589\nTRAIN: \t Epoch: 162 \t Loss: -0.01200722801414403\nTRAIN: \t Epoch: 162 \t Loss: -0.012035019618148604\nTRAIN: \t Epoch: 162 \t Loss: -0.011972837006816497\nTRAIN: \t Epoch: 162 \t Loss: -0.01199366504858647\nTRAIN: \t Epoch: 162 \t Loss: -0.012037212401628495\nTRAIN: \t Epoch: 162 \t Loss: -0.012047465017531067\nTRAIN: \t Epoch: 162 \t Loss: -0.01207582697723851\nVALD: \t Epoch: 162 \t Loss: -0.013771952129900455\nVALD: \t Epoch: 162 \t Loss: -0.012501449324190617\nVALD: \t Epoch: 162 \t Loss: -0.010033037047833204\nVALD: \t Epoch: 162 \t Loss: -0.010078051489984204\n******************************\nEpoch: social-tag : 162\ntrain_loss -0.01207582697723851\nval_loss -0.010078051489984204\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 163 \t Loss: -0.010974030010402203\nTRAIN: \t Epoch: 163 \t Loss: -0.011645759921520948\nTRAIN: \t Epoch: 163 \t Loss: -0.01173536665737629\nTRAIN: \t Epoch: 163 \t Loss: -0.011640361975878477\nTRAIN: \t Epoch: 163 \t Loss: -0.011838749051094055\nTRAIN: \t Epoch: 163 \t Loss: -0.01182871637865901\nTRAIN: \t Epoch: 163 \t Loss: -0.011917326865451676\nTRAIN: \t Epoch: 163 \t Loss: -0.01183170429430902\nTRAIN: \t Epoch: 163 \t Loss: -0.011812715170284113\nTRAIN: \t Epoch: 163 \t Loss: -0.011775841657072305\nTRAIN: \t Epoch: 163 \t Loss: -0.011725753884423862\nTRAIN: \t Epoch: 163 \t Loss: -0.011775868634382883\nTRAIN: \t Epoch: 163 \t Loss: -0.011874797324148508\nTRAIN: \t Epoch: 163 \t Loss: -0.011933956827436174\nTRAIN: \t Epoch: 163 \t Loss: -0.012010518399377664\nTRAIN: \t Epoch: 163 \t Loss: -0.01200728991534561\nTRAIN: \t Epoch: 163 \t Loss: -0.012025147450692726\nVALD: \t Epoch: 163 \t Loss: -0.01404544711112976\nVALD: \t Epoch: 163 \t Loss: -0.01358212623745203\nVALD: \t Epoch: 163 \t Loss: -0.011338194366544485\nVALD: \t Epoch: 163 \t Loss: -0.011259752952172132\n******************************\nEpoch: social-tag : 163\ntrain_loss -0.012025147450692726\nval_loss -0.011259752952172132\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 164 \t Loss: -0.01315070129930973\nTRAIN: \t Epoch: 164 \t Loss: -0.012821487616747618\nTRAIN: \t Epoch: 164 \t Loss: -0.012808633657793203\nTRAIN: \t Epoch: 164 \t Loss: -0.012714959681034088\nTRAIN: \t Epoch: 164 \t Loss: -0.012406484968960286\nTRAIN: \t Epoch: 164 \t Loss: -0.012365651627381643\nTRAIN: \t Epoch: 164 \t Loss: -0.012358100286551885\nTRAIN: \t Epoch: 164 \t Loss: -0.012169624445959926\nTRAIN: \t Epoch: 164 \t Loss: -0.012160965965853797\nTRAIN: \t Epoch: 164 \t Loss: -0.012039289996027946\nTRAIN: \t Epoch: 164 \t Loss: -0.012030752629718998\nTRAIN: \t Epoch: 164 \t Loss: -0.012057965698962411\nTRAIN: \t Epoch: 164 \t Loss: -0.012059538768461118\nTRAIN: \t Epoch: 164 \t Loss: -0.01208758247750146\nTRAIN: \t Epoch: 164 \t Loss: -0.012093880524237951\nTRAIN: \t Epoch: 164 \t Loss: -0.01207214524038136\nTRAIN: \t Epoch: 164 \t Loss: -0.01204718341768691\nVALD: \t Epoch: 164 \t Loss: -0.013459100387990475\nVALD: \t Epoch: 164 \t Loss: -0.012479853816330433\nVALD: \t Epoch: 164 \t Loss: -0.009972588314364353\nVALD: \t Epoch: 164 \t Loss: -0.010125595771386\n******************************\nEpoch: social-tag : 164\ntrain_loss -0.01204718341768691\nval_loss -0.010125595771386\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 165 \t Loss: -0.012173769064247608\nTRAIN: \t Epoch: 165 \t Loss: -0.012312083970755339\nTRAIN: \t Epoch: 165 \t Loss: -0.012447006379564604\nTRAIN: \t Epoch: 165 \t Loss: -0.01243741437792778\nTRAIN: \t Epoch: 165 \t Loss: -0.012333454005420208\nTRAIN: \t Epoch: 165 \t Loss: -0.012289627299954494\nTRAIN: \t Epoch: 165 \t Loss: -0.012369856238365173\nTRAIN: \t Epoch: 165 \t Loss: -0.012384728994220495\nTRAIN: \t Epoch: 165 \t Loss: -0.012289401868151294\nTRAIN: \t Epoch: 165 \t Loss: -0.012220996897667646\nTRAIN: \t Epoch: 165 \t Loss: -0.01229058172215115\nTRAIN: \t Epoch: 165 \t Loss: -0.01220545475371182\nTRAIN: \t Epoch: 165 \t Loss: -0.012185938369769316\nTRAIN: \t Epoch: 165 \t Loss: -0.012101183445858104\nTRAIN: \t Epoch: 165 \t Loss: -0.012116843648254872\nTRAIN: \t Epoch: 165 \t Loss: -0.01207745703868568\nTRAIN: \t Epoch: 165 \t Loss: -0.01206218112598766\nVALD: \t Epoch: 165 \t Loss: -0.01396720390766859\nVALD: \t Epoch: 165 \t Loss: -0.013492618221789598\nVALD: \t Epoch: 165 \t Loss: -0.01139130008717378\nVALD: \t Epoch: 165 \t Loss: -0.011284688989559333\n******************************\nEpoch: social-tag : 165\ntrain_loss -0.01206218112598766\nval_loss -0.011284688989559333\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 166 \t Loss: -0.012569545768201351\nTRAIN: \t Epoch: 166 \t Loss: -0.012718395795673132\nTRAIN: \t Epoch: 166 \t Loss: -0.012529863975942135\nTRAIN: \t Epoch: 166 \t Loss: -0.012339900946244597\nTRAIN: \t Epoch: 166 \t Loss: -0.012204184941947461\nTRAIN: \t Epoch: 166 \t Loss: -0.01225497325261434\nTRAIN: \t Epoch: 166 \t Loss: -0.01207624256078686\nTRAIN: \t Epoch: 166 \t Loss: -0.012148189241997898\nTRAIN: \t Epoch: 166 \t Loss: -0.012121824754609002\nTRAIN: \t Epoch: 166 \t Loss: -0.012179844453930855\nTRAIN: \t Epoch: 166 \t Loss: -0.012214823879978874\nTRAIN: \t Epoch: 166 \t Loss: -0.012200538689891497\nTRAIN: \t Epoch: 166 \t Loss: -0.012146007699462084\nTRAIN: \t Epoch: 166 \t Loss: -0.012089372080351626\nTRAIN: \t Epoch: 166 \t Loss: -0.012077907286584377\nTRAIN: \t Epoch: 166 \t Loss: -0.012093539407942444\nTRAIN: \t Epoch: 166 \t Loss: -0.012048606958353159\nVALD: \t Epoch: 166 \t Loss: -0.013707151636481285\nVALD: \t Epoch: 166 \t Loss: -0.012953364755958319\nVALD: \t Epoch: 166 \t Loss: -0.01026775361970067\nVALD: \t Epoch: 166 \t Loss: -0.010394915968120217\n******************************\nEpoch: social-tag : 166\ntrain_loss -0.012048606958353159\nval_loss -0.010394915968120217\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 167 \t Loss: -0.01204493548721075\nTRAIN: \t Epoch: 167 \t Loss: -0.01156594417989254\nTRAIN: \t Epoch: 167 \t Loss: -0.011811280623078346\nTRAIN: \t Epoch: 167 \t Loss: -0.011663294397294521\nTRAIN: \t Epoch: 167 \t Loss: -0.01188470795750618\nTRAIN: \t Epoch: 167 \t Loss: -0.011919426110883554\nTRAIN: \t Epoch: 167 \t Loss: -0.011799052623765809\nTRAIN: \t Epoch: 167 \t Loss: -0.01202120294328779\nTRAIN: \t Epoch: 167 \t Loss: -0.012073659441537328\nTRAIN: \t Epoch: 167 \t Loss: -0.012041476927697659\nTRAIN: \t Epoch: 167 \t Loss: -0.012025316669182344\nTRAIN: \t Epoch: 167 \t Loss: -0.012028876536836227\nTRAIN: \t Epoch: 167 \t Loss: -0.012076663426481761\nTRAIN: \t Epoch: 167 \t Loss: -0.01208933675661683\nTRAIN: \t Epoch: 167 \t Loss: -0.012081255142887434\nTRAIN: \t Epoch: 167 \t Loss: -0.012061381130479276\nTRAIN: \t Epoch: 167 \t Loss: -0.012064690165447466\nVALD: \t Epoch: 167 \t Loss: -0.014191431924700737\nVALD: \t Epoch: 167 \t Loss: -0.013235042337328196\nVALD: \t Epoch: 167 \t Loss: -0.010810629619906345\nVALD: \t Epoch: 167 \t Loss: -0.010827106748988291\n******************************\nEpoch: social-tag : 167\ntrain_loss -0.012064690165447466\nval_loss -0.010827106748988291\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 168 \t Loss: -0.011676298454403877\nTRAIN: \t Epoch: 168 \t Loss: -0.012143843807280064\nTRAIN: \t Epoch: 168 \t Loss: -0.011994332075119019\nTRAIN: \t Epoch: 168 \t Loss: -0.01224120962433517\nTRAIN: \t Epoch: 168 \t Loss: -0.012157968431711196\nTRAIN: \t Epoch: 168 \t Loss: -0.012049439207961163\nTRAIN: \t Epoch: 168 \t Loss: -0.012077629566192627\nTRAIN: \t Epoch: 168 \t Loss: -0.012134870165027678\nTRAIN: \t Epoch: 168 \t Loss: -0.012087688884801336\nTRAIN: \t Epoch: 168 \t Loss: -0.012138653732836246\nTRAIN: \t Epoch: 168 \t Loss: -0.012105341238731688\nTRAIN: \t Epoch: 168 \t Loss: -0.012146151391789317\nTRAIN: \t Epoch: 168 \t Loss: -0.01212376313140759\nTRAIN: \t Epoch: 168 \t Loss: -0.01207270207149642\nTRAIN: \t Epoch: 168 \t Loss: -0.012083875760436059\nTRAIN: \t Epoch: 168 \t Loss: -0.012061194749549031\nTRAIN: \t Epoch: 168 \t Loss: -0.01207625541384473\nVALD: \t Epoch: 168 \t Loss: -0.01374876219779253\nVALD: \t Epoch: 168 \t Loss: -0.013140290975570679\nVALD: \t Epoch: 168 \t Loss: -0.010733764308194319\nVALD: \t Epoch: 168 \t Loss: -0.010741076545563048\n******************************\nEpoch: social-tag : 168\ntrain_loss -0.01207625541384473\nval_loss -0.010741076545563048\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 169 \t Loss: -0.012505735270678997\nTRAIN: \t Epoch: 169 \t Loss: -0.012975814286619425\nTRAIN: \t Epoch: 169 \t Loss: -0.013120531104505062\nTRAIN: \t Epoch: 169 \t Loss: -0.012748772278428078\nTRAIN: \t Epoch: 169 \t Loss: -0.012713469564914703\nTRAIN: \t Epoch: 169 \t Loss: -0.012371123613168796\nTRAIN: \t Epoch: 169 \t Loss: -0.01225740129926375\nTRAIN: \t Epoch: 169 \t Loss: -0.012411799863912165\nTRAIN: \t Epoch: 169 \t Loss: -0.012449504083229436\nTRAIN: \t Epoch: 169 \t Loss: -0.012394131068140269\nTRAIN: \t Epoch: 169 \t Loss: -0.012245881540531462\nTRAIN: \t Epoch: 169 \t Loss: -0.012252880803619822\nTRAIN: \t Epoch: 169 \t Loss: -0.012262302880676893\nTRAIN: \t Epoch: 169 \t Loss: -0.012225272533084666\nTRAIN: \t Epoch: 169 \t Loss: -0.012138468399643897\nTRAIN: \t Epoch: 169 \t Loss: -0.012119842111133039\nTRAIN: \t Epoch: 169 \t Loss: -0.012093686103595026\nVALD: \t Epoch: 169 \t Loss: -0.013642712496221066\nVALD: \t Epoch: 169 \t Loss: -0.012990264222025871\nVALD: \t Epoch: 169 \t Loss: -0.010489028878509998\nVALD: \t Epoch: 169 \t Loss: -0.010618189137852835\n******************************\nEpoch: social-tag : 169\ntrain_loss -0.012093686103595026\nval_loss -0.010618189137852835\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 170 \t Loss: -0.011981794610619545\nTRAIN: \t Epoch: 170 \t Loss: -0.012348432559520006\nTRAIN: \t Epoch: 170 \t Loss: -0.011990549663702646\nTRAIN: \t Epoch: 170 \t Loss: -0.01203221594914794\nTRAIN: \t Epoch: 170 \t Loss: -0.011955893225967884\nTRAIN: \t Epoch: 170 \t Loss: -0.012069317201773325\nTRAIN: \t Epoch: 170 \t Loss: -0.01190186504806791\nTRAIN: \t Epoch: 170 \t Loss: -0.012004120973870158\nTRAIN: \t Epoch: 170 \t Loss: -0.01205078698694706\nTRAIN: \t Epoch: 170 \t Loss: -0.012080394476652146\nTRAIN: \t Epoch: 170 \t Loss: -0.012105671180920168\nTRAIN: \t Epoch: 170 \t Loss: -0.012124535317222277\nTRAIN: \t Epoch: 170 \t Loss: -0.012073214810628157\nTRAIN: \t Epoch: 170 \t Loss: -0.012110043250556504\nTRAIN: \t Epoch: 170 \t Loss: -0.012072156742215156\nTRAIN: \t Epoch: 170 \t Loss: -0.012062056339345872\nTRAIN: \t Epoch: 170 \t Loss: -0.012077879471083483\nVALD: \t Epoch: 170 \t Loss: -0.013791880570352077\nVALD: \t Epoch: 170 \t Loss: -0.012607372365891933\nVALD: \t Epoch: 170 \t Loss: -0.010245217010378838\nVALD: \t Epoch: 170 \t Loss: -0.01033609927057506\n******************************\nEpoch: social-tag : 170\ntrain_loss -0.012077879471083483\nval_loss -0.01033609927057506\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 171 \t Loss: -0.012008327059447765\nTRAIN: \t Epoch: 171 \t Loss: -0.012110001407563686\nTRAIN: \t Epoch: 171 \t Loss: -0.012070011657973131\nTRAIN: \t Epoch: 171 \t Loss: -0.011948768980801105\nTRAIN: \t Epoch: 171 \t Loss: -0.012019800394773484\nTRAIN: \t Epoch: 171 \t Loss: -0.012079806067049503\nTRAIN: \t Epoch: 171 \t Loss: -0.012020281116877283\nTRAIN: \t Epoch: 171 \t Loss: -0.011942475568503141\nTRAIN: \t Epoch: 171 \t Loss: -0.012066234524051348\nTRAIN: \t Epoch: 171 \t Loss: -0.012014769110828638\nTRAIN: \t Epoch: 171 \t Loss: -0.012012024410068989\nTRAIN: \t Epoch: 171 \t Loss: -0.012035923389097055\nTRAIN: \t Epoch: 171 \t Loss: -0.012067419118606128\nTRAIN: \t Epoch: 171 \t Loss: -0.012130871348615204\nTRAIN: \t Epoch: 171 \t Loss: -0.012190504868825277\nTRAIN: \t Epoch: 171 \t Loss: -0.012140150065533817\nTRAIN: \t Epoch: 171 \t Loss: -0.012102047235450962\nVALD: \t Epoch: 171 \t Loss: -0.014091945253312588\nVALD: \t Epoch: 171 \t Loss: -0.01338182482868433\nVALD: \t Epoch: 171 \t Loss: -0.011231816528985897\nVALD: \t Epoch: 171 \t Loss: -0.01116635758957701\n******************************\nEpoch: social-tag : 171\ntrain_loss -0.012102047235450962\nval_loss -0.01116635758957701\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 172 \t Loss: -0.012273984961211681\nTRAIN: \t Epoch: 172 \t Loss: -0.012363294139504433\nTRAIN: \t Epoch: 172 \t Loss: -0.01156020537018776\nTRAIN: \t Epoch: 172 \t Loss: -0.011819536564871669\nTRAIN: \t Epoch: 172 \t Loss: -0.011846992745995521\nTRAIN: \t Epoch: 172 \t Loss: -0.012168434138099352\nTRAIN: \t Epoch: 172 \t Loss: -0.012181682645210199\nTRAIN: \t Epoch: 172 \t Loss: -0.012126590823754668\nTRAIN: \t Epoch: 172 \t Loss: -0.012140286465485891\nTRAIN: \t Epoch: 172 \t Loss: -0.012176624312996865\nTRAIN: \t Epoch: 172 \t Loss: -0.012202392586252907\nTRAIN: \t Epoch: 172 \t Loss: -0.01223426490711669\nTRAIN: \t Epoch: 172 \t Loss: -0.012233730119008284\nTRAIN: \t Epoch: 172 \t Loss: -0.012255198388759578\nTRAIN: \t Epoch: 172 \t Loss: -0.012206818163394927\nTRAIN: \t Epoch: 172 \t Loss: -0.012176120828371495\nTRAIN: \t Epoch: 172 \t Loss: -0.012127143528425333\nVALD: \t Epoch: 172 \t Loss: -0.013504940085113049\nVALD: \t Epoch: 172 \t Loss: -0.013290912844240665\nVALD: \t Epoch: 172 \t Loss: -0.011100406448046366\nVALD: \t Epoch: 172 \t Loss: -0.011079540033778269\n******************************\nEpoch: social-tag : 172\ntrain_loss -0.012127143528425333\nval_loss -0.011079540033778269\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 173 \t Loss: -0.012084049172699451\nTRAIN: \t Epoch: 173 \t Loss: -0.012136092409491539\nTRAIN: \t Epoch: 173 \t Loss: -0.012389694650967916\nTRAIN: \t Epoch: 173 \t Loss: -0.012196098919957876\nTRAIN: \t Epoch: 173 \t Loss: -0.012279503978788853\nTRAIN: \t Epoch: 173 \t Loss: -0.012114075012505054\nTRAIN: \t Epoch: 173 \t Loss: -0.012183382974139281\nTRAIN: \t Epoch: 173 \t Loss: -0.012201351462863386\nTRAIN: \t Epoch: 173 \t Loss: -0.01225825295680099\nTRAIN: \t Epoch: 173 \t Loss: -0.012105958629399538\nTRAIN: \t Epoch: 173 \t Loss: -0.01214557530527765\nTRAIN: \t Epoch: 173 \t Loss: -0.01213234942406416\nTRAIN: \t Epoch: 173 \t Loss: -0.012103882618248463\nTRAIN: \t Epoch: 173 \t Loss: -0.012109489712331976\nTRAIN: \t Epoch: 173 \t Loss: -0.01208829420308272\nTRAIN: \t Epoch: 173 \t Loss: -0.01211059675551951\nTRAIN: \t Epoch: 173 \t Loss: -0.012139979467699022\nVALD: \t Epoch: 173 \t Loss: -0.013980802148580551\nVALD: \t Epoch: 173 \t Loss: -0.012986557558178902\nVALD: \t Epoch: 173 \t Loss: -0.0105201774276793\nVALD: \t Epoch: 173 \t Loss: -0.010550337042399272\n******************************\nEpoch: social-tag : 173\ntrain_loss -0.012139979467699022\nval_loss -0.010550337042399272\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 174 \t Loss: -0.012140687555074692\nTRAIN: \t Epoch: 174 \t Loss: -0.011964987963438034\nTRAIN: \t Epoch: 174 \t Loss: -0.011905831595261892\nTRAIN: \t Epoch: 174 \t Loss: -0.01196487550623715\nTRAIN: \t Epoch: 174 \t Loss: -0.012069384008646012\nTRAIN: \t Epoch: 174 \t Loss: -0.01207789033651352\nTRAIN: \t Epoch: 174 \t Loss: -0.012058820309383529\nTRAIN: \t Epoch: 174 \t Loss: -0.01206124504096806\nTRAIN: \t Epoch: 174 \t Loss: -0.012109545783864127\nTRAIN: \t Epoch: 174 \t Loss: -0.012052989471703768\nTRAIN: \t Epoch: 174 \t Loss: -0.012079788456586275\nTRAIN: \t Epoch: 174 \t Loss: -0.012075911508873105\nTRAIN: \t Epoch: 174 \t Loss: -0.012068321068699542\nTRAIN: \t Epoch: 174 \t Loss: -0.012074139900505543\nTRAIN: \t Epoch: 174 \t Loss: -0.012050494365394116\nTRAIN: \t Epoch: 174 \t Loss: -0.012134485295973718\nTRAIN: \t Epoch: 174 \t Loss: -0.012116877277466383\nVALD: \t Epoch: 174 \t Loss: -0.014280195347964764\nVALD: \t Epoch: 174 \t Loss: -0.013515705708414316\nVALD: \t Epoch: 174 \t Loss: -0.01149178979297479\nVALD: \t Epoch: 174 \t Loss: -0.011341360038863922\n******************************\nEpoch: social-tag : 174\ntrain_loss -0.012116877277466383\nval_loss -0.011341360038863922\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 175 \t Loss: -0.011564156971871853\nTRAIN: \t Epoch: 175 \t Loss: -0.011779260355979204\nTRAIN: \t Epoch: 175 \t Loss: -0.011589994964500269\nTRAIN: \t Epoch: 175 \t Loss: -0.011933746049180627\nTRAIN: \t Epoch: 175 \t Loss: -0.011966250091791152\nTRAIN: \t Epoch: 175 \t Loss: -0.011973839563628038\nTRAIN: \t Epoch: 175 \t Loss: -0.012029660865664482\nTRAIN: \t Epoch: 175 \t Loss: -0.01203927828464657\nTRAIN: \t Epoch: 175 \t Loss: -0.012045008337332143\nTRAIN: \t Epoch: 175 \t Loss: -0.012040205020457505\nTRAIN: \t Epoch: 175 \t Loss: -0.012019850228997793\nTRAIN: \t Epoch: 175 \t Loss: -0.012014323379844427\nTRAIN: \t Epoch: 175 \t Loss: -0.012046256747383337\nTRAIN: \t Epoch: 175 \t Loss: -0.01209720057834472\nTRAIN: \t Epoch: 175 \t Loss: -0.012065547021726767\nTRAIN: \t Epoch: 175 \t Loss: -0.012083832232747227\nTRAIN: \t Epoch: 175 \t Loss: -0.012119080730234131\nVALD: \t Epoch: 175 \t Loss: -0.014154748991131783\nVALD: \t Epoch: 175 \t Loss: -0.01354289660230279\nVALD: \t Epoch: 175 \t Loss: -0.011264110449701548\nVALD: \t Epoch: 175 \t Loss: -0.01117169797539473\n******************************\nEpoch: social-tag : 175\ntrain_loss -0.012119080730234131\nval_loss -0.01117169797539473\n{'min_val_epoch': 155, 'min_val_loss': -0.011456328237841942}\n******************************\nTRAIN: \t Epoch: 176 \t Loss: -0.011671987362205982\nTRAIN: \t Epoch: 176 \t Loss: -0.011831695213913918\nTRAIN: \t Epoch: 176 \t Loss: -0.012073184673984846\nTRAIN: \t Epoch: 176 \t Loss: -0.011854982934892178\nTRAIN: \t Epoch: 176 \t Loss: -0.01191455703228712\nTRAIN: \t Epoch: 176 \t Loss: -0.012037130848815044\nTRAIN: \t Epoch: 176 \t Loss: -0.012116031721234322\nTRAIN: \t Epoch: 176 \t Loss: -0.012104253517463803\nTRAIN: \t Epoch: 176 \t Loss: -0.012150956731703546\nTRAIN: \t Epoch: 176 \t Loss: -0.01215022662654519\nTRAIN: \t Epoch: 176 \t Loss: -0.012134385633875023\nTRAIN: \t Epoch: 176 \t Loss: -0.012151975107068816\nTRAIN: \t Epoch: 176 \t Loss: -0.012135103703118287\nTRAIN: \t Epoch: 176 \t Loss: -0.012107589215572392\nTRAIN: \t Epoch: 176 \t Loss: -0.012130775737265747\nTRAIN: \t Epoch: 176 \t Loss: -0.012081300606951118\nTRAIN: \t Epoch: 176 \t Loss: -0.012104150218268236\nVALD: \t Epoch: 176 \t Loss: -0.014255117624998093\nVALD: \t Epoch: 176 \t Loss: -0.013688040431588888\nVALD: \t Epoch: 176 \t Loss: -0.011633107749124369\nVALD: \t Epoch: 176 \t Loss: -0.011466639008588658\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:33<00:00,  9.84it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.3347452546310046  FDE: 0.5204325974316134\n**************************************************\n******************************\nEpoch: social-tag : 176\ntrain_loss -0.012104150218268236\nval_loss -0.011466639008588658\n{'min_val_epoch': 176, 'min_val_loss': -0.011466639008588658}\n******************************\nTRAIN: \t Epoch: 177 \t Loss: -0.012420679442584515\nTRAIN: \t Epoch: 177 \t Loss: -0.01186539139598608\nTRAIN: \t Epoch: 177 \t Loss: -0.011807109539707502\nTRAIN: \t Epoch: 177 \t Loss: -0.0119352787733078\nTRAIN: \t Epoch: 177 \t Loss: -0.012031741440296173\nTRAIN: \t Epoch: 177 \t Loss: -0.012093350136031708\nTRAIN: \t Epoch: 177 \t Loss: -0.011970138443367822\nTRAIN: \t Epoch: 177 \t Loss: -0.012042887508869171\nTRAIN: \t Epoch: 177 \t Loss: -0.012071703664130636\nTRAIN: \t Epoch: 177 \t Loss: -0.012113823648542165\nTRAIN: \t Epoch: 177 \t Loss: -0.012068258852443912\nTRAIN: \t Epoch: 177 \t Loss: -0.012128602170074979\nTRAIN: \t Epoch: 177 \t Loss: -0.012209449249964494\nTRAIN: \t Epoch: 177 \t Loss: -0.012187657412141562\nTRAIN: \t Epoch: 177 \t Loss: -0.012176194787025451\nTRAIN: \t Epoch: 177 \t Loss: -0.01219427160685882\nTRAIN: \t Epoch: 177 \t Loss: -0.012116045493519667\nVALD: \t Epoch: 177 \t Loss: -0.013747748918831348\nVALD: \t Epoch: 177 \t Loss: -0.013374280650168657\nVALD: \t Epoch: 177 \t Loss: -0.011382158224781355\nVALD: \t Epoch: 177 \t Loss: -0.011176583057868028\n******************************\nEpoch: social-tag : 177\ntrain_loss -0.012116045493519667\nval_loss -0.011176583057868028\n{'min_val_epoch': 176, 'min_val_loss': -0.011466639008588658}\n******************************\nTRAIN: \t Epoch: 178 \t Loss: -0.011280640959739685\nTRAIN: \t Epoch: 178 \t Loss: -0.012197972275316715\nTRAIN: \t Epoch: 178 \t Loss: -0.012105057947337627\nTRAIN: \t Epoch: 178 \t Loss: -0.011851266957819462\nTRAIN: \t Epoch: 178 \t Loss: -0.011809452436864376\nTRAIN: \t Epoch: 178 \t Loss: -0.0118241379968822\nTRAIN: \t Epoch: 178 \t Loss: -0.011743291414209775\nTRAIN: \t Epoch: 178 \t Loss: -0.011764169903472066\nTRAIN: \t Epoch: 178 \t Loss: -0.011886015223960081\nTRAIN: \t Epoch: 178 \t Loss: -0.011946006026118994\nTRAIN: \t Epoch: 178 \t Loss: -0.011915618638423357\nTRAIN: \t Epoch: 178 \t Loss: -0.012016136354456345\nTRAIN: \t Epoch: 178 \t Loss: -0.012095965516681854\nTRAIN: \t Epoch: 178 \t Loss: -0.012092960865369864\nTRAIN: \t Epoch: 178 \t Loss: -0.012085854696730772\nTRAIN: \t Epoch: 178 \t Loss: -0.012061520013958216\nTRAIN: \t Epoch: 178 \t Loss: -0.01211665373182658\nVALD: \t Epoch: 178 \t Loss: -0.014093746431171894\nVALD: \t Epoch: 178 \t Loss: -0.013331476598978043\nVALD: \t Epoch: 178 \t Loss: -0.011423290086289247\nVALD: \t Epoch: 178 \t Loss: -0.011225272319511977\n******************************\nEpoch: social-tag : 178\ntrain_loss -0.01211665373182658\nval_loss -0.011225272319511977\n{'min_val_epoch': 176, 'min_val_loss': -0.011466639008588658}\n******************************\nTRAIN: \t Epoch: 179 \t Loss: -0.012106788344681263\nTRAIN: \t Epoch: 179 \t Loss: -0.011962632182985544\nTRAIN: \t Epoch: 179 \t Loss: -0.012100751201311747\nTRAIN: \t Epoch: 179 \t Loss: -0.012023664778098464\nTRAIN: \t Epoch: 179 \t Loss: -0.01211971826851368\nTRAIN: \t Epoch: 179 \t Loss: -0.012120567262172699\nTRAIN: \t Epoch: 179 \t Loss: -0.012061318515666894\nTRAIN: \t Epoch: 179 \t Loss: -0.012063785339705646\nTRAIN: \t Epoch: 179 \t Loss: -0.012129598297178745\nTRAIN: \t Epoch: 179 \t Loss: -0.012113593332469464\nTRAIN: \t Epoch: 179 \t Loss: -0.01216540587219325\nTRAIN: \t Epoch: 179 \t Loss: -0.012183458311483264\nTRAIN: \t Epoch: 179 \t Loss: -0.012185132704102077\nTRAIN: \t Epoch: 179 \t Loss: -0.0121416285900133\nTRAIN: \t Epoch: 179 \t Loss: -0.012172934785485267\nTRAIN: \t Epoch: 179 \t Loss: -0.012168226414360106\nTRAIN: \t Epoch: 179 \t Loss: -0.012131963038083279\nVALD: \t Epoch: 179 \t Loss: -0.014121421612799168\nVALD: \t Epoch: 179 \t Loss: -0.013528707902878523\nVALD: \t Epoch: 179 \t Loss: -0.011637264862656593\nVALD: \t Epoch: 179 \t Loss: -0.011467672393707459\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:33<00:00,  9.86it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.33715730355356055  FDE: 0.5226733836120543\n**************************************************\n******************************\nEpoch: social-tag : 179\ntrain_loss -0.012131963038083279\nval_loss -0.011467672393707459\n{'min_val_epoch': 179, 'min_val_loss': -0.011467672393707459}\n******************************\nTRAIN: \t Epoch: 180 \t Loss: -0.011694144457578659\nTRAIN: \t Epoch: 180 \t Loss: -0.012288207653909922\nTRAIN: \t Epoch: 180 \t Loss: -0.012411021006604036\nTRAIN: \t Epoch: 180 \t Loss: -0.012439250480383635\nTRAIN: \t Epoch: 180 \t Loss: -0.012159619480371475\nTRAIN: \t Epoch: 180 \t Loss: -0.0123477002295355\nTRAIN: \t Epoch: 180 \t Loss: -0.012387130010340894\nTRAIN: \t Epoch: 180 \t Loss: -0.012257009278982878\nTRAIN: \t Epoch: 180 \t Loss: -0.012218395041094886\nTRAIN: \t Epoch: 180 \t Loss: -0.012173546198755502\nTRAIN: \t Epoch: 180 \t Loss: -0.012068157084286213\nTRAIN: \t Epoch: 180 \t Loss: -0.012042307062074542\nTRAIN: \t Epoch: 180 \t Loss: -0.012027522190832175\nTRAIN: \t Epoch: 180 \t Loss: -0.012089664781732219\nTRAIN: \t Epoch: 180 \t Loss: -0.012114097985128561\nTRAIN: \t Epoch: 180 \t Loss: -0.01214082638034597\nTRAIN: \t Epoch: 180 \t Loss: -0.012135651357697718\nVALD: \t Epoch: 180 \t Loss: -0.013501404784619808\nVALD: \t Epoch: 180 \t Loss: -0.012503426987677813\nVALD: \t Epoch: 180 \t Loss: -0.009997511748224497\nVALD: \t Epoch: 180 \t Loss: -0.010126325303684928\n******************************\nEpoch: social-tag : 180\ntrain_loss -0.012135651357697718\nval_loss -0.010126325303684928\n{'min_val_epoch': 179, 'min_val_loss': -0.011467672393707459}\n******************************\nTRAIN: \t Epoch: 181 \t Loss: -0.01233198493719101\nTRAIN: \t Epoch: 181 \t Loss: -0.012021777220070362\nTRAIN: \t Epoch: 181 \t Loss: -0.01205617468804121\nTRAIN: \t Epoch: 181 \t Loss: -0.012051919242367148\nTRAIN: \t Epoch: 181 \t Loss: -0.011953461915254593\nTRAIN: \t Epoch: 181 \t Loss: -0.012040066222349802\nTRAIN: \t Epoch: 181 \t Loss: -0.01205181676362242\nTRAIN: \t Epoch: 181 \t Loss: -0.012058086460456252\nTRAIN: \t Epoch: 181 \t Loss: -0.012086521937615342\nTRAIN: \t Epoch: 181 \t Loss: -0.012078907527029514\nTRAIN: \t Epoch: 181 \t Loss: -0.012123280712826685\nTRAIN: \t Epoch: 181 \t Loss: -0.01209649397060275\nTRAIN: \t Epoch: 181 \t Loss: -0.012160576306856595\nTRAIN: \t Epoch: 181 \t Loss: -0.012167382187076978\nTRAIN: \t Epoch: 181 \t Loss: -0.012161535707612832\nTRAIN: \t Epoch: 181 \t Loss: -0.012142374995164573\nTRAIN: \t Epoch: 181 \t Loss: -0.01215682774217743\nVALD: \t Epoch: 181 \t Loss: -0.014198429882526398\nVALD: \t Epoch: 181 \t Loss: -0.013607341796159744\nVALD: \t Epoch: 181 \t Loss: -0.011329559609293938\nVALD: \t Epoch: 181 \t Loss: -0.011251081725556455\n******************************\nEpoch: social-tag : 181\ntrain_loss -0.01215682774217743\nval_loss -0.011251081725556455\n{'min_val_epoch': 179, 'min_val_loss': -0.011467672393707459}\n******************************\nTRAIN: \t Epoch: 182 \t Loss: -0.012749477289617062\nTRAIN: \t Epoch: 182 \t Loss: -0.012309351470321417\nTRAIN: \t Epoch: 182 \t Loss: -0.011918227809170881\nTRAIN: \t Epoch: 182 \t Loss: -0.011939855990931392\nTRAIN: \t Epoch: 182 \t Loss: -0.011909389123320579\nTRAIN: \t Epoch: 182 \t Loss: -0.011979330175866684\nTRAIN: \t Epoch: 182 \t Loss: -0.01209575603050845\nTRAIN: \t Epoch: 182 \t Loss: -0.011905706720426679\nTRAIN: \t Epoch: 182 \t Loss: -0.011981881637540128\nTRAIN: \t Epoch: 182 \t Loss: -0.012080908194184304\nTRAIN: \t Epoch: 182 \t Loss: -0.012062562036920677\nTRAIN: \t Epoch: 182 \t Loss: -0.012112950828547278\nTRAIN: \t Epoch: 182 \t Loss: -0.012085849992357768\nTRAIN: \t Epoch: 182 \t Loss: -0.012145346496254206\nTRAIN: \t Epoch: 182 \t Loss: -0.012157389397422473\nTRAIN: \t Epoch: 182 \t Loss: -0.012162720202468336\nTRAIN: \t Epoch: 182 \t Loss: -0.012148792149894165\nVALD: \t Epoch: 182 \t Loss: -0.01411751564592123\nVALD: \t Epoch: 182 \t Loss: -0.013662602752447128\nVALD: \t Epoch: 182 \t Loss: -0.011683841856817404\nVALD: \t Epoch: 182 \t Loss: -0.011532564125137177\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:33<00:00,  9.84it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.34134521070364837  FDE: 0.5335576055776223\n**************************************************\n******************************\nEpoch: social-tag : 182\ntrain_loss -0.012148792149894165\nval_loss -0.011532564125137177\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 183 \t Loss: -0.011596936732530594\nTRAIN: \t Epoch: 183 \t Loss: -0.012811702210456133\nTRAIN: \t Epoch: 183 \t Loss: -0.012807559532423815\nTRAIN: \t Epoch: 183 \t Loss: -0.012442910112440586\nTRAIN: \t Epoch: 183 \t Loss: -0.012270738184452058\nTRAIN: \t Epoch: 183 \t Loss: -0.012229831423610449\nTRAIN: \t Epoch: 183 \t Loss: -0.012317716277071409\nTRAIN: \t Epoch: 183 \t Loss: -0.012313807616010308\nTRAIN: \t Epoch: 183 \t Loss: -0.012217713416450553\nTRAIN: \t Epoch: 183 \t Loss: -0.012246084492653608\nTRAIN: \t Epoch: 183 \t Loss: -0.012265288609672676\nTRAIN: \t Epoch: 183 \t Loss: -0.01219557086005807\nTRAIN: \t Epoch: 183 \t Loss: -0.012185580312059475\nTRAIN: \t Epoch: 183 \t Loss: -0.012178596375244004\nTRAIN: \t Epoch: 183 \t Loss: -0.01218450851738453\nTRAIN: \t Epoch: 183 \t Loss: -0.012200825556647032\nTRAIN: \t Epoch: 183 \t Loss: -0.012151743906239668\nVALD: \t Epoch: 183 \t Loss: -0.013692657463252544\nVALD: \t Epoch: 183 \t Loss: -0.013256919104605913\nVALD: \t Epoch: 183 \t Loss: -0.011447852166990439\nVALD: \t Epoch: 183 \t Loss: -0.011334543932459787\n******************************\nEpoch: social-tag : 183\ntrain_loss -0.012151743906239668\nval_loss -0.011334543932459787\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 184 \t Loss: -0.012090804055333138\nTRAIN: \t Epoch: 184 \t Loss: -0.011907238513231277\nTRAIN: \t Epoch: 184 \t Loss: -0.011900964813927809\nTRAIN: \t Epoch: 184 \t Loss: -0.011844035470858216\nTRAIN: \t Epoch: 184 \t Loss: -0.01186367254704237\nTRAIN: \t Epoch: 184 \t Loss: -0.011866920161992311\nTRAIN: \t Epoch: 184 \t Loss: -0.011965969577431679\nTRAIN: \t Epoch: 184 \t Loss: -0.012006742297671735\nTRAIN: \t Epoch: 184 \t Loss: -0.01208459192679988\nTRAIN: \t Epoch: 184 \t Loss: -0.012109194416552782\nTRAIN: \t Epoch: 184 \t Loss: -0.012184161523526365\nTRAIN: \t Epoch: 184 \t Loss: -0.012210739155610403\nTRAIN: \t Epoch: 184 \t Loss: -0.012125108104485732\nTRAIN: \t Epoch: 184 \t Loss: -0.012128263312791075\nTRAIN: \t Epoch: 184 \t Loss: -0.012115409163137277\nTRAIN: \t Epoch: 184 \t Loss: -0.012147798668593168\nTRAIN: \t Epoch: 184 \t Loss: -0.012158097558175072\nVALD: \t Epoch: 184 \t Loss: -0.01372506096959114\nVALD: \t Epoch: 184 \t Loss: -0.013011969160288572\nVALD: \t Epoch: 184 \t Loss: -0.010732832054297129\nVALD: \t Epoch: 184 \t Loss: -0.010752507312569076\n******************************\nEpoch: social-tag : 184\ntrain_loss -0.012158097558175072\nval_loss -0.010752507312569076\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 185 \t Loss: -0.012622793205082417\nTRAIN: \t Epoch: 185 \t Loss: -0.012477388605475426\nTRAIN: \t Epoch: 185 \t Loss: -0.012128107870618502\nTRAIN: \t Epoch: 185 \t Loss: -0.012233203742653131\nTRAIN: \t Epoch: 185 \t Loss: -0.012171781435608865\nTRAIN: \t Epoch: 185 \t Loss: -0.012224224706490835\nTRAIN: \t Epoch: 185 \t Loss: -0.012207883676247937\nTRAIN: \t Epoch: 185 \t Loss: -0.012159866280853748\nTRAIN: \t Epoch: 185 \t Loss: -0.012039311540623506\nTRAIN: \t Epoch: 185 \t Loss: -0.012027670256793499\nTRAIN: \t Epoch: 185 \t Loss: -0.011989474635232578\nTRAIN: \t Epoch: 185 \t Loss: -0.012014575923482576\nTRAIN: \t Epoch: 185 \t Loss: -0.012061696213025313\nTRAIN: \t Epoch: 185 \t Loss: -0.012152628906603371\nTRAIN: \t Epoch: 185 \t Loss: -0.012143833252290884\nTRAIN: \t Epoch: 185 \t Loss: -0.012109998613595963\nTRAIN: \t Epoch: 185 \t Loss: -0.012154264093348474\nVALD: \t Epoch: 185 \t Loss: -0.013808224350214005\nVALD: \t Epoch: 185 \t Loss: -0.012713639065623283\nVALD: \t Epoch: 185 \t Loss: -0.010215081429729858\nVALD: \t Epoch: 185 \t Loss: -0.010284165541330973\n******************************\nEpoch: social-tag : 185\ntrain_loss -0.012154264093348474\nval_loss -0.010284165541330973\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 186 \t Loss: -0.011578869074583054\nTRAIN: \t Epoch: 186 \t Loss: -0.01225859671831131\nTRAIN: \t Epoch: 186 \t Loss: -0.012481564345459143\nTRAIN: \t Epoch: 186 \t Loss: -0.012629271484911442\nTRAIN: \t Epoch: 186 \t Loss: -0.012493500672280788\nTRAIN: \t Epoch: 186 \t Loss: -0.012406855200727781\nTRAIN: \t Epoch: 186 \t Loss: -0.012483080716005393\nTRAIN: \t Epoch: 186 \t Loss: -0.012346946517936885\nTRAIN: \t Epoch: 186 \t Loss: -0.012280003167688847\nTRAIN: \t Epoch: 186 \t Loss: -0.012255520187318325\nTRAIN: \t Epoch: 186 \t Loss: -0.012222431684759531\nTRAIN: \t Epoch: 186 \t Loss: -0.01218901116711398\nTRAIN: \t Epoch: 186 \t Loss: -0.0122419331366053\nTRAIN: \t Epoch: 186 \t Loss: -0.012211800087243319\nTRAIN: \t Epoch: 186 \t Loss: -0.012174028592805067\nTRAIN: \t Epoch: 186 \t Loss: -0.012199084158055484\nTRAIN: \t Epoch: 186 \t Loss: -0.012188173802287289\nVALD: \t Epoch: 186 \t Loss: -0.013830907642841339\nVALD: \t Epoch: 186 \t Loss: -0.012977788224816322\nVALD: \t Epoch: 186 \t Loss: -0.010799146878222624\nVALD: \t Epoch: 186 \t Loss: -0.010821752443522988\n******************************\nEpoch: social-tag : 186\ntrain_loss -0.012188173802287289\nval_loss -0.010821752443522988\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 187 \t Loss: -0.012075986713171005\nTRAIN: \t Epoch: 187 \t Loss: -0.011804924812167883\nTRAIN: \t Epoch: 187 \t Loss: -0.012017674744129181\nTRAIN: \t Epoch: 187 \t Loss: -0.011954201618209481\nTRAIN: \t Epoch: 187 \t Loss: -0.012048575282096862\nTRAIN: \t Epoch: 187 \t Loss: -0.012023722752928734\nTRAIN: \t Epoch: 187 \t Loss: -0.012164802423545293\nTRAIN: \t Epoch: 187 \t Loss: -0.01216772198677063\nTRAIN: \t Epoch: 187 \t Loss: -0.01228700919697682\nTRAIN: \t Epoch: 187 \t Loss: -0.012248188629746437\nTRAIN: \t Epoch: 187 \t Loss: -0.012285995635796677\nTRAIN: \t Epoch: 187 \t Loss: -0.01223895481477181\nTRAIN: \t Epoch: 187 \t Loss: -0.012231851569735087\nTRAIN: \t Epoch: 187 \t Loss: -0.012240280929420675\nTRAIN: \t Epoch: 187 \t Loss: -0.012198051686088245\nTRAIN: \t Epoch: 187 \t Loss: -0.012163647392299026\nTRAIN: \t Epoch: 187 \t Loss: -0.012196216506488396\nVALD: \t Epoch: 187 \t Loss: -0.013588559813797474\nVALD: \t Epoch: 187 \t Loss: -0.012840733863413334\nVALD: \t Epoch: 187 \t Loss: -0.010422401285419861\nVALD: \t Epoch: 187 \t Loss: -0.010490571072477542\n******************************\nEpoch: social-tag : 187\ntrain_loss -0.012196216506488396\nval_loss -0.010490571072477542\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 188 \t Loss: -0.011756653897464275\nTRAIN: \t Epoch: 188 \t Loss: -0.01201363792642951\nTRAIN: \t Epoch: 188 \t Loss: -0.011928069405257702\nTRAIN: \t Epoch: 188 \t Loss: -0.011953482637181878\nTRAIN: \t Epoch: 188 \t Loss: -0.012090018391609192\nTRAIN: \t Epoch: 188 \t Loss: -0.012164350443830093\nTRAIN: \t Epoch: 188 \t Loss: -0.012249685291733061\nTRAIN: \t Epoch: 188 \t Loss: -0.01229133631568402\nTRAIN: \t Epoch: 188 \t Loss: -0.012375303647584386\nTRAIN: \t Epoch: 188 \t Loss: -0.012386692781001329\nTRAIN: \t Epoch: 188 \t Loss: -0.012354640544138172\nTRAIN: \t Epoch: 188 \t Loss: -0.012240256260459622\nTRAIN: \t Epoch: 188 \t Loss: -0.012195608650262538\nTRAIN: \t Epoch: 188 \t Loss: -0.012249695203666176\nTRAIN: \t Epoch: 188 \t Loss: -0.012179904369016488\nTRAIN: \t Epoch: 188 \t Loss: -0.012157171091530472\nTRAIN: \t Epoch: 188 \t Loss: -0.012171665234773449\nVALD: \t Epoch: 188 \t Loss: -0.01313931867480278\nVALD: \t Epoch: 188 \t Loss: -0.011747434735298157\nVALD: \t Epoch: 188 \t Loss: -0.008823711269845566\nVALD: \t Epoch: 188 \t Loss: -0.009058618616915034\n******************************\nEpoch: social-tag : 188\ntrain_loss -0.012171665234773449\nval_loss -0.009058618616915034\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 189 \t Loss: -0.01188153587281704\nTRAIN: \t Epoch: 189 \t Loss: -0.011391208972781897\nTRAIN: \t Epoch: 189 \t Loss: -0.01174214823792378\nTRAIN: \t Epoch: 189 \t Loss: -0.011919983895495534\nTRAIN: \t Epoch: 189 \t Loss: -0.011882211081683636\nTRAIN: \t Epoch: 189 \t Loss: -0.011925309741248688\nTRAIN: \t Epoch: 189 \t Loss: -0.01202059377517019\nTRAIN: \t Epoch: 189 \t Loss: -0.01196857646573335\nTRAIN: \t Epoch: 189 \t Loss: -0.012107708801825842\nTRAIN: \t Epoch: 189 \t Loss: -0.012109300401061774\nTRAIN: \t Epoch: 189 \t Loss: -0.012228375639427792\nTRAIN: \t Epoch: 189 \t Loss: -0.01212590110177795\nTRAIN: \t Epoch: 189 \t Loss: -0.012180276644917635\nTRAIN: \t Epoch: 189 \t Loss: -0.012116209869938237\nTRAIN: \t Epoch: 189 \t Loss: -0.012166916268567245\nTRAIN: \t Epoch: 189 \t Loss: -0.012167883629444987\nTRAIN: \t Epoch: 189 \t Loss: -0.012184258155298956\nVALD: \t Epoch: 189 \t Loss: -0.014089931733906269\nVALD: \t Epoch: 189 \t Loss: -0.013314160518348217\nVALD: \t Epoch: 189 \t Loss: -0.011009295781453451\nVALD: \t Epoch: 189 \t Loss: -0.01098762348502458\n******************************\nEpoch: social-tag : 189\ntrain_loss -0.012184258155298956\nval_loss -0.01098762348502458\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 190 \t Loss: -0.013188788667321205\nTRAIN: \t Epoch: 190 \t Loss: -0.011912494897842407\nTRAIN: \t Epoch: 190 \t Loss: -0.011968806075553099\nTRAIN: \t Epoch: 190 \t Loss: -0.011893162503838539\nTRAIN: \t Epoch: 190 \t Loss: -0.011851110309362412\nTRAIN: \t Epoch: 190 \t Loss: -0.012003510569532713\nTRAIN: \t Epoch: 190 \t Loss: -0.012095662632158824\nTRAIN: \t Epoch: 190 \t Loss: -0.012127506546676159\nTRAIN: \t Epoch: 190 \t Loss: -0.01217336787117852\nTRAIN: \t Epoch: 190 \t Loss: -0.012143566366285085\nTRAIN: \t Epoch: 190 \t Loss: -0.012174849652431229\nTRAIN: \t Epoch: 190 \t Loss: -0.012243244719381133\nTRAIN: \t Epoch: 190 \t Loss: -0.01226218338482655\nTRAIN: \t Epoch: 190 \t Loss: -0.012307472793119294\nTRAIN: \t Epoch: 190 \t Loss: -0.012315470414857069\nTRAIN: \t Epoch: 190 \t Loss: -0.012254905479494482\nTRAIN: \t Epoch: 190 \t Loss: -0.012187257013989218\nVALD: \t Epoch: 190 \t Loss: -0.01401064358651638\nVALD: \t Epoch: 190 \t Loss: -0.013536716811358929\nVALD: \t Epoch: 190 \t Loss: -0.011627336964011192\nVALD: \t Epoch: 190 \t Loss: -0.011450047026613277\n******************************\nEpoch: social-tag : 190\ntrain_loss -0.012187257013989218\nval_loss -0.011450047026613277\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 191 \t Loss: -0.012549880892038345\nTRAIN: \t Epoch: 191 \t Loss: -0.012105133850127459\nTRAIN: \t Epoch: 191 \t Loss: -0.01209802677234014\nTRAIN: \t Epoch: 191 \t Loss: -0.012053405633196235\nTRAIN: \t Epoch: 191 \t Loss: -0.011969536356627942\nTRAIN: \t Epoch: 191 \t Loss: -0.011894946917891502\nTRAIN: \t Epoch: 191 \t Loss: -0.01200804061123303\nTRAIN: \t Epoch: 191 \t Loss: -0.01211710087954998\nTRAIN: \t Epoch: 191 \t Loss: -0.012116342989934815\nTRAIN: \t Epoch: 191 \t Loss: -0.012196752149611712\nTRAIN: \t Epoch: 191 \t Loss: -0.01218202667818828\nTRAIN: \t Epoch: 191 \t Loss: -0.012139374855905771\nTRAIN: \t Epoch: 191 \t Loss: -0.012155984026881365\nTRAIN: \t Epoch: 191 \t Loss: -0.012135443676795279\nTRAIN: \t Epoch: 191 \t Loss: -0.01215679906308651\nTRAIN: \t Epoch: 191 \t Loss: -0.01216875814134255\nTRAIN: \t Epoch: 191 \t Loss: -0.012187416947474985\nVALD: \t Epoch: 191 \t Loss: -0.014283926226198673\nVALD: \t Epoch: 191 \t Loss: -0.013491358608007431\nVALD: \t Epoch: 191 \t Loss: -0.01136774088566502\nVALD: \t Epoch: 191 \t Loss: -0.011212586523767954\n******************************\nEpoch: social-tag : 191\ntrain_loss -0.012187416947474985\nval_loss -0.011212586523767954\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 192 \t Loss: -0.012260018847882748\nTRAIN: \t Epoch: 192 \t Loss: -0.012651491444557905\nTRAIN: \t Epoch: 192 \t Loss: -0.01235050397614638\nTRAIN: \t Epoch: 192 \t Loss: -0.011963703902438283\nTRAIN: \t Epoch: 192 \t Loss: -0.011975893937051296\nTRAIN: \t Epoch: 192 \t Loss: -0.012090451239297787\nTRAIN: \t Epoch: 192 \t Loss: -0.012170820230884212\nTRAIN: \t Epoch: 192 \t Loss: -0.012199534918181598\nTRAIN: \t Epoch: 192 \t Loss: -0.012217796821561124\nTRAIN: \t Epoch: 192 \t Loss: -0.01209722263738513\nTRAIN: \t Epoch: 192 \t Loss: -0.012087664905596863\nTRAIN: \t Epoch: 192 \t Loss: -0.012099690735340118\nTRAIN: \t Epoch: 192 \t Loss: -0.012112388387322426\nTRAIN: \t Epoch: 192 \t Loss: -0.012127918457346303\nTRAIN: \t Epoch: 192 \t Loss: -0.012116846318046251\nTRAIN: \t Epoch: 192 \t Loss: -0.012201775971334428\nTRAIN: \t Epoch: 192 \t Loss: -0.012184926336913399\nVALD: \t Epoch: 192 \t Loss: -0.013764717616140842\nVALD: \t Epoch: 192 \t Loss: -0.012456187978386879\nVALD: \t Epoch: 192 \t Loss: -0.009790816809982061\nVALD: \t Epoch: 192 \t Loss: -0.0099526161919097\n******************************\nEpoch: social-tag : 192\ntrain_loss -0.012184926336913399\nval_loss -0.0099526161919097\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 193 \t Loss: -0.012732395902276039\nTRAIN: \t Epoch: 193 \t Loss: -0.012522995471954346\nTRAIN: \t Epoch: 193 \t Loss: -0.012442837158838907\nTRAIN: \t Epoch: 193 \t Loss: -0.012659228639677167\nTRAIN: \t Epoch: 193 \t Loss: -0.012524698488414287\nTRAIN: \t Epoch: 193 \t Loss: -0.01260610700895389\nTRAIN: \t Epoch: 193 \t Loss: -0.012460348994604178\nTRAIN: \t Epoch: 193 \t Loss: -0.01245033007580787\nTRAIN: \t Epoch: 193 \t Loss: -0.01244216039776802\nTRAIN: \t Epoch: 193 \t Loss: -0.01239021671935916\nTRAIN: \t Epoch: 193 \t Loss: -0.012419946407052603\nTRAIN: \t Epoch: 193 \t Loss: -0.012365775493284067\nTRAIN: \t Epoch: 193 \t Loss: -0.012283392752019258\nTRAIN: \t Epoch: 193 \t Loss: -0.012240463269076176\nTRAIN: \t Epoch: 193 \t Loss: -0.012186072332163652\nTRAIN: \t Epoch: 193 \t Loss: -0.012170913396403193\nTRAIN: \t Epoch: 193 \t Loss: -0.012181958155424305\nVALD: \t Epoch: 193 \t Loss: -0.01337367668747902\nVALD: \t Epoch: 193 \t Loss: -0.012116394005715847\nVALD: \t Epoch: 193 \t Loss: -0.010375084510693947\nVALD: \t Epoch: 193 \t Loss: -0.010273572570549514\n******************************\nEpoch: social-tag : 193\ntrain_loss -0.012181958155424305\nval_loss -0.010273572570549514\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 194 \t Loss: -0.012356054969131947\nTRAIN: \t Epoch: 194 \t Loss: -0.012645929586142302\nTRAIN: \t Epoch: 194 \t Loss: -0.012609094691773256\nTRAIN: \t Epoch: 194 \t Loss: -0.012636864092200994\nTRAIN: \t Epoch: 194 \t Loss: -0.012606334500014782\nTRAIN: \t Epoch: 194 \t Loss: -0.012508122716099024\nTRAIN: \t Epoch: 194 \t Loss: -0.012365394005818027\nTRAIN: \t Epoch: 194 \t Loss: -0.012343783746473491\nTRAIN: \t Epoch: 194 \t Loss: -0.012319822899169393\nTRAIN: \t Epoch: 194 \t Loss: -0.012252602074295283\nTRAIN: \t Epoch: 194 \t Loss: -0.012308676134456287\nTRAIN: \t Epoch: 194 \t Loss: -0.012150703851754466\nTRAIN: \t Epoch: 194 \t Loss: -0.012178785167634487\nTRAIN: \t Epoch: 194 \t Loss: -0.012249279567705733\nTRAIN: \t Epoch: 194 \t Loss: -0.012291472529371579\nTRAIN: \t Epoch: 194 \t Loss: -0.012239450821653008\nTRAIN: \t Epoch: 194 \t Loss: -0.012216069707364746\nVALD: \t Epoch: 194 \t Loss: -0.013862372376024723\nVALD: \t Epoch: 194 \t Loss: -0.01292258920148015\nVALD: \t Epoch: 194 \t Loss: -0.010941472680618366\nVALD: \t Epoch: 194 \t Loss: -0.010964862957686007\n******************************\nEpoch: social-tag : 194\ntrain_loss -0.012216069707364746\nval_loss -0.010964862957686007\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 195 \t Loss: -0.013027898967266083\nTRAIN: \t Epoch: 195 \t Loss: -0.012790793552994728\nTRAIN: \t Epoch: 195 \t Loss: -0.012154278655846914\nTRAIN: \t Epoch: 195 \t Loss: -0.012134670978412032\nTRAIN: \t Epoch: 195 \t Loss: -0.012198095396161079\nTRAIN: \t Epoch: 195 \t Loss: -0.012311712683488926\nTRAIN: \t Epoch: 195 \t Loss: -0.012263731631849493\nTRAIN: \t Epoch: 195 \t Loss: -0.01227988803293556\nTRAIN: \t Epoch: 195 \t Loss: -0.012278443719777796\nTRAIN: \t Epoch: 195 \t Loss: -0.012245317548513412\nTRAIN: \t Epoch: 195 \t Loss: -0.012235334651036696\nTRAIN: \t Epoch: 195 \t Loss: -0.012241921853274107\nTRAIN: \t Epoch: 195 \t Loss: -0.012259784226234142\nTRAIN: \t Epoch: 195 \t Loss: -0.012233825733086892\nTRAIN: \t Epoch: 195 \t Loss: -0.012177143556376299\nTRAIN: \t Epoch: 195 \t Loss: -0.012182892533019185\nTRAIN: \t Epoch: 195 \t Loss: -0.012225020253522829\nVALD: \t Epoch: 195 \t Loss: -0.013489870354533195\nVALD: \t Epoch: 195 \t Loss: -0.013329514302313328\nVALD: \t Epoch: 195 \t Loss: -0.011618344113230705\nVALD: \t Epoch: 195 \t Loss: -0.011480356642823972\n******************************\nEpoch: social-tag : 195\ntrain_loss -0.012225020253522829\nval_loss -0.011480356642823972\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 196 \t Loss: -0.012671156786382198\nTRAIN: \t Epoch: 196 \t Loss: -0.01234060525894165\nTRAIN: \t Epoch: 196 \t Loss: -0.0122714980194966\nTRAIN: \t Epoch: 196 \t Loss: -0.012208762345835567\nTRAIN: \t Epoch: 196 \t Loss: -0.012194861099123955\nTRAIN: \t Epoch: 196 \t Loss: -0.011920791119337082\nTRAIN: \t Epoch: 196 \t Loss: -0.011852932428675038\nTRAIN: \t Epoch: 196 \t Loss: -0.011956053669564426\nTRAIN: \t Epoch: 196 \t Loss: -0.012050269068115287\nTRAIN: \t Epoch: 196 \t Loss: -0.012046497780829668\nTRAIN: \t Epoch: 196 \t Loss: -0.012054961936717684\nTRAIN: \t Epoch: 196 \t Loss: -0.01209815739033123\nTRAIN: \t Epoch: 196 \t Loss: -0.012127531620745476\nTRAIN: \t Epoch: 196 \t Loss: -0.012113719114235469\nTRAIN: \t Epoch: 196 \t Loss: -0.012108331422011058\nTRAIN: \t Epoch: 196 \t Loss: -0.012170415255241096\nTRAIN: \t Epoch: 196 \t Loss: -0.012218226932666519\nVALD: \t Epoch: 196 \t Loss: -0.014131451956927776\nVALD: \t Epoch: 196 \t Loss: -0.0132213793694973\nVALD: \t Epoch: 196 \t Loss: -0.010891979715476433\nVALD: \t Epoch: 196 \t Loss: -0.010829839282882904\n******************************\nEpoch: social-tag : 196\ntrain_loss -0.012218226932666519\nval_loss -0.010829839282882904\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 197 \t Loss: -0.013216339983046055\nTRAIN: \t Epoch: 197 \t Loss: -0.012713147327303886\nTRAIN: \t Epoch: 197 \t Loss: -0.012228028538326422\nTRAIN: \t Epoch: 197 \t Loss: -0.012214546091854572\nTRAIN: \t Epoch: 197 \t Loss: -0.011944195814430713\nTRAIN: \t Epoch: 197 \t Loss: -0.011950303645183643\nTRAIN: \t Epoch: 197 \t Loss: -0.012025384764586176\nTRAIN: \t Epoch: 197 \t Loss: -0.012142712366767228\nTRAIN: \t Epoch: 197 \t Loss: -0.012171614294250807\nTRAIN: \t Epoch: 197 \t Loss: -0.012114143744111062\nTRAIN: \t Epoch: 197 \t Loss: -0.012137571180408651\nTRAIN: \t Epoch: 197 \t Loss: -0.012110652091602484\nTRAIN: \t Epoch: 197 \t Loss: -0.01211439030101666\nTRAIN: \t Epoch: 197 \t Loss: -0.012123680008309228\nTRAIN: \t Epoch: 197 \t Loss: -0.012173951355119545\nTRAIN: \t Epoch: 197 \t Loss: -0.012186063861008734\nTRAIN: \t Epoch: 197 \t Loss: -0.012188990431075747\nVALD: \t Epoch: 197 \t Loss: -0.014097009785473347\nVALD: \t Epoch: 197 \t Loss: -0.013442967552691698\nVALD: \t Epoch: 197 \t Loss: -0.011575022091468176\nVALD: \t Epoch: 197 \t Loss: -0.011427181923460818\n******************************\nEpoch: social-tag : 197\ntrain_loss -0.012188990431075747\nval_loss -0.011427181923460818\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 198 \t Loss: -0.013079400174319744\nTRAIN: \t Epoch: 198 \t Loss: -0.012736348435282707\nTRAIN: \t Epoch: 198 \t Loss: -0.012506366396943728\nTRAIN: \t Epoch: 198 \t Loss: -0.012516675516963005\nTRAIN: \t Epoch: 198 \t Loss: -0.012631281279027462\nTRAIN: \t Epoch: 198 \t Loss: -0.012253873671094576\nTRAIN: \t Epoch: 198 \t Loss: -0.012369769625365734\nTRAIN: \t Epoch: 198 \t Loss: -0.012429618393070996\nTRAIN: \t Epoch: 198 \t Loss: -0.012228518620961241\nTRAIN: \t Epoch: 198 \t Loss: -0.01218287879601121\nTRAIN: \t Epoch: 198 \t Loss: -0.0121366601775993\nTRAIN: \t Epoch: 198 \t Loss: -0.01218656993781527\nTRAIN: \t Epoch: 198 \t Loss: -0.012177972982709225\nTRAIN: \t Epoch: 198 \t Loss: -0.012169181635337216\nTRAIN: \t Epoch: 198 \t Loss: -0.012212070636451244\nTRAIN: \t Epoch: 198 \t Loss: -0.012259733222890645\nTRAIN: \t Epoch: 198 \t Loss: -0.01223242161513278\nVALD: \t Epoch: 198 \t Loss: -0.014127044007182121\nVALD: \t Epoch: 198 \t Loss: -0.013658160343766212\nVALD: \t Epoch: 198 \t Loss: -0.011634244583547115\nVALD: \t Epoch: 198 \t Loss: -0.011495922140018669\n******************************\nEpoch: social-tag : 198\ntrain_loss -0.01223242161513278\nval_loss -0.011495922140018669\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 199 \t Loss: -0.010896729305386543\nTRAIN: \t Epoch: 199 \t Loss: -0.01159761380404234\nTRAIN: \t Epoch: 199 \t Loss: -0.011680981454749903\nTRAIN: \t Epoch: 199 \t Loss: -0.012120438041165471\nTRAIN: \t Epoch: 199 \t Loss: -0.012132821790874004\nTRAIN: \t Epoch: 199 \t Loss: -0.012112049696346125\nTRAIN: \t Epoch: 199 \t Loss: -0.01220961513796023\nTRAIN: \t Epoch: 199 \t Loss: -0.012248138664290309\nTRAIN: \t Epoch: 199 \t Loss: -0.012324431290229162\nTRAIN: \t Epoch: 199 \t Loss: -0.012255720980465411\nTRAIN: \t Epoch: 199 \t Loss: -0.012297107922759924\nTRAIN: \t Epoch: 199 \t Loss: -0.012351681478321552\nTRAIN: \t Epoch: 199 \t Loss: -0.012339579586226206\nTRAIN: \t Epoch: 199 \t Loss: -0.012338753390525068\nTRAIN: \t Epoch: 199 \t Loss: -0.01230729849388202\nTRAIN: \t Epoch: 199 \t Loss: -0.01223811408272013\nTRAIN: \t Epoch: 199 \t Loss: -0.012227216312153772\nVALD: \t Epoch: 199 \t Loss: -0.013634873554110527\nVALD: \t Epoch: 199 \t Loss: -0.013425794430077076\nVALD: \t Epoch: 199 \t Loss: -0.011557014659047127\nVALD: \t Epoch: 199 \t Loss: -0.011403107357596209\n******************************\nEpoch: social-tag : 199\ntrain_loss -0.012227216312153772\nval_loss -0.011403107357596209\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 200 \t Loss: -0.012267446145415306\nTRAIN: \t Epoch: 200 \t Loss: -0.012102515902370214\nTRAIN: \t Epoch: 200 \t Loss: -0.012519915588200092\nTRAIN: \t Epoch: 200 \t Loss: -0.012440366670489311\nTRAIN: \t Epoch: 200 \t Loss: -0.01216219700872898\nTRAIN: \t Epoch: 200 \t Loss: -0.012114757051070532\nTRAIN: \t Epoch: 200 \t Loss: -0.012084293179214\nTRAIN: \t Epoch: 200 \t Loss: -0.01206973334774375\nTRAIN: \t Epoch: 200 \t Loss: -0.012155557672182718\nTRAIN: \t Epoch: 200 \t Loss: -0.012086873315274716\nTRAIN: \t Epoch: 200 \t Loss: -0.012144253081218763\nTRAIN: \t Epoch: 200 \t Loss: -0.01217256773573657\nTRAIN: \t Epoch: 200 \t Loss: -0.012200753347804913\nTRAIN: \t Epoch: 200 \t Loss: -0.01225970805223499\nTRAIN: \t Epoch: 200 \t Loss: -0.012188488493363063\nTRAIN: \t Epoch: 200 \t Loss: -0.012200805533211678\nTRAIN: \t Epoch: 200 \t Loss: -0.012213444478358284\nVALD: \t Epoch: 200 \t Loss: -0.013692588545382023\nVALD: \t Epoch: 200 \t Loss: -0.013261982705444098\nVALD: \t Epoch: 200 \t Loss: -0.011258603694538275\nVALD: \t Epoch: 200 \t Loss: -0.011252800385633152\n******************************\nEpoch: social-tag : 200\ntrain_loss -0.012213444478358284\nval_loss -0.011252800385633152\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 201 \t Loss: -0.012187211774289608\nTRAIN: \t Epoch: 201 \t Loss: -0.012324823997914791\nTRAIN: \t Epoch: 201 \t Loss: -0.012194495337704817\nTRAIN: \t Epoch: 201 \t Loss: -0.01195874810218811\nTRAIN: \t Epoch: 201 \t Loss: -0.011865796148777008\nTRAIN: \t Epoch: 201 \t Loss: -0.01185671209047238\nTRAIN: \t Epoch: 201 \t Loss: -0.01183927564748696\nTRAIN: \t Epoch: 201 \t Loss: -0.011919774929992855\nTRAIN: \t Epoch: 201 \t Loss: -0.011853586261471113\nTRAIN: \t Epoch: 201 \t Loss: -0.011993130948394538\nTRAIN: \t Epoch: 201 \t Loss: -0.01205456265333024\nTRAIN: \t Epoch: 201 \t Loss: -0.01213202408204476\nTRAIN: \t Epoch: 201 \t Loss: -0.012160671946520988\nTRAIN: \t Epoch: 201 \t Loss: -0.012155031984938043\nTRAIN: \t Epoch: 201 \t Loss: -0.012139987262586752\nTRAIN: \t Epoch: 201 \t Loss: -0.012216528120916337\nTRAIN: \t Epoch: 201 \t Loss: -0.012245738737736687\nVALD: \t Epoch: 201 \t Loss: -0.013978740200400352\nVALD: \t Epoch: 201 \t Loss: -0.013358998578041792\nVALD: \t Epoch: 201 \t Loss: -0.01137962844222784\nVALD: \t Epoch: 201 \t Loss: -0.011251709656325166\n******************************\nEpoch: social-tag : 201\ntrain_loss -0.012245738737736687\nval_loss -0.011251709656325166\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 202 \t Loss: -0.012284928932785988\nTRAIN: \t Epoch: 202 \t Loss: -0.012693258933722973\nTRAIN: \t Epoch: 202 \t Loss: -0.012145770092805227\nTRAIN: \t Epoch: 202 \t Loss: -0.011988705256953835\nTRAIN: \t Epoch: 202 \t Loss: -0.012059702537953854\nTRAIN: \t Epoch: 202 \t Loss: -0.012233078634987274\nTRAIN: \t Epoch: 202 \t Loss: -0.012220746172325951\nTRAIN: \t Epoch: 202 \t Loss: -0.012066496536135674\nTRAIN: \t Epoch: 202 \t Loss: -0.01209044156389104\nTRAIN: \t Epoch: 202 \t Loss: -0.012172841560095548\nTRAIN: \t Epoch: 202 \t Loss: -0.01220320842482827\nTRAIN: \t Epoch: 202 \t Loss: -0.012255247915163636\nTRAIN: \t Epoch: 202 \t Loss: -0.012222059787465977\nTRAIN: \t Epoch: 202 \t Loss: -0.01229442995307701\nTRAIN: \t Epoch: 202 \t Loss: -0.012311110521356266\nTRAIN: \t Epoch: 202 \t Loss: -0.012239550880622119\nTRAIN: \t Epoch: 202 \t Loss: -0.012225962102864727\nVALD: \t Epoch: 202 \t Loss: -0.013892460614442825\nVALD: \t Epoch: 202 \t Loss: -0.012991823721677065\nVALD: \t Epoch: 202 \t Loss: -0.010845527984201908\nVALD: \t Epoch: 202 \t Loss: -0.010714638494921777\n******************************\nEpoch: social-tag : 202\ntrain_loss -0.012225962102864727\nval_loss -0.010714638494921777\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 203 \t Loss: -0.012634756043553352\nTRAIN: \t Epoch: 203 \t Loss: -0.01294276723638177\nTRAIN: \t Epoch: 203 \t Loss: -0.012815988001724085\nTRAIN: \t Epoch: 203 \t Loss: -0.012639110442250967\nTRAIN: \t Epoch: 203 \t Loss: -0.012606726586818695\nTRAIN: \t Epoch: 203 \t Loss: -0.012644735785822073\nTRAIN: \t Epoch: 203 \t Loss: -0.0125044138569917\nTRAIN: \t Epoch: 203 \t Loss: -0.012529012630693614\nTRAIN: \t Epoch: 203 \t Loss: -0.012516326167517237\nTRAIN: \t Epoch: 203 \t Loss: -0.012410038523375988\nTRAIN: \t Epoch: 203 \t Loss: -0.012362506325271997\nTRAIN: \t Epoch: 203 \t Loss: -0.012313658371567726\nTRAIN: \t Epoch: 203 \t Loss: -0.012323163593044648\nTRAIN: \t Epoch: 203 \t Loss: -0.012358213774859905\nTRAIN: \t Epoch: 203 \t Loss: -0.012280444304148356\nTRAIN: \t Epoch: 203 \t Loss: -0.012244440382346511\nTRAIN: \t Epoch: 203 \t Loss: -0.012217750857499514\nVALD: \t Epoch: 203 \t Loss: -0.013934051617980003\nVALD: \t Epoch: 203 \t Loss: -0.01326041016727686\nVALD: \t Epoch: 203 \t Loss: -0.01111562317237258\nVALD: \t Epoch: 203 \t Loss: -0.011079991767982285\n******************************\nEpoch: social-tag : 203\ntrain_loss -0.012217750857499514\nval_loss -0.011079991767982285\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 204 \t Loss: -0.012087523937225342\nTRAIN: \t Epoch: 204 \t Loss: -0.012503047939389944\nTRAIN: \t Epoch: 204 \t Loss: -0.012639173306524754\nTRAIN: \t Epoch: 204 \t Loss: -0.01268753525801003\nTRAIN: \t Epoch: 204 \t Loss: -0.012619025446474553\nTRAIN: \t Epoch: 204 \t Loss: -0.012531063674638668\nTRAIN: \t Epoch: 204 \t Loss: -0.012502421225820268\nTRAIN: \t Epoch: 204 \t Loss: -0.012446927838027477\nTRAIN: \t Epoch: 204 \t Loss: -0.012266513994998403\nTRAIN: \t Epoch: 204 \t Loss: -0.012255361117422581\nTRAIN: \t Epoch: 204 \t Loss: -0.012257448482242498\nTRAIN: \t Epoch: 204 \t Loss: -0.012337748737384876\nTRAIN: \t Epoch: 204 \t Loss: -0.012318689805956988\nTRAIN: \t Epoch: 204 \t Loss: -0.012288830376097135\nTRAIN: \t Epoch: 204 \t Loss: -0.012239126674830913\nTRAIN: \t Epoch: 204 \t Loss: -0.01220780739095062\nTRAIN: \t Epoch: 204 \t Loss: -0.012204735398744091\nVALD: \t Epoch: 204 \t Loss: -0.014155724085867405\nVALD: \t Epoch: 204 \t Loss: -0.013308101333677769\nVALD: \t Epoch: 204 \t Loss: -0.011294804513454437\nVALD: \t Epoch: 204 \t Loss: -0.011179825265012577\n******************************\nEpoch: social-tag : 204\ntrain_loss -0.012204735398744091\nval_loss -0.011179825265012577\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 205 \t Loss: -0.012025059200823307\nTRAIN: \t Epoch: 205 \t Loss: -0.012346075382083654\nTRAIN: \t Epoch: 205 \t Loss: -0.012133701704442501\nTRAIN: \t Epoch: 205 \t Loss: -0.012411734322085977\nTRAIN: \t Epoch: 205 \t Loss: -0.012303446233272553\nTRAIN: \t Epoch: 205 \t Loss: -0.012216762794802586\nTRAIN: \t Epoch: 205 \t Loss: -0.012169448126639639\nTRAIN: \t Epoch: 205 \t Loss: -0.01224759768228978\nTRAIN: \t Epoch: 205 \t Loss: -0.012254460611277156\nTRAIN: \t Epoch: 205 \t Loss: -0.01225379966199398\nTRAIN: \t Epoch: 205 \t Loss: -0.0123008516701785\nTRAIN: \t Epoch: 205 \t Loss: -0.012204655523722371\nTRAIN: \t Epoch: 205 \t Loss: -0.012195019911114987\nTRAIN: \t Epoch: 205 \t Loss: -0.012194275124264615\nTRAIN: \t Epoch: 205 \t Loss: -0.01222765160103639\nTRAIN: \t Epoch: 205 \t Loss: -0.012261749943718314\nTRAIN: \t Epoch: 205 \t Loss: -0.012225540891063936\nVALD: \t Epoch: 205 \t Loss: -0.014064932242035866\nVALD: \t Epoch: 205 \t Loss: -0.01341503532603383\nVALD: \t Epoch: 205 \t Loss: -0.011657753959298134\nVALD: \t Epoch: 205 \t Loss: -0.011380854718937369\n******************************\nEpoch: social-tag : 205\ntrain_loss -0.012225540891063936\nval_loss -0.011380854718937369\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 206 \t Loss: -0.012041039764881134\nTRAIN: \t Epoch: 206 \t Loss: -0.012410598807036877\nTRAIN: \t Epoch: 206 \t Loss: -0.012013591825962067\nTRAIN: \t Epoch: 206 \t Loss: -0.012061497429385781\nTRAIN: \t Epoch: 206 \t Loss: -0.01197852697223425\nTRAIN: \t Epoch: 206 \t Loss: -0.012170338537544012\nTRAIN: \t Epoch: 206 \t Loss: -0.012253659378205026\nTRAIN: \t Epoch: 206 \t Loss: -0.012315641390159726\nTRAIN: \t Epoch: 206 \t Loss: -0.012367111630737782\nTRAIN: \t Epoch: 206 \t Loss: -0.012265130504965782\nTRAIN: \t Epoch: 206 \t Loss: -0.012251212176951494\nTRAIN: \t Epoch: 206 \t Loss: -0.01227263625090321\nTRAIN: \t Epoch: 206 \t Loss: -0.012253115168557717\nTRAIN: \t Epoch: 206 \t Loss: -0.012273498877350773\nTRAIN: \t Epoch: 206 \t Loss: -0.012292731491227945\nTRAIN: \t Epoch: 206 \t Loss: -0.012248114799149334\nTRAIN: \t Epoch: 206 \t Loss: -0.012237474830313162\nVALD: \t Epoch: 206 \t Loss: -0.0141216441988945\nVALD: \t Epoch: 206 \t Loss: -0.013631573878228664\nVALD: \t Epoch: 206 \t Loss: -0.01156735218440493\nVALD: \t Epoch: 206 \t Loss: -0.011380674001461493\n******************************\nEpoch: social-tag : 206\ntrain_loss -0.012237474830313162\nval_loss -0.011380674001461493\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 207 \t Loss: -0.01187578309327364\nTRAIN: \t Epoch: 207 \t Loss: -0.012497433926910162\nTRAIN: \t Epoch: 207 \t Loss: -0.012417157491048178\nTRAIN: \t Epoch: 207 \t Loss: -0.012417741352692246\nTRAIN: \t Epoch: 207 \t Loss: -0.012302640452980995\nTRAIN: \t Epoch: 207 \t Loss: -0.012324747939904531\nTRAIN: \t Epoch: 207 \t Loss: -0.012287866457232408\nTRAIN: \t Epoch: 207 \t Loss: -0.012310814578086138\nTRAIN: \t Epoch: 207 \t Loss: -0.012347799311909411\nTRAIN: \t Epoch: 207 \t Loss: -0.01233476810157299\nTRAIN: \t Epoch: 207 \t Loss: -0.012391575358130714\nTRAIN: \t Epoch: 207 \t Loss: -0.012330698470274607\nTRAIN: \t Epoch: 207 \t Loss: -0.012323463049072485\nTRAIN: \t Epoch: 207 \t Loss: -0.012309980379151446\nTRAIN: \t Epoch: 207 \t Loss: -0.012222803259889285\nTRAIN: \t Epoch: 207 \t Loss: -0.012216984468977898\nTRAIN: \t Epoch: 207 \t Loss: -0.012189999787193356\nVALD: \t Epoch: 207 \t Loss: -0.014246952719986439\nVALD: \t Epoch: 207 \t Loss: -0.01367869833484292\nVALD: \t Epoch: 207 \t Loss: -0.011628468210498491\nVALD: \t Epoch: 207 \t Loss: -0.011471874223735756\n******************************\nEpoch: social-tag : 207\ntrain_loss -0.012189999787193356\nval_loss -0.011471874223735756\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 208 \t Loss: -0.012208799831569195\nTRAIN: \t Epoch: 208 \t Loss: -0.011959344614297152\nTRAIN: \t Epoch: 208 \t Loss: -0.012188464403152466\nTRAIN: \t Epoch: 208 \t Loss: -0.0124041554518044\nTRAIN: \t Epoch: 208 \t Loss: -0.012343260459601879\nTRAIN: \t Epoch: 208 \t Loss: -0.012271586650361618\nTRAIN: \t Epoch: 208 \t Loss: -0.012212428929550307\nTRAIN: \t Epoch: 208 \t Loss: -0.01222085312474519\nTRAIN: \t Epoch: 208 \t Loss: -0.012279582416845692\nTRAIN: \t Epoch: 208 \t Loss: -0.01229716967791319\nTRAIN: \t Epoch: 208 \t Loss: -0.01228750166906552\nTRAIN: \t Epoch: 208 \t Loss: -0.012368273921310902\nTRAIN: \t Epoch: 208 \t Loss: -0.012250289965707522\nTRAIN: \t Epoch: 208 \t Loss: -0.01226985886959093\nTRAIN: \t Epoch: 208 \t Loss: -0.01229097880423069\nTRAIN: \t Epoch: 208 \t Loss: -0.012218196119647473\nTRAIN: \t Epoch: 208 \t Loss: -0.012237737293947826\nVALD: \t Epoch: 208 \t Loss: -0.013287374749779701\nVALD: \t Epoch: 208 \t Loss: -0.012479479424655437\nVALD: \t Epoch: 208 \t Loss: -0.01084522157907486\nVALD: \t Epoch: 208 \t Loss: -0.010741416089786978\n******************************\nEpoch: social-tag : 208\ntrain_loss -0.012237737293947826\nval_loss -0.010741416089786978\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 209 \t Loss: -0.012543752789497375\nTRAIN: \t Epoch: 209 \t Loss: -0.012281838804483414\nTRAIN: \t Epoch: 209 \t Loss: -0.012399284479518732\nTRAIN: \t Epoch: 209 \t Loss: -0.012419088277965784\nTRAIN: \t Epoch: 209 \t Loss: -0.012390646710991859\nTRAIN: \t Epoch: 209 \t Loss: -0.012306926771998405\nTRAIN: \t Epoch: 209 \t Loss: -0.01229877583682537\nTRAIN: \t Epoch: 209 \t Loss: -0.012258718139491975\nTRAIN: \t Epoch: 209 \t Loss: -0.012297907947666116\nTRAIN: \t Epoch: 209 \t Loss: -0.01228617588058114\nTRAIN: \t Epoch: 209 \t Loss: -0.012324193069203333\nTRAIN: \t Epoch: 209 \t Loss: -0.012330990517511964\nTRAIN: \t Epoch: 209 \t Loss: -0.01237244696284716\nTRAIN: \t Epoch: 209 \t Loss: -0.01223164184817246\nTRAIN: \t Epoch: 209 \t Loss: -0.01224339585751295\nTRAIN: \t Epoch: 209 \t Loss: -0.012240889249369502\nTRAIN: \t Epoch: 209 \t Loss: -0.012252236660005468\nVALD: \t Epoch: 209 \t Loss: -0.01294674538075924\nVALD: \t Epoch: 209 \t Loss: -0.012019640300422907\nVALD: \t Epoch: 209 \t Loss: -0.010200656640032927\nVALD: \t Epoch: 209 \t Loss: -0.010152400372746938\n******************************\nEpoch: social-tag : 209\ntrain_loss -0.012252236660005468\nval_loss -0.010152400372746938\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 210 \t Loss: -0.011837665922939777\nTRAIN: \t Epoch: 210 \t Loss: -0.011850981041789055\nTRAIN: \t Epoch: 210 \t Loss: -0.011747358677287897\nTRAIN: \t Epoch: 210 \t Loss: -0.011684686876833439\nTRAIN: \t Epoch: 210 \t Loss: -0.01174206379801035\nTRAIN: \t Epoch: 210 \t Loss: -0.01190189536040028\nTRAIN: \t Epoch: 210 \t Loss: -0.01207059302500316\nTRAIN: \t Epoch: 210 \t Loss: -0.01210036117117852\nTRAIN: \t Epoch: 210 \t Loss: -0.012156317424443033\nTRAIN: \t Epoch: 210 \t Loss: -0.012130811903625726\nTRAIN: \t Epoch: 210 \t Loss: -0.012159018607979471\nTRAIN: \t Epoch: 210 \t Loss: -0.012225089517111579\nTRAIN: \t Epoch: 210 \t Loss: -0.012217337408891091\nTRAIN: \t Epoch: 210 \t Loss: -0.012307656795850821\nTRAIN: \t Epoch: 210 \t Loss: -0.012235773044327896\nTRAIN: \t Epoch: 210 \t Loss: -0.012245257559698075\nTRAIN: \t Epoch: 210 \t Loss: -0.012253624923301466\nVALD: \t Epoch: 210 \t Loss: -0.013409971259534359\nVALD: \t Epoch: 210 \t Loss: -0.012283246498554945\nVALD: \t Epoch: 210 \t Loss: -0.010171398675690094\nVALD: \t Epoch: 210 \t Loss: -0.010229902710029465\n******************************\nEpoch: social-tag : 210\ntrain_loss -0.012253624923301466\nval_loss -0.010229902710029465\n{'min_val_epoch': 182, 'min_val_loss': -0.011532564125137177}\n******************************\nTRAIN: \t Epoch: 211 \t Loss: -0.011779719963669777\nTRAIN: \t Epoch: 211 \t Loss: -0.012061223853379488\nTRAIN: \t Epoch: 211 \t Loss: -0.012154233331481615\nTRAIN: \t Epoch: 211 \t Loss: -0.012131660478189588\nTRAIN: \t Epoch: 211 \t Loss: -0.012345243990421296\nTRAIN: \t Epoch: 211 \t Loss: -0.012358735936383406\nTRAIN: \t Epoch: 211 \t Loss: -0.012259074220699924\nTRAIN: \t Epoch: 211 \t Loss: -0.012293275445699692\nTRAIN: \t Epoch: 211 \t Loss: -0.01225866015172667\nTRAIN: \t Epoch: 211 \t Loss: -0.012216930650174618\nTRAIN: \t Epoch: 211 \t Loss: -0.012231515889818018\nTRAIN: \t Epoch: 211 \t Loss: -0.012255514195809761\nTRAIN: \t Epoch: 211 \t Loss: -0.012319248312940964\nTRAIN: \t Epoch: 211 \t Loss: -0.01227578753605485\nTRAIN: \t Epoch: 211 \t Loss: -0.012300229320923487\nTRAIN: \t Epoch: 211 \t Loss: -0.012249141931533813\nTRAIN: \t Epoch: 211 \t Loss: -0.012243538445821314\nVALD: \t Epoch: 211 \t Loss: -0.013983388431370258\nVALD: \t Epoch: 211 \t Loss: -0.01385377673432231\nVALD: \t Epoch: 211 \t Loss: -0.012182677164673805\nVALD: \t Epoch: 211 \t Loss: -0.011872185918385398\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 921/921 [01:34<00:00,  9.70it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.34325901825389826  FDE: 0.5560145048561989\n**************************************************\n******************************\nEpoch: social-tag : 211\ntrain_loss -0.012243538445821314\nval_loss -0.011872185918385398\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 212 \t Loss: -0.01174123864620924\nTRAIN: \t Epoch: 212 \t Loss: -0.012139569967985153\nTRAIN: \t Epoch: 212 \t Loss: -0.012118939310312271\nTRAIN: \t Epoch: 212 \t Loss: -0.012288224417716265\nTRAIN: \t Epoch: 212 \t Loss: -0.012338905222713947\nTRAIN: \t Epoch: 212 \t Loss: -0.012388760223984718\nTRAIN: \t Epoch: 212 \t Loss: -0.012461910689515727\nTRAIN: \t Epoch: 212 \t Loss: -0.01229353272356093\nTRAIN: \t Epoch: 212 \t Loss: -0.01221213500118918\nTRAIN: \t Epoch: 212 \t Loss: -0.012292828410863876\nTRAIN: \t Epoch: 212 \t Loss: -0.012323652986775745\nTRAIN: \t Epoch: 212 \t Loss: -0.012294557799274722\nTRAIN: \t Epoch: 212 \t Loss: -0.012282741685899405\nTRAIN: \t Epoch: 212 \t Loss: -0.012297202699950762\nTRAIN: \t Epoch: 212 \t Loss: -0.012362865917384624\nTRAIN: \t Epoch: 212 \t Loss: -0.012293157633394003\nTRAIN: \t Epoch: 212 \t Loss: -0.012241417739653227\nVALD: \t Epoch: 212 \t Loss: -0.014289915561676025\nVALD: \t Epoch: 212 \t Loss: -0.013813296798616648\nVALD: \t Epoch: 212 \t Loss: -0.011915597754220167\nVALD: \t Epoch: 212 \t Loss: -0.011722127834479967\n******************************\nEpoch: social-tag : 212\ntrain_loss -0.012241417739653227\nval_loss -0.011722127834479967\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 213 \t Loss: -0.013150610029697418\nTRAIN: \t Epoch: 213 \t Loss: -0.011945347767323256\nTRAIN: \t Epoch: 213 \t Loss: -0.011955896702905497\nTRAIN: \t Epoch: 213 \t Loss: -0.012013347120955586\nTRAIN: \t Epoch: 213 \t Loss: -0.01210790779441595\nTRAIN: \t Epoch: 213 \t Loss: -0.012145680841058493\nTRAIN: \t Epoch: 213 \t Loss: -0.01226789437766586\nTRAIN: \t Epoch: 213 \t Loss: -0.012260583112947643\nTRAIN: \t Epoch: 213 \t Loss: -0.012160035781562328\nTRAIN: \t Epoch: 213 \t Loss: -0.012149111926555633\nTRAIN: \t Epoch: 213 \t Loss: -0.012105730023573746\nTRAIN: \t Epoch: 213 \t Loss: -0.01216467097401619\nTRAIN: \t Epoch: 213 \t Loss: -0.012202290603174614\nTRAIN: \t Epoch: 213 \t Loss: -0.012257520442030259\nTRAIN: \t Epoch: 213 \t Loss: -0.012268133896092574\nTRAIN: \t Epoch: 213 \t Loss: -0.012296343455091119\nTRAIN: \t Epoch: 213 \t Loss: -0.012269609523090448\nVALD: \t Epoch: 213 \t Loss: -0.013801837339997292\nVALD: \t Epoch: 213 \t Loss: -0.013321477454155684\nVALD: \t Epoch: 213 \t Loss: -0.011441329183677832\nVALD: \t Epoch: 213 \t Loss: -0.011302350286000264\n******************************\nEpoch: social-tag : 213\ntrain_loss -0.012269609523090448\nval_loss -0.011302350286000264\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 214 \t Loss: -0.010973910801112652\nTRAIN: \t Epoch: 214 \t Loss: -0.011810501106083393\nTRAIN: \t Epoch: 214 \t Loss: -0.011891433037817478\nTRAIN: \t Epoch: 214 \t Loss: -0.012083160923793912\nTRAIN: \t Epoch: 214 \t Loss: -0.012158151157200337\nTRAIN: \t Epoch: 214 \t Loss: -0.012273780691126982\nTRAIN: \t Epoch: 214 \t Loss: -0.012296660004981927\nTRAIN: \t Epoch: 214 \t Loss: -0.012332524987868965\nTRAIN: \t Epoch: 214 \t Loss: -0.012299894148276912\nTRAIN: \t Epoch: 214 \t Loss: -0.0122589573264122\nTRAIN: \t Epoch: 214 \t Loss: -0.012237769297578117\nTRAIN: \t Epoch: 214 \t Loss: -0.012215701397508383\nTRAIN: \t Epoch: 214 \t Loss: -0.01225898379030136\nTRAIN: \t Epoch: 214 \t Loss: -0.012277636810072831\nTRAIN: \t Epoch: 214 \t Loss: -0.0122874166816473\nTRAIN: \t Epoch: 214 \t Loss: -0.012265302881132811\nTRAIN: \t Epoch: 214 \t Loss: -0.01227962239786531\nVALD: \t Epoch: 214 \t Loss: -0.014389227144420147\nVALD: \t Epoch: 214 \t Loss: -0.01377933518961072\nVALD: \t Epoch: 214 \t Loss: -0.011881887602309385\nVALD: \t Epoch: 214 \t Loss: -0.011650914679506344\n******************************\nEpoch: social-tag : 214\ntrain_loss -0.01227962239786531\nval_loss -0.011650914679506344\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 215 \t Loss: -0.012400881387293339\nTRAIN: \t Epoch: 215 \t Loss: -0.012005962897092104\nTRAIN: \t Epoch: 215 \t Loss: -0.011702848287920157\nTRAIN: \t Epoch: 215 \t Loss: -0.011781071312725544\nTRAIN: \t Epoch: 215 \t Loss: -0.011916746571660041\nTRAIN: \t Epoch: 215 \t Loss: -0.012078231200575829\nTRAIN: \t Epoch: 215 \t Loss: -0.012051135035497802\nTRAIN: \t Epoch: 215 \t Loss: -0.01209129043854773\nTRAIN: \t Epoch: 215 \t Loss: -0.012074029590520594\nTRAIN: \t Epoch: 215 \t Loss: -0.012062618974596262\nTRAIN: \t Epoch: 215 \t Loss: -0.012084391137415712\nTRAIN: \t Epoch: 215 \t Loss: -0.012069269937152663\nTRAIN: \t Epoch: 215 \t Loss: -0.012156206756257094\nTRAIN: \t Epoch: 215 \t Loss: -0.012204087605433804\nTRAIN: \t Epoch: 215 \t Loss: -0.012268390754858654\nTRAIN: \t Epoch: 215 \t Loss: -0.012266012956388295\nTRAIN: \t Epoch: 215 \t Loss: -0.01226843949971777\nVALD: \t Epoch: 215 \t Loss: -0.014133748598396778\nVALD: \t Epoch: 215 \t Loss: -0.013370761647820473\nVALD: \t Epoch: 215 \t Loss: -0.011265932582318783\nVALD: \t Epoch: 215 \t Loss: -0.011158276698784438\n******************************\nEpoch: social-tag : 215\ntrain_loss -0.01226843949971777\nval_loss -0.011158276698784438\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 216 \t Loss: -0.011910946108400822\nTRAIN: \t Epoch: 216 \t Loss: -0.011562409345060587\nTRAIN: \t Epoch: 216 \t Loss: -0.01191005731622378\nTRAIN: \t Epoch: 216 \t Loss: -0.011936272494494915\nTRAIN: \t Epoch: 216 \t Loss: -0.011940724588930607\nTRAIN: \t Epoch: 216 \t Loss: -0.012043916465093693\nTRAIN: \t Epoch: 216 \t Loss: -0.012194095977715083\nTRAIN: \t Epoch: 216 \t Loss: -0.012165602529421449\nTRAIN: \t Epoch: 216 \t Loss: -0.012091976176533435\nTRAIN: \t Epoch: 216 \t Loss: -0.012186300661414861\nTRAIN: \t Epoch: 216 \t Loss: -0.012199715626510706\nTRAIN: \t Epoch: 216 \t Loss: -0.012238105603804192\nTRAIN: \t Epoch: 216 \t Loss: -0.012309983587608887\nTRAIN: \t Epoch: 216 \t Loss: -0.01232266113428133\nTRAIN: \t Epoch: 216 \t Loss: -0.012305680165688197\nTRAIN: \t Epoch: 216 \t Loss: -0.012268580263480544\nTRAIN: \t Epoch: 216 \t Loss: -0.01227357867879398\nVALD: \t Epoch: 216 \t Loss: -0.014284943230450153\nVALD: \t Epoch: 216 \t Loss: -0.013485669624060392\nVALD: \t Epoch: 216 \t Loss: -0.011557722153762976\nVALD: \t Epoch: 216 \t Loss: -0.011425972460748668\n******************************\nEpoch: social-tag : 216\ntrain_loss -0.01227357867879398\nval_loss -0.011425972460748668\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 217 \t Loss: -0.011890240013599396\nTRAIN: \t Epoch: 217 \t Loss: -0.011983734089881182\nTRAIN: \t Epoch: 217 \t Loss: -0.012312404500941435\nTRAIN: \t Epoch: 217 \t Loss: -0.012462098617106676\nTRAIN: \t Epoch: 217 \t Loss: -0.01240047737956047\nTRAIN: \t Epoch: 217 \t Loss: -0.012314207231005033\nTRAIN: \t Epoch: 217 \t Loss: -0.012267201207578182\nTRAIN: \t Epoch: 217 \t Loss: -0.01229805441107601\nTRAIN: \t Epoch: 217 \t Loss: -0.012326258234679699\nTRAIN: \t Epoch: 217 \t Loss: -0.012388432864099741\nTRAIN: \t Epoch: 217 \t Loss: -0.012385418723252687\nTRAIN: \t Epoch: 217 \t Loss: -0.012302088706443707\nTRAIN: \t Epoch: 217 \t Loss: -0.012185514689638065\nTRAIN: \t Epoch: 217 \t Loss: -0.012227429143552269\nTRAIN: \t Epoch: 217 \t Loss: -0.012227357489367326\nTRAIN: \t Epoch: 217 \t Loss: -0.012284126365557313\nTRAIN: \t Epoch: 217 \t Loss: -0.012270450366265846\nVALD: \t Epoch: 217 \t Loss: -0.013302543200552464\nVALD: \t Epoch: 217 \t Loss: -0.012246518395841122\nVALD: \t Epoch: 217 \t Loss: -0.010298282063255707\nVALD: \t Epoch: 217 \t Loss: -0.010252771858207717\n******************************\nEpoch: social-tag : 217\ntrain_loss -0.012270450366265846\nval_loss -0.010252771858207717\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 218 \t Loss: -0.012441182509064674\nTRAIN: \t Epoch: 218 \t Loss: -0.01252780295908451\nTRAIN: \t Epoch: 218 \t Loss: -0.012570720786849657\nTRAIN: \t Epoch: 218 \t Loss: -0.012702983804047108\nTRAIN: \t Epoch: 218 \t Loss: -0.012448079511523246\nTRAIN: \t Epoch: 218 \t Loss: -0.012501260886589685\nTRAIN: \t Epoch: 218 \t Loss: -0.01235107212726559\nTRAIN: \t Epoch: 218 \t Loss: -0.012345995521172881\nTRAIN: \t Epoch: 218 \t Loss: -0.012352412152621482\nTRAIN: \t Epoch: 218 \t Loss: -0.012272205110639334\nTRAIN: \t Epoch: 218 \t Loss: -0.01231091588058255\nTRAIN: \t Epoch: 218 \t Loss: -0.012285490520298481\nTRAIN: \t Epoch: 218 \t Loss: -0.012253836871912846\nTRAIN: \t Epoch: 218 \t Loss: -0.012260260792183024\nTRAIN: \t Epoch: 218 \t Loss: -0.012311750960846742\nTRAIN: \t Epoch: 218 \t Loss: -0.012324454146437347\nTRAIN: \t Epoch: 218 \t Loss: -0.012304545803503557\nVALD: \t Epoch: 218 \t Loss: -0.01366338413208723\nVALD: \t Epoch: 218 \t Loss: -0.012729974929243326\nVALD: \t Epoch: 218 \t Loss: -0.01053206172461311\nVALD: \t Epoch: 218 \t Loss: -0.010540694176793812\n******************************\nEpoch: social-tag : 218\ntrain_loss -0.012304545803503557\nval_loss -0.010540694176793812\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 219 \t Loss: -0.01001071184873581\nTRAIN: \t Epoch: 219 \t Loss: -0.011250081472098827\nTRAIN: \t Epoch: 219 \t Loss: -0.01139328752954801\nTRAIN: \t Epoch: 219 \t Loss: -0.011955765075981617\nTRAIN: \t Epoch: 219 \t Loss: -0.012189782969653607\nTRAIN: \t Epoch: 219 \t Loss: -0.012284393111864725\nTRAIN: \t Epoch: 219 \t Loss: -0.01231904314564807\nTRAIN: \t Epoch: 219 \t Loss: -0.012203500024043024\nTRAIN: \t Epoch: 219 \t Loss: -0.012306421788202392\nTRAIN: \t Epoch: 219 \t Loss: -0.012281086202710867\nTRAIN: \t Epoch: 219 \t Loss: -0.012320257554000074\nTRAIN: \t Epoch: 219 \t Loss: -0.012304663580531875\nTRAIN: \t Epoch: 219 \t Loss: -0.01234384561673953\nTRAIN: \t Epoch: 219 \t Loss: -0.012354641421032804\nTRAIN: \t Epoch: 219 \t Loss: -0.012291249260306359\nTRAIN: \t Epoch: 219 \t Loss: -0.012292986211832613\nTRAIN: \t Epoch: 219 \t Loss: -0.012277078843026451\nVALD: \t Epoch: 219 \t Loss: -0.013914940878748894\nVALD: \t Epoch: 219 \t Loss: -0.013030684553086758\nVALD: \t Epoch: 219 \t Loss: -0.011120327903578678\nVALD: \t Epoch: 219 \t Loss: -0.011115283428313966\n******************************\nEpoch: social-tag : 219\ntrain_loss -0.012277078843026451\nval_loss -0.011115283428313966\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 220 \t Loss: -0.011738747358322144\nTRAIN: \t Epoch: 220 \t Loss: -0.012715731281787157\nTRAIN: \t Epoch: 220 \t Loss: -0.012715126077334086\nTRAIN: \t Epoch: 220 \t Loss: -0.012631682213395834\nTRAIN: \t Epoch: 220 \t Loss: -0.012677175737917424\nTRAIN: \t Epoch: 220 \t Loss: -0.01266805004949371\nTRAIN: \t Epoch: 220 \t Loss: -0.012639473990670272\nTRAIN: \t Epoch: 220 \t Loss: -0.01253110880497843\nTRAIN: \t Epoch: 220 \t Loss: -0.012498346467812857\nTRAIN: \t Epoch: 220 \t Loss: -0.012444190308451652\nTRAIN: \t Epoch: 220 \t Loss: -0.012399000623686747\nTRAIN: \t Epoch: 220 \t Loss: -0.01241162815131247\nTRAIN: \t Epoch: 220 \t Loss: -0.012410196977166029\nTRAIN: \t Epoch: 220 \t Loss: -0.012337474085922753\nTRAIN: \t Epoch: 220 \t Loss: -0.012281859355668227\nTRAIN: \t Epoch: 220 \t Loss: -0.012286183016840369\nTRAIN: \t Epoch: 220 \t Loss: -0.012287599740154816\nVALD: \t Epoch: 220 \t Loss: -0.01327346172183752\nVALD: \t Epoch: 220 \t Loss: -0.012276336550712585\nVALD: \t Epoch: 220 \t Loss: -0.009390891452009479\nVALD: \t Epoch: 220 \t Loss: -0.009602034341789292\n******************************\nEpoch: social-tag : 220\ntrain_loss -0.012287599740154816\nval_loss -0.009602034341789292\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 221 \t Loss: -0.012225918471813202\nTRAIN: \t Epoch: 221 \t Loss: -0.01232623029500246\nTRAIN: \t Epoch: 221 \t Loss: -0.012254004056255022\nTRAIN: \t Epoch: 221 \t Loss: -0.012160042766481638\nTRAIN: \t Epoch: 221 \t Loss: -0.012235089018940926\nTRAIN: \t Epoch: 221 \t Loss: -0.012339769707371792\nTRAIN: \t Epoch: 221 \t Loss: -0.01225695280092103\nTRAIN: \t Epoch: 221 \t Loss: -0.012204070808365941\nTRAIN: \t Epoch: 221 \t Loss: -0.012190356643663513\nTRAIN: \t Epoch: 221 \t Loss: -0.012274777237325906\nTRAIN: \t Epoch: 221 \t Loss: -0.012286450967869976\nTRAIN: \t Epoch: 221 \t Loss: -0.01226054683017234\nTRAIN: \t Epoch: 221 \t Loss: -0.012262514075980736\nTRAIN: \t Epoch: 221 \t Loss: -0.012264326214790344\nTRAIN: \t Epoch: 221 \t Loss: -0.012231248120466868\nTRAIN: \t Epoch: 221 \t Loss: -0.012226552178617567\nTRAIN: \t Epoch: 221 \t Loss: -0.012256106277081099\nVALD: \t Epoch: 221 \t Loss: -0.013951101340353489\nVALD: \t Epoch: 221 \t Loss: -0.01313720177859068\nVALD: \t Epoch: 221 \t Loss: -0.010875386962046226\nVALD: \t Epoch: 221 \t Loss: -0.010836428391957236\n******************************\nEpoch: social-tag : 221\ntrain_loss -0.012256106277081099\nval_loss -0.010836428391957236\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 222 \t Loss: -0.011637563817203045\nTRAIN: \t Epoch: 222 \t Loss: -0.012164975516498089\nTRAIN: \t Epoch: 222 \t Loss: -0.01235118011633555\nTRAIN: \t Epoch: 222 \t Loss: -0.012487638741731644\nTRAIN: \t Epoch: 222 \t Loss: -0.012430448457598687\nTRAIN: \t Epoch: 222 \t Loss: -0.012223624313871065\nTRAIN: \t Epoch: 222 \t Loss: -0.012055536732077599\nTRAIN: \t Epoch: 222 \t Loss: -0.01213283883407712\nTRAIN: \t Epoch: 222 \t Loss: -0.012144168321457174\nTRAIN: \t Epoch: 222 \t Loss: -0.012186197005212307\nTRAIN: \t Epoch: 222 \t Loss: -0.012152687900445679\nTRAIN: \t Epoch: 222 \t Loss: -0.012207088448728124\nTRAIN: \t Epoch: 222 \t Loss: -0.012212221080867143\nTRAIN: \t Epoch: 222 \t Loss: -0.012257718414600407\nTRAIN: \t Epoch: 222 \t Loss: -0.012262885955472787\nTRAIN: \t Epoch: 222 \t Loss: -0.012311791244428605\nTRAIN: \t Epoch: 222 \t Loss: -0.012273088097572327\nVALD: \t Epoch: 222 \t Loss: -0.013995125889778137\nVALD: \t Epoch: 222 \t Loss: -0.013147818855941296\nVALD: \t Epoch: 222 \t Loss: -0.010929494320104519\nVALD: \t Epoch: 222 \t Loss: -0.010933497113857917\n******************************\nEpoch: social-tag : 222\ntrain_loss -0.012273088097572327\nval_loss -0.010933497113857917\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 223 \t Loss: -0.011274111457169056\nTRAIN: \t Epoch: 223 \t Loss: -0.011783103458583355\nTRAIN: \t Epoch: 223 \t Loss: -0.012046217918395996\nTRAIN: \t Epoch: 223 \t Loss: -0.012140902690589428\nTRAIN: \t Epoch: 223 \t Loss: -0.012212255969643593\nTRAIN: \t Epoch: 223 \t Loss: -0.012364257282267014\nTRAIN: \t Epoch: 223 \t Loss: -0.01226403737174613\nTRAIN: \t Epoch: 223 \t Loss: -0.012144700158387423\nTRAIN: \t Epoch: 223 \t Loss: -0.012161024949616857\nTRAIN: \t Epoch: 223 \t Loss: -0.012160829361528158\nTRAIN: \t Epoch: 223 \t Loss: -0.012207496657290241\nTRAIN: \t Epoch: 223 \t Loss: -0.012199676440407833\nTRAIN: \t Epoch: 223 \t Loss: -0.01224293695906034\nTRAIN: \t Epoch: 223 \t Loss: -0.012266818167907851\nTRAIN: \t Epoch: 223 \t Loss: -0.012284548146029313\nTRAIN: \t Epoch: 223 \t Loss: -0.012327038100920618\nTRAIN: \t Epoch: 223 \t Loss: -0.012314715930684046\nVALD: \t Epoch: 223 \t Loss: -0.013564703986048698\nVALD: \t Epoch: 223 \t Loss: -0.012707642279565334\nVALD: \t Epoch: 223 \t Loss: -0.010357749803612629\nVALD: \t Epoch: 223 \t Loss: -0.010356634319899324\n******************************\nEpoch: social-tag : 223\ntrain_loss -0.012314715930684046\nval_loss -0.010356634319899324\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 224 \t Loss: -0.012298781424760818\nTRAIN: \t Epoch: 224 \t Loss: -0.012062621768563986\nTRAIN: \t Epoch: 224 \t Loss: -0.012608321073154608\nTRAIN: \t Epoch: 224 \t Loss: -0.01249333145096898\nTRAIN: \t Epoch: 224 \t Loss: -0.012268297746777535\nTRAIN: \t Epoch: 224 \t Loss: -0.012280882025758425\nTRAIN: \t Epoch: 224 \t Loss: -0.012182132207921572\nTRAIN: \t Epoch: 224 \t Loss: -0.012240573996677995\nTRAIN: \t Epoch: 224 \t Loss: -0.012197066615853045\nTRAIN: \t Epoch: 224 \t Loss: -0.012218993715941906\nTRAIN: \t Epoch: 224 \t Loss: -0.012256935154172506\nTRAIN: \t Epoch: 224 \t Loss: -0.012300256562108794\nTRAIN: \t Epoch: 224 \t Loss: -0.01233154427833282\nTRAIN: \t Epoch: 224 \t Loss: -0.012371046202523368\nTRAIN: \t Epoch: 224 \t Loss: -0.01230738361676534\nTRAIN: \t Epoch: 224 \t Loss: -0.012314204534050077\nTRAIN: \t Epoch: 224 \t Loss: -0.01229678656002789\nVALD: \t Epoch: 224 \t Loss: -0.013938591815531254\nVALD: \t Epoch: 224 \t Loss: -0.013102438300848007\nVALD: \t Epoch: 224 \t Loss: -0.010483771717796722\nVALD: \t Epoch: 224 \t Loss: -0.010615675868150478\n******************************\nEpoch: social-tag : 224\ntrain_loss -0.01229678656002789\nval_loss -0.010615675868150478\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 225 \t Loss: -0.01234870683401823\nTRAIN: \t Epoch: 225 \t Loss: -0.01202043378725648\nTRAIN: \t Epoch: 225 \t Loss: -0.012231257123251757\nTRAIN: \t Epoch: 225 \t Loss: -0.011989399557933211\nTRAIN: \t Epoch: 225 \t Loss: -0.012269898690283299\nTRAIN: \t Epoch: 225 \t Loss: -0.012224179226905107\nTRAIN: \t Epoch: 225 \t Loss: -0.012243880491171564\nTRAIN: \t Epoch: 225 \t Loss: -0.012197894509881735\nTRAIN: \t Epoch: 225 \t Loss: -0.01222269143909216\nTRAIN: \t Epoch: 225 \t Loss: -0.012241530418395995\nTRAIN: \t Epoch: 225 \t Loss: -0.012282043949446896\nTRAIN: \t Epoch: 225 \t Loss: -0.012351454158003131\nTRAIN: \t Epoch: 225 \t Loss: -0.012401515474686256\nTRAIN: \t Epoch: 225 \t Loss: -0.012359484697559051\nTRAIN: \t Epoch: 225 \t Loss: -0.012272682351370653\nTRAIN: \t Epoch: 225 \t Loss: -0.012333867780398577\nTRAIN: \t Epoch: 225 \t Loss: -0.012292370369488543\nVALD: \t Epoch: 225 \t Loss: -0.013078891672194004\nVALD: \t Epoch: 225 \t Loss: -0.011838245671242476\nVALD: \t Epoch: 225 \t Loss: -0.008838759114344915\nVALD: \t Epoch: 225 \t Loss: -0.009135497782282724\n******************************\nEpoch: social-tag : 225\ntrain_loss -0.012292370369488543\nval_loss -0.009135497782282724\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 226 \t Loss: -0.011566342785954475\nTRAIN: \t Epoch: 226 \t Loss: -0.012516184244304895\nTRAIN: \t Epoch: 226 \t Loss: -0.012111859706540903\nTRAIN: \t Epoch: 226 \t Loss: -0.012034098850563169\nTRAIN: \t Epoch: 226 \t Loss: -0.012270275503396988\nTRAIN: \t Epoch: 226 \t Loss: -0.012082198324302832\nTRAIN: \t Epoch: 226 \t Loss: -0.012180465805743421\nTRAIN: \t Epoch: 226 \t Loss: -0.012318249209783971\nTRAIN: \t Epoch: 226 \t Loss: -0.012301188065773912\nTRAIN: \t Epoch: 226 \t Loss: -0.012312768492847681\nTRAIN: \t Epoch: 226 \t Loss: -0.012300232763994823\nTRAIN: \t Epoch: 226 \t Loss: -0.012259432735542456\nTRAIN: \t Epoch: 226 \t Loss: -0.012250988887479672\nTRAIN: \t Epoch: 226 \t Loss: -0.012273927618350302\nTRAIN: \t Epoch: 226 \t Loss: -0.012240005594988664\nTRAIN: \t Epoch: 226 \t Loss: -0.012286702927667648\nTRAIN: \t Epoch: 226 \t Loss: -0.01230332562982133\nVALD: \t Epoch: 226 \t Loss: -0.014344336465001106\nVALD: \t Epoch: 226 \t Loss: -0.013670115731656551\nVALD: \t Epoch: 226 \t Loss: -0.011626710494359335\nVALD: \t Epoch: 226 \t Loss: -0.01147683556684239\n******************************\nEpoch: social-tag : 226\ntrain_loss -0.01230332562982133\nval_loss -0.01147683556684239\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 227 \t Loss: -0.01176813617348671\nTRAIN: \t Epoch: 227 \t Loss: -0.012110215611755848\nTRAIN: \t Epoch: 227 \t Loss: -0.012222969283660253\nTRAIN: \t Epoch: 227 \t Loss: -0.012275167973712087\nTRAIN: \t Epoch: 227 \t Loss: -0.012315515987575054\nTRAIN: \t Epoch: 227 \t Loss: -0.012227131053805351\nTRAIN: \t Epoch: 227 \t Loss: -0.01233620529196092\nTRAIN: \t Epoch: 227 \t Loss: -0.012262382195331156\nTRAIN: \t Epoch: 227 \t Loss: -0.012238484393391345\nTRAIN: \t Epoch: 227 \t Loss: -0.012212086655199527\nTRAIN: \t Epoch: 227 \t Loss: -0.01221042913808064\nTRAIN: \t Epoch: 227 \t Loss: -0.012361975309128562\nTRAIN: \t Epoch: 227 \t Loss: -0.012290668960374136\nTRAIN: \t Epoch: 227 \t Loss: -0.012301205923514707\nTRAIN: \t Epoch: 227 \t Loss: -0.012319473549723625\nTRAIN: \t Epoch: 227 \t Loss: -0.012316368462052196\nTRAIN: \t Epoch: 227 \t Loss: -0.01229215661684672\nVALD: \t Epoch: 227 \t Loss: -0.01399631891399622\nVALD: \t Epoch: 227 \t Loss: -0.013144396245479584\nVALD: \t Epoch: 227 \t Loss: -0.011174687029172977\nVALD: \t Epoch: 227 \t Loss: -0.011131202746294215\n******************************\nEpoch: social-tag : 227\ntrain_loss -0.01229215661684672\nval_loss -0.011131202746294215\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 228 \t Loss: -0.012006992474198341\nTRAIN: \t Epoch: 228 \t Loss: -0.011753004975616932\nTRAIN: \t Epoch: 228 \t Loss: -0.011968779688080152\nTRAIN: \t Epoch: 228 \t Loss: -0.012136573670431972\nTRAIN: \t Epoch: 228 \t Loss: -0.012172996625304222\nTRAIN: \t Epoch: 228 \t Loss: -0.012318441644310951\nTRAIN: \t Epoch: 228 \t Loss: -0.012420643786234515\nTRAIN: \t Epoch: 228 \t Loss: -0.01248717971611768\nTRAIN: \t Epoch: 228 \t Loss: -0.012533546735843023\nTRAIN: \t Epoch: 228 \t Loss: -0.012452846486121416\nTRAIN: \t Epoch: 228 \t Loss: -0.01240040251815861\nTRAIN: \t Epoch: 228 \t Loss: -0.012398635890955726\nTRAIN: \t Epoch: 228 \t Loss: -0.012337264748146901\nTRAIN: \t Epoch: 228 \t Loss: -0.012334151126976525\nTRAIN: \t Epoch: 228 \t Loss: -0.012339094529549281\nTRAIN: \t Epoch: 228 \t Loss: -0.012294408632442355\nTRAIN: \t Epoch: 228 \t Loss: -0.012285942606853716\nVALD: \t Epoch: 228 \t Loss: -0.013899322599172592\nVALD: \t Epoch: 228 \t Loss: -0.012933398131281137\nVALD: \t Epoch: 228 \t Loss: -0.010934250894933939\nVALD: \t Epoch: 228 \t Loss: -0.010884580378998778\n******************************\nEpoch: social-tag : 228\ntrain_loss -0.012285942606853716\nval_loss -0.010884580378998778\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 229 \t Loss: -0.012144609354436398\nTRAIN: \t Epoch: 229 \t Loss: -0.01259393198415637\nTRAIN: \t Epoch: 229 \t Loss: -0.012485131000479063\nTRAIN: \t Epoch: 229 \t Loss: -0.012413215124979615\nTRAIN: \t Epoch: 229 \t Loss: -0.012287808582186699\nTRAIN: \t Epoch: 229 \t Loss: -0.012403264796982208\nTRAIN: \t Epoch: 229 \t Loss: -0.012294285265462739\nTRAIN: \t Epoch: 229 \t Loss: -0.012280348339118063\nTRAIN: \t Epoch: 229 \t Loss: -0.01232756684637732\nTRAIN: \t Epoch: 229 \t Loss: -0.012257017474621535\nTRAIN: \t Epoch: 229 \t Loss: -0.012230274182828989\nTRAIN: \t Epoch: 229 \t Loss: -0.01224580112223824\nTRAIN: \t Epoch: 229 \t Loss: -0.012319160410417961\nTRAIN: \t Epoch: 229 \t Loss: -0.012330398429185152\nTRAIN: \t Epoch: 229 \t Loss: -0.01231721993535757\nTRAIN: \t Epoch: 229 \t Loss: -0.012338754662778229\nTRAIN: \t Epoch: 229 \t Loss: -0.012325775555588982\nVALD: \t Epoch: 229 \t Loss: -0.013854200951755047\nVALD: \t Epoch: 229 \t Loss: -0.013266056776046753\nVALD: \t Epoch: 229 \t Loss: -0.011627910969158014\nVALD: \t Epoch: 229 \t Loss: -0.011435991989638277\n******************************\nEpoch: social-tag : 229\ntrain_loss -0.012325775555588982\nval_loss -0.011435991989638277\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 230 \t Loss: -0.012316180393099785\nTRAIN: \t Epoch: 230 \t Loss: -0.01262591127306223\nTRAIN: \t Epoch: 230 \t Loss: -0.01264195516705513\nTRAIN: \t Epoch: 230 \t Loss: -0.012418983969837427\nTRAIN: \t Epoch: 230 \t Loss: -0.012336045689880847\nTRAIN: \t Epoch: 230 \t Loss: -0.012462151547273\nTRAIN: \t Epoch: 230 \t Loss: -0.012387356055634362\nTRAIN: \t Epoch: 230 \t Loss: -0.012287318939343095\nTRAIN: \t Epoch: 230 \t Loss: -0.012224443153374724\nTRAIN: \t Epoch: 230 \t Loss: -0.012246901076287032\nTRAIN: \t Epoch: 230 \t Loss: -0.012237167070535097\nTRAIN: \t Epoch: 230 \t Loss: -0.012278431871285042\nTRAIN: \t Epoch: 230 \t Loss: -0.012332837025706585\nTRAIN: \t Epoch: 230 \t Loss: -0.012339662693973099\nTRAIN: \t Epoch: 230 \t Loss: -0.012340027838945389\nTRAIN: \t Epoch: 230 \t Loss: -0.012311372789554298\nTRAIN: \t Epoch: 230 \t Loss: -0.01230244313112714\nVALD: \t Epoch: 230 \t Loss: -0.012807553634047508\nVALD: \t Epoch: 230 \t Loss: -0.011370365042239428\nVALD: \t Epoch: 230 \t Loss: -0.008504721568897367\nVALD: \t Epoch: 230 \t Loss: -0.008804902047692183\n******************************\nEpoch: social-tag : 230\ntrain_loss -0.01230244313112714\nval_loss -0.008804902047692183\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 231 \t Loss: -0.012379740364849567\nTRAIN: \t Epoch: 231 \t Loss: -0.012227473314851522\nTRAIN: \t Epoch: 231 \t Loss: -0.012352478690445423\nTRAIN: \t Epoch: 231 \t Loss: -0.012247883016243577\nTRAIN: \t Epoch: 231 \t Loss: -0.012142215110361575\nTRAIN: \t Epoch: 231 \t Loss: -0.012207654304802418\nTRAIN: \t Epoch: 231 \t Loss: -0.01229400746524334\nTRAIN: \t Epoch: 231 \t Loss: -0.012236221344210207\nTRAIN: \t Epoch: 231 \t Loss: -0.01236686110496521\nTRAIN: \t Epoch: 231 \t Loss: -0.012294100690633058\nTRAIN: \t Epoch: 231 \t Loss: -0.01230429375374859\nTRAIN: \t Epoch: 231 \t Loss: -0.012272028252482414\nTRAIN: \t Epoch: 231 \t Loss: -0.012239498731035452\nTRAIN: \t Epoch: 231 \t Loss: -0.012265279955629791\nTRAIN: \t Epoch: 231 \t Loss: -0.012287560167411964\nTRAIN: \t Epoch: 231 \t Loss: -0.012323069386184216\nTRAIN: \t Epoch: 231 \t Loss: -0.012341557437497559\nVALD: \t Epoch: 231 \t Loss: -0.01402365230023861\nVALD: \t Epoch: 231 \t Loss: -0.013098548166453838\nVALD: \t Epoch: 231 \t Loss: -0.010872166138142347\nVALD: \t Epoch: 231 \t Loss: -0.010910068800349436\n******************************\nEpoch: social-tag : 231\ntrain_loss -0.012341557437497559\nval_loss -0.010910068800349436\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 232 \t Loss: -0.011433340609073639\nTRAIN: \t Epoch: 232 \t Loss: -0.011961859185248613\nTRAIN: \t Epoch: 232 \t Loss: -0.01167757778118054\nTRAIN: \t Epoch: 232 \t Loss: -0.0118949837051332\nTRAIN: \t Epoch: 232 \t Loss: -0.011888361535966396\nTRAIN: \t Epoch: 232 \t Loss: -0.01208715777223309\nTRAIN: \t Epoch: 232 \t Loss: -0.012209469718592507\nTRAIN: \t Epoch: 232 \t Loss: -0.01220501505304128\nTRAIN: \t Epoch: 232 \t Loss: -0.012221782675219907\nTRAIN: \t Epoch: 232 \t Loss: -0.01213226355612278\nTRAIN: \t Epoch: 232 \t Loss: -0.012156842191788282\nTRAIN: \t Epoch: 232 \t Loss: -0.012212781546016535\nTRAIN: \t Epoch: 232 \t Loss: -0.012287320354237007\nTRAIN: \t Epoch: 232 \t Loss: -0.012353900620447738\nTRAIN: \t Epoch: 232 \t Loss: -0.012311846017837524\nTRAIN: \t Epoch: 232 \t Loss: -0.012313119659665972\nTRAIN: \t Epoch: 232 \t Loss: -0.012285868101047747\nVALD: \t Epoch: 232 \t Loss: -0.013304862193763256\nVALD: \t Epoch: 232 \t Loss: -0.012059420812875032\nVALD: \t Epoch: 232 \t Loss: -0.009850950135538975\nVALD: \t Epoch: 232 \t Loss: -0.009878522026800585\n******************************\nEpoch: social-tag : 232\ntrain_loss -0.012285868101047747\nval_loss -0.009878522026800585\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 233 \t Loss: -0.012437751516699791\nTRAIN: \t Epoch: 233 \t Loss: -0.012031952384859324\nTRAIN: \t Epoch: 233 \t Loss: -0.01188229334851106\nTRAIN: \t Epoch: 233 \t Loss: -0.012038291664794087\nTRAIN: \t Epoch: 233 \t Loss: -0.012090731412172318\nTRAIN: \t Epoch: 233 \t Loss: -0.012052860576659441\nTRAIN: \t Epoch: 233 \t Loss: -0.01210183636950595\nTRAIN: \t Epoch: 233 \t Loss: -0.01209759630728513\nTRAIN: \t Epoch: 233 \t Loss: -0.012189141992065642\nTRAIN: \t Epoch: 233 \t Loss: -0.01224436955526471\nTRAIN: \t Epoch: 233 \t Loss: -0.012145482088354502\nTRAIN: \t Epoch: 233 \t Loss: -0.012199479698513946\nTRAIN: \t Epoch: 233 \t Loss: -0.012291298462794377\nTRAIN: \t Epoch: 233 \t Loss: -0.012266532917107855\nTRAIN: \t Epoch: 233 \t Loss: -0.012245821952819824\nTRAIN: \t Epoch: 233 \t Loss: -0.012295364227611572\nTRAIN: \t Epoch: 233 \t Loss: -0.01231052235446193\nVALD: \t Epoch: 233 \t Loss: -0.013228134252130985\nVALD: \t Epoch: 233 \t Loss: -0.012778258416801691\nVALD: \t Epoch: 233 \t Loss: -0.01036878644178311\nVALD: \t Epoch: 233 \t Loss: -0.010411003868498964\n******************************\nEpoch: social-tag : 233\ntrain_loss -0.01231052235446193\nval_loss -0.010411003868498964\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 234 \t Loss: -0.012943115085363388\nTRAIN: \t Epoch: 234 \t Loss: -0.01267288625240326\nTRAIN: \t Epoch: 234 \t Loss: -0.012749953816334406\nTRAIN: \t Epoch: 234 \t Loss: -0.012529187835752964\nTRAIN: \t Epoch: 234 \t Loss: -0.012444393150508404\nTRAIN: \t Epoch: 234 \t Loss: -0.012561811910321316\nTRAIN: \t Epoch: 234 \t Loss: -0.012381061911582947\nTRAIN: \t Epoch: 234 \t Loss: -0.012361803906969726\nTRAIN: \t Epoch: 234 \t Loss: -0.012475442555215623\nTRAIN: \t Epoch: 234 \t Loss: -0.012460720259696245\nTRAIN: \t Epoch: 234 \t Loss: -0.012532431886277416\nTRAIN: \t Epoch: 234 \t Loss: -0.012457988457754254\nTRAIN: \t Epoch: 234 \t Loss: -0.01238752743945672\nTRAIN: \t Epoch: 234 \t Loss: -0.012379953504673072\nTRAIN: \t Epoch: 234 \t Loss: -0.012347718762854735\nTRAIN: \t Epoch: 234 \t Loss: -0.012390947435051203\nTRAIN: \t Epoch: 234 \t Loss: -0.012336598314119108\nVALD: \t Epoch: 234 \t Loss: -0.013595853932201862\nVALD: \t Epoch: 234 \t Loss: -0.012515445239841938\nVALD: \t Epoch: 234 \t Loss: -0.010496847797185183\nVALD: \t Epoch: 234 \t Loss: -0.010491166880982603\n******************************\nEpoch: social-tag : 234\ntrain_loss -0.012336598314119108\nval_loss -0.010491166880982603\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 235 \t Loss: -0.01206295471638441\nTRAIN: \t Epoch: 235 \t Loss: -0.012110068928450346\nTRAIN: \t Epoch: 235 \t Loss: -0.011933966850241026\nTRAIN: \t Epoch: 235 \t Loss: -0.011886997381225228\nTRAIN: \t Epoch: 235 \t Loss: -0.012319555878639222\nTRAIN: \t Epoch: 235 \t Loss: -0.012112088656673828\nTRAIN: \t Epoch: 235 \t Loss: -0.012189238199165888\nTRAIN: \t Epoch: 235 \t Loss: -0.012140391860157251\nTRAIN: \t Epoch: 235 \t Loss: -0.012142190606229834\nTRAIN: \t Epoch: 235 \t Loss: -0.012196301948279142\nTRAIN: \t Epoch: 235 \t Loss: -0.012238699519498781\nTRAIN: \t Epoch: 235 \t Loss: -0.012150322242329517\nTRAIN: \t Epoch: 235 \t Loss: -0.012209189554246573\nTRAIN: \t Epoch: 235 \t Loss: -0.012237818712102515\nTRAIN: \t Epoch: 235 \t Loss: -0.012265359672407309\nTRAIN: \t Epoch: 235 \t Loss: -0.012337985914200544\nTRAIN: \t Epoch: 235 \t Loss: -0.012339846993034536\nVALD: \t Epoch: 235 \t Loss: -0.014235828071832657\nVALD: \t Epoch: 235 \t Loss: -0.01356625510379672\nVALD: \t Epoch: 235 \t Loss: -0.01169569045305252\nVALD: \t Epoch: 235 \t Loss: -0.011563108352843873\n******************************\nEpoch: social-tag : 235\ntrain_loss -0.012339846993034536\nval_loss -0.011563108352843873\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 236 \t Loss: -0.011783117428421974\nTRAIN: \t Epoch: 236 \t Loss: -0.01184183731675148\nTRAIN: \t Epoch: 236 \t Loss: -0.012005265181263288\nTRAIN: \t Epoch: 236 \t Loss: -0.01210480323061347\nTRAIN: \t Epoch: 236 \t Loss: -0.012243066914379597\nTRAIN: \t Epoch: 236 \t Loss: -0.012384002096951008\nTRAIN: \t Epoch: 236 \t Loss: -0.012305377450372492\nTRAIN: \t Epoch: 236 \t Loss: -0.012354493490420282\nTRAIN: \t Epoch: 236 \t Loss: -0.012345064845350053\nTRAIN: \t Epoch: 236 \t Loss: -0.012343805190175772\nTRAIN: \t Epoch: 236 \t Loss: -0.012272664345800877\nTRAIN: \t Epoch: 236 \t Loss: -0.012333873038490614\nTRAIN: \t Epoch: 236 \t Loss: -0.012387926905201031\nTRAIN: \t Epoch: 236 \t Loss: -0.012346431879060609\nTRAIN: \t Epoch: 236 \t Loss: -0.01237628497183323\nTRAIN: \t Epoch: 236 \t Loss: -0.012382028857246041\nTRAIN: \t Epoch: 236 \t Loss: -0.012352395774514385\nVALD: \t Epoch: 236 \t Loss: -0.013206932693719864\nVALD: \t Epoch: 236 \t Loss: -0.012904123868793249\nVALD: \t Epoch: 236 \t Loss: -0.010782777642210325\nVALD: \t Epoch: 236 \t Loss: -0.010741655460136855\n******************************\nEpoch: social-tag : 236\ntrain_loss -0.012352395774514385\nval_loss -0.010741655460136855\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 237 \t Loss: -0.012847076170146465\nTRAIN: \t Epoch: 237 \t Loss: -0.012709800153970718\nTRAIN: \t Epoch: 237 \t Loss: -0.012661234475672245\nTRAIN: \t Epoch: 237 \t Loss: -0.012584734708070755\nTRAIN: \t Epoch: 237 \t Loss: -0.01243133544921875\nTRAIN: \t Epoch: 237 \t Loss: -0.012364108115434647\nTRAIN: \t Epoch: 237 \t Loss: -0.012313176345612322\nTRAIN: \t Epoch: 237 \t Loss: -0.012375017045997083\nTRAIN: \t Epoch: 237 \t Loss: -0.012301998730334971\nTRAIN: \t Epoch: 237 \t Loss: -0.01226946385577321\nTRAIN: \t Epoch: 237 \t Loss: -0.012273298237811436\nTRAIN: \t Epoch: 237 \t Loss: -0.01228085944118599\nTRAIN: \t Epoch: 237 \t Loss: -0.012314545563780345\nTRAIN: \t Epoch: 237 \t Loss: -0.012267388004277433\nTRAIN: \t Epoch: 237 \t Loss: -0.01230039019137621\nTRAIN: \t Epoch: 237 \t Loss: -0.012334555387496948\nTRAIN: \t Epoch: 237 \t Loss: -0.012340127349351391\nVALD: \t Epoch: 237 \t Loss: -0.014255867339670658\nVALD: \t Epoch: 237 \t Loss: -0.01334696775302291\nVALD: \t Epoch: 237 \t Loss: -0.011300688919921717\nVALD: \t Epoch: 237 \t Loss: -0.011128202169955134\n******************************\nEpoch: social-tag : 237\ntrain_loss -0.012340127349351391\nval_loss -0.011128202169955134\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 238 \t Loss: -0.012568168342113495\nTRAIN: \t Epoch: 238 \t Loss: -0.012669962830841541\nTRAIN: \t Epoch: 238 \t Loss: -0.012630481272935867\nTRAIN: \t Epoch: 238 \t Loss: -0.012763386825099587\nTRAIN: \t Epoch: 238 \t Loss: -0.012473046779632568\nTRAIN: \t Epoch: 238 \t Loss: -0.01223394414409995\nTRAIN: \t Epoch: 238 \t Loss: -0.012260186219854015\nTRAIN: \t Epoch: 238 \t Loss: -0.012257098569534719\nTRAIN: \t Epoch: 238 \t Loss: -0.012241153046488762\nTRAIN: \t Epoch: 238 \t Loss: -0.01234903447329998\nTRAIN: \t Epoch: 238 \t Loss: -0.012286201712082733\nTRAIN: \t Epoch: 238 \t Loss: -0.012333206599578261\nTRAIN: \t Epoch: 238 \t Loss: -0.012353783831573449\nTRAIN: \t Epoch: 238 \t Loss: -0.012347628894661154\nTRAIN: \t Epoch: 238 \t Loss: -0.012337932114799817\nTRAIN: \t Epoch: 238 \t Loss: -0.012306504999287426\nTRAIN: \t Epoch: 238 \t Loss: -0.012312515356549711\nVALD: \t Epoch: 238 \t Loss: -0.014079383574426174\nVALD: \t Epoch: 238 \t Loss: -0.013028958346694708\nVALD: \t Epoch: 238 \t Loss: -0.010771884117275476\nVALD: \t Epoch: 238 \t Loss: -0.010704637168648239\n******************************\nEpoch: social-tag : 238\ntrain_loss -0.012312515356549711\nval_loss -0.010704637168648239\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 239 \t Loss: -0.0124816307798028\nTRAIN: \t Epoch: 239 \t Loss: -0.01240299828350544\nTRAIN: \t Epoch: 239 \t Loss: -0.012325425321857134\nTRAIN: \t Epoch: 239 \t Loss: -0.012470862129703164\nTRAIN: \t Epoch: 239 \t Loss: -0.012492775544524192\nTRAIN: \t Epoch: 239 \t Loss: -0.012494037548700968\nTRAIN: \t Epoch: 239 \t Loss: -0.012398139307541507\nTRAIN: \t Epoch: 239 \t Loss: -0.012307780794799328\nTRAIN: \t Epoch: 239 \t Loss: -0.012346300503445996\nTRAIN: \t Epoch: 239 \t Loss: -0.012249777838587762\nTRAIN: \t Epoch: 239 \t Loss: -0.012240501882677729\nTRAIN: \t Epoch: 239 \t Loss: -0.012320547209431728\nTRAIN: \t Epoch: 239 \t Loss: -0.012378511663812857\nTRAIN: \t Epoch: 239 \t Loss: -0.012385768217167683\nTRAIN: \t Epoch: 239 \t Loss: -0.012378430366516114\nTRAIN: \t Epoch: 239 \t Loss: -0.012302356190048158\nTRAIN: \t Epoch: 239 \t Loss: -0.01235131278746959\nVALD: \t Epoch: 239 \t Loss: -0.013024671003222466\nVALD: \t Epoch: 239 \t Loss: -0.012064373586326838\nVALD: \t Epoch: 239 \t Loss: -0.010356316498170296\nVALD: \t Epoch: 239 \t Loss: -0.010244030319526047\n******************************\nEpoch: social-tag : 239\ntrain_loss -0.01235131278746959\nval_loss -0.010244030319526047\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 240 \t Loss: -0.011772765778005123\nTRAIN: \t Epoch: 240 \t Loss: -0.011963881552219391\nTRAIN: \t Epoch: 240 \t Loss: -0.011921754417320093\nTRAIN: \t Epoch: 240 \t Loss: -0.012336306041106582\nTRAIN: \t Epoch: 240 \t Loss: -0.012205398082733155\nTRAIN: \t Epoch: 240 \t Loss: -0.012381890323013067\nTRAIN: \t Epoch: 240 \t Loss: -0.012295073164360864\nTRAIN: \t Epoch: 240 \t Loss: -0.012375797028653324\nTRAIN: \t Epoch: 240 \t Loss: -0.012339516956773069\nTRAIN: \t Epoch: 240 \t Loss: -0.012381606176495552\nTRAIN: \t Epoch: 240 \t Loss: -0.012379635802724144\nTRAIN: \t Epoch: 240 \t Loss: -0.012322367557014028\nTRAIN: \t Epoch: 240 \t Loss: -0.012326646023071729\nTRAIN: \t Epoch: 240 \t Loss: -0.012312125015471662\nTRAIN: \t Epoch: 240 \t Loss: -0.012322432237366836\nTRAIN: \t Epoch: 240 \t Loss: -0.012374566635116935\nTRAIN: \t Epoch: 240 \t Loss: -0.012339536523954435\nVALD: \t Epoch: 240 \t Loss: -0.013807473704218864\nVALD: \t Epoch: 240 \t Loss: -0.013132710475474596\nVALD: \t Epoch: 240 \t Loss: -0.010916678079714378\nVALD: \t Epoch: 240 \t Loss: -0.010936799520503976\n******************************\nEpoch: social-tag : 240\ntrain_loss -0.012339536523954435\nval_loss -0.010936799520503976\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 241 \t Loss: -0.011687913909554482\nTRAIN: \t Epoch: 241 \t Loss: -0.012378277722746134\nTRAIN: \t Epoch: 241 \t Loss: -0.012316110854347547\nTRAIN: \t Epoch: 241 \t Loss: -0.012066359398886561\nTRAIN: \t Epoch: 241 \t Loss: -0.012243978492915631\nTRAIN: \t Epoch: 241 \t Loss: -0.012197390974809727\nTRAIN: \t Epoch: 241 \t Loss: -0.012385848643524306\nTRAIN: \t Epoch: 241 \t Loss: -0.012468982371501625\nTRAIN: \t Epoch: 241 \t Loss: -0.012431884080999427\nTRAIN: \t Epoch: 241 \t Loss: -0.012411831319332123\nTRAIN: \t Epoch: 241 \t Loss: -0.012426965277303349\nTRAIN: \t Epoch: 241 \t Loss: -0.012440217814097801\nTRAIN: \t Epoch: 241 \t Loss: -0.012458212673664093\nTRAIN: \t Epoch: 241 \t Loss: -0.012413396899189268\nTRAIN: \t Epoch: 241 \t Loss: -0.012379123953481515\nTRAIN: \t Epoch: 241 \t Loss: -0.012349751603323966\nTRAIN: \t Epoch: 241 \t Loss: -0.012341022068126635\nVALD: \t Epoch: 241 \t Loss: -0.01413597073405981\nVALD: \t Epoch: 241 \t Loss: -0.013311731163412333\nVALD: \t Epoch: 241 \t Loss: -0.01133433310315013\nVALD: \t Epoch: 241 \t Loss: -0.011237890539530985\n******************************\nEpoch: social-tag : 241\ntrain_loss -0.012341022068126635\nval_loss -0.011237890539530985\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 242 \t Loss: -0.011711063794791698\nTRAIN: \t Epoch: 242 \t Loss: -0.011712585110217333\nTRAIN: \t Epoch: 242 \t Loss: -0.012084826827049255\nTRAIN: \t Epoch: 242 \t Loss: -0.0120550945866853\nTRAIN: \t Epoch: 242 \t Loss: -0.012298998422920704\nTRAIN: \t Epoch: 242 \t Loss: -0.012304877396672964\nTRAIN: \t Epoch: 242 \t Loss: -0.012222163379192352\nTRAIN: \t Epoch: 242 \t Loss: -0.012307364260777831\nTRAIN: \t Epoch: 242 \t Loss: -0.012255522732933363\nTRAIN: \t Epoch: 242 \t Loss: -0.012174363899976014\nTRAIN: \t Epoch: 242 \t Loss: -0.01223548628728498\nTRAIN: \t Epoch: 242 \t Loss: -0.012290026294067502\nTRAIN: \t Epoch: 242 \t Loss: -0.012308620561200839\nTRAIN: \t Epoch: 242 \t Loss: -0.012349381311131375\nTRAIN: \t Epoch: 242 \t Loss: -0.01236954815685749\nTRAIN: \t Epoch: 242 \t Loss: -0.012382881192024797\nTRAIN: \t Epoch: 242 \t Loss: -0.012373353044191996\nVALD: \t Epoch: 242 \t Loss: -0.013705315999686718\nVALD: \t Epoch: 242 \t Loss: -0.012977522797882557\nVALD: \t Epoch: 242 \t Loss: -0.011352079920470715\nVALD: \t Epoch: 242 \t Loss: -0.011169704610478141\n******************************\nEpoch: social-tag : 242\ntrain_loss -0.012373353044191996\nval_loss -0.011169704610478141\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 243 \t Loss: -0.011813297867774963\nTRAIN: \t Epoch: 243 \t Loss: -0.012351512908935547\nTRAIN: \t Epoch: 243 \t Loss: -0.012489279732108116\nTRAIN: \t Epoch: 243 \t Loss: -0.012690783478319645\nTRAIN: \t Epoch: 243 \t Loss: -0.012783867865800857\nTRAIN: \t Epoch: 243 \t Loss: -0.012749067197243372\nTRAIN: \t Epoch: 243 \t Loss: -0.01270197584692921\nTRAIN: \t Epoch: 243 \t Loss: -0.01262850093189627\nTRAIN: \t Epoch: 243 \t Loss: -0.012657923934360346\nTRAIN: \t Epoch: 243 \t Loss: -0.012547590304166079\nTRAIN: \t Epoch: 243 \t Loss: -0.012489984997294167\nTRAIN: \t Epoch: 243 \t Loss: -0.012528625161697468\nTRAIN: \t Epoch: 243 \t Loss: -0.012459063042814914\nTRAIN: \t Epoch: 243 \t Loss: -0.012391872970121247\nTRAIN: \t Epoch: 243 \t Loss: -0.01233834692587455\nTRAIN: \t Epoch: 243 \t Loss: -0.01236627547768876\nTRAIN: \t Epoch: 243 \t Loss: -0.012343029519825272\nVALD: \t Epoch: 243 \t Loss: -0.014020796865224838\nVALD: \t Epoch: 243 \t Loss: -0.013143393211066723\nVALD: \t Epoch: 243 \t Loss: -0.011294864118099213\nVALD: \t Epoch: 243 \t Loss: -0.011160018201359732\n******************************\nEpoch: social-tag : 243\ntrain_loss -0.012343029519825272\nval_loss -0.011160018201359732\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 244 \t Loss: -0.012258170172572136\nTRAIN: \t Epoch: 244 \t Loss: -0.012195168063044548\nTRAIN: \t Epoch: 244 \t Loss: -0.011807850562036037\nTRAIN: \t Epoch: 244 \t Loss: -0.012084700632840395\nTRAIN: \t Epoch: 244 \t Loss: -0.012193589843809605\nTRAIN: \t Epoch: 244 \t Loss: -0.012160827871412039\nTRAIN: \t Epoch: 244 \t Loss: -0.012185431750757354\nTRAIN: \t Epoch: 244 \t Loss: -0.012152947834692895\nTRAIN: \t Epoch: 244 \t Loss: -0.012199620198872354\nTRAIN: \t Epoch: 244 \t Loss: -0.012212331872433424\nTRAIN: \t Epoch: 244 \t Loss: -0.012327045625583693\nTRAIN: \t Epoch: 244 \t Loss: -0.012286086411525806\nTRAIN: \t Epoch: 244 \t Loss: -0.012266782613900991\nTRAIN: \t Epoch: 244 \t Loss: -0.012274352700582572\nTRAIN: \t Epoch: 244 \t Loss: -0.01229085580756267\nTRAIN: \t Epoch: 244 \t Loss: -0.01231259509222582\nTRAIN: \t Epoch: 244 \t Loss: -0.012335731337467829\nVALD: \t Epoch: 244 \t Loss: -0.013980783522129059\nVALD: \t Epoch: 244 \t Loss: -0.013255390338599682\nVALD: \t Epoch: 244 \t Loss: -0.011354651612540087\nVALD: \t Epoch: 244 \t Loss: -0.01127795306984298\n******************************\nEpoch: social-tag : 244\ntrain_loss -0.012335731337467829\nval_loss -0.01127795306984298\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 245 \t Loss: -0.012235082685947418\nTRAIN: \t Epoch: 245 \t Loss: -0.012104963418096304\nTRAIN: \t Epoch: 245 \t Loss: -0.01208951169004043\nTRAIN: \t Epoch: 245 \t Loss: -0.011954329907894135\nTRAIN: \t Epoch: 245 \t Loss: -0.012107361480593682\nTRAIN: \t Epoch: 245 \t Loss: -0.012212771301468214\nTRAIN: \t Epoch: 245 \t Loss: -0.012257196009159088\nTRAIN: \t Epoch: 245 \t Loss: -0.012334253056906164\nTRAIN: \t Epoch: 245 \t Loss: -0.012292833170957036\nTRAIN: \t Epoch: 245 \t Loss: -0.012147711310535669\nTRAIN: \t Epoch: 245 \t Loss: -0.012113282626325434\nTRAIN: \t Epoch: 245 \t Loss: -0.012198083257923523\nTRAIN: \t Epoch: 245 \t Loss: -0.012314973399043083\nTRAIN: \t Epoch: 245 \t Loss: -0.012381099896239383\nTRAIN: \t Epoch: 245 \t Loss: -0.012353788626690706\nTRAIN: \t Epoch: 245 \t Loss: -0.012378773535601795\nTRAIN: \t Epoch: 245 \t Loss: -0.012346170616872383\nVALD: \t Epoch: 245 \t Loss: -0.014249362051486969\nVALD: \t Epoch: 245 \t Loss: -0.013536192011088133\nVALD: \t Epoch: 245 \t Loss: -0.011812256028254827\nVALD: \t Epoch: 245 \t Loss: -0.011513503011829126\n******************************\nEpoch: social-tag : 245\ntrain_loss -0.012346170616872383\nval_loss -0.011513503011829126\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 246 \t Loss: -0.012969824485480785\nTRAIN: \t Epoch: 246 \t Loss: -0.012563536874949932\nTRAIN: \t Epoch: 246 \t Loss: -0.012432473401228586\nTRAIN: \t Epoch: 246 \t Loss: -0.012734427116811275\nTRAIN: \t Epoch: 246 \t Loss: -0.012417691759765148\nTRAIN: \t Epoch: 246 \t Loss: -0.012609116888294617\nTRAIN: \t Epoch: 246 \t Loss: -0.012580390886536666\nTRAIN: \t Epoch: 246 \t Loss: -0.012693725759163499\nTRAIN: \t Epoch: 246 \t Loss: -0.0126099634087748\nTRAIN: \t Epoch: 246 \t Loss: -0.012538095470517874\nTRAIN: \t Epoch: 246 \t Loss: -0.012471039186824451\nTRAIN: \t Epoch: 246 \t Loss: -0.012482083247353634\nTRAIN: \t Epoch: 246 \t Loss: -0.01238560898659321\nTRAIN: \t Epoch: 246 \t Loss: -0.012433121978704418\nTRAIN: \t Epoch: 246 \t Loss: -0.01237962357699871\nTRAIN: \t Epoch: 246 \t Loss: -0.012367039627861232\nTRAIN: \t Epoch: 246 \t Loss: -0.012375822319000057\nVALD: \t Epoch: 246 \t Loss: -0.013796059414744377\nVALD: \t Epoch: 246 \t Loss: -0.013443165458738804\nVALD: \t Epoch: 246 \t Loss: -0.011443502735346556\nVALD: \t Epoch: 246 \t Loss: -0.011384386621311516\n******************************\nEpoch: social-tag : 246\ntrain_loss -0.012375822319000057\nval_loss -0.011384386621311516\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 247 \t Loss: -0.010325437411665916\nTRAIN: \t Epoch: 247 \t Loss: -0.011763406917452812\nTRAIN: \t Epoch: 247 \t Loss: -0.012049981082479158\nTRAIN: \t Epoch: 247 \t Loss: -0.011878413613885641\nTRAIN: \t Epoch: 247 \t Loss: -0.012013905867934226\nTRAIN: \t Epoch: 247 \t Loss: -0.012003889462600151\nTRAIN: \t Epoch: 247 \t Loss: -0.012082842710827078\nTRAIN: \t Epoch: 247 \t Loss: -0.012117193080484867\nTRAIN: \t Epoch: 247 \t Loss: -0.012097697808510728\nTRAIN: \t Epoch: 247 \t Loss: -0.012160720489919185\nTRAIN: \t Epoch: 247 \t Loss: -0.01224317630244927\nTRAIN: \t Epoch: 247 \t Loss: -0.01232212440421184\nTRAIN: \t Epoch: 247 \t Loss: -0.012290090322494507\nTRAIN: \t Epoch: 247 \t Loss: -0.01235962353114571\nTRAIN: \t Epoch: 247 \t Loss: -0.01233142763376236\nTRAIN: \t Epoch: 247 \t Loss: -0.012330524739809334\nTRAIN: \t Epoch: 247 \t Loss: -0.012351798853187851\nVALD: \t Epoch: 247 \t Loss: -0.014330070465803146\nVALD: \t Epoch: 247 \t Loss: -0.01383233442902565\nVALD: \t Epoch: 247 \t Loss: -0.011986229568719864\nVALD: \t Epoch: 247 \t Loss: -0.011796779261377757\n******************************\nEpoch: social-tag : 247\ntrain_loss -0.012351798853187851\nval_loss -0.011796779261377757\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 248 \t Loss: -0.01247322652488947\nTRAIN: \t Epoch: 248 \t Loss: -0.012809660751372576\nTRAIN: \t Epoch: 248 \t Loss: -0.012960472765068213\nTRAIN: \t Epoch: 248 \t Loss: -0.012675217119976878\nTRAIN: \t Epoch: 248 \t Loss: -0.012564896978437901\nTRAIN: \t Epoch: 248 \t Loss: -0.012458020355552435\nTRAIN: \t Epoch: 248 \t Loss: -0.012461051877055849\nTRAIN: \t Epoch: 248 \t Loss: -0.012397549231536686\nTRAIN: \t Epoch: 248 \t Loss: -0.012347171497013833\nTRAIN: \t Epoch: 248 \t Loss: -0.012415161170065404\nTRAIN: \t Epoch: 248 \t Loss: -0.01235524116253311\nTRAIN: \t Epoch: 248 \t Loss: -0.012358464999124408\nTRAIN: \t Epoch: 248 \t Loss: -0.012359532312704967\nTRAIN: \t Epoch: 248 \t Loss: -0.012360516403402601\nTRAIN: \t Epoch: 248 \t Loss: -0.012366838194429875\nTRAIN: \t Epoch: 248 \t Loss: -0.012387811613734812\nTRAIN: \t Epoch: 248 \t Loss: -0.01235259569842707\nVALD: \t Epoch: 248 \t Loss: -0.013076246716082096\nVALD: \t Epoch: 248 \t Loss: -0.011753940489143133\nVALD: \t Epoch: 248 \t Loss: -0.009242207122345766\nVALD: \t Epoch: 248 \t Loss: -0.00941998444631428\n******************************\nEpoch: social-tag : 248\ntrain_loss -0.01235259569842707\nval_loss -0.00941998444631428\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\nTRAIN: \t Epoch: 249 \t Loss: -0.013580990023911\nTRAIN: \t Epoch: 249 \t Loss: -0.012844845652580261\nTRAIN: \t Epoch: 249 \t Loss: -0.012869394073883692\nTRAIN: \t Epoch: 249 \t Loss: -0.012705736327916384\nTRAIN: \t Epoch: 249 \t Loss: -0.012780355475842952\nTRAIN: \t Epoch: 249 \t Loss: -0.01267705981930097\nTRAIN: \t Epoch: 249 \t Loss: -0.012632264223481928\nTRAIN: \t Epoch: 249 \t Loss: -0.01266715326346457\nTRAIN: \t Epoch: 249 \t Loss: -0.012594284903672006\nTRAIN: \t Epoch: 249 \t Loss: -0.012545709032565355\nTRAIN: \t Epoch: 249 \t Loss: -0.012482820671390404\nTRAIN: \t Epoch: 249 \t Loss: -0.01246452440197269\nTRAIN: \t Epoch: 249 \t Loss: -0.012449361097354155\nTRAIN: \t Epoch: 249 \t Loss: -0.012358379882893391\nTRAIN: \t Epoch: 249 \t Loss: -0.012392566353082658\nTRAIN: \t Epoch: 249 \t Loss: -0.01241237937938422\nTRAIN: \t Epoch: 249 \t Loss: -0.012378582759111217\nVALD: \t Epoch: 249 \t Loss: -0.013558318838477135\nVALD: \t Epoch: 249 \t Loss: -0.013120830524712801\nVALD: \t Epoch: 249 \t Loss: -0.011522733606398106\nVALD: \t Epoch: 249 \t Loss: -0.011350884646950606\n******************************\nEpoch: social-tag : 249\ntrain_loss -0.012378582759111217\nval_loss -0.011350884646950606\n{'min_val_epoch': 211, 'min_val_loss': -0.011872185918385398}\n******************************\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}