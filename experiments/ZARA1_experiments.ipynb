{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8544056,
          "sourceType": "datasetVersion",
          "datasetId": 5104693
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Defining the model\n",
        "\n",
        "model = social_stgcnn(n_stgcnn =args.n_stgcnn,n_txpcnn=args.n_txpcnn,\n",
        "output_feat=args.output_size,seq_len=args.obs_seq_len,\n",
        "kernel_size=args.kernel_size,pred_seq_len=args.pred_seq_len)\n",
        "\n",
        "\n",
        "#Training settings\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(),lr=args.lr)\n",
        "\n",
        "if args.use_lrschd:\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_sh_rate, gamma=0.1)\n",
        "\n",
        "\n",
        "\n",
        "checkpoint_dir = './checkpoint/'+args.tag+'/'\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "with open(checkpoint_dir+'args.pkl', 'wb') as fp:\n",
        "    pickle.dump(args, fp)\n",
        "\n",
        "\n",
        "\n",
        "print('Data and model loaded')\n",
        "print('Checkpoint dir:', checkpoint_dir)\n",
        "\n",
        "#Training\n",
        "metrics = {'train_loss':[],  'val_loss':[]}\n",
        "constant_metrics = {'min_val_epoch':-1, 'min_val_loss':9999999999999999}\n",
        "\n",
        "def train(epoch):\n",
        "    global metrics,loader_train\n",
        "    model.train()\n",
        "    loss_batch = 0\n",
        "    batch_count = 0\n",
        "    is_fst_loss = True\n",
        "    loader_len = len(loader_train)\n",
        "    turn_point =int(loader_len/args.batch_size)*args.batch_size+ loader_len%args.batch_size -1\n",
        "\n",
        "\n",
        "    for cnt,batch in enumerate(loader_train):\n",
        "        batch_count+=1\n",
        "\n",
        "        #Get data\n",
        "        batch = [tensor for tensor in batch]\n",
        "        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, non_linear_ped,\\\n",
        "         loss_mask,V_obs,A_obs,A_dir_obs, V_tr,A_tr,A_dir_tr = batch\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        #Forward\n",
        "        #V_obs = batch,seq,node,feat\n",
        "        #V_obs_tmp = batch,feat,seq,node\n",
        "        V_obs_tmp =V_obs.permute(0,3,1,2)\n",
        "\n",
        "        V_pred,_,_ = model(V_obs_tmp,A_obs.squeeze(), A_dir_obs.squeeze())\n",
        "\n",
        "        V_pred = V_pred.permute(0,2,3,1)\n",
        "\n",
        "\n",
        "\n",
        "        V_tr = V_tr.squeeze()\n",
        "        A_tr = A_tr.squeeze()\n",
        "        A_dir_tr = A_dir_tr.squeeze()\n",
        "        V_pred = V_pred.squeeze()\n",
        "\n",
        "        if batch_count%args.batch_size !=0 and cnt != turn_point :\n",
        "            l = graph_loss(V_pred,V_tr)\n",
        "            if is_fst_loss :\n",
        "                loss = l\n",
        "                is_fst_loss = False\n",
        "            else:\n",
        "                loss += l\n",
        "\n",
        "        else:\n",
        "            loss = loss/args.batch_size\n",
        "            is_fst_loss = True\n",
        "            loss.backward()\n",
        "\n",
        "            if args.clip_grad is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(),args.clip_grad)\n",
        "\n",
        "\n",
        "            optimizer.step()\n",
        "            #Metrics\n",
        "            loss_batch += loss.item()\n",
        "            print('TRAIN:','\\t Epoch:', epoch,'\\t Loss:',loss_batch/batch_count)\n",
        "\n",
        "    metrics['train_loss'].append(loss_batch/batch_count)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def vald(epoch):\n",
        "    global metrics,loader_val,constant_metrics\n",
        "    model.eval()\n",
        "    loss_batch = 0\n",
        "    batch_count = 0\n",
        "    is_fst_loss = True\n",
        "    loader_len = len(loader_val)\n",
        "    turn_point =int(loader_len/args.batch_size)*args.batch_size+ loader_len%args.batch_size -1\n",
        "\n",
        "    for cnt,batch in enumerate(loader_val):\n",
        "        batch_count+=1\n",
        "\n",
        "        #Get data\n",
        "        batch = [tensor for tensor in batch]\n",
        "        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, non_linear_ped,\\\n",
        "         loss_mask,V_obs,A_obs,A_dir_obs, V_tr,A_tr,A_dir_tr = batch\n",
        "\n",
        "\n",
        "        V_obs_tmp =V_obs.permute(0,3,1,2)\n",
        "\n",
        "        V_pred,_,_ = model(V_obs_tmp,A_obs.squeeze(), A_dir_obs.squeeze())\n",
        "\n",
        "        V_pred = V_pred.permute(0,2,3,1)\n",
        "\n",
        "        V_tr = V_tr.squeeze()\n",
        "        A_tr = A_tr.squeeze()\n",
        "        A_dir_tr = A_dir_tr.squeeze()\n",
        "        V_pred = V_pred.squeeze()\n",
        "\n",
        "        if batch_count%args.batch_size !=0 and cnt != turn_point :\n",
        "            l = graph_loss(V_pred,V_tr)\n",
        "            if is_fst_loss :\n",
        "                loss = l\n",
        "                is_fst_loss = False\n",
        "            else:\n",
        "                loss += l\n",
        "\n",
        "        else:\n",
        "            loss = loss/args.batch_size\n",
        "            is_fst_loss = True\n",
        "            #Metrics\n",
        "            loss_batch += loss.item()\n",
        "            print('VALD:','\\t Epoch:', epoch,'\\t Loss:',loss_batch/batch_count)\n",
        "\n",
        "    metrics['val_loss'].append(loss_batch/batch_count)\n",
        "\n",
        "    if  metrics['val_loss'][-1]< constant_metrics['min_val_loss']:\n",
        "        constant_metrics['min_val_loss'] =  metrics['val_loss'][-1]\n",
        "        constant_metrics['min_val_epoch'] = epoch\n",
        "        torch.save(model.state_dict(),checkpoint_dir+'val_best.pth')  # OK\n",
        "        check_test_performance()\n",
        "\n",
        "print('Training started ...')\n",
        "for epoch in range(args.num_epochs):\n",
        "    train(epoch)\n",
        "    vald(epoch)\n",
        "    if args.use_lrschd:\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "    print('*'*30)\n",
        "    print('Epoch:',args.tag,\":\", epoch)\n",
        "    for k,v in metrics.items():\n",
        "        if len(v)>0:\n",
        "            print(k,v[-1])\n",
        "\n",
        "\n",
        "    print(constant_metrics)\n",
        "    print('*'*30)\n",
        "\n",
        "    with open(checkpoint_dir+'metrics.pkl', 'wb') as fp:\n",
        "        pickle.dump(metrics, fp)\n",
        "\n",
        "    with open(checkpoint_dir+'constant_metrics.pkl', 'wb') as fp:\n",
        "        pickle.dump(constant_metrics, fp)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-04T02:16:43.754358Z",
          "iopub.execute_input": "2024-06-04T02:16:43.754766Z"
        },
        "trusted": true,
        "id": "8YdzPV1KcM9v",
        "outputId": "54135389-f6c8-4033-d94e-3c87ed3f1e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Data and model loaded\nCheckpoint dir: ./checkpoint/social-tag/\nTraining started ...\nTRAIN: \t Epoch: 0 \t Loss: 0.017905406653881073\nTRAIN: \t Epoch: 0 \t Loss: 0.01704898662865162\nTRAIN: \t Epoch: 0 \t Loss: 0.01653238416959842\nTRAIN: \t Epoch: 0 \t Loss: 0.0161204282194376\nTRAIN: \t Epoch: 0 \t Loss: 0.01576061323285103\nTRAIN: \t Epoch: 0 \t Loss: 0.015433841695388159\nTRAIN: \t Epoch: 0 \t Loss: 0.015113428235054016\nTRAIN: \t Epoch: 0 \t Loss: 0.014816240523941815\nTRAIN: \t Epoch: 0 \t Loss: 0.014531371908055412\nTRAIN: \t Epoch: 0 \t Loss: 0.014240691997110843\nTRAIN: \t Epoch: 0 \t Loss: 0.013945008882067421\nTRAIN: \t Epoch: 0 \t Loss: 0.013651493548726043\nTRAIN: \t Epoch: 0 \t Loss: 0.01337091288027855\nTRAIN: \t Epoch: 0 \t Loss: 0.01307677710428834\nTRAIN: \t Epoch: 0 \t Loss: 0.012784409647186598\nTRAIN: \t Epoch: 0 \t Loss: 0.012504966114647686\nTRAIN: \t Epoch: 0 \t Loss: 0.012229107758578132\nTRAIN: \t Epoch: 0 \t Loss: 0.011943436150128642\nTRAIN: \t Epoch: 0 \t Loss: 0.011900724969887713\nVALD: \t Epoch: 0 \t Loss: 0.006534758023917675\nVALD: \t Epoch: 0 \t Loss: 0.010889844968914986\nVALD: \t Epoch: 0 \t Loss: 0.012241586421926817\nVALD: \t Epoch: 0 \t Loss: 0.011362972669303417\nVALD: \t Epoch: 0 \t Loss: 0.011174220487105944\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.70it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 1.2063701968762728  FDE: 1.6796964878864178\n**************************************************\n******************************\nEpoch: social-tag : 0\ntrain_loss 0.011900724969887713\nval_loss 0.011174220487105944\n{'min_val_epoch': 0, 'min_val_loss': 0.011174220487105944}\n******************************\nTRAIN: \t Epoch: 1 \t Loss: 0.006804140284657478\nTRAIN: \t Epoch: 1 \t Loss: 0.0066112675704061985\nTRAIN: \t Epoch: 1 \t Loss: 0.006436558905988932\nTRAIN: \t Epoch: 1 \t Loss: 0.006359525490552187\nTRAIN: \t Epoch: 1 \t Loss: 0.006419632025063038\nTRAIN: \t Epoch: 1 \t Loss: 0.006731942916909854\nTRAIN: \t Epoch: 1 \t Loss: 0.006728938980294126\nTRAIN: \t Epoch: 1 \t Loss: 0.0065947421244345605\nTRAIN: \t Epoch: 1 \t Loss: 0.006387129063821501\nTRAIN: \t Epoch: 1 \t Loss: 0.006208326434716582\nTRAIN: \t Epoch: 1 \t Loss: 0.00607248078185049\nTRAIN: \t Epoch: 1 \t Loss: 0.005951552108551065\nTRAIN: \t Epoch: 1 \t Loss: 0.00578877363855449\nTRAIN: \t Epoch: 1 \t Loss: 0.00568496181430029\nTRAIN: \t Epoch: 1 \t Loss: 0.005747311453645428\nTRAIN: \t Epoch: 1 \t Loss: 0.005888719417271204\nTRAIN: \t Epoch: 1 \t Loss: 0.005907940828953595\nTRAIN: \t Epoch: 1 \t Loss: 0.00587408359731651\nTRAIN: \t Epoch: 1 \t Loss: 0.00585638382291609\nVALD: \t Epoch: 1 \t Loss: 0.0037289310712367296\nVALD: \t Epoch: 1 \t Loss: 0.007208870840258896\nVALD: \t Epoch: 1 \t Loss: 0.00882205250672996\nVALD: \t Epoch: 1 \t Loss: 0.010635459388140589\nVALD: \t Epoch: 1 \t Loss: 0.009923081407862262\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.72it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\nADE: 1.0306519957545333  FDE: 1.5225974661262602\n**************************************************\n******************************\nEpoch: social-tag : 1\ntrain_loss 0.00585638382291609\nval_loss 0.009923081407862262\n{'min_val_epoch': 1, 'min_val_loss': 0.009923081407862262}\n******************************\nTRAIN: \t Epoch: 2 \t Loss: 0.004136004019528627\nTRAIN: \t Epoch: 2 \t Loss: 0.003728829207830131\nTRAIN: \t Epoch: 2 \t Loss: 0.0035657569921265044\nTRAIN: \t Epoch: 2 \t Loss: 0.0033884821459650993\nTRAIN: \t Epoch: 2 \t Loss: 0.00344188716262579\nTRAIN: \t Epoch: 2 \t Loss: 0.00402212228315572\nTRAIN: \t Epoch: 2 \t Loss: 0.0043294950654464105\nTRAIN: \t Epoch: 2 \t Loss: 0.004379657038953155\nTRAIN: \t Epoch: 2 \t Loss: 0.0042949268956565196\nTRAIN: \t Epoch: 2 \t Loss: 0.0041786970803514125\nTRAIN: \t Epoch: 2 \t Loss: 0.003982016181742603\nTRAIN: \t Epoch: 2 \t Loss: 0.0038172414254707596\nTRAIN: \t Epoch: 2 \t Loss: 0.0037527420880416264\nTRAIN: \t Epoch: 2 \t Loss: 0.0038898419755111846\nTRAIN: \t Epoch: 2 \t Loss: 0.004020008056734999\nTRAIN: \t Epoch: 2 \t Loss: 0.004008237752714194\nTRAIN: \t Epoch: 2 \t Loss: 0.003940355813349871\nTRAIN: \t Epoch: 2 \t Loss: 0.0038136893935087654\nTRAIN: \t Epoch: 2 \t Loss: 0.003792498188540917\nVALD: \t Epoch: 2 \t Loss: 0.000580865831580013\nVALD: \t Epoch: 2 \t Loss: 0.0054054017818998545\nVALD: \t Epoch: 2 \t Loss: 0.008293151525625339\nVALD: \t Epoch: 2 \t Loss: 0.007796207893989049\nVALD: \t Epoch: 2 \t Loss: 0.00737967465288383\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.74it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.8708259260301278  FDE: 1.3716798658387632\n**************************************************\n******************************\nEpoch: social-tag : 2\ntrain_loss 0.003792498188540917\nval_loss 0.00737967465288383\n{'min_val_epoch': 2, 'min_val_loss': 0.00737967465288383}\n******************************\nTRAIN: \t Epoch: 3 \t Loss: 0.0017879689112305641\nTRAIN: \t Epoch: 3 \t Loss: 0.0015540827880613506\nTRAIN: \t Epoch: 3 \t Loss: 0.001750631839968264\nTRAIN: \t Epoch: 3 \t Loss: 0.001910809165565297\nTRAIN: \t Epoch: 3 \t Loss: 0.0025126678636297585\nTRAIN: \t Epoch: 3 \t Loss: 0.0028879010545400283\nTRAIN: \t Epoch: 3 \t Loss: 0.002923060358235879\nTRAIN: \t Epoch: 3 \t Loss: 0.0027346972783561796\nTRAIN: \t Epoch: 3 \t Loss: 0.0025367544601774877\nTRAIN: \t Epoch: 3 \t Loss: 0.0023411210800986736\nTRAIN: \t Epoch: 3 \t Loss: 0.00219853303860873\nTRAIN: \t Epoch: 3 \t Loss: 0.002365852133758987\nTRAIN: \t Epoch: 3 \t Loss: 0.002706732860623071\nTRAIN: \t Epoch: 3 \t Loss: 0.0028540989068070693\nTRAIN: \t Epoch: 3 \t Loss: 0.002898322457137207\nTRAIN: \t Epoch: 3 \t Loss: 0.002862679488316644\nTRAIN: \t Epoch: 3 \t Loss: 0.0027408541817053713\nTRAIN: \t Epoch: 3 \t Loss: 0.0026339661918528792\nTRAIN: \t Epoch: 3 \t Loss: 0.0026144850779522104\nVALD: \t Epoch: 3 \t Loss: -0.0009485485497862101\nVALD: \t Epoch: 3 \t Loss: 0.0032654417445883155\nVALD: \t Epoch: 3 \t Loss: 0.004057231592014432\nVALD: \t Epoch: 3 \t Loss: 0.003560521930921823\nVALD: \t Epoch: 3 \t Loss: 0.003328342600302263\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.72it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\nADE: 0.8173278609453442  FDE: 1.2501915517723794\n**************************************************\n******************************\nEpoch: social-tag : 3\ntrain_loss 0.0026144850779522104\nval_loss 0.003328342600302263\n{'min_val_epoch': 3, 'min_val_loss': 0.003328342600302263}\n******************************\nTRAIN: \t Epoch: 4 \t Loss: 0.0009173312573693693\nTRAIN: \t Epoch: 4 \t Loss: 0.0008598185668233782\nTRAIN: \t Epoch: 4 \t Loss: 0.001502081227954477\nTRAIN: \t Epoch: 4 \t Loss: 0.002691817076993175\nTRAIN: \t Epoch: 4 \t Loss: 0.0029748866916634144\nTRAIN: \t Epoch: 4 \t Loss: 0.0029179005747816214\nTRAIN: \t Epoch: 4 \t Loss: 0.0027520880019957466\nTRAIN: \t Epoch: 4 \t Loss: 0.002447407710860716\nTRAIN: \t Epoch: 4 \t Loss: 0.002162297734078796\nTRAIN: \t Epoch: 4 \t Loss: 0.0018659915185708087\nTRAIN: \t Epoch: 4 \t Loss: 0.0019057382801706394\nTRAIN: \t Epoch: 4 \t Loss: 0.00208582960901064\nTRAIN: \t Epoch: 4 \t Loss: 0.002086514884117955\nTRAIN: \t Epoch: 4 \t Loss: 0.001981057928787777\nTRAIN: \t Epoch: 4 \t Loss: 0.0018357843514725876\nTRAIN: \t Epoch: 4 \t Loss: 0.001685202584212675\nTRAIN: \t Epoch: 4 \t Loss: 0.001571837861881838\nTRAIN: \t Epoch: 4 \t Loss: 0.0014451551392590369\nTRAIN: \t Epoch: 4 \t Loss: 0.0014490995930730784\nVALD: \t Epoch: 4 \t Loss: -0.00187349459156394\nVALD: \t Epoch: 4 \t Loss: 0.0028572736773639917\nVALD: \t Epoch: 4 \t Loss: 0.0032319696620106697\nVALD: \t Epoch: 4 \t Loss: 0.002912541152909398\nVALD: \t Epoch: 4 \t Loss: 0.0028181221613214034\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.70it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.7166088990754149  FDE: 1.0394295261202176\n**************************************************\n******************************\nEpoch: social-tag : 4\ntrain_loss 0.0014490995930730784\nval_loss 0.0028181221613214034\n{'min_val_epoch': 4, 'min_val_loss': 0.0028181221613214034}\n******************************\nTRAIN: \t Epoch: 5 \t Loss: -0.000568624644074589\nTRAIN: \t Epoch: 5 \t Loss: -0.0004378141456982121\nTRAIN: \t Epoch: 5 \t Loss: 0.0005689465227381637\nTRAIN: \t Epoch: 5 \t Loss: 0.001271901659492869\nTRAIN: \t Epoch: 5 \t Loss: 0.0013710361963603646\nTRAIN: \t Epoch: 5 \t Loss: 0.0012853637211568032\nTRAIN: \t Epoch: 5 \t Loss: 0.0010673785769280844\nTRAIN: \t Epoch: 5 \t Loss: 0.0008060494164965348\nTRAIN: \t Epoch: 5 \t Loss: 0.0006948846146567828\nTRAIN: \t Epoch: 5 \t Loss: 0.0007464195427019149\nTRAIN: \t Epoch: 5 \t Loss: 0.0007651963810944421\nTRAIN: \t Epoch: 5 \t Loss: 0.0007325031814010193\nTRAIN: \t Epoch: 5 \t Loss: 0.0006477369864184696\nTRAIN: \t Epoch: 5 \t Loss: 0.0005184209148865193\nTRAIN: \t Epoch: 5 \t Loss: 0.00046940829876499873\nTRAIN: \t Epoch: 5 \t Loss: 0.0005379356352932518\nTRAIN: \t Epoch: 5 \t Loss: 0.0006362256124916979\nTRAIN: \t Epoch: 5 \t Loss: 0.0006134928415930416\nTRAIN: \t Epoch: 5 \t Loss: 0.0006099286158091864\nVALD: \t Epoch: 5 \t Loss: -0.00079114775871858\nVALD: \t Epoch: 5 \t Loss: 0.0033608554222155362\nVALD: \t Epoch: 5 \t Loss: 0.006204128905665129\nVALD: \t Epoch: 5 \t Loss: 0.010443378254421987\nVALD: \t Epoch: 5 \t Loss: 0.009893618844264795\n******************************\nEpoch: social-tag : 5\ntrain_loss 0.0006099286158091864\nval_loss 0.009893618844264795\n{'min_val_epoch': 4, 'min_val_loss': 0.0028181221613214034}\n******************************\nTRAIN: \t Epoch: 6 \t Loss: -0.0006768889725208282\nTRAIN: \t Epoch: 6 \t Loss: -0.0011459646630100906\nTRAIN: \t Epoch: 6 \t Loss: -0.001187731356670459\nTRAIN: \t Epoch: 6 \t Loss: -0.0011999525013379753\nTRAIN: \t Epoch: 6 \t Loss: -0.0006409654859453439\nTRAIN: \t Epoch: 6 \t Loss: -0.00036600932556514937\nTRAIN: \t Epoch: 6 \t Loss: -0.0003658625563340528\nTRAIN: \t Epoch: 6 \t Loss: -0.0005197203863644972\nTRAIN: \t Epoch: 6 \t Loss: -0.0005886423298054271\nTRAIN: \t Epoch: 6 \t Loss: -0.0007079364149831235\nTRAIN: \t Epoch: 6 \t Loss: -0.000731490203179419\nTRAIN: \t Epoch: 6 \t Loss: -0.00045854205382056534\nTRAIN: \t Epoch: 6 \t Loss: -0.00014082207165371912\nTRAIN: \t Epoch: 6 \t Loss: 5.6790745085371395e-05\nTRAIN: \t Epoch: 6 \t Loss: 0.00017329411736379067\nTRAIN: \t Epoch: 6 \t Loss: 0.0001528637285446166\nTRAIN: \t Epoch: 6 \t Loss: 8.424192663583467e-05\nTRAIN: \t Epoch: 6 \t Loss: 1.4906230919425272e-05\nTRAIN: \t Epoch: 6 \t Loss: 1.681504472705129e-05\nVALD: \t Epoch: 6 \t Loss: -0.003475051373243332\nVALD: \t Epoch: 6 \t Loss: -0.0005180672742426395\nVALD: \t Epoch: 6 \t Loss: -0.0006447505244674782\nVALD: \t Epoch: 6 \t Loss: -0.0008689643145771697\nVALD: \t Epoch: 6 \t Loss: -0.0007150098558299798\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.72it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\nADE: 0.6651893291894969  FDE: 0.9827832241303112\n**************************************************\n******************************\nEpoch: social-tag : 6\ntrain_loss 1.681504472705129e-05\nval_loss -0.0007150098558299798\n{'min_val_epoch': 6, 'min_val_loss': -0.0007150098558299798}\n******************************\nTRAIN: \t Epoch: 7 \t Loss: -0.0013768228236585855\nTRAIN: \t Epoch: 7 \t Loss: -0.0017660707235336304\nTRAIN: \t Epoch: 7 \t Loss: -0.0014899747135738532\nTRAIN: \t Epoch: 7 \t Loss: -0.0001577502116560936\nTRAIN: \t Epoch: 7 \t Loss: 0.00012692685704678296\nTRAIN: \t Epoch: 7 \t Loss: 0.00012429483710244918\nTRAIN: \t Epoch: 7 \t Loss: -8.814577137984867e-05\nTRAIN: \t Epoch: 7 \t Loss: -0.0003262827649450628\nTRAIN: \t Epoch: 7 \t Loss: -0.0005208968763731213\nTRAIN: \t Epoch: 7 \t Loss: -0.0006765678394003771\nTRAIN: \t Epoch: 7 \t Loss: -0.0008135197819104757\nTRAIN: \t Epoch: 7 \t Loss: -0.0008841505902334271\nTRAIN: \t Epoch: 7 \t Loss: -0.0007843506833663783\nTRAIN: \t Epoch: 7 \t Loss: -0.0006379853491255615\nTRAIN: \t Epoch: 7 \t Loss: -0.0006195990591853236\nTRAIN: \t Epoch: 7 \t Loss: -0.0007351196172749042\nTRAIN: \t Epoch: 7 \t Loss: -0.0008089296196190202\nTRAIN: \t Epoch: 7 \t Loss: -0.000918209493748792\nTRAIN: \t Epoch: 7 \t Loss: -0.0009377725096700214\nVALD: \t Epoch: 7 \t Loss: -0.004320197273045778\nVALD: \t Epoch: 7 \t Loss: -0.0012434420641511679\nVALD: \t Epoch: 7 \t Loss: -0.0013871124247089028\nVALD: \t Epoch: 7 \t Loss: -0.0014200319128576666\nVALD: \t Epoch: 7 \t Loss: -0.0014067285193884668\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.79it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\nADE: 0.6697924225601775  FDE: 1.0109885554909908\n**************************************************\n******************************\nEpoch: social-tag : 7\ntrain_loss -0.0009377725096700214\nval_loss -0.0014067285193884668\n{'min_val_epoch': 7, 'min_val_loss': -0.0014067285193884668}\n******************************\nTRAIN: \t Epoch: 8 \t Loss: -0.0015573439886793494\nTRAIN: \t Epoch: 8 \t Loss: -0.0021197505411691964\nTRAIN: \t Epoch: 8 \t Loss: -0.0015781483768175046\nTRAIN: \t Epoch: 8 \t Loss: -0.0007593790360260755\nTRAIN: \t Epoch: 8 \t Loss: -0.0005212583928368986\nTRAIN: \t Epoch: 8 \t Loss: -0.0006443468252352128\nTRAIN: \t Epoch: 8 \t Loss: -0.0008676301970678781\nTRAIN: \t Epoch: 8 \t Loss: -0.0009641512224334292\nTRAIN: \t Epoch: 8 \t Loss: -0.001223196734726015\nTRAIN: \t Epoch: 8 \t Loss: -0.0013191928330343217\nTRAIN: \t Epoch: 8 \t Loss: -0.0012802294698882508\nTRAIN: \t Epoch: 8 \t Loss: -0.0011317133078894888\nTRAIN: \t Epoch: 8 \t Loss: -0.001148106266135493\nTRAIN: \t Epoch: 8 \t Loss: -0.001244755066831463\nTRAIN: \t Epoch: 8 \t Loss: -0.0013625237508676947\nTRAIN: \t Epoch: 8 \t Loss: -0.001411574619851308\nTRAIN: \t Epoch: 8 \t Loss: -0.0011422304361236885\nTRAIN: \t Epoch: 8 \t Loss: -0.0009544221716674252\nTRAIN: \t Epoch: 8 \t Loss: -0.00093797154557746\nVALD: \t Epoch: 8 \t Loss: 0.0004211667983327061\nVALD: \t Epoch: 8 \t Loss: 0.0008029005985008553\nVALD: \t Epoch: 8 \t Loss: 0.0006154400325613096\nVALD: \t Epoch: 8 \t Loss: 0.0005687837292498443\nVALD: \t Epoch: 8 \t Loss: 0.000553439511371053\n******************************\nEpoch: social-tag : 8\ntrain_loss -0.00093797154557746\nval_loss 0.000553439511371053\n{'min_val_epoch': 7, 'min_val_loss': -0.0014067285193884668}\n******************************\nTRAIN: \t Epoch: 9 \t Loss: 0.0007554794428870082\nTRAIN: \t Epoch: 9 \t Loss: 0.00024330349697265774\nTRAIN: \t Epoch: 9 \t Loss: -0.0002603388760083665\nTRAIN: \t Epoch: 9 \t Loss: -0.0010301336224074475\nTRAIN: \t Epoch: 9 \t Loss: -0.0014879473892506212\nTRAIN: \t Epoch: 9 \t Loss: -0.001573626102375177\nTRAIN: \t Epoch: 9 \t Loss: -0.001173315116570198\nTRAIN: \t Epoch: 9 \t Loss: -0.0009451615078432951\nTRAIN: \t Epoch: 9 \t Loss: -0.0010149317774145554\nTRAIN: \t Epoch: 9 \t Loss: -0.0012052221485646442\nTRAIN: \t Epoch: 9 \t Loss: -0.0013855477948461405\nTRAIN: \t Epoch: 9 \t Loss: -0.0014828156781732105\nTRAIN: \t Epoch: 9 \t Loss: -0.001616341887991159\nTRAIN: \t Epoch: 9 \t Loss: -0.0017216750940341236\nTRAIN: \t Epoch: 9 \t Loss: -0.0018264595438571027\nTRAIN: \t Epoch: 9 \t Loss: -0.001679201670413022\nTRAIN: \t Epoch: 9 \t Loss: -0.0015020662223618915\nTRAIN: \t Epoch: 9 \t Loss: -0.0014498952611271914\nTRAIN: \t Epoch: 9 \t Loss: -0.0014584661396436912\nVALD: \t Epoch: 9 \t Loss: -0.003344477154314518\nVALD: \t Epoch: 9 \t Loss: -0.002364212996326387\nVALD: \t Epoch: 9 \t Loss: -0.0026093081881602607\nVALD: \t Epoch: 9 \t Loss: -0.002705773222260177\nVALD: \t Epoch: 9 \t Loss: -0.0026168891467338753\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.78it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\nADE: 0.6489242497668557  FDE: 0.8877569502103559\n**************************************************\n******************************\nEpoch: social-tag : 9\ntrain_loss -0.0014584661396436912\nval_loss -0.0026168891467338753\n{'min_val_epoch': 9, 'min_val_loss': -0.0026168891467338753}\n******************************\nTRAIN: \t Epoch: 10 \t Loss: -0.0024762977845966816\nTRAIN: \t Epoch: 10 \t Loss: -0.0028990620048716664\nTRAIN: \t Epoch: 10 \t Loss: -0.0031323440683384738\nTRAIN: \t Epoch: 10 \t Loss: -0.002911987481638789\nTRAIN: \t Epoch: 10 \t Loss: -0.002327101005175791\nTRAIN: \t Epoch: 10 \t Loss: -0.0019178117580243754\nTRAIN: \t Epoch: 10 \t Loss: -0.0018999047605185687\nTRAIN: \t Epoch: 10 \t Loss: -0.0018946390972587324\nTRAIN: \t Epoch: 10 \t Loss: -0.0020007684538035798\nTRAIN: \t Epoch: 10 \t Loss: -0.0021710020649152286\nTRAIN: \t Epoch: 10 \t Loss: -0.002215327476650683\nTRAIN: \t Epoch: 10 \t Loss: -0.0021248801239532136\nTRAIN: \t Epoch: 10 \t Loss: -0.0019862236210358855\nTRAIN: \t Epoch: 10 \t Loss: -0.0020262985562372444\nTRAIN: \t Epoch: 10 \t Loss: -0.0021317684003709777\nTRAIN: \t Epoch: 10 \t Loss: -0.002249262462044044\nTRAIN: \t Epoch: 10 \t Loss: -0.002314442870502877\nTRAIN: \t Epoch: 10 \t Loss: -0.0020115715895675144\nTRAIN: \t Epoch: 10 \t Loss: -0.001991446729952792\nVALD: \t Epoch: 10 \t Loss: 0.0005815462791360915\nVALD: \t Epoch: 10 \t Loss: 0.0013280774292070419\nVALD: \t Epoch: 10 \t Loss: 0.0010776114456045132\nVALD: \t Epoch: 10 \t Loss: 0.0009813366195885465\nVALD: \t Epoch: 10 \t Loss: 0.0010577897271834128\n******************************\nEpoch: social-tag : 10\ntrain_loss -0.001991446729952792\nval_loss 0.0010577897271834128\n{'min_val_epoch': 9, 'min_val_loss': -0.0026168891467338753}\n******************************\nTRAIN: \t Epoch: 11 \t Loss: 0.0014187172055244446\nTRAIN: \t Epoch: 11 \t Loss: 0.0008161288642440923\nTRAIN: \t Epoch: 11 \t Loss: 0.00029435665055643767\nTRAIN: \t Epoch: 11 \t Loss: -0.0002193023065046873\nTRAIN: \t Epoch: 11 \t Loss: -0.0007874649454606697\nTRAIN: \t Epoch: 11 \t Loss: -0.0013516062147876557\nTRAIN: \t Epoch: 11 \t Loss: -0.0016452945814567751\nTRAIN: \t Epoch: 11 \t Loss: -0.0015768598805152578\nTRAIN: \t Epoch: 11 \t Loss: -0.0015401258893285154\nTRAIN: \t Epoch: 11 \t Loss: -0.001621753720974084\nTRAIN: \t Epoch: 11 \t Loss: -0.0017810629625190895\nTRAIN: \t Epoch: 11 \t Loss: -0.0019580612218608926\nTRAIN: \t Epoch: 11 \t Loss: -0.0020828566753502507\nTRAIN: \t Epoch: 11 \t Loss: -0.001866156625120701\nTRAIN: \t Epoch: 11 \t Loss: -0.0017638089610651755\nTRAIN: \t Epoch: 11 \t Loss: -0.0017761670032996335\nTRAIN: \t Epoch: 11 \t Loss: -0.0018491238254629185\nTRAIN: \t Epoch: 11 \t Loss: -0.0019477217388662717\nTRAIN: \t Epoch: 11 \t Loss: -0.001969009288887491\nVALD: \t Epoch: 11 \t Loss: -0.006042440887540579\nVALD: \t Epoch: 11 \t Loss: -0.004311225144192576\nVALD: \t Epoch: 11 \t Loss: -0.004541972341636817\nVALD: \t Epoch: 11 \t Loss: -0.004315478261560202\nVALD: \t Epoch: 11 \t Loss: -0.004251795760856187\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.75it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.5760224526089711  FDE: 0.8001560428669847\n**************************************************\n******************************\nEpoch: social-tag : 11\ntrain_loss -0.001969009288887491\nval_loss -0.004251795760856187\n{'min_val_epoch': 11, 'min_val_loss': -0.004251795760856187}\n******************************\nTRAIN: \t Epoch: 12 \t Loss: -0.004207749851047993\nTRAIN: \t Epoch: 12 \t Loss: -0.0032379741314798594\nTRAIN: \t Epoch: 12 \t Loss: -0.0022999098097595074\nTRAIN: \t Epoch: 12 \t Loss: -0.002151256936485879\nTRAIN: \t Epoch: 12 \t Loss: -0.0024260166916064917\nTRAIN: \t Epoch: 12 \t Loss: -0.0027108080588125936\nTRAIN: \t Epoch: 12 \t Loss: -0.0029353496003230767\nTRAIN: \t Epoch: 12 \t Loss: -0.0031274831781047396\nTRAIN: \t Epoch: 12 \t Loss: -0.0030946597495737174\nTRAIN: \t Epoch: 12 \t Loss: -0.002752531468286179\nTRAIN: \t Epoch: 12 \t Loss: -0.00246924235728908\nTRAIN: \t Epoch: 12 \t Loss: -0.002336571349587757\nTRAIN: \t Epoch: 12 \t Loss: -0.002384993666335224\nTRAIN: \t Epoch: 12 \t Loss: -0.0025233807204391007\nTRAIN: \t Epoch: 12 \t Loss: -0.002585503945980842\nTRAIN: \t Epoch: 12 \t Loss: -0.002690347077077604\nTRAIN: \t Epoch: 12 \t Loss: -0.0027371779656010296\nTRAIN: \t Epoch: 12 \t Loss: -0.002676517565204348\nTRAIN: \t Epoch: 12 \t Loss: -0.002671455391694717\nVALD: \t Epoch: 12 \t Loss: -0.0037702752742916346\nVALD: \t Epoch: 12 \t Loss: -0.0022595271875616163\nVALD: \t Epoch: 12 \t Loss: -0.0025508176962224147\nVALD: \t Epoch: 12 \t Loss: -0.0017403030942659825\nVALD: \t Epoch: 12 \t Loss: -0.0017499269779063453\n******************************\nEpoch: social-tag : 12\ntrain_loss -0.002671455391694717\nval_loss -0.0017499269779063453\n{'min_val_epoch': 11, 'min_val_loss': -0.004251795760856187}\n******************************\nTRAIN: \t Epoch: 13 \t Loss: -0.0021602539345622063\nTRAIN: \t Epoch: 13 \t Loss: -0.0021004470763728023\nTRAIN: \t Epoch: 13 \t Loss: -0.00263411117096742\nTRAIN: \t Epoch: 13 \t Loss: -0.002865872869733721\nTRAIN: \t Epoch: 13 \t Loss: -0.0029969992116093635\nTRAIN: \t Epoch: 13 \t Loss: -0.00275866196413214\nTRAIN: \t Epoch: 13 \t Loss: -0.0026450062536501457\nTRAIN: \t Epoch: 13 \t Loss: -0.002736923939664848\nTRAIN: \t Epoch: 13 \t Loss: -0.002926314831711352\nTRAIN: \t Epoch: 13 \t Loss: -0.0030669870437122883\nTRAIN: \t Epoch: 13 \t Loss: -0.0030803757571530614\nTRAIN: \t Epoch: 13 \t Loss: -0.0030904754627651223\nTRAIN: \t Epoch: 13 \t Loss: -0.003037767422098953\nTRAIN: \t Epoch: 13 \t Loss: -0.002905398590623268\nTRAIN: \t Epoch: 13 \t Loss: -0.002917939517647028\nTRAIN: \t Epoch: 13 \t Loss: -0.003023654979187995\nTRAIN: \t Epoch: 13 \t Loss: -0.0031040917041108887\nTRAIN: \t Epoch: 13 \t Loss: -0.0031715256886349786\nTRAIN: \t Epoch: 13 \t Loss: -0.003176414464743973\nVALD: \t Epoch: 13 \t Loss: -0.00641383184120059\nVALD: \t Epoch: 13 \t Loss: -0.004957412020303309\nVALD: \t Epoch: 13 \t Loss: -0.0052066880743950605\nVALD: \t Epoch: 13 \t Loss: -0.005270914232823998\nVALD: \t Epoch: 13 \t Loss: -0.0050775253083095075\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.71it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\nADE: 0.5851047769736725  FDE: 0.8257716856032145\n**************************************************\n******************************\nEpoch: social-tag : 13\ntrain_loss -0.003176414464743973\nval_loss -0.0050775253083095075\n{'min_val_epoch': 13, 'min_val_loss': -0.0050775253083095075}\n******************************\nTRAIN: \t Epoch: 14 \t Loss: -0.004145297687500715\nTRAIN: \t Epoch: 14 \t Loss: -0.004263467155396938\nTRAIN: \t Epoch: 14 \t Loss: -0.0028585350009961985\nTRAIN: \t Epoch: 14 \t Loss: -0.002437176010062103\nTRAIN: \t Epoch: 14 \t Loss: -0.0023819867070415056\nTRAIN: \t Epoch: 14 \t Loss: -0.0026649612712693247\nTRAIN: \t Epoch: 14 \t Loss: -0.0029049995033087078\nTRAIN: \t Epoch: 14 \t Loss: -0.0030513661049553775\nTRAIN: \t Epoch: 14 \t Loss: -0.003225613256897001\nTRAIN: \t Epoch: 14 \t Loss: -0.0032841082669619937\nTRAIN: \t Epoch: 14 \t Loss: -0.003383034544557714\nTRAIN: \t Epoch: 14 \t Loss: -0.0034898986347495034\nTRAIN: \t Epoch: 14 \t Loss: -0.0034185778176134382\nTRAIN: \t Epoch: 14 \t Loss: -0.0033388726717800766\nTRAIN: \t Epoch: 14 \t Loss: -0.003285435510042589\nTRAIN: \t Epoch: 14 \t Loss: -0.00330948182136126\nTRAIN: \t Epoch: 14 \t Loss: -0.003411026932179298\nTRAIN: \t Epoch: 14 \t Loss: -0.0034523441123989564\nTRAIN: \t Epoch: 14 \t Loss: -0.0034669371491055823\nVALD: \t Epoch: 14 \t Loss: -0.006676057819277048\nVALD: \t Epoch: 14 \t Loss: -0.005458904430270195\nVALD: \t Epoch: 14 \t Loss: -0.005694864317774773\nVALD: \t Epoch: 14 \t Loss: -0.005444095120765269\nVALD: \t Epoch: 14 \t Loss: -0.005304025372197805\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.79it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\nADE: 0.5896568058665109  FDE: 0.9124532596067546\n**************************************************\n******************************\nEpoch: social-tag : 14\ntrain_loss -0.0034669371491055823\nval_loss -0.005304025372197805\n{'min_val_epoch': 14, 'min_val_loss': -0.005304025372197805}\n******************************\nTRAIN: \t Epoch: 15 \t Loss: -0.004576674196869135\nTRAIN: \t Epoch: 15 \t Loss: -0.0043716453947126865\nTRAIN: \t Epoch: 15 \t Loss: -0.0038044715765863657\nTRAIN: \t Epoch: 15 \t Loss: -0.0030250522249843925\nTRAIN: \t Epoch: 15 \t Loss: -0.0030150248901918532\nTRAIN: \t Epoch: 15 \t Loss: -0.003232818504329771\nTRAIN: \t Epoch: 15 \t Loss: -0.003397610387764871\nTRAIN: \t Epoch: 15 \t Loss: -0.0035166589805157855\nTRAIN: \t Epoch: 15 \t Loss: -0.0035968036520191366\nTRAIN: \t Epoch: 15 \t Loss: -0.003509157185908407\nTRAIN: \t Epoch: 15 \t Loss: -0.003315858680500903\nTRAIN: \t Epoch: 15 \t Loss: -0.0033070138403369733\nTRAIN: \t Epoch: 15 \t Loss: -0.0033620063913986087\nTRAIN: \t Epoch: 15 \t Loss: -0.003475142701063305\nTRAIN: \t Epoch: 15 \t Loss: -0.0035435732376451292\nTRAIN: \t Epoch: 15 \t Loss: -0.0036138609793852083\nTRAIN: \t Epoch: 15 \t Loss: -0.003567334422019913\nTRAIN: \t Epoch: 15 \t Loss: -0.0033629912286414765\nTRAIN: \t Epoch: 15 \t Loss: -0.0033525607535017142\nVALD: \t Epoch: 15 \t Loss: -0.004363388754427433\nVALD: \t Epoch: 15 \t Loss: -0.003913134569302201\nVALD: \t Epoch: 15 \t Loss: -0.004143264765540759\nVALD: \t Epoch: 15 \t Loss: -0.003950089565478265\nVALD: \t Epoch: 15 \t Loss: -0.0038856778263060515\n******************************\nEpoch: social-tag : 15\ntrain_loss -0.0033525607535017142\nval_loss -0.0038856778263060515\n{'min_val_epoch': 14, 'min_val_loss': -0.005304025372197805}\n******************************\nTRAIN: \t Epoch: 16 \t Loss: -0.0034371980000287294\nTRAIN: \t Epoch: 16 \t Loss: -0.0038622141582891345\nTRAIN: \t Epoch: 16 \t Loss: -0.0043515552921841545\nTRAIN: \t Epoch: 16 \t Loss: -0.004445038910489529\nTRAIN: \t Epoch: 16 \t Loss: -0.004131059115752578\nTRAIN: \t Epoch: 16 \t Loss: -0.0033591020037420094\nTRAIN: \t Epoch: 16 \t Loss: -0.0030101074454640703\nTRAIN: \t Epoch: 16 \t Loss: -0.0029635848259204067\nTRAIN: \t Epoch: 16 \t Loss: -0.0030916373879234823\nTRAIN: \t Epoch: 16 \t Loss: -0.0032987469632644205\nTRAIN: \t Epoch: 16 \t Loss: -0.0033604505482468417\nTRAIN: \t Epoch: 16 \t Loss: -0.0034607383277034387\nTRAIN: \t Epoch: 16 \t Loss: -0.0035616479894647803\nTRAIN: \t Epoch: 16 \t Loss: -0.0034918826048461987\nTRAIN: \t Epoch: 16 \t Loss: -0.003321212091638396\nTRAIN: \t Epoch: 16 \t Loss: -0.0033031307211786043\nTRAIN: \t Epoch: 16 \t Loss: -0.0033695584211005446\nTRAIN: \t Epoch: 16 \t Loss: -0.003433141918827055\nTRAIN: \t Epoch: 16 \t Loss: -0.0034207261354372892\nVALD: \t Epoch: 16 \t Loss: -0.00681984331458807\nVALD: \t Epoch: 16 \t Loss: -0.005744182970374823\nVALD: \t Epoch: 16 \t Loss: -0.006014868927498658\nVALD: \t Epoch: 16 \t Loss: -0.005851172492839396\nVALD: \t Epoch: 16 \t Loss: -0.005661091627168261\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.75it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.5810241853099536  FDE: 0.8737117585416048\n**************************************************\n******************************\nEpoch: social-tag : 16\ntrain_loss -0.0034207261354372892\nval_loss -0.005661091627168261\n{'min_val_epoch': 16, 'min_val_loss': -0.005661091627168261}\n******************************\nTRAIN: \t Epoch: 17 \t Loss: -0.005039736162871122\nTRAIN: \t Epoch: 17 \t Loss: -0.005257095443084836\nTRAIN: \t Epoch: 17 \t Loss: -0.0050488718164463835\nTRAIN: \t Epoch: 17 \t Loss: -0.004581007931847125\nTRAIN: \t Epoch: 17 \t Loss: -0.004254097491502762\nTRAIN: \t Epoch: 17 \t Loss: -0.00413753732573241\nTRAIN: \t Epoch: 17 \t Loss: -0.004238134017214179\nTRAIN: \t Epoch: 17 \t Loss: -0.004381051781820133\nTRAIN: \t Epoch: 17 \t Loss: -0.00445038760598335\nTRAIN: \t Epoch: 17 \t Loss: -0.004539744998328388\nTRAIN: \t Epoch: 17 \t Loss: -0.004530481380325827\nTRAIN: \t Epoch: 17 \t Loss: -0.004283848907410477\nTRAIN: \t Epoch: 17 \t Loss: -0.004051374978958988\nTRAIN: \t Epoch: 17 \t Loss: -0.003983400005381554\nTRAIN: \t Epoch: 17 \t Loss: -0.004028283098402123\nTRAIN: \t Epoch: 17 \t Loss: -0.00405116150068352\nTRAIN: \t Epoch: 17 \t Loss: -0.00409491771749933\nTRAIN: \t Epoch: 17 \t Loss: -0.004124480202638854\nTRAIN: \t Epoch: 17 \t Loss: -0.0041348841570760545\nVALD: \t Epoch: 17 \t Loss: -0.0066957599483430386\nVALD: \t Epoch: 17 \t Loss: -0.005772012053057551\nVALD: \t Epoch: 17 \t Loss: -0.006047957732031743\nVALD: \t Epoch: 17 \t Loss: -0.006059509119950235\nVALD: \t Epoch: 17 \t Loss: -0.005821622156899822\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.71it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.5670286414530893  FDE: 0.8166626519080572\n**************************************************\n******************************\nEpoch: social-tag : 17\ntrain_loss -0.0041348841570760545\nval_loss -0.005821622156899822\n{'min_val_epoch': 17, 'min_val_loss': -0.005821622156899822}\n******************************\nTRAIN: \t Epoch: 18 \t Loss: -0.005362479016184807\nTRAIN: \t Epoch: 18 \t Loss: -0.004325827700085938\nTRAIN: \t Epoch: 18 \t Loss: -0.0031673709745518863\nTRAIN: \t Epoch: 18 \t Loss: -0.0030788170552114025\nTRAIN: \t Epoch: 18 \t Loss: -0.00345656304853037\nTRAIN: \t Epoch: 18 \t Loss: -0.0036648418851351985\nTRAIN: \t Epoch: 18 \t Loss: -0.0038491178919295116\nTRAIN: \t Epoch: 18 \t Loss: -0.003971904639911372\nTRAIN: \t Epoch: 18 \t Loss: -0.003977147040940408\nTRAIN: \t Epoch: 18 \t Loss: -0.003740701760398224\nTRAIN: \t Epoch: 18 \t Loss: -0.0037027344964867966\nTRAIN: \t Epoch: 18 \t Loss: -0.0037748110335087404\nTRAIN: \t Epoch: 18 \t Loss: -0.003899535867206466\nTRAIN: \t Epoch: 18 \t Loss: -0.004050917744669797\nTRAIN: \t Epoch: 18 \t Loss: -0.004131467473537972\nTRAIN: \t Epoch: 18 \t Loss: -0.004093397907126928\nTRAIN: \t Epoch: 18 \t Loss: -0.004057181975119473\nTRAIN: \t Epoch: 18 \t Loss: -0.00410450599479696\nTRAIN: \t Epoch: 18 \t Loss: -0.004119212364554919\nVALD: \t Epoch: 18 \t Loss: -0.007420856971293688\nVALD: \t Epoch: 18 \t Loss: -0.005970770958811045\nVALD: \t Epoch: 18 \t Loss: -0.0062634774173299474\nVALD: \t Epoch: 18 \t Loss: -0.006327914423309267\nVALD: \t Epoch: 18 \t Loss: -0.0061029853899616845\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.77it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\nADE: 0.5259236695115342  FDE: 0.7255095242217855\n**************************************************\n******************************\nEpoch: social-tag : 18\ntrain_loss -0.004119212364554919\nval_loss -0.0061029853899616845\n{'min_val_epoch': 18, 'min_val_loss': -0.0061029853899616845}\n******************************\nTRAIN: \t Epoch: 19 \t Loss: -0.005487572401762009\nTRAIN: \t Epoch: 19 \t Loss: -0.005437687737867236\nTRAIN: \t Epoch: 19 \t Loss: -0.005477829681088527\nTRAIN: \t Epoch: 19 \t Loss: -0.004826100077480078\nTRAIN: \t Epoch: 19 \t Loss: -0.004558539437130094\nTRAIN: \t Epoch: 19 \t Loss: -0.004633941105566919\nTRAIN: \t Epoch: 19 \t Loss: -0.004612061960090484\nTRAIN: \t Epoch: 19 \t Loss: -0.004751440399559215\nTRAIN: \t Epoch: 19 \t Loss: -0.004580992817257841\nTRAIN: \t Epoch: 19 \t Loss: -0.004191549168899656\nTRAIN: \t Epoch: 19 \t Loss: -0.004069754451682622\nTRAIN: \t Epoch: 19 \t Loss: -0.004100033450716485\nTRAIN: \t Epoch: 19 \t Loss: -0.004190679550027618\nTRAIN: \t Epoch: 19 \t Loss: -0.004339626075566879\nTRAIN: \t Epoch: 19 \t Loss: -0.004381285343940059\nTRAIN: \t Epoch: 19 \t Loss: -0.00439038647164125\nTRAIN: \t Epoch: 19 \t Loss: -0.0042713937787886925\nTRAIN: \t Epoch: 19 \t Loss: -0.0041850022066177595\nTRAIN: \t Epoch: 19 \t Loss: -0.004189681897098704\nVALD: \t Epoch: 19 \t Loss: -0.006198437884449959\nVALD: \t Epoch: 19 \t Loss: -0.005504105472937226\nVALD: \t Epoch: 19 \t Loss: -0.005825601673374574\nVALD: \t Epoch: 19 \t Loss: -0.005971228005364537\nVALD: \t Epoch: 19 \t Loss: -0.005757203377968024\n******************************\nEpoch: social-tag : 19\ntrain_loss -0.004189681897098704\nval_loss -0.005757203377968024\n{'min_val_epoch': 18, 'min_val_loss': -0.0061029853899616845}\n******************************\nTRAIN: \t Epoch: 20 \t Loss: -0.0052923886105418205\nTRAIN: \t Epoch: 20 \t Loss: -0.005229663103818893\nTRAIN: \t Epoch: 20 \t Loss: -0.005571974441409111\nTRAIN: \t Epoch: 20 \t Loss: -0.005422599264420569\nTRAIN: \t Epoch: 20 \t Loss: -0.004834736231714487\nTRAIN: \t Epoch: 20 \t Loss: -0.004599930446905394\nTRAIN: \t Epoch: 20 \t Loss: -0.004707924722294722\nTRAIN: \t Epoch: 20 \t Loss: -0.004832219594391063\nTRAIN: \t Epoch: 20 \t Loss: -0.004980182611486978\nTRAIN: \t Epoch: 20 \t Loss: -0.0050361801637336615\nTRAIN: \t Epoch: 20 \t Loss: -0.004881444302472201\nTRAIN: \t Epoch: 20 \t Loss: -0.00480719100839148\nTRAIN: \t Epoch: 20 \t Loss: -0.004772344425034065\nTRAIN: \t Epoch: 20 \t Loss: -0.004817088234371373\nTRAIN: \t Epoch: 20 \t Loss: -0.004919689428061247\nTRAIN: \t Epoch: 20 \t Loss: -0.004979297023965046\nTRAIN: \t Epoch: 20 \t Loss: -0.004887895206646884\nTRAIN: \t Epoch: 20 \t Loss: -0.0047506757934267325\nTRAIN: \t Epoch: 20 \t Loss: -0.004744471374074515\nVALD: \t Epoch: 20 \t Loss: -0.004758790601044893\nVALD: \t Epoch: 20 \t Loss: -0.0035224497551098466\nVALD: \t Epoch: 20 \t Loss: -0.0038364642144491277\nVALD: \t Epoch: 20 \t Loss: -0.0038120485260151327\nVALD: \t Epoch: 20 \t Loss: -0.003619368140362511\n******************************\nEpoch: social-tag : 20\ntrain_loss -0.004744471374074515\nval_loss -0.003619368140362511\n{'min_val_epoch': 18, 'min_val_loss': -0.0061029853899616845}\n******************************\nTRAIN: \t Epoch: 21 \t Loss: -0.00402683112770319\nTRAIN: \t Epoch: 21 \t Loss: -0.004580169916152954\nTRAIN: \t Epoch: 21 \t Loss: -0.00502416988213857\nTRAIN: \t Epoch: 21 \t Loss: -0.005226504406891763\nTRAIN: \t Epoch: 21 \t Loss: -0.00515118045732379\nTRAIN: \t Epoch: 21 \t Loss: -0.0048143803142011166\nTRAIN: \t Epoch: 21 \t Loss: -0.004457365422110472\nTRAIN: \t Epoch: 21 \t Loss: -0.0044630485062953085\nTRAIN: \t Epoch: 21 \t Loss: -0.004505031302364336\nTRAIN: \t Epoch: 21 \t Loss: -0.004678755509667098\nTRAIN: \t Epoch: 21 \t Loss: -0.004849611760371111\nTRAIN: \t Epoch: 21 \t Loss: -0.0049558647636634605\nTRAIN: \t Epoch: 21 \t Loss: -0.005077890102536633\nTRAIN: \t Epoch: 21 \t Loss: -0.0050833211779328326\nTRAIN: \t Epoch: 21 \t Loss: -0.005013484026615818\nTRAIN: \t Epoch: 21 \t Loss: -0.004912770702503622\nTRAIN: \t Epoch: 21 \t Loss: -0.004845750381183975\nTRAIN: \t Epoch: 21 \t Loss: -0.004897329568242033\nTRAIN: \t Epoch: 21 \t Loss: -0.004909700451741231\nVALD: \t Epoch: 21 \t Loss: -0.008016396313905716\nVALD: \t Epoch: 21 \t Loss: -0.006921096704900265\nVALD: \t Epoch: 21 \t Loss: -0.007215336586038272\nVALD: \t Epoch: 21 \t Loss: -0.007243421860039234\nVALD: \t Epoch: 21 \t Loss: -0.006851656032987863\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.77it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.49929606520439035  FDE: 0.6908752038889294\n**************************************************\n******************************\nEpoch: social-tag : 21\ntrain_loss -0.004909700451741231\nval_loss -0.006851656032987863\n{'min_val_epoch': 21, 'min_val_loss': -0.006851656032987863}\n******************************\nTRAIN: \t Epoch: 22 \t Loss: -0.005330049432814121\nTRAIN: \t Epoch: 22 \t Loss: -0.005766931921243668\nTRAIN: \t Epoch: 22 \t Loss: -0.005765566757569711\nTRAIN: \t Epoch: 22 \t Loss: -0.004887439077720046\nTRAIN: \t Epoch: 22 \t Loss: -0.0040566634852439165\nTRAIN: \t Epoch: 22 \t Loss: -0.003930362484728296\nTRAIN: \t Epoch: 22 \t Loss: -0.004066964677934136\nTRAIN: \t Epoch: 22 \t Loss: -0.004315664758905768\nTRAIN: \t Epoch: 22 \t Loss: -0.0044280098130305605\nTRAIN: \t Epoch: 22 \t Loss: -0.004638647940009832\nTRAIN: \t Epoch: 22 \t Loss: -0.004707224370742386\nTRAIN: \t Epoch: 22 \t Loss: -0.004476645680066819\nTRAIN: \t Epoch: 22 \t Loss: -0.004287114608674669\nTRAIN: \t Epoch: 22 \t Loss: -0.0042663860450764856\nTRAIN: \t Epoch: 22 \t Loss: -0.0043649675246948995\nTRAIN: \t Epoch: 22 \t Loss: -0.0044716228658217005\nTRAIN: \t Epoch: 22 \t Loss: -0.00452488988829667\nTRAIN: \t Epoch: 22 \t Loss: -0.004627154978354358\nTRAIN: \t Epoch: 22 \t Loss: -0.0045961283578904\nVALD: \t Epoch: 22 \t Loss: -0.007339459843933582\nVALD: \t Epoch: 22 \t Loss: -0.006453942507505417\nVALD: \t Epoch: 22 \t Loss: -0.006561503124733766\nVALD: \t Epoch: 22 \t Loss: -0.0064219742780551314\nVALD: \t Epoch: 22 \t Loss: -0.006186721935745113\n******************************\nEpoch: social-tag : 22\ntrain_loss -0.0045961283578904\nval_loss -0.006186721935745113\n{'min_val_epoch': 21, 'min_val_loss': -0.006851656032987863}\n******************************\nTRAIN: \t Epoch: 23 \t Loss: -0.005834903102368116\nTRAIN: \t Epoch: 23 \t Loss: -0.005876624491065741\nTRAIN: \t Epoch: 23 \t Loss: -0.0049734332133084536\nTRAIN: \t Epoch: 23 \t Loss: -0.003938486624974757\nTRAIN: \t Epoch: 23 \t Loss: -0.0039059156086295845\nTRAIN: \t Epoch: 23 \t Loss: -0.004080848031056424\nTRAIN: \t Epoch: 23 \t Loss: -0.004327486102868404\nTRAIN: \t Epoch: 23 \t Loss: -0.00453880088753067\nTRAIN: \t Epoch: 23 \t Loss: -0.004673237989967068\nTRAIN: \t Epoch: 23 \t Loss: -0.004893889906816185\nTRAIN: \t Epoch: 23 \t Loss: -0.004962098551914096\nTRAIN: \t Epoch: 23 \t Loss: -0.0049378894618712366\nTRAIN: \t Epoch: 23 \t Loss: -0.004665184282482817\nTRAIN: \t Epoch: 23 \t Loss: -0.00454203287206058\nTRAIN: \t Epoch: 23 \t Loss: -0.004600698857878645\nTRAIN: \t Epoch: 23 \t Loss: -0.004682258047978394\nTRAIN: \t Epoch: 23 \t Loss: -0.0047907773542272694\nTRAIN: \t Epoch: 23 \t Loss: -0.004894405693954064\nTRAIN: \t Epoch: 23 \t Loss: -0.004857524286098875\nVALD: \t Epoch: 23 \t Loss: -0.008542661555111408\nVALD: \t Epoch: 23 \t Loss: -0.00738507229834795\nVALD: \t Epoch: 23 \t Loss: -0.007552667831381162\nVALD: \t Epoch: 23 \t Loss: -0.007339911418966949\nVALD: \t Epoch: 23 \t Loss: -0.007035584193615874\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:31<00:00, 18.82it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.495055480597773  FDE: 0.6648032350814848\n**************************************************\n******************************\nEpoch: social-tag : 23\ntrain_loss -0.004857524286098875\nval_loss -0.007035584193615874\n{'min_val_epoch': 23, 'min_val_loss': -0.007035584193615874}\n******************************\nTRAIN: \t Epoch: 24 \t Loss: -0.00621259119361639\nTRAIN: \t Epoch: 24 \t Loss: -0.00550882751122117\nTRAIN: \t Epoch: 24 \t Loss: -0.004484084978078802\nTRAIN: \t Epoch: 24 \t Loss: -0.004541063739452511\nTRAIN: \t Epoch: 24 \t Loss: -0.004918306181207299\nTRAIN: \t Epoch: 24 \t Loss: -0.005190074482622246\nTRAIN: \t Epoch: 24 \t Loss: -0.0052559267662997755\nTRAIN: \t Epoch: 24 \t Loss: -0.005060166382463649\nTRAIN: \t Epoch: 24 \t Loss: -0.004758633724931214\nTRAIN: \t Epoch: 24 \t Loss: -0.004804803477600217\nTRAIN: \t Epoch: 24 \t Loss: -0.004952745977789164\nTRAIN: \t Epoch: 24 \t Loss: -0.005016302457079291\nTRAIN: \t Epoch: 24 \t Loss: -0.0051241185014637616\nTRAIN: \t Epoch: 24 \t Loss: -0.005165120753060494\nTRAIN: \t Epoch: 24 \t Loss: -0.005206533210972945\nTRAIN: \t Epoch: 24 \t Loss: -0.005156445113243535\nTRAIN: \t Epoch: 24 \t Loss: -0.0050813080058159195\nTRAIN: \t Epoch: 24 \t Loss: -0.005065381643362343\nTRAIN: \t Epoch: 24 \t Loss: -0.005066805415683323\nVALD: \t Epoch: 24 \t Loss: -0.007833627983927727\nVALD: \t Epoch: 24 \t Loss: -0.006872015772387385\nVALD: \t Epoch: 24 \t Loss: -0.007254193692157666\nVALD: \t Epoch: 24 \t Loss: -0.007409867248497903\nVALD: \t Epoch: 24 \t Loss: -0.007078086196883651\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\nADE: 0.5381391898980811  FDE: 0.7814830892897583\n**************************************************\n******************************\nEpoch: social-tag : 24\ntrain_loss -0.005066805415683323\nval_loss -0.007078086196883651\n{'min_val_epoch': 24, 'min_val_loss': -0.007078086196883651}\n******************************\nTRAIN: \t Epoch: 25 \t Loss: -0.00510098971426487\nTRAIN: \t Epoch: 25 \t Loss: -0.005698296008631587\nTRAIN: \t Epoch: 25 \t Loss: -0.006102972198277712\nTRAIN: \t Epoch: 25 \t Loss: -0.006220324547030032\nTRAIN: \t Epoch: 25 \t Loss: -0.006284197699278593\nTRAIN: \t Epoch: 25 \t Loss: -0.005564797398013373\nTRAIN: \t Epoch: 25 \t Loss: -0.005156948630298887\nTRAIN: \t Epoch: 25 \t Loss: -0.005107241391669959\nTRAIN: \t Epoch: 25 \t Loss: -0.005263078129953808\nTRAIN: \t Epoch: 25 \t Loss: -0.00540184723213315\nTRAIN: \t Epoch: 25 \t Loss: -0.005456199488517913\nTRAIN: \t Epoch: 25 \t Loss: -0.005483140819706023\nTRAIN: \t Epoch: 25 \t Loss: -0.00553340192597646\nTRAIN: \t Epoch: 25 \t Loss: -0.005549477380035179\nTRAIN: \t Epoch: 25 \t Loss: -0.005479820631444454\nTRAIN: \t Epoch: 25 \t Loss: -0.00544175403774716\nTRAIN: \t Epoch: 25 \t Loss: -0.005466049810981049\nTRAIN: \t Epoch: 25 \t Loss: -0.005558409598759479\nTRAIN: \t Epoch: 25 \t Loss: -0.0055704340467054615\nVALD: \t Epoch: 25 \t Loss: -0.008985397405922413\nVALD: \t Epoch: 25 \t Loss: -0.007417415035888553\nVALD: \t Epoch: 25 \t Loss: -0.00788078565771381\nVALD: \t Epoch: 25 \t Loss: -0.008059524116106331\nVALD: \t Epoch: 25 \t Loss: -0.007647999751666361\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\nADE: 0.5316605749476427  FDE: 0.7986941262538421\n**************************************************\n******************************\nEpoch: social-tag : 25\ntrain_loss -0.0055704340467054615\nval_loss -0.007647999751666361\n{'min_val_epoch': 25, 'min_val_loss': -0.007647999751666361}\n******************************\nTRAIN: \t Epoch: 26 \t Loss: -0.0067050522193312645\nTRAIN: \t Epoch: 26 \t Loss: -0.0062075050082057714\nTRAIN: \t Epoch: 26 \t Loss: -0.004833074752241373\nTRAIN: \t Epoch: 26 \t Loss: -0.004418967000674456\nTRAIN: \t Epoch: 26 \t Loss: -0.004557524016126991\nTRAIN: \t Epoch: 26 \t Loss: -0.00495795482614388\nTRAIN: \t Epoch: 26 \t Loss: -0.00526271464436182\nTRAIN: \t Epoch: 26 \t Loss: -0.005471860320540145\nTRAIN: \t Epoch: 26 \t Loss: -0.005483581762139996\nTRAIN: \t Epoch: 26 \t Loss: -0.005412237788550556\nTRAIN: \t Epoch: 26 \t Loss: -0.005284924716265364\nTRAIN: \t Epoch: 26 \t Loss: -0.00530584065321212\nTRAIN: \t Epoch: 26 \t Loss: -0.005391006913179388\nTRAIN: \t Epoch: 26 \t Loss: -0.0055182536764602575\nTRAIN: \t Epoch: 26 \t Loss: -0.005612782311315337\nTRAIN: \t Epoch: 26 \t Loss: -0.005685579948476516\nTRAIN: \t Epoch: 26 \t Loss: -0.005676566061618573\nTRAIN: \t Epoch: 26 \t Loss: -0.005632825338074731\nTRAIN: \t Epoch: 26 \t Loss: -0.005624849403808078\nVALD: \t Epoch: 26 \t Loss: -0.007362480275332928\nVALD: \t Epoch: 26 \t Loss: -0.0059748683124780655\nVALD: \t Epoch: 26 \t Loss: -0.006290053793539603\nVALD: \t Epoch: 26 \t Loss: -0.006359789869748056\nVALD: \t Epoch: 26 \t Loss: -0.006105000815115684\n******************************\nEpoch: social-tag : 26\ntrain_loss -0.005624849403808078\nval_loss -0.006105000815115684\n{'min_val_epoch': 25, 'min_val_loss': -0.007647999751666361}\n******************************\nTRAIN: \t Epoch: 27 \t Loss: -0.0059457821771502495\nTRAIN: \t Epoch: 27 \t Loss: -0.006193374516442418\nTRAIN: \t Epoch: 27 \t Loss: -0.006144676047066848\nTRAIN: \t Epoch: 27 \t Loss: -0.005700650857761502\nTRAIN: \t Epoch: 27 \t Loss: -0.005553771276026964\nTRAIN: \t Epoch: 27 \t Loss: -0.00538993370719254\nTRAIN: \t Epoch: 27 \t Loss: -0.005487180647573301\nTRAIN: \t Epoch: 27 \t Loss: -0.005655668384861201\nTRAIN: \t Epoch: 27 \t Loss: -0.005726958573278453\nTRAIN: \t Epoch: 27 \t Loss: -0.005567694688215852\nTRAIN: \t Epoch: 27 \t Loss: -0.0053928560281003065\nTRAIN: \t Epoch: 27 \t Loss: -0.005360166910880555\nTRAIN: \t Epoch: 27 \t Loss: -0.005489941775942078\nTRAIN: \t Epoch: 27 \t Loss: -0.0056607968207182625\nTRAIN: \t Epoch: 27 \t Loss: -0.005707514549915989\nTRAIN: \t Epoch: 27 \t Loss: -0.005803146414109506\nTRAIN: \t Epoch: 27 \t Loss: -0.005815852282787947\nTRAIN: \t Epoch: 27 \t Loss: -0.005683555526451932\nTRAIN: \t Epoch: 27 \t Loss: -0.005679219313010792\nVALD: \t Epoch: 27 \t Loss: -0.006621206179261208\nVALD: \t Epoch: 27 \t Loss: -0.005745294503867626\nVALD: \t Epoch: 27 \t Loss: -0.0060364855453372\nVALD: \t Epoch: 27 \t Loss: -0.006172165274620056\nVALD: \t Epoch: 27 \t Loss: -0.005856664121643571\n******************************\nEpoch: social-tag : 27\ntrain_loss -0.005679219313010792\nval_loss -0.005856664121643571\n{'min_val_epoch': 25, 'min_val_loss': -0.007647999751666361}\n******************************\nTRAIN: \t Epoch: 28 \t Loss: -0.005439593456685543\nTRAIN: \t Epoch: 28 \t Loss: -0.0058720409870147705\nTRAIN: \t Epoch: 28 \t Loss: -0.006394023075699806\nTRAIN: \t Epoch: 28 \t Loss: -0.006515774060972035\nTRAIN: \t Epoch: 28 \t Loss: -0.006316827051341534\nTRAIN: \t Epoch: 28 \t Loss: -0.00589924674325933\nTRAIN: \t Epoch: 28 \t Loss: -0.005590585625863501\nTRAIN: \t Epoch: 28 \t Loss: -0.005678478482877836\nTRAIN: \t Epoch: 28 \t Loss: -0.005847232856063379\nTRAIN: \t Epoch: 28 \t Loss: -0.00604557937476784\nTRAIN: \t Epoch: 28 \t Loss: -0.006115594518963586\nTRAIN: \t Epoch: 28 \t Loss: -0.0061156425896721585\nTRAIN: \t Epoch: 28 \t Loss: -0.006019299115555791\nTRAIN: \t Epoch: 28 \t Loss: -0.005855490586587361\nTRAIN: \t Epoch: 28 \t Loss: -0.005836524038265149\nTRAIN: \t Epoch: 28 \t Loss: -0.005905037687625736\nTRAIN: \t Epoch: 28 \t Loss: -0.005980802864274558\nTRAIN: \t Epoch: 28 \t Loss: -0.006051480977071656\nTRAIN: \t Epoch: 28 \t Loss: -0.00606059217560815\nVALD: \t Epoch: 28 \t Loss: -0.009744813665747643\nVALD: \t Epoch: 28 \t Loss: -0.008123306790366769\nVALD: \t Epoch: 28 \t Loss: -0.008242470988382896\nVALD: \t Epoch: 28 \t Loss: -0.007982829120010138\nVALD: \t Epoch: 28 \t Loss: -0.007591391791982099\n******************************\nEpoch: social-tag : 28\ntrain_loss -0.00606059217560815\nval_loss -0.007591391791982099\n{'min_val_epoch': 25, 'min_val_loss': -0.007647999751666361}\n******************************\nTRAIN: \t Epoch: 29 \t Loss: -0.007219494320452213\nTRAIN: \t Epoch: 29 \t Loss: -0.005959587171673775\nTRAIN: \t Epoch: 29 \t Loss: -0.004693911178037524\nTRAIN: \t Epoch: 29 \t Loss: -0.004725719627458602\nTRAIN: \t Epoch: 29 \t Loss: -0.005075133265927434\nTRAIN: \t Epoch: 29 \t Loss: -0.0053205765240515275\nTRAIN: \t Epoch: 29 \t Loss: -0.005625079086582575\nTRAIN: \t Epoch: 29 \t Loss: -0.005923190590692684\nTRAIN: \t Epoch: 29 \t Loss: -0.0058909733521027696\nTRAIN: \t Epoch: 29 \t Loss: -0.005871010129339993\nTRAIN: \t Epoch: 29 \t Loss: -0.005863898497244174\nTRAIN: \t Epoch: 29 \t Loss: -0.005944074112145851\nTRAIN: \t Epoch: 29 \t Loss: -0.0059121266329804295\nTRAIN: \t Epoch: 29 \t Loss: -0.005726222397892603\nTRAIN: \t Epoch: 29 \t Loss: -0.005667912292604645\nTRAIN: \t Epoch: 29 \t Loss: -0.005720041794120334\nTRAIN: \t Epoch: 29 \t Loss: -0.0058032114414826915\nTRAIN: \t Epoch: 29 \t Loss: -0.005880841222177777\nTRAIN: \t Epoch: 29 \t Loss: -0.005878175080181913\nVALD: \t Epoch: 29 \t Loss: -0.009779322892427444\nVALD: \t Epoch: 29 \t Loss: -0.008206731639802456\nVALD: \t Epoch: 29 \t Loss: -0.008427817995349566\nVALD: \t Epoch: 29 \t Loss: -0.008424669271335006\nVALD: \t Epoch: 29 \t Loss: -0.008061851942834776\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.72it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.4702093988667636  FDE: 0.651516929876885\n**************************************************\n******************************\nEpoch: social-tag : 29\ntrain_loss -0.005878175080181913\nval_loss -0.008061851942834776\n{'min_val_epoch': 29, 'min_val_loss': -0.008061851942834776}\n******************************\nTRAIN: \t Epoch: 30 \t Loss: -0.006834216881543398\nTRAIN: \t Epoch: 30 \t Loss: -0.00696910941042006\nTRAIN: \t Epoch: 30 \t Loss: -0.006582963125159343\nTRAIN: \t Epoch: 30 \t Loss: -0.005821294616907835\nTRAIN: \t Epoch: 30 \t Loss: -0.005672771204262972\nTRAIN: \t Epoch: 30 \t Loss: -0.005794484323511521\nTRAIN: \t Epoch: 30 \t Loss: -0.005957357984568391\nTRAIN: \t Epoch: 30 \t Loss: -0.006123849656432867\nTRAIN: \t Epoch: 30 \t Loss: -0.006263350912680228\nTRAIN: \t Epoch: 30 \t Loss: -0.006148360623046756\nTRAIN: \t Epoch: 30 \t Loss: -0.006046798334202983\nTRAIN: \t Epoch: 30 \t Loss: -0.006065575483565529\nTRAIN: \t Epoch: 30 \t Loss: -0.006138902050084793\nTRAIN: \t Epoch: 30 \t Loss: -0.0062584341836294955\nTRAIN: \t Epoch: 30 \t Loss: -0.0062874242973824344\nTRAIN: \t Epoch: 30 \t Loss: -0.006213639018824324\nTRAIN: \t Epoch: 30 \t Loss: -0.006092038032982279\nTRAIN: \t Epoch: 30 \t Loss: -0.006102874771588379\nTRAIN: \t Epoch: 30 \t Loss: -0.006115523092352451\nVALD: \t Epoch: 30 \t Loss: -0.009177370928227901\nVALD: \t Epoch: 30 \t Loss: -0.008140966761857271\nVALD: \t Epoch: 30 \t Loss: -0.008454664299885431\nVALD: \t Epoch: 30 \t Loss: -0.008562367176637053\nVALD: \t Epoch: 30 \t Loss: -0.00823032836283534\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.76it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\nADE: 0.4617743602206721  FDE: 0.6085423516123403\n**************************************************\n******************************\nEpoch: social-tag : 30\ntrain_loss -0.006115523092352451\nval_loss -0.00823032836283534\n{'min_val_epoch': 30, 'min_val_loss': -0.00823032836283534}\n******************************\nTRAIN: \t Epoch: 31 \t Loss: -0.00681677320972085\nTRAIN: \t Epoch: 31 \t Loss: -0.007732949452474713\nTRAIN: \t Epoch: 31 \t Loss: -0.007702232804149389\nTRAIN: \t Epoch: 31 \t Loss: -0.0075719490414485335\nTRAIN: \t Epoch: 31 \t Loss: -0.00742467101663351\nTRAIN: \t Epoch: 31 \t Loss: -0.007179258391261101\nTRAIN: \t Epoch: 31 \t Loss: -0.006922670906143529\nTRAIN: \t Epoch: 31 \t Loss: -0.006416470598196611\nTRAIN: \t Epoch: 31 \t Loss: -0.006279996518666546\nTRAIN: \t Epoch: 31 \t Loss: -0.0063562355237081645\nTRAIN: \t Epoch: 31 \t Loss: -0.006364489545706998\nTRAIN: \t Epoch: 31 \t Loss: -0.006486312466828774\nTRAIN: \t Epoch: 31 \t Loss: -0.006522586587100075\nTRAIN: \t Epoch: 31 \t Loss: -0.006449569898125317\nTRAIN: \t Epoch: 31 \t Loss: -0.006240055756643414\nTRAIN: \t Epoch: 31 \t Loss: -0.0061634697340196\nTRAIN: \t Epoch: 31 \t Loss: -0.006229016970952644\nTRAIN: \t Epoch: 31 \t Loss: -0.006315879847130014\nTRAIN: \t Epoch: 31 \t Loss: -0.006320627048470458\nVALD: \t Epoch: 31 \t Loss: -0.009433228522539139\nVALD: \t Epoch: 31 \t Loss: -0.008271277882158756\nVALD: \t Epoch: 31 \t Loss: -0.008842694262663523\nVALD: \t Epoch: 31 \t Loss: -0.009061847580596805\nVALD: \t Epoch: 31 \t Loss: -0.008501666143906019\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.73it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.4629085973738792  FDE: 0.655552431778502\n**************************************************\n******************************\nEpoch: social-tag : 31\ntrain_loss -0.006320627048470458\nval_loss -0.008501666143906019\n{'min_val_epoch': 31, 'min_val_loss': -0.008501666143906019}\n******************************\nTRAIN: \t Epoch: 32 \t Loss: -0.007371230516582727\nTRAIN: \t Epoch: 32 \t Loss: -0.007056837901473045\nTRAIN: \t Epoch: 32 \t Loss: -0.00707016559317708\nTRAIN: \t Epoch: 32 \t Loss: -0.006688984809443355\nTRAIN: \t Epoch: 32 \t Loss: -0.005944758374243975\nTRAIN: \t Epoch: 32 \t Loss: -0.005776634206995368\nTRAIN: \t Epoch: 32 \t Loss: -0.005830049714339631\nTRAIN: \t Epoch: 32 \t Loss: -0.006118647928815335\nTRAIN: \t Epoch: 32 \t Loss: -0.006280897288686699\nTRAIN: \t Epoch: 32 \t Loss: -0.006375887989997863\nTRAIN: \t Epoch: 32 \t Loss: -0.0063403095542029905\nTRAIN: \t Epoch: 32 \t Loss: -0.0061667364013070864\nTRAIN: \t Epoch: 32 \t Loss: -0.006069425350198379\nTRAIN: \t Epoch: 32 \t Loss: -0.006160410985882793\nTRAIN: \t Epoch: 32 \t Loss: -0.006283862081666787\nTRAIN: \t Epoch: 32 \t Loss: -0.006351267278660089\nTRAIN: \t Epoch: 32 \t Loss: -0.006423682083978373\nTRAIN: \t Epoch: 32 \t Loss: -0.0065057907356984085\nTRAIN: \t Epoch: 32 \t Loss: -0.006520041751512467\nVALD: \t Epoch: 32 \t Loss: -0.007377936504781246\nVALD: \t Epoch: 32 \t Loss: -0.0060139724519103765\nVALD: \t Epoch: 32 \t Loss: -0.0069283062281707926\nVALD: \t Epoch: 32 \t Loss: -0.007148343720473349\nVALD: \t Epoch: 32 \t Loss: -0.006449684968664627\n******************************\nEpoch: social-tag : 32\ntrain_loss -0.006520041751512467\nval_loss -0.006449684968664627\n{'min_val_epoch': 31, 'min_val_loss': -0.008501666143906019}\n******************************\nTRAIN: \t Epoch: 33 \t Loss: -0.008511288091540337\nTRAIN: \t Epoch: 33 \t Loss: -0.008001909824088216\nTRAIN: \t Epoch: 33 \t Loss: -0.007474622378746669\nTRAIN: \t Epoch: 33 \t Loss: -0.0065823482000269\nTRAIN: \t Epoch: 33 \t Loss: -0.006402773829177022\nTRAIN: \t Epoch: 33 \t Loss: -0.006441008493614693\nTRAIN: \t Epoch: 33 \t Loss: -0.006619981722906232\nTRAIN: \t Epoch: 33 \t Loss: -0.006657614285359159\nTRAIN: \t Epoch: 33 \t Loss: -0.006730293311799566\nTRAIN: \t Epoch: 33 \t Loss: -0.006668054708279669\nTRAIN: \t Epoch: 33 \t Loss: -0.006433528657494621\nTRAIN: \t Epoch: 33 \t Loss: -0.006273451358235131\nTRAIN: \t Epoch: 33 \t Loss: -0.006301732071173878\nTRAIN: \t Epoch: 33 \t Loss: -0.006414652319758066\nTRAIN: \t Epoch: 33 \t Loss: -0.006494920405869683\nTRAIN: \t Epoch: 33 \t Loss: -0.006534550091600977\nTRAIN: \t Epoch: 33 \t Loss: -0.006585753481725559\nTRAIN: \t Epoch: 33 \t Loss: -0.006489281193353236\nTRAIN: \t Epoch: 33 \t Loss: -0.0064760497406214914\nVALD: \t Epoch: 33 \t Loss: -0.006036295089870691\nVALD: \t Epoch: 33 \t Loss: -0.0051275319419801235\nVALD: \t Epoch: 33 \t Loss: -0.005591079592704773\nVALD: \t Epoch: 33 \t Loss: -0.005819964339025319\nVALD: \t Epoch: 33 \t Loss: -0.0053769860878463615\n******************************\nEpoch: social-tag : 33\ntrain_loss -0.0064760497406214914\nval_loss -0.0053769860878463615\n{'min_val_epoch': 31, 'min_val_loss': -0.008501666143906019}\n******************************\nTRAIN: \t Epoch: 34 \t Loss: -0.004756989423185587\nTRAIN: \t Epoch: 34 \t Loss: -0.0057860983069986105\nTRAIN: \t Epoch: 34 \t Loss: -0.006539755345632632\nTRAIN: \t Epoch: 34 \t Loss: -0.006638594903051853\nTRAIN: \t Epoch: 34 \t Loss: -0.006742278393357992\nTRAIN: \t Epoch: 34 \t Loss: -0.006709833707039555\nTRAIN: \t Epoch: 34 \t Loss: -0.006524115667811462\nTRAIN: \t Epoch: 34 \t Loss: -0.006236359884496778\nTRAIN: \t Epoch: 34 \t Loss: -0.006345697212964296\nTRAIN: \t Epoch: 34 \t Loss: -0.006516543729230761\nTRAIN: \t Epoch: 34 \t Loss: -0.006637108131227168\nTRAIN: \t Epoch: 34 \t Loss: -0.006749254069291055\nTRAIN: \t Epoch: 34 \t Loss: -0.00670426102498403\nTRAIN: \t Epoch: 34 \t Loss: -0.006474398226211113\nTRAIN: \t Epoch: 34 \t Loss: -0.006334008136764169\nTRAIN: \t Epoch: 34 \t Loss: -0.006408512897905894\nTRAIN: \t Epoch: 34 \t Loss: -0.006531431955997558\nTRAIN: \t Epoch: 34 \t Loss: -0.0065940715139731765\nTRAIN: \t Epoch: 34 \t Loss: -0.006607330757980199\nVALD: \t Epoch: 34 \t Loss: -0.009613546542823315\nVALD: \t Epoch: 34 \t Loss: -0.008385882945731282\nVALD: \t Epoch: 34 \t Loss: -0.008936680698146423\nVALD: \t Epoch: 34 \t Loss: -0.009140090900473297\nVALD: \t Epoch: 34 \t Loss: -0.00856328847979711\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.74it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.4527945269045283  FDE: 0.6289154554913888\n**************************************************\n******************************\nEpoch: social-tag : 34\ntrain_loss -0.006607330757980199\nval_loss -0.00856328847979711\n{'min_val_epoch': 34, 'min_val_loss': -0.00856328847979711}\n******************************\nTRAIN: \t Epoch: 35 \t Loss: -0.007249289192259312\nTRAIN: \t Epoch: 35 \t Loss: -0.00702439364977181\nTRAIN: \t Epoch: 35 \t Loss: -0.007367106309781472\nTRAIN: \t Epoch: 35 \t Loss: -0.007266898988746107\nTRAIN: \t Epoch: 35 \t Loss: -0.00674655856564641\nTRAIN: \t Epoch: 35 \t Loss: -0.006413766493399938\nTRAIN: \t Epoch: 35 \t Loss: -0.006513195977147136\nTRAIN: \t Epoch: 35 \t Loss: -0.0066743920324370265\nTRAIN: \t Epoch: 35 \t Loss: -0.00675935417206751\nTRAIN: \t Epoch: 35 \t Loss: -0.006958708120509982\nTRAIN: \t Epoch: 35 \t Loss: -0.0069262387484989385\nTRAIN: \t Epoch: 35 \t Loss: -0.006382450582653594\nTRAIN: \t Epoch: 35 \t Loss: -0.00612749356794386\nTRAIN: \t Epoch: 35 \t Loss: -0.006124071947332206\nTRAIN: \t Epoch: 35 \t Loss: -0.006185486047373464\nTRAIN: \t Epoch: 35 \t Loss: -0.00634771178374649\nTRAIN: \t Epoch: 35 \t Loss: -0.00643346765804488\nTRAIN: \t Epoch: 35 \t Loss: -0.0064365102734882385\nTRAIN: \t Epoch: 35 \t Loss: -0.0064218432564708714\nVALD: \t Epoch: 35 \t Loss: -0.008273999206721783\nVALD: \t Epoch: 35 \t Loss: -0.0070348873268812895\nVALD: \t Epoch: 35 \t Loss: -0.007324856550743182\nVALD: \t Epoch: 35 \t Loss: -0.0074634148040786386\nVALD: \t Epoch: 35 \t Loss: -0.007006410232260208\n******************************\nEpoch: social-tag : 35\ntrain_loss -0.0064218432564708714\nval_loss -0.007006410232260208\n{'min_val_epoch': 34, 'min_val_loss': -0.00856328847979711}\n******************************\nTRAIN: \t Epoch: 36 \t Loss: -0.005902053322643042\nTRAIN: \t Epoch: 36 \t Loss: -0.006273830309510231\nTRAIN: \t Epoch: 36 \t Loss: -0.006909672481318315\nTRAIN: \t Epoch: 36 \t Loss: -0.007148818811401725\nTRAIN: \t Epoch: 36 \t Loss: -0.007138380408287048\nTRAIN: \t Epoch: 36 \t Loss: -0.007134255487471819\nTRAIN: \t Epoch: 36 \t Loss: -0.006951561862868922\nTRAIN: \t Epoch: 36 \t Loss: -0.006813319225329906\nTRAIN: \t Epoch: 36 \t Loss: -0.006861968172921075\nTRAIN: \t Epoch: 36 \t Loss: -0.006982370745390654\nTRAIN: \t Epoch: 36 \t Loss: -0.006937513085590167\nTRAIN: \t Epoch: 36 \t Loss: -0.006798559062493344\nTRAIN: \t Epoch: 36 \t Loss: -0.0067374873189972\nTRAIN: \t Epoch: 36 \t Loss: -0.006764507253787347\nTRAIN: \t Epoch: 36 \t Loss: -0.006905849402149518\nTRAIN: \t Epoch: 36 \t Loss: -0.006999360804911703\nTRAIN: \t Epoch: 36 \t Loss: -0.007062734707313425\nTRAIN: \t Epoch: 36 \t Loss: -0.0070713486832877\nTRAIN: \t Epoch: 36 \t Loss: -0.007065221548568166\nVALD: \t Epoch: 36 \t Loss: -0.009078563190996647\nVALD: \t Epoch: 36 \t Loss: -0.007771125761792064\nVALD: \t Epoch: 36 \t Loss: -0.007861572162558636\nVALD: \t Epoch: 36 \t Loss: -0.007724549621343613\nVALD: \t Epoch: 36 \t Loss: -0.007442337914931873\n******************************\nEpoch: social-tag : 36\ntrain_loss -0.007065221548568166\nval_loss -0.007442337914931873\n{'min_val_epoch': 34, 'min_val_loss': -0.00856328847979711}\n******************************\nTRAIN: \t Epoch: 37 \t Loss: -0.006556439679116011\nTRAIN: \t Epoch: 37 \t Loss: -0.007061343872919679\nTRAIN: \t Epoch: 37 \t Loss: -0.0073201474733650684\nTRAIN: \t Epoch: 37 \t Loss: -0.007530856062658131\nTRAIN: \t Epoch: 37 \t Loss: -0.007464507129043341\nTRAIN: \t Epoch: 37 \t Loss: -0.007352613688757022\nTRAIN: \t Epoch: 37 \t Loss: -0.007250706332602671\nTRAIN: \t Epoch: 37 \t Loss: -0.007293240807484835\nTRAIN: \t Epoch: 37 \t Loss: -0.0073221167032089494\nTRAIN: \t Epoch: 37 \t Loss: -0.007294598966836929\nTRAIN: \t Epoch: 37 \t Loss: -0.007275031329217282\nTRAIN: \t Epoch: 37 \t Loss: -0.00703115474122266\nTRAIN: \t Epoch: 37 \t Loss: -0.006936301513073536\nTRAIN: \t Epoch: 37 \t Loss: -0.007010304335770863\nTRAIN: \t Epoch: 37 \t Loss: -0.0070594974172612035\nTRAIN: \t Epoch: 37 \t Loss: -0.007139962661312893\nTRAIN: \t Epoch: 37 \t Loss: -0.007196405765545719\nTRAIN: \t Epoch: 37 \t Loss: -0.007106542457929916\nTRAIN: \t Epoch: 37 \t Loss: -0.007090846360576861\nVALD: \t Epoch: 37 \t Loss: -0.007849368266761303\nVALD: \t Epoch: 37 \t Loss: -0.00675323698669672\nVALD: \t Epoch: 37 \t Loss: -0.007152017205953598\nVALD: \t Epoch: 37 \t Loss: -0.007346129044890404\nVALD: \t Epoch: 37 \t Loss: -0.006861708282439176\n******************************\nEpoch: social-tag : 37\ntrain_loss -0.007090846360576861\nval_loss -0.006861708282439176\n{'min_val_epoch': 34, 'min_val_loss': -0.00856328847979711}\n******************************\nTRAIN: \t Epoch: 38 \t Loss: -0.0064633795991539955\nTRAIN: \t Epoch: 38 \t Loss: -0.006931013660505414\nTRAIN: \t Epoch: 38 \t Loss: -0.007340929936617613\nTRAIN: \t Epoch: 38 \t Loss: -0.007074640016071498\nTRAIN: \t Epoch: 38 \t Loss: -0.006788146030157804\nTRAIN: \t Epoch: 38 \t Loss: -0.006459352017069857\nTRAIN: \t Epoch: 38 \t Loss: -0.00665428615840418\nTRAIN: \t Epoch: 38 \t Loss: -0.0068947746767662466\nTRAIN: \t Epoch: 38 \t Loss: -0.007036802203704913\nTRAIN: \t Epoch: 38 \t Loss: -0.007016111444681883\nTRAIN: \t Epoch: 38 \t Loss: -0.007085153536701744\nTRAIN: \t Epoch: 38 \t Loss: -0.00703058911797901\nTRAIN: \t Epoch: 38 \t Loss: -0.0069829996746893115\nTRAIN: \t Epoch: 38 \t Loss: -0.006998570569391761\nTRAIN: \t Epoch: 38 \t Loss: -0.00710543841123581\nTRAIN: \t Epoch: 38 \t Loss: -0.007198585313744843\nTRAIN: \t Epoch: 38 \t Loss: -0.007092693847987582\nTRAIN: \t Epoch: 38 \t Loss: -0.006953745873437988\nTRAIN: \t Epoch: 38 \t Loss: -0.0069633959427565775\nVALD: \t Epoch: 38 \t Loss: -0.00809638760983944\nVALD: \t Epoch: 38 \t Loss: -0.007394324755296111\nVALD: \t Epoch: 38 \t Loss: -0.007752460893243551\nVALD: \t Epoch: 38 \t Loss: -0.007911088992841542\nVALD: \t Epoch: 38 \t Loss: -0.007596374247684952\n******************************\nEpoch: social-tag : 38\ntrain_loss -0.0069633959427565775\nval_loss -0.007596374247684952\n{'min_val_epoch': 34, 'min_val_loss': -0.00856328847979711}\n******************************\nTRAIN: \t Epoch: 39 \t Loss: -0.006731156725436449\nTRAIN: \t Epoch: 39 \t Loss: -0.007712164660915732\nTRAIN: \t Epoch: 39 \t Loss: -0.00777693356697758\nTRAIN: \t Epoch: 39 \t Loss: -0.007783150067552924\nTRAIN: \t Epoch: 39 \t Loss: -0.007715368643403053\nTRAIN: \t Epoch: 39 \t Loss: -0.007677505491301417\nTRAIN: \t Epoch: 39 \t Loss: -0.007619625556149653\nTRAIN: \t Epoch: 39 \t Loss: -0.007169275952037424\nTRAIN: \t Epoch: 39 \t Loss: -0.007047045975923538\nTRAIN: \t Epoch: 39 \t Loss: -0.00713795954361558\nTRAIN: \t Epoch: 39 \t Loss: -0.007251144471493634\nTRAIN: \t Epoch: 39 \t Loss: -0.00735419235813121\nTRAIN: \t Epoch: 39 \t Loss: -0.00750349354572021\nTRAIN: \t Epoch: 39 \t Loss: -0.007575174727077995\nTRAIN: \t Epoch: 39 \t Loss: -0.007395719519505898\nTRAIN: \t Epoch: 39 \t Loss: -0.007291167479706928\nTRAIN: \t Epoch: 39 \t Loss: -0.007315095842761152\nTRAIN: \t Epoch: 39 \t Loss: -0.007335829858978589\nTRAIN: \t Epoch: 39 \t Loss: -0.007346063120728623\nVALD: \t Epoch: 39 \t Loss: -0.010306966491043568\nVALD: \t Epoch: 39 \t Loss: -0.008973876014351845\nVALD: \t Epoch: 39 \t Loss: -0.00945823701719443\nVALD: \t Epoch: 39 \t Loss: -0.009582596831023693\nVALD: \t Epoch: 39 \t Loss: -0.009069025615030084\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.70it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\nADE: 0.4781785910057503  FDE: 0.7278599716364577\n**************************************************\n******************************\nEpoch: social-tag : 39\ntrain_loss -0.007346063120728623\nval_loss -0.009069025615030084\n{'min_val_epoch': 39, 'min_val_loss': -0.009069025615030084}\n******************************\nTRAIN: \t Epoch: 40 \t Loss: -0.008532887324690819\nTRAIN: \t Epoch: 40 \t Loss: -0.007481707725673914\nTRAIN: \t Epoch: 40 \t Loss: -0.00801252294331789\nTRAIN: \t Epoch: 40 \t Loss: -0.008137302473187447\nTRAIN: \t Epoch: 40 \t Loss: -0.007392137590795755\nTRAIN: \t Epoch: 40 \t Loss: -0.006536137351455788\nTRAIN: \t Epoch: 40 \t Loss: -0.006347454984539321\nTRAIN: \t Epoch: 40 \t Loss: -0.0064785856229718775\nTRAIN: \t Epoch: 40 \t Loss: -0.006763230362493131\nTRAIN: \t Epoch: 40 \t Loss: -0.006993071804754436\nTRAIN: \t Epoch: 40 \t Loss: -0.007149747128344395\nTRAIN: \t Epoch: 40 \t Loss: -0.007155327184591442\nTRAIN: \t Epoch: 40 \t Loss: -0.006918526917266158\nTRAIN: \t Epoch: 40 \t Loss: -0.006684974950206067\nTRAIN: \t Epoch: 40 \t Loss: -0.006688280294959744\nTRAIN: \t Epoch: 40 \t Loss: -0.0067731643357547\nTRAIN: \t Epoch: 40 \t Loss: -0.006852440221016021\nTRAIN: \t Epoch: 40 \t Loss: -0.006978105249193807\nTRAIN: \t Epoch: 40 \t Loss: -0.00698722308560962\nVALD: \t Epoch: 40 \t Loss: -0.01102429162710905\nVALD: \t Epoch: 40 \t Loss: -0.00979159027338028\nVALD: \t Epoch: 40 \t Loss: -0.010348860174417496\nVALD: \t Epoch: 40 \t Loss: -0.010525727411732078\nVALD: \t Epoch: 40 \t Loss: -0.009911016294778871\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.76it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\nADE: 0.4431156312730187  FDE: 0.6508880095575207\n**************************************************\n******************************\nEpoch: social-tag : 40\ntrain_loss -0.00698722308560962\nval_loss -0.009911016294778871\n{'min_val_epoch': 40, 'min_val_loss': -0.009911016294778871}\n******************************\nTRAIN: \t Epoch: 41 \t Loss: -0.008160914294421673\nTRAIN: \t Epoch: 41 \t Loss: -0.008432160131633282\nTRAIN: \t Epoch: 41 \t Loss: -0.008362440702815851\nTRAIN: \t Epoch: 41 \t Loss: -0.008357515558600426\nTRAIN: \t Epoch: 41 \t Loss: -0.008147560432553292\nTRAIN: \t Epoch: 41 \t Loss: -0.007713489001616836\nTRAIN: \t Epoch: 41 \t Loss: -0.0075945197604596615\nTRAIN: \t Epoch: 41 \t Loss: -0.007683757285121828\nTRAIN: \t Epoch: 41 \t Loss: -0.007795643734021319\nTRAIN: \t Epoch: 41 \t Loss: -0.007800763612613082\nTRAIN: \t Epoch: 41 \t Loss: -0.007513937879015099\nTRAIN: \t Epoch: 41 \t Loss: -0.007207182585261762\nTRAIN: \t Epoch: 41 \t Loss: -0.007154363422439649\nTRAIN: \t Epoch: 41 \t Loss: -0.007233185161437307\nTRAIN: \t Epoch: 41 \t Loss: -0.0072417387117942175\nTRAIN: \t Epoch: 41 \t Loss: -0.007244370208354667\nTRAIN: \t Epoch: 41 \t Loss: -0.0072111239492454945\nTRAIN: \t Epoch: 41 \t Loss: -0.007210249339954721\nTRAIN: \t Epoch: 41 \t Loss: -0.007235126831375454\nVALD: \t Epoch: 41 \t Loss: -0.0089782839640975\nVALD: \t Epoch: 41 \t Loss: -0.007441024761646986\nVALD: \t Epoch: 41 \t Loss: -0.008023564703762531\nVALD: \t Epoch: 41 \t Loss: -0.008157905424013734\nVALD: \t Epoch: 41 \t Loss: -0.0075864015531933995\n******************************\nEpoch: social-tag : 41\ntrain_loss -0.007235126831375454\nval_loss -0.0075864015531933995\n{'min_val_epoch': 40, 'min_val_loss': -0.009911016294778871}\n******************************\nTRAIN: \t Epoch: 42 \t Loss: -0.006868711672723293\nTRAIN: \t Epoch: 42 \t Loss: -0.007508112117648125\nTRAIN: \t Epoch: 42 \t Loss: -0.007688483223319054\nTRAIN: \t Epoch: 42 \t Loss: -0.006910409429110587\nTRAIN: \t Epoch: 42 \t Loss: -0.0066385816782712935\nTRAIN: \t Epoch: 42 \t Loss: -0.006863814312964678\nTRAIN: \t Epoch: 42 \t Loss: -0.00713132308529956\nTRAIN: \t Epoch: 42 \t Loss: -0.007237139507196844\nTRAIN: \t Epoch: 42 \t Loss: -0.007212168588820431\nTRAIN: \t Epoch: 42 \t Loss: -0.007156742922961712\nTRAIN: \t Epoch: 42 \t Loss: -0.00708516953851689\nTRAIN: \t Epoch: 42 \t Loss: -0.0070992091204971075\nTRAIN: \t Epoch: 42 \t Loss: -0.007166141930681009\nTRAIN: \t Epoch: 42 \t Loss: -0.0072911618543522695\nTRAIN: \t Epoch: 42 \t Loss: -0.007290229760110378\nTRAIN: \t Epoch: 42 \t Loss: -0.0071232516202144325\nTRAIN: \t Epoch: 42 \t Loss: -0.007059457218822311\nTRAIN: \t Epoch: 42 \t Loss: -0.0071191696139673395\nTRAIN: \t Epoch: 42 \t Loss: -0.007119399491411976\nVALD: \t Epoch: 42 \t Loss: -0.009855394251644611\nVALD: \t Epoch: 42 \t Loss: -0.00902681378647685\nVALD: \t Epoch: 42 \t Loss: -0.00959159837414821\nVALD: \t Epoch: 42 \t Loss: -0.009754723170772195\nVALD: \t Epoch: 42 \t Loss: -0.009184616262262519\n******************************\nEpoch: social-tag : 42\ntrain_loss -0.007119399491411976\nval_loss -0.009184616262262519\n{'min_val_epoch': 40, 'min_val_loss': -0.009911016294778871}\n******************************\nTRAIN: \t Epoch: 43 \t Loss: -0.008727912791073322\nTRAIN: \t Epoch: 43 \t Loss: -0.00829132366925478\nTRAIN: \t Epoch: 43 \t Loss: -0.007844930980354548\nTRAIN: \t Epoch: 43 \t Loss: -0.007130638230592012\nTRAIN: \t Epoch: 43 \t Loss: -0.007143933977931738\nTRAIN: \t Epoch: 43 \t Loss: -0.007194344885647297\nTRAIN: \t Epoch: 43 \t Loss: -0.007328814826905727\nTRAIN: \t Epoch: 43 \t Loss: -0.007562939426861703\nTRAIN: \t Epoch: 43 \t Loss: -0.007624158325294654\nTRAIN: \t Epoch: 43 \t Loss: -0.0076983367092907425\nTRAIN: \t Epoch: 43 \t Loss: -0.007743303833360022\nTRAIN: \t Epoch: 43 \t Loss: -0.00771142002971222\nTRAIN: \t Epoch: 43 \t Loss: -0.007522810931102588\nTRAIN: \t Epoch: 43 \t Loss: -0.007408406352624297\nTRAIN: \t Epoch: 43 \t Loss: -0.007451458182185888\nTRAIN: \t Epoch: 43 \t Loss: -0.007566601474536583\nTRAIN: \t Epoch: 43 \t Loss: -0.007603537154329174\nTRAIN: \t Epoch: 43 \t Loss: -0.007619162995575203\nTRAIN: \t Epoch: 43 \t Loss: -0.00761798004925405\nVALD: \t Epoch: 43 \t Loss: -0.010474003851413727\nVALD: \t Epoch: 43 \t Loss: -0.009069742634892464\nVALD: \t Epoch: 43 \t Loss: -0.00944300057987372\nVALD: \t Epoch: 43 \t Loss: -0.00948782218620181\nVALD: \t Epoch: 43 \t Loss: -0.00902170386196168\n******************************\nEpoch: social-tag : 43\ntrain_loss -0.00761798004925405\nval_loss -0.00902170386196168\n{'min_val_epoch': 40, 'min_val_loss': -0.009911016294778871}\n******************************\nTRAIN: \t Epoch: 44 \t Loss: -0.008257642388343811\nTRAIN: \t Epoch: 44 \t Loss: -0.007322855293750763\nTRAIN: \t Epoch: 44 \t Loss: -0.006763046452154716\nTRAIN: \t Epoch: 44 \t Loss: -0.007047867984510958\nTRAIN: \t Epoch: 44 \t Loss: -0.007419479358941316\nTRAIN: \t Epoch: 44 \t Loss: -0.0074865862261503935\nTRAIN: \t Epoch: 44 \t Loss: -0.007734637374856642\nTRAIN: \t Epoch: 44 \t Loss: -0.007830536982510239\nTRAIN: \t Epoch: 44 \t Loss: -0.007767578276495139\nTRAIN: \t Epoch: 44 \t Loss: -0.0074902237858623264\nTRAIN: \t Epoch: 44 \t Loss: -0.007414297920397737\nTRAIN: \t Epoch: 44 \t Loss: -0.0074844103849803405\nTRAIN: \t Epoch: 44 \t Loss: -0.0075798282901254985\nTRAIN: \t Epoch: 44 \t Loss: -0.0076084784564695185\nTRAIN: \t Epoch: 44 \t Loss: -0.007607411655286948\nTRAIN: \t Epoch: 44 \t Loss: -0.0076297124032862484\nTRAIN: \t Epoch: 44 \t Loss: -0.0075277832939344294\nTRAIN: \t Epoch: 44 \t Loss: -0.007425861976419886\nTRAIN: \t Epoch: 44 \t Loss: -0.007431316373165469\nVALD: \t Epoch: 44 \t Loss: -0.008714115247130394\nVALD: \t Epoch: 44 \t Loss: -0.00810275785624981\nVALD: \t Epoch: 44 \t Loss: -0.008475688596566519\nVALD: \t Epoch: 44 \t Loss: -0.008530768100172281\nVALD: \t Epoch: 44 \t Loss: -0.008063837713446499\n******************************\nEpoch: social-tag : 44\ntrain_loss -0.007431316373165469\nval_loss -0.008063837713446499\n{'min_val_epoch': 40, 'min_val_loss': -0.009911016294778871}\n******************************\nTRAIN: \t Epoch: 45 \t Loss: -0.007526622153818607\nTRAIN: \t Epoch: 45 \t Loss: -0.008054704871028662\nTRAIN: \t Epoch: 45 \t Loss: -0.008200567526121935\nTRAIN: \t Epoch: 45 \t Loss: -0.008273148443549871\nTRAIN: \t Epoch: 45 \t Loss: -0.008451893739402295\nTRAIN: \t Epoch: 45 \t Loss: -0.008105723963429531\nTRAIN: \t Epoch: 45 \t Loss: -0.007671263468052659\nTRAIN: \t Epoch: 45 \t Loss: -0.0076085294131189585\nTRAIN: \t Epoch: 45 \t Loss: -0.007724677316016621\nTRAIN: \t Epoch: 45 \t Loss: -0.007832603249698877\nTRAIN: \t Epoch: 45 \t Loss: -0.007851462895897303\nTRAIN: \t Epoch: 45 \t Loss: -0.007382697697418432\nTRAIN: \t Epoch: 45 \t Loss: -0.0070511600575768035\nTRAIN: \t Epoch: 45 \t Loss: -0.006967068683089954\nTRAIN: \t Epoch: 45 \t Loss: -0.006947483060260614\nTRAIN: \t Epoch: 45 \t Loss: -0.0070227227988652885\nTRAIN: \t Epoch: 45 \t Loss: -0.0071292572161730594\nTRAIN: \t Epoch: 45 \t Loss: -0.007222469844337966\nTRAIN: \t Epoch: 45 \t Loss: -0.007239467131485721\nVALD: \t Epoch: 45 \t Loss: -0.011169259436428547\nVALD: \t Epoch: 45 \t Loss: -0.009500551968812943\nVALD: \t Epoch: 45 \t Loss: -0.01001207965115706\nVALD: \t Epoch: 45 \t Loss: -0.010019316105172038\nVALD: \t Epoch: 45 \t Loss: -0.009459238801120727\n******************************\nEpoch: social-tag : 45\ntrain_loss -0.007239467131485721\nval_loss -0.009459238801120727\n{'min_val_epoch': 40, 'min_val_loss': -0.009911016294778871}\n******************************\nTRAIN: \t Epoch: 46 \t Loss: -0.008374727331101894\nTRAIN: \t Epoch: 46 \t Loss: -0.008409674279391766\nTRAIN: \t Epoch: 46 \t Loss: -0.00835210271179676\nTRAIN: \t Epoch: 46 \t Loss: -0.008130442001856863\nTRAIN: \t Epoch: 46 \t Loss: -0.008113958034664392\nTRAIN: \t Epoch: 46 \t Loss: -0.008097148112331828\nTRAIN: \t Epoch: 46 \t Loss: -0.008031183454607214\nTRAIN: \t Epoch: 46 \t Loss: -0.008012651465833187\nTRAIN: \t Epoch: 46 \t Loss: -0.00793414143845439\nTRAIN: \t Epoch: 46 \t Loss: -0.00784621681086719\nTRAIN: \t Epoch: 46 \t Loss: -0.007821626622568478\nTRAIN: \t Epoch: 46 \t Loss: -0.007886609450603524\nTRAIN: \t Epoch: 46 \t Loss: -0.00789753569719883\nTRAIN: \t Epoch: 46 \t Loss: -0.007842190231063537\nTRAIN: \t Epoch: 46 \t Loss: -0.007773796965678533\nTRAIN: \t Epoch: 46 \t Loss: -0.0077848146902397275\nTRAIN: \t Epoch: 46 \t Loss: -0.007831640925039263\nTRAIN: \t Epoch: 46 \t Loss: -0.007831510001172623\nTRAIN: \t Epoch: 46 \t Loss: -0.007832982242877477\nVALD: \t Epoch: 46 \t Loss: -0.010294022969901562\nVALD: \t Epoch: 46 \t Loss: -0.009166332427412271\nVALD: \t Epoch: 46 \t Loss: -0.009495338425040245\nVALD: \t Epoch: 46 \t Loss: -0.009498490951955318\nVALD: \t Epoch: 46 \t Loss: -0.009081641110506924\n******************************\nEpoch: social-tag : 46\ntrain_loss -0.007832982242877477\nval_loss -0.009081641110506924\n{'min_val_epoch': 40, 'min_val_loss': -0.009911016294778871}\n******************************\nTRAIN: \t Epoch: 47 \t Loss: -0.00833173468708992\nTRAIN: \t Epoch: 47 \t Loss: -0.008396877907216549\nTRAIN: \t Epoch: 47 \t Loss: -0.008096142827222744\nTRAIN: \t Epoch: 47 \t Loss: -0.0076746343402192\nTRAIN: \t Epoch: 47 \t Loss: -0.007524554245173931\nTRAIN: \t Epoch: 47 \t Loss: -0.007783766835927963\nTRAIN: \t Epoch: 47 \t Loss: -0.008074733694749219\nTRAIN: \t Epoch: 47 \t Loss: -0.00817059155087918\nTRAIN: \t Epoch: 47 \t Loss: -0.008188444500168165\nTRAIN: \t Epoch: 47 \t Loss: -0.008120760880410671\nTRAIN: \t Epoch: 47 \t Loss: -0.008010043550960043\nTRAIN: \t Epoch: 47 \t Loss: -0.007902112401401004\nTRAIN: \t Epoch: 47 \t Loss: -0.007803779525252489\nTRAIN: \t Epoch: 47 \t Loss: -0.007829735188611917\nTRAIN: \t Epoch: 47 \t Loss: -0.007923968074222406\nTRAIN: \t Epoch: 47 \t Loss: -0.007892078720033169\nTRAIN: \t Epoch: 47 \t Loss: -0.007812884718398838\nTRAIN: \t Epoch: 47 \t Loss: -0.007760548239780797\nTRAIN: \t Epoch: 47 \t Loss: -0.007768887407198536\nVALD: \t Epoch: 47 \t Loss: -0.00883776880800724\nVALD: \t Epoch: 47 \t Loss: -0.008346314541995525\nVALD: \t Epoch: 47 \t Loss: -0.008930801724394163\nVALD: \t Epoch: 47 \t Loss: -0.009111876832321286\nVALD: \t Epoch: 47 \t Loss: -0.00857076250817165\n******************************\nEpoch: social-tag : 47\ntrain_loss -0.007768887407198536\nval_loss -0.00857076250817165\n{'min_val_epoch': 40, 'min_val_loss': -0.009911016294778871}\n******************************\nTRAIN: \t Epoch: 48 \t Loss: -0.00901359785348177\nTRAIN: \t Epoch: 48 \t Loss: -0.008817837107926607\nTRAIN: \t Epoch: 48 \t Loss: -0.00875154323875904\nTRAIN: \t Epoch: 48 \t Loss: -0.008251467370428145\nTRAIN: \t Epoch: 48 \t Loss: -0.007252306118607521\nTRAIN: \t Epoch: 48 \t Loss: -0.006991430496176084\nTRAIN: \t Epoch: 48 \t Loss: -0.007149842567741871\nTRAIN: \t Epoch: 48 \t Loss: -0.007269800873473287\nTRAIN: \t Epoch: 48 \t Loss: -0.007406803261902597\nTRAIN: \t Epoch: 48 \t Loss: -0.007473179791122675\nTRAIN: \t Epoch: 48 \t Loss: -0.007568589242344553\nTRAIN: \t Epoch: 48 \t Loss: -0.0076178630503515405\nTRAIN: \t Epoch: 48 \t Loss: -0.007574356555079038\nTRAIN: \t Epoch: 48 \t Loss: -0.007508035383320281\nTRAIN: \t Epoch: 48 \t Loss: -0.007552978613724311\nTRAIN: \t Epoch: 48 \t Loss: -0.007656290632439777\nTRAIN: \t Epoch: 48 \t Loss: -0.0077340724768445775\nTRAIN: \t Epoch: 48 \t Loss: -0.007668774445644683\nTRAIN: \t Epoch: 48 \t Loss: -0.0076426864921738625\nVALD: \t Epoch: 48 \t Loss: -0.007914337329566479\nVALD: \t Epoch: 48 \t Loss: -0.006703187944367528\nVALD: \t Epoch: 48 \t Loss: -0.0069207786582410336\nVALD: \t Epoch: 48 \t Loss: -0.007084235665388405\nVALD: \t Epoch: 48 \t Loss: -0.006622462873616494\n******************************\nEpoch: social-tag : 48\ntrain_loss -0.0076426864921738625\nval_loss -0.006622462873616494\n{'min_val_epoch': 40, 'min_val_loss': -0.009911016294778871}\n******************************\nTRAIN: \t Epoch: 49 \t Loss: -0.007056784816086292\nTRAIN: \t Epoch: 49 \t Loss: -0.007630667183548212\nTRAIN: \t Epoch: 49 \t Loss: -0.0074953425986071425\nTRAIN: \t Epoch: 49 \t Loss: -0.007712810416705906\nTRAIN: \t Epoch: 49 \t Loss: -0.007608436606824398\nTRAIN: \t Epoch: 49 \t Loss: -0.0077352155931293964\nTRAIN: \t Epoch: 49 \t Loss: -0.007783512584865093\nTRAIN: \t Epoch: 49 \t Loss: -0.007764047710224986\nTRAIN: \t Epoch: 49 \t Loss: -0.007859460181660123\nTRAIN: \t Epoch: 49 \t Loss: -0.007901782728731632\nTRAIN: \t Epoch: 49 \t Loss: -0.007935147732496262\nTRAIN: \t Epoch: 49 \t Loss: -0.007885105015399555\nTRAIN: \t Epoch: 49 \t Loss: -0.00788180547981308\nTRAIN: \t Epoch: 49 \t Loss: -0.007901471008413605\nTRAIN: \t Epoch: 49 \t Loss: -0.007977038839211066\nTRAIN: \t Epoch: 49 \t Loss: -0.007980597583809868\nTRAIN: \t Epoch: 49 \t Loss: -0.007905527812373988\nTRAIN: \t Epoch: 49 \t Loss: -0.007832353468984365\nTRAIN: \t Epoch: 49 \t Loss: -0.007836632578153225\nVALD: \t Epoch: 49 \t Loss: -0.009697242639958858\nVALD: \t Epoch: 49 \t Loss: -0.008616551524028182\nVALD: \t Epoch: 49 \t Loss: -0.00897855389242371\nVALD: \t Epoch: 49 \t Loss: -0.008992605493403971\nVALD: \t Epoch: 49 \t Loss: -0.008605888165718268\n******************************\nEpoch: social-tag : 49\ntrain_loss -0.007836632578153225\nval_loss -0.008605888165718268\n{'min_val_epoch': 40, 'min_val_loss': -0.009911016294778871}\n******************************\nTRAIN: \t Epoch: 50 \t Loss: -0.007367750629782677\nTRAIN: \t Epoch: 50 \t Loss: -0.008153995499014854\nTRAIN: \t Epoch: 50 \t Loss: -0.008448297468324503\nTRAIN: \t Epoch: 50 \t Loss: -0.008735480718314648\nTRAIN: \t Epoch: 50 \t Loss: -0.008530774246901273\nTRAIN: \t Epoch: 50 \t Loss: -0.007834688682729999\nTRAIN: \t Epoch: 50 \t Loss: -0.007496194514845099\nTRAIN: \t Epoch: 50 \t Loss: -0.007577185519039631\nTRAIN: \t Epoch: 50 \t Loss: -0.007705427188840177\nTRAIN: \t Epoch: 50 \t Loss: -0.007907758187502622\nTRAIN: \t Epoch: 50 \t Loss: -0.00804779077456756\nTRAIN: \t Epoch: 50 \t Loss: -0.008151063307498893\nTRAIN: \t Epoch: 50 \t Loss: -0.0080468741317208\nTRAIN: \t Epoch: 50 \t Loss: -0.00786754987867815\nTRAIN: \t Epoch: 50 \t Loss: -0.007773976121097803\nTRAIN: \t Epoch: 50 \t Loss: -0.007846175780287012\nTRAIN: \t Epoch: 50 \t Loss: -0.007892759215524969\nTRAIN: \t Epoch: 50 \t Loss: -0.007945401309471991\nTRAIN: \t Epoch: 50 \t Loss: -0.007950682555622935\nVALD: \t Epoch: 50 \t Loss: -0.010982127860188484\nVALD: \t Epoch: 50 \t Loss: -0.00980048906058073\nVALD: \t Epoch: 50 \t Loss: -0.01042644896854957\nVALD: \t Epoch: 50 \t Loss: -0.010634067468345165\nVALD: \t Epoch: 50 \t Loss: -0.009944642476799074\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.75it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.4279430259795322  FDE: 0.6173245056953546\n**************************************************\n******************************\nEpoch: social-tag : 50\ntrain_loss -0.007950682555622935\nval_loss -0.009944642476799074\n{'min_val_epoch': 50, 'min_val_loss': -0.009944642476799074}\n******************************\nTRAIN: \t Epoch: 51 \t Loss: -0.009199930354952812\nTRAIN: \t Epoch: 51 \t Loss: -0.008632427081465721\nTRAIN: \t Epoch: 51 \t Loss: -0.008937788816789785\nTRAIN: \t Epoch: 51 \t Loss: -0.009019970428198576\nTRAIN: \t Epoch: 51 \t Loss: -0.009046808443963527\nTRAIN: \t Epoch: 51 \t Loss: -0.008729542372748256\nTRAIN: \t Epoch: 51 \t Loss: -0.00818422163969704\nTRAIN: \t Epoch: 51 \t Loss: -0.008004941802937537\nTRAIN: \t Epoch: 51 \t Loss: -0.008138322530107366\nTRAIN: \t Epoch: 51 \t Loss: -0.008160800533369183\nTRAIN: \t Epoch: 51 \t Loss: -0.008176522541113874\nTRAIN: \t Epoch: 51 \t Loss: -0.008152022763776282\nTRAIN: \t Epoch: 51 \t Loss: -0.008140489351577483\nTRAIN: \t Epoch: 51 \t Loss: -0.008015230564134461\nTRAIN: \t Epoch: 51 \t Loss: -0.00791747954984506\nTRAIN: \t Epoch: 51 \t Loss: -0.007978744164574891\nTRAIN: \t Epoch: 51 \t Loss: -0.008034890298457706\nTRAIN: \t Epoch: 51 \t Loss: -0.00811688881367445\nTRAIN: \t Epoch: 51 \t Loss: -0.008109685222872571\nVALD: \t Epoch: 51 \t Loss: -0.011332021094858646\nVALD: \t Epoch: 51 \t Loss: -0.010218606796115637\nVALD: \t Epoch: 51 \t Loss: -0.010864180512726307\nVALD: \t Epoch: 51 \t Loss: -0.010971535462886095\nVALD: \t Epoch: 51 \t Loss: -0.010393912437533544\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.77it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\nADE: 0.4607735914099717  FDE: 0.7215658365577455\n**************************************************\n******************************\nEpoch: social-tag : 51\ntrain_loss -0.008109685222872571\nval_loss -0.010393912437533544\n{'min_val_epoch': 51, 'min_val_loss': -0.010393912437533544}\n******************************\nTRAIN: \t Epoch: 52 \t Loss: -0.009191579185426235\nTRAIN: \t Epoch: 52 \t Loss: -0.008652191143482924\nTRAIN: \t Epoch: 52 \t Loss: -0.007806108177949985\nTRAIN: \t Epoch: 52 \t Loss: -0.007825083914212883\nTRAIN: \t Epoch: 52 \t Loss: -0.007916189264506102\nTRAIN: \t Epoch: 52 \t Loss: -0.00800875675243636\nTRAIN: \t Epoch: 52 \t Loss: -0.008125327594046081\nTRAIN: \t Epoch: 52 \t Loss: -0.008225030323956162\nTRAIN: \t Epoch: 52 \t Loss: -0.00814114324748516\nTRAIN: \t Epoch: 52 \t Loss: -0.007967989472672344\nTRAIN: \t Epoch: 52 \t Loss: -0.007988123180852695\nTRAIN: \t Epoch: 52 \t Loss: -0.008084537034543851\nTRAIN: \t Epoch: 52 \t Loss: -0.008125180127815558\nTRAIN: \t Epoch: 52 \t Loss: -0.008184834815827864\nTRAIN: \t Epoch: 52 \t Loss: -0.008163394934187333\nTRAIN: \t Epoch: 52 \t Loss: -0.008019454660825431\nTRAIN: \t Epoch: 52 \t Loss: -0.007949300384258522\nTRAIN: \t Epoch: 52 \t Loss: -0.008015250217997365\nTRAIN: \t Epoch: 52 \t Loss: -0.008012872789308595\nVALD: \t Epoch: 52 \t Loss: -0.011379757896065712\nVALD: \t Epoch: 52 \t Loss: -0.010048840660601854\nVALD: \t Epoch: 52 \t Loss: -0.010703690660496553\nVALD: \t Epoch: 52 \t Loss: -0.010696981102228165\nVALD: \t Epoch: 52 \t Loss: -0.010070151533962282\n******************************\nEpoch: social-tag : 52\ntrain_loss -0.008012872789308595\nval_loss -0.010070151533962282\n{'min_val_epoch': 51, 'min_val_loss': -0.010393912437533544}\n******************************\nTRAIN: \t Epoch: 53 \t Loss: -0.009474198333919048\nTRAIN: \t Epoch: 53 \t Loss: -0.009251900482922792\nTRAIN: \t Epoch: 53 \t Loss: -0.00960103701800108\nTRAIN: \t Epoch: 53 \t Loss: -0.009493623627349734\nTRAIN: \t Epoch: 53 \t Loss: -0.008977394364774228\nTRAIN: \t Epoch: 53 \t Loss: -0.008708563866093755\nTRAIN: \t Epoch: 53 \t Loss: -0.00874233225892697\nTRAIN: \t Epoch: 53 \t Loss: -0.008764362486544997\nTRAIN: \t Epoch: 53 \t Loss: -0.008861211387233602\nTRAIN: \t Epoch: 53 \t Loss: -0.00886968788690865\nTRAIN: \t Epoch: 53 \t Loss: -0.00884280332618139\nTRAIN: \t Epoch: 53 \t Loss: -0.008760392083786428\nTRAIN: \t Epoch: 53 \t Loss: -0.008569317619101359\nTRAIN: \t Epoch: 53 \t Loss: -0.008588733251339622\nTRAIN: \t Epoch: 53 \t Loss: -0.008644388398776452\nTRAIN: \t Epoch: 53 \t Loss: -0.008637085877126083\nTRAIN: \t Epoch: 53 \t Loss: -0.008610962698345674\nTRAIN: \t Epoch: 53 \t Loss: -0.008528016455885436\nTRAIN: \t Epoch: 53 \t Loss: -0.008511548212101294\nVALD: \t Epoch: 53 \t Loss: -0.009191145189106464\nVALD: \t Epoch: 53 \t Loss: -0.007765515940263867\nVALD: \t Epoch: 53 \t Loss: -0.008541676991929611\nVALD: \t Epoch: 53 \t Loss: -0.008637115941382945\nVALD: \t Epoch: 53 \t Loss: -0.008036233029089684\n******************************\nEpoch: social-tag : 53\ntrain_loss -0.008511548212101294\nval_loss -0.008036233029089684\n{'min_val_epoch': 51, 'min_val_loss': -0.010393912437533544}\n******************************\nTRAIN: \t Epoch: 54 \t Loss: -0.00854753702878952\nTRAIN: \t Epoch: 54 \t Loss: -0.008388052694499493\nTRAIN: \t Epoch: 54 \t Loss: -0.008318468617896238\nTRAIN: \t Epoch: 54 \t Loss: -0.007985750562511384\nTRAIN: \t Epoch: 54 \t Loss: -0.008056922350078821\nTRAIN: \t Epoch: 54 \t Loss: -0.008076292850698033\nTRAIN: \t Epoch: 54 \t Loss: -0.008229660202882119\nTRAIN: \t Epoch: 54 \t Loss: -0.008251130057033151\nTRAIN: \t Epoch: 54 \t Loss: -0.008104744884702895\nTRAIN: \t Epoch: 54 \t Loss: -0.007947069266811014\nTRAIN: \t Epoch: 54 \t Loss: -0.00801619171926921\nTRAIN: \t Epoch: 54 \t Loss: -0.008061445667408407\nTRAIN: \t Epoch: 54 \t Loss: -0.008153480011969805\nTRAIN: \t Epoch: 54 \t Loss: -0.008204153239993113\nTRAIN: \t Epoch: 54 \t Loss: -0.008116852988799413\nTRAIN: \t Epoch: 54 \t Loss: -0.008003081631613895\nTRAIN: \t Epoch: 54 \t Loss: -0.008025420747478218\nTRAIN: \t Epoch: 54 \t Loss: -0.008113590250205662\nTRAIN: \t Epoch: 54 \t Loss: -0.008117670166040678\nVALD: \t Epoch: 54 \t Loss: -0.01182958111166954\nVALD: \t Epoch: 54 \t Loss: -0.010529492516070604\nVALD: \t Epoch: 54 \t Loss: -0.011159975081682205\nVALD: \t Epoch: 54 \t Loss: -0.011178246000781655\nVALD: \t Epoch: 54 \t Loss: -0.010694698262805781\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.71it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.419796620831048  FDE: 0.6162759476045224\n**************************************************\n******************************\nEpoch: social-tag : 54\ntrain_loss -0.008117670166040678\nval_loss -0.010694698262805781\n{'min_val_epoch': 54, 'min_val_loss': -0.010694698262805781}\n******************************\nTRAIN: \t Epoch: 55 \t Loss: -0.009234823286533356\nTRAIN: \t Epoch: 55 \t Loss: -0.008166228188201785\nTRAIN: \t Epoch: 55 \t Loss: -0.008005414623767138\nTRAIN: \t Epoch: 55 \t Loss: -0.00807736988645047\nTRAIN: \t Epoch: 55 \t Loss: -0.00822582533583045\nTRAIN: \t Epoch: 55 \t Loss: -0.008065161062404513\nTRAIN: \t Epoch: 55 \t Loss: -0.007980895295206989\nTRAIN: \t Epoch: 55 \t Loss: -0.008137658762279898\nTRAIN: \t Epoch: 55 \t Loss: -0.008284389610505767\nTRAIN: \t Epoch: 55 \t Loss: -0.008261931361630559\nTRAIN: \t Epoch: 55 \t Loss: -0.00826196385208856\nTRAIN: \t Epoch: 55 \t Loss: -0.008314583566971123\nTRAIN: \t Epoch: 55 \t Loss: -0.008321273176429363\nTRAIN: \t Epoch: 55 \t Loss: -0.008301901198657495\nTRAIN: \t Epoch: 55 \t Loss: -0.00828419200455149\nTRAIN: \t Epoch: 55 \t Loss: -0.00831716021639295\nTRAIN: \t Epoch: 55 \t Loss: -0.008371509200728992\nTRAIN: \t Epoch: 55 \t Loss: -0.008318419102579355\nTRAIN: \t Epoch: 55 \t Loss: -0.008307264121465905\nVALD: \t Epoch: 55 \t Loss: -0.008320852182805538\nVALD: \t Epoch: 55 \t Loss: -0.00794891663827002\nVALD: \t Epoch: 55 \t Loss: -0.008563812666883072\nVALD: \t Epoch: 55 \t Loss: -0.008525771903805435\nVALD: \t Epoch: 55 \t Loss: -0.008254862718345706\n******************************\nEpoch: social-tag : 55\ntrain_loss -0.008307264121465905\nval_loss -0.008254862718345706\n{'min_val_epoch': 54, 'min_val_loss': -0.010694698262805781}\n******************************\nTRAIN: \t Epoch: 56 \t Loss: -0.007109759375452995\nTRAIN: \t Epoch: 56 \t Loss: -0.00811024522408843\nTRAIN: \t Epoch: 56 \t Loss: -0.008339377430578073\nTRAIN: \t Epoch: 56 \t Loss: -0.008500501047819853\nTRAIN: \t Epoch: 56 \t Loss: -0.008568417467176914\nTRAIN: \t Epoch: 56 \t Loss: -0.00853566313162446\nTRAIN: \t Epoch: 56 \t Loss: -0.00828792414228831\nTRAIN: \t Epoch: 56 \t Loss: -0.008173711248673499\nTRAIN: \t Epoch: 56 \t Loss: -0.008367093176477484\nTRAIN: \t Epoch: 56 \t Loss: -0.008540121559053659\nTRAIN: \t Epoch: 56 \t Loss: -0.008573485199700703\nTRAIN: \t Epoch: 56 \t Loss: -0.008588544403513273\nTRAIN: \t Epoch: 56 \t Loss: -0.008498763271535818\nTRAIN: \t Epoch: 56 \t Loss: -0.008443249056914024\nTRAIN: \t Epoch: 56 \t Loss: -0.00847958525021871\nTRAIN: \t Epoch: 56 \t Loss: -0.008579128712881356\nTRAIN: \t Epoch: 56 \t Loss: -0.008629697648917927\nTRAIN: \t Epoch: 56 \t Loss: -0.008495927581356632\nTRAIN: \t Epoch: 56 \t Loss: -0.008474402617371155\nVALD: \t Epoch: 56 \t Loss: -0.006960823200643063\nVALD: \t Epoch: 56 \t Loss: -0.006673274328932166\nVALD: \t Epoch: 56 \t Loss: -0.007202314368138711\nVALD: \t Epoch: 56 \t Loss: -0.007076504174619913\nVALD: \t Epoch: 56 \t Loss: -0.00686517618904429\n******************************\nEpoch: social-tag : 56\ntrain_loss -0.008474402617371155\nval_loss -0.00686517618904429\n{'min_val_epoch': 54, 'min_val_loss': -0.010694698262805781}\n******************************\nTRAIN: \t Epoch: 57 \t Loss: -0.0055281613022089005\nTRAIN: \t Epoch: 57 \t Loss: -0.006913608871400356\nTRAIN: \t Epoch: 57 \t Loss: -0.007787854100267093\nTRAIN: \t Epoch: 57 \t Loss: -0.00843219319358468\nTRAIN: \t Epoch: 57 \t Loss: -0.00858028158545494\nTRAIN: \t Epoch: 57 \t Loss: -0.00868072547018528\nTRAIN: \t Epoch: 57 \t Loss: -0.008730853508625711\nTRAIN: \t Epoch: 57 \t Loss: -0.008749259752221406\nTRAIN: \t Epoch: 57 \t Loss: -0.008685356316467127\nTRAIN: \t Epoch: 57 \t Loss: -0.008438562555238605\nTRAIN: \t Epoch: 57 \t Loss: -0.00840948907319795\nTRAIN: \t Epoch: 57 \t Loss: -0.008523783142057558\nTRAIN: \t Epoch: 57 \t Loss: -0.00860586457957442\nTRAIN: \t Epoch: 57 \t Loss: -0.008668870565348439\nTRAIN: \t Epoch: 57 \t Loss: -0.008643052695939938\nTRAIN: \t Epoch: 57 \t Loss: -0.008473735389998183\nTRAIN: \t Epoch: 57 \t Loss: -0.008383749765070045\nTRAIN: \t Epoch: 57 \t Loss: -0.008469096229722103\nTRAIN: \t Epoch: 57 \t Loss: -0.008470017921811646\nVALD: \t Epoch: 57 \t Loss: -0.012124108150601387\nVALD: \t Epoch: 57 \t Loss: -0.010607382748275995\nVALD: \t Epoch: 57 \t Loss: -0.011242669696609179\nVALD: \t Epoch: 57 \t Loss: -0.011263360269367695\nVALD: \t Epoch: 57 \t Loss: -0.010722912638640602\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\nADE: 0.400935567223137  FDE: 0.5602834322757487\n**************************************************\n******************************\nEpoch: social-tag : 57\ntrain_loss -0.008470017921811646\nval_loss -0.010722912638640602\n{'min_val_epoch': 57, 'min_val_loss': -0.010722912638640602}\n******************************\nTRAIN: \t Epoch: 58 \t Loss: -0.008350498043000698\nTRAIN: \t Epoch: 58 \t Loss: -0.008853699546307325\nTRAIN: \t Epoch: 58 \t Loss: -0.009165399086972078\nTRAIN: \t Epoch: 58 \t Loss: -0.008995009819045663\nTRAIN: \t Epoch: 58 \t Loss: -0.008459614496678114\nTRAIN: \t Epoch: 58 \t Loss: -0.008249260718002915\nTRAIN: \t Epoch: 58 \t Loss: -0.00848065669249211\nTRAIN: \t Epoch: 58 \t Loss: -0.008649149851407856\nTRAIN: \t Epoch: 58 \t Loss: -0.008678735492544042\nTRAIN: \t Epoch: 58 \t Loss: -0.00856966241262853\nTRAIN: \t Epoch: 58 \t Loss: -0.00852324165911837\nTRAIN: \t Epoch: 58 \t Loss: -0.008530150827330848\nTRAIN: \t Epoch: 58 \t Loss: -0.008608210939340867\nTRAIN: \t Epoch: 58 \t Loss: -0.008673268603160977\nTRAIN: \t Epoch: 58 \t Loss: -0.008564187865704297\nTRAIN: \t Epoch: 58 \t Loss: -0.008410089678363875\nTRAIN: \t Epoch: 58 \t Loss: -0.008395872873199336\nTRAIN: \t Epoch: 58 \t Loss: -0.008432482602074742\nTRAIN: \t Epoch: 58 \t Loss: -0.00844904473637015\nVALD: \t Epoch: 58 \t Loss: -0.011580731719732285\nVALD: \t Epoch: 58 \t Loss: -0.010323184076696634\nVALD: \t Epoch: 58 \t Loss: -0.010962214941779772\nVALD: \t Epoch: 58 \t Loss: -0.01094900188036263\nVALD: \t Epoch: 58 \t Loss: -0.010363350229815018\n******************************\nEpoch: social-tag : 58\ntrain_loss -0.00844904473637015\nval_loss -0.010363350229815018\n{'min_val_epoch': 57, 'min_val_loss': -0.010722912638640602}\n******************************\nTRAIN: \t Epoch: 59 \t Loss: -0.010152710601687431\nTRAIN: \t Epoch: 59 \t Loss: -0.01003223191946745\nTRAIN: \t Epoch: 59 \t Loss: -0.010012665142615637\nTRAIN: \t Epoch: 59 \t Loss: -0.009750583674758673\nTRAIN: \t Epoch: 59 \t Loss: -0.009224145393818616\nTRAIN: \t Epoch: 59 \t Loss: -0.009042551508173347\nTRAIN: \t Epoch: 59 \t Loss: -0.008980974887630768\nTRAIN: \t Epoch: 59 \t Loss: -0.008966545283328742\nTRAIN: \t Epoch: 59 \t Loss: -0.008943072571936581\nTRAIN: \t Epoch: 59 \t Loss: -0.008945393888279796\nTRAIN: \t Epoch: 59 \t Loss: -0.008848080707883293\nTRAIN: \t Epoch: 59 \t Loss: -0.008798992144875228\nTRAIN: \t Epoch: 59 \t Loss: -0.008795862193577565\nTRAIN: \t Epoch: 59 \t Loss: -0.008826087661353605\nTRAIN: \t Epoch: 59 \t Loss: -0.008875953623404105\nTRAIN: \t Epoch: 59 \t Loss: -0.008826801524264738\nTRAIN: \t Epoch: 59 \t Loss: -0.008728131487527314\nTRAIN: \t Epoch: 59 \t Loss: -0.0086995928755237\nTRAIN: \t Epoch: 59 \t Loss: -0.00867443621915851\nVALD: \t Epoch: 59 \t Loss: -0.011126342229545116\nVALD: \t Epoch: 59 \t Loss: -0.009804525412619114\nVALD: \t Epoch: 59 \t Loss: -0.01038306268552939\nVALD: \t Epoch: 59 \t Loss: -0.010318436892703176\nVALD: \t Epoch: 59 \t Loss: -0.009807175053052666\n******************************\nEpoch: social-tag : 59\ntrain_loss -0.00867443621915851\nval_loss -0.009807175053052666\n{'min_val_epoch': 57, 'min_val_loss': -0.010722912638640602}\n******************************\nTRAIN: \t Epoch: 60 \t Loss: -0.009072510525584221\nTRAIN: \t Epoch: 60 \t Loss: -0.008899287786334753\nTRAIN: \t Epoch: 60 \t Loss: -0.009107162555058798\nTRAIN: \t Epoch: 60 \t Loss: -0.009069062070921063\nTRAIN: \t Epoch: 60 \t Loss: -0.008472329750657082\nTRAIN: \t Epoch: 60 \t Loss: -0.008179174270480871\nTRAIN: \t Epoch: 60 \t Loss: -0.008279026485979557\nTRAIN: \t Epoch: 60 \t Loss: -0.008547069854103029\nTRAIN: \t Epoch: 60 \t Loss: -0.008685116552644305\nTRAIN: \t Epoch: 60 \t Loss: -0.008597321528941392\nTRAIN: \t Epoch: 60 \t Loss: -0.00851877990432761\nTRAIN: \t Epoch: 60 \t Loss: -0.008422232310598096\nTRAIN: \t Epoch: 60 \t Loss: -0.008488278836011887\nTRAIN: \t Epoch: 60 \t Loss: -0.008597357198596\nTRAIN: \t Epoch: 60 \t Loss: -0.008580392971634866\nTRAIN: \t Epoch: 60 \t Loss: -0.008544837241061032\nTRAIN: \t Epoch: 60 \t Loss: -0.008468619328649604\nTRAIN: \t Epoch: 60 \t Loss: -0.008540113560027547\nTRAIN: \t Epoch: 60 \t Loss: -0.008524682497947408\nVALD: \t Epoch: 60 \t Loss: -0.011379582807421684\nVALD: \t Epoch: 60 \t Loss: -0.010022386442869902\nVALD: \t Epoch: 60 \t Loss: -0.010902667418122292\nVALD: \t Epoch: 60 \t Loss: -0.011018133955076337\nVALD: \t Epoch: 60 \t Loss: -0.010251790235850437\n******************************\nEpoch: social-tag : 60\ntrain_loss -0.008524682497947408\nval_loss -0.010251790235850437\n{'min_val_epoch': 57, 'min_val_loss': -0.010722912638640602}\n******************************\nTRAIN: \t Epoch: 61 \t Loss: -0.009549492970108986\nTRAIN: \t Epoch: 61 \t Loss: -0.00939654465764761\nTRAIN: \t Epoch: 61 \t Loss: -0.009416372515261173\nTRAIN: \t Epoch: 61 \t Loss: -0.008865540148690343\nTRAIN: \t Epoch: 61 \t Loss: -0.008588922396302223\nTRAIN: \t Epoch: 61 \t Loss: -0.008592460149278244\nTRAIN: \t Epoch: 61 \t Loss: -0.00876917690038681\nTRAIN: \t Epoch: 61 \t Loss: -0.008908654446713626\nTRAIN: \t Epoch: 61 \t Loss: -0.008962093128098382\nTRAIN: \t Epoch: 61 \t Loss: -0.008858262095600367\nTRAIN: \t Epoch: 61 \t Loss: -0.008623339574445377\nTRAIN: \t Epoch: 61 \t Loss: -0.00856229243800044\nTRAIN: \t Epoch: 61 \t Loss: -0.008704997527484711\nTRAIN: \t Epoch: 61 \t Loss: -0.008901036544037717\nTRAIN: \t Epoch: 61 \t Loss: -0.008913447583715121\nTRAIN: \t Epoch: 61 \t Loss: -0.008781311014899984\nTRAIN: \t Epoch: 61 \t Loss: -0.008705238784279893\nTRAIN: \t Epoch: 61 \t Loss: -0.008732166218881806\nTRAIN: \t Epoch: 61 \t Loss: -0.008719301565159609\nVALD: \t Epoch: 61 \t Loss: -0.011453226208686829\nVALD: \t Epoch: 61 \t Loss: -0.010193243622779846\nVALD: \t Epoch: 61 \t Loss: -0.010839104652404785\nVALD: \t Epoch: 61 \t Loss: -0.010728807654231787\nVALD: \t Epoch: 61 \t Loss: -0.010170002220090756\n******************************\nEpoch: social-tag : 61\ntrain_loss -0.008719301565159609\nval_loss -0.010170002220090756\n{'min_val_epoch': 57, 'min_val_loss': -0.010722912638640602}\n******************************\nTRAIN: \t Epoch: 62 \t Loss: -0.009466452524065971\nTRAIN: \t Epoch: 62 \t Loss: -0.009351438377052546\nTRAIN: \t Epoch: 62 \t Loss: -0.009013453498482704\nTRAIN: \t Epoch: 62 \t Loss: -0.008411173359490931\nTRAIN: \t Epoch: 62 \t Loss: -0.008332462701946496\nTRAIN: \t Epoch: 62 \t Loss: -0.008507611462846398\nTRAIN: \t Epoch: 62 \t Loss: -0.008763826784810849\nTRAIN: \t Epoch: 62 \t Loss: -0.008828352962154895\nTRAIN: \t Epoch: 62 \t Loss: -0.008781120853705538\nTRAIN: \t Epoch: 62 \t Loss: -0.008598624635487795\nTRAIN: \t Epoch: 62 \t Loss: -0.008600851266898892\nTRAIN: \t Epoch: 62 \t Loss: -0.008679147887354096\nTRAIN: \t Epoch: 62 \t Loss: -0.008748721689558946\nTRAIN: \t Epoch: 62 \t Loss: -0.008821378062878336\nTRAIN: \t Epoch: 62 \t Loss: -0.008863591278592745\nTRAIN: \t Epoch: 62 \t Loss: -0.008815414912533015\nTRAIN: \t Epoch: 62 \t Loss: -0.008687540560084231\nTRAIN: \t Epoch: 62 \t Loss: -0.008692869088715978\nTRAIN: \t Epoch: 62 \t Loss: -0.008705925944034238\nVALD: \t Epoch: 62 \t Loss: -0.01174276415258646\nVALD: \t Epoch: 62 \t Loss: -0.010148482862859964\nVALD: \t Epoch: 62 \t Loss: -0.010563179850578308\nVALD: \t Epoch: 62 \t Loss: -0.010348897892981768\nVALD: \t Epoch: 62 \t Loss: -0.009837318057856283\n******************************\nEpoch: social-tag : 62\ntrain_loss -0.008705925944034238\nval_loss -0.009837318057856283\n{'min_val_epoch': 57, 'min_val_loss': -0.010722912638640602}\n******************************\nTRAIN: \t Epoch: 63 \t Loss: -0.010209992527961731\nTRAIN: \t Epoch: 63 \t Loss: -0.009806206915527582\nTRAIN: \t Epoch: 63 \t Loss: -0.009845606361826261\nTRAIN: \t Epoch: 63 \t Loss: -0.009765672730281949\nTRAIN: \t Epoch: 63 \t Loss: -0.009848987311124801\nTRAIN: \t Epoch: 63 \t Loss: -0.009649695983777443\nTRAIN: \t Epoch: 63 \t Loss: -0.009286276863089629\nTRAIN: \t Epoch: 63 \t Loss: -0.009039722383022308\nTRAIN: \t Epoch: 63 \t Loss: -0.009124805633392599\nTRAIN: \t Epoch: 63 \t Loss: -0.009282297547906637\nTRAIN: \t Epoch: 63 \t Loss: -0.009332652119073\nTRAIN: \t Epoch: 63 \t Loss: -0.009323786478489637\nTRAIN: \t Epoch: 63 \t Loss: -0.009171365258785395\nTRAIN: \t Epoch: 63 \t Loss: -0.009129467087664775\nTRAIN: \t Epoch: 63 \t Loss: -0.009174056351184845\nTRAIN: \t Epoch: 63 \t Loss: -0.009173995873425156\nTRAIN: \t Epoch: 63 \t Loss: -0.009161615470314728\nTRAIN: \t Epoch: 63 \t Loss: -0.00917022049220072\nTRAIN: \t Epoch: 63 \t Loss: -0.009151923804705149\nVALD: \t Epoch: 63 \t Loss: -0.011055823415517807\nVALD: \t Epoch: 63 \t Loss: -0.009901431389153004\nVALD: \t Epoch: 63 \t Loss: -0.010648904368281364\nVALD: \t Epoch: 63 \t Loss: -0.010660738684237003\nVALD: \t Epoch: 63 \t Loss: -0.01021354356087929\n******************************\nEpoch: social-tag : 63\ntrain_loss -0.009151923804705149\nval_loss -0.01021354356087929\n{'min_val_epoch': 57, 'min_val_loss': -0.010722912638640602}\n******************************\nTRAIN: \t Epoch: 64 \t Loss: -0.0095906937494874\nTRAIN: \t Epoch: 64 \t Loss: -0.009535051882266998\nTRAIN: \t Epoch: 64 \t Loss: -0.009405688072244326\nTRAIN: \t Epoch: 64 \t Loss: -0.0093163235578686\nTRAIN: \t Epoch: 64 \t Loss: -0.009166658110916614\nTRAIN: \t Epoch: 64 \t Loss: -0.009269976833214363\nTRAIN: \t Epoch: 64 \t Loss: -0.009221081089760576\nTRAIN: \t Epoch: 64 \t Loss: -0.009158973582088947\nTRAIN: \t Epoch: 64 \t Loss: -0.009132335065967508\nTRAIN: \t Epoch: 64 \t Loss: -0.009119474794715643\nTRAIN: \t Epoch: 64 \t Loss: -0.009050165641714226\nTRAIN: \t Epoch: 64 \t Loss: -0.00909909939703842\nTRAIN: \t Epoch: 64 \t Loss: -0.009103147814480158\nTRAIN: \t Epoch: 64 \t Loss: -0.008972385332786612\nTRAIN: \t Epoch: 64 \t Loss: -0.008806237702568371\nTRAIN: \t Epoch: 64 \t Loss: -0.008800057228654623\nTRAIN: \t Epoch: 64 \t Loss: -0.008959873141173054\nTRAIN: \t Epoch: 64 \t Loss: -0.008993558844344484\nTRAIN: \t Epoch: 64 \t Loss: -0.008994082295360696\nVALD: \t Epoch: 64 \t Loss: -0.01109803281724453\nVALD: \t Epoch: 64 \t Loss: -0.009615841787308455\nVALD: \t Epoch: 64 \t Loss: -0.01039252212891976\nVALD: \t Epoch: 64 \t Loss: -0.010545625118538737\nVALD: \t Epoch: 64 \t Loss: -0.009946368745535858\n******************************\nEpoch: social-tag : 64\ntrain_loss -0.008994082295360696\nval_loss -0.009946368745535858\n{'min_val_epoch': 57, 'min_val_loss': -0.010722912638640602}\n******************************\nTRAIN: \t Epoch: 65 \t Loss: -0.010221006348729134\nTRAIN: \t Epoch: 65 \t Loss: -0.010200594551861286\nTRAIN: \t Epoch: 65 \t Loss: -0.01003385583559672\nTRAIN: \t Epoch: 65 \t Loss: -0.009374527144245803\nTRAIN: \t Epoch: 65 \t Loss: -0.008958759997040034\nTRAIN: \t Epoch: 65 \t Loss: -0.008950820891186595\nTRAIN: \t Epoch: 65 \t Loss: -0.00911649629207594\nTRAIN: \t Epoch: 65 \t Loss: -0.009324918442871422\nTRAIN: \t Epoch: 65 \t Loss: -0.009469646805276474\nTRAIN: \t Epoch: 65 \t Loss: -0.009030208922922611\nTRAIN: \t Epoch: 65 \t Loss: -0.008593014695427635\nTRAIN: \t Epoch: 65 \t Loss: -0.008432439722431203\nTRAIN: \t Epoch: 65 \t Loss: -0.008471502098613061\nTRAIN: \t Epoch: 65 \t Loss: -0.00853429757989943\nTRAIN: \t Epoch: 65 \t Loss: -0.00864780629053712\nTRAIN: \t Epoch: 65 \t Loss: -0.008777121576713398\nTRAIN: \t Epoch: 65 \t Loss: -0.008811791893094778\nTRAIN: \t Epoch: 65 \t Loss: -0.008854439404482642\nTRAIN: \t Epoch: 65 \t Loss: -0.008850071537504927\nVALD: \t Epoch: 65 \t Loss: -0.010422772727906704\nVALD: \t Epoch: 65 \t Loss: -0.009408209007233381\nVALD: \t Epoch: 65 \t Loss: -0.01028535421937704\nVALD: \t Epoch: 65 \t Loss: -0.010292928200215101\nVALD: \t Epoch: 65 \t Loss: -0.009819189379037904\n******************************\nEpoch: social-tag : 65\ntrain_loss -0.008850071537504927\nval_loss -0.009819189379037904\n{'min_val_epoch': 57, 'min_val_loss': -0.010722912638640602}\n******************************\nTRAIN: \t Epoch: 66 \t Loss: -0.009270023554563522\nTRAIN: \t Epoch: 66 \t Loss: -0.009135723114013672\nTRAIN: \t Epoch: 66 \t Loss: -0.008546810752401749\nTRAIN: \t Epoch: 66 \t Loss: -0.0088369658915326\nTRAIN: \t Epoch: 66 \t Loss: -0.008868312928825617\nTRAIN: \t Epoch: 66 \t Loss: -0.008842547812188664\nTRAIN: \t Epoch: 66 \t Loss: -0.008926098168428456\nTRAIN: \t Epoch: 66 \t Loss: -0.008947553753387183\nTRAIN: \t Epoch: 66 \t Loss: -0.008923144855846962\nTRAIN: \t Epoch: 66 \t Loss: -0.008853220799937844\nTRAIN: \t Epoch: 66 \t Loss: -0.008877131283621897\nTRAIN: \t Epoch: 66 \t Loss: -0.008933699030118683\nTRAIN: \t Epoch: 66 \t Loss: -0.008948394348128485\nTRAIN: \t Epoch: 66 \t Loss: -0.009004922616960747\nTRAIN: \t Epoch: 66 \t Loss: -0.009054373484104872\nTRAIN: \t Epoch: 66 \t Loss: -0.009129397483775392\nTRAIN: \t Epoch: 66 \t Loss: -0.009144262766794246\nTRAIN: \t Epoch: 66 \t Loss: -0.009066278140784966\nTRAIN: \t Epoch: 66 \t Loss: -0.009070245531434956\nVALD: \t Epoch: 66 \t Loss: -0.009844991378486156\nVALD: \t Epoch: 66 \t Loss: -0.009074619505554438\nVALD: \t Epoch: 66 \t Loss: -0.009966688541074594\nVALD: \t Epoch: 66 \t Loss: -0.010025929659605026\nVALD: \t Epoch: 66 \t Loss: -0.009543807841529531\n******************************\nEpoch: social-tag : 66\ntrain_loss -0.009070245531434956\nval_loss -0.009543807841529531\n{'min_val_epoch': 57, 'min_val_loss': -0.010722912638640602}\n******************************\nTRAIN: \t Epoch: 67 \t Loss: -0.008579409681260586\nTRAIN: \t Epoch: 67 \t Loss: -0.009055925533175468\nTRAIN: \t Epoch: 67 \t Loss: -0.00952447143693765\nTRAIN: \t Epoch: 67 \t Loss: -0.009432217804715037\nTRAIN: \t Epoch: 67 \t Loss: -0.008993894141167403\nTRAIN: \t Epoch: 67 \t Loss: -0.008882768064116439\nTRAIN: \t Epoch: 67 \t Loss: -0.009031211624720268\nTRAIN: \t Epoch: 67 \t Loss: -0.009048624022398144\nTRAIN: \t Epoch: 67 \t Loss: -0.009175041017846929\nTRAIN: \t Epoch: 67 \t Loss: -0.009157850267365574\nTRAIN: \t Epoch: 67 \t Loss: -0.008991961663758213\nTRAIN: \t Epoch: 67 \t Loss: -0.008860642090439796\nTRAIN: \t Epoch: 67 \t Loss: -0.008843955368949817\nTRAIN: \t Epoch: 67 \t Loss: -0.008879173280937331\nTRAIN: \t Epoch: 67 \t Loss: -0.008974737736086051\nTRAIN: \t Epoch: 67 \t Loss: -0.009051420784089714\nTRAIN: \t Epoch: 67 \t Loss: -0.009070499690578264\nTRAIN: \t Epoch: 67 \t Loss: -0.009111266221023269\nTRAIN: \t Epoch: 67 \t Loss: -0.009104637218948892\nVALD: \t Epoch: 67 \t Loss: -0.011033399030566216\nVALD: \t Epoch: 67 \t Loss: -0.00935263279825449\nVALD: \t Epoch: 67 \t Loss: -0.010276993736624718\nVALD: \t Epoch: 67 \t Loss: -0.010425349231809378\nVALD: \t Epoch: 67 \t Loss: -0.009876176739527174\n******************************\nEpoch: social-tag : 67\ntrain_loss -0.009104637218948892\nval_loss -0.009876176739527174\n{'min_val_epoch': 57, 'min_val_loss': -0.010722912638640602}\n******************************\nTRAIN: \t Epoch: 68 \t Loss: -0.00877082534134388\nTRAIN: \t Epoch: 68 \t Loss: -0.009052866604179144\nTRAIN: \t Epoch: 68 \t Loss: -0.008721877199908098\nTRAIN: \t Epoch: 68 \t Loss: -0.008948941249400377\nTRAIN: \t Epoch: 68 \t Loss: -0.009182172641158105\nTRAIN: \t Epoch: 68 \t Loss: -0.008889162990575036\nTRAIN: \t Epoch: 68 \t Loss: -0.008558574132621288\nTRAIN: \t Epoch: 68 \t Loss: -0.008611823548562825\nTRAIN: \t Epoch: 68 \t Loss: -0.008716515368885465\nTRAIN: \t Epoch: 68 \t Loss: -0.008838606160134077\nTRAIN: \t Epoch: 68 \t Loss: -0.008855928582223978\nTRAIN: \t Epoch: 68 \t Loss: -0.008891505965342125\nTRAIN: \t Epoch: 68 \t Loss: -0.008840959089306684\nTRAIN: \t Epoch: 68 \t Loss: -0.008782688328730208\nTRAIN: \t Epoch: 68 \t Loss: -0.008827914856374264\nTRAIN: \t Epoch: 68 \t Loss: -0.00895667338045314\nTRAIN: \t Epoch: 68 \t Loss: -0.009048813077456811\nTRAIN: \t Epoch: 68 \t Loss: -0.009038906854887804\nTRAIN: \t Epoch: 68 \t Loss: -0.009005848125816932\nVALD: \t Epoch: 68 \t Loss: -0.009943789802491665\nVALD: \t Epoch: 68 \t Loss: -0.00848409510217607\nVALD: \t Epoch: 68 \t Loss: -0.009067786702265343\nVALD: \t Epoch: 68 \t Loss: -0.009205490699969232\nVALD: \t Epoch: 68 \t Loss: -0.008706677551111899\n******************************\nEpoch: social-tag : 68\ntrain_loss -0.009005848125816932\nval_loss -0.008706677551111899\n{'min_val_epoch': 57, 'min_val_loss': -0.010722912638640602}\n******************************\nTRAIN: \t Epoch: 69 \t Loss: -0.008219626732170582\nTRAIN: \t Epoch: 69 \t Loss: -0.009190568700432777\nTRAIN: \t Epoch: 69 \t Loss: -0.009405481939514479\nTRAIN: \t Epoch: 69 \t Loss: -0.009647228755056858\nTRAIN: \t Epoch: 69 \t Loss: -0.009924099780619144\nTRAIN: \t Epoch: 69 \t Loss: -0.009627507999539375\nTRAIN: \t Epoch: 69 \t Loss: -0.009445381882999624\nTRAIN: \t Epoch: 69 \t Loss: -0.009496932732872665\nTRAIN: \t Epoch: 69 \t Loss: -0.009639178816643026\nTRAIN: \t Epoch: 69 \t Loss: -0.009577389154583216\nTRAIN: \t Epoch: 69 \t Loss: -0.009469936009157787\nTRAIN: \t Epoch: 69 \t Loss: -0.009329230020133158\nTRAIN: \t Epoch: 69 \t Loss: -0.009394785210203666\nTRAIN: \t Epoch: 69 \t Loss: -0.00945951180931713\nTRAIN: \t Epoch: 69 \t Loss: -0.0094809351178507\nTRAIN: \t Epoch: 69 \t Loss: -0.009444417111808434\nTRAIN: \t Epoch: 69 \t Loss: -0.00938607650973341\nTRAIN: \t Epoch: 69 \t Loss: -0.009429690298727818\nTRAIN: \t Epoch: 69 \t Loss: -0.009451959497142105\nVALD: \t Epoch: 69 \t Loss: -0.012115247547626495\nVALD: \t Epoch: 69 \t Loss: -0.01043873792514205\nVALD: \t Epoch: 69 \t Loss: -0.01126719731837511\nVALD: \t Epoch: 69 \t Loss: -0.01133757270872593\nVALD: \t Epoch: 69 \t Loss: -0.010654228777924844\n******************************\nEpoch: social-tag : 69\ntrain_loss -0.009451959497142105\nval_loss -0.010654228777924844\n{'min_val_epoch': 57, 'min_val_loss': -0.010722912638640602}\n******************************\nTRAIN: \t Epoch: 70 \t Loss: -0.009868338704109192\nTRAIN: \t Epoch: 70 \t Loss: -0.010013082064688206\nTRAIN: \t Epoch: 70 \t Loss: -0.009874493504563967\nTRAIN: \t Epoch: 70 \t Loss: -0.008951059076935053\nTRAIN: \t Epoch: 70 \t Loss: -0.008631524816155433\nTRAIN: \t Epoch: 70 \t Loss: -0.008689740672707558\nTRAIN: \t Epoch: 70 \t Loss: -0.008890544197389058\nTRAIN: \t Epoch: 70 \t Loss: -0.00908227835316211\nTRAIN: \t Epoch: 70 \t Loss: -0.009201007584730784\nTRAIN: \t Epoch: 70 \t Loss: -0.008992349915206432\nTRAIN: \t Epoch: 70 \t Loss: -0.008812945505434816\nTRAIN: \t Epoch: 70 \t Loss: -0.008885777012134591\nTRAIN: \t Epoch: 70 \t Loss: -0.009000111514559159\nTRAIN: \t Epoch: 70 \t Loss: -0.009098282349961144\nTRAIN: \t Epoch: 70 \t Loss: -0.009170113131403923\nTRAIN: \t Epoch: 70 \t Loss: -0.009211395110469311\nTRAIN: \t Epoch: 70 \t Loss: -0.009193839395747465\nTRAIN: \t Epoch: 70 \t Loss: -0.009098418522626162\nTRAIN: \t Epoch: 70 \t Loss: -0.009094106564482772\nVALD: \t Epoch: 70 \t Loss: -0.009951804764568806\nVALD: \t Epoch: 70 \t Loss: -0.008264526724815369\nVALD: \t Epoch: 70 \t Loss: -0.009014709542195002\nVALD: \t Epoch: 70 \t Loss: -0.009075428126379848\nVALD: \t Epoch: 70 \t Loss: -0.008586185136117227\n******************************\nEpoch: social-tag : 70\ntrain_loss -0.009094106564482772\nval_loss -0.008586185136117227\n{'min_val_epoch': 57, 'min_val_loss': -0.010722912638640602}\n******************************\nTRAIN: \t Epoch: 71 \t Loss: -0.008522003889083862\nTRAIN: \t Epoch: 71 \t Loss: -0.009338165633380413\nTRAIN: \t Epoch: 71 \t Loss: -0.009906205038229624\nTRAIN: \t Epoch: 71 \t Loss: -0.00990485050715506\nTRAIN: \t Epoch: 71 \t Loss: -0.009657105803489685\nTRAIN: \t Epoch: 71 \t Loss: -0.00941925977046291\nTRAIN: \t Epoch: 71 \t Loss: -0.009368964337876864\nTRAIN: \t Epoch: 71 \t Loss: -0.009496321901679039\nTRAIN: \t Epoch: 71 \t Loss: -0.009589221018056074\nTRAIN: \t Epoch: 71 \t Loss: -0.009480239078402519\nTRAIN: \t Epoch: 71 \t Loss: -0.009279268624430353\nTRAIN: \t Epoch: 71 \t Loss: -0.0092660883286347\nTRAIN: \t Epoch: 71 \t Loss: -0.009388622756187733\nTRAIN: \t Epoch: 71 \t Loss: -0.009502740909478493\nTRAIN: \t Epoch: 71 \t Loss: -0.009534810855984687\nTRAIN: \t Epoch: 71 \t Loss: -0.009507070702966303\nTRAIN: \t Epoch: 71 \t Loss: -0.009361871271668112\nTRAIN: \t Epoch: 71 \t Loss: -0.0093689924882104\nTRAIN: \t Epoch: 71 \t Loss: -0.009377062051179181\nVALD: \t Epoch: 71 \t Loss: -0.012126531451940536\nVALD: \t Epoch: 71 \t Loss: -0.010079022496938705\nVALD: \t Epoch: 71 \t Loss: -0.011312698945403099\nVALD: \t Epoch: 71 \t Loss: -0.01147802290506661\nVALD: \t Epoch: 71 \t Loss: -0.010849787381069719\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.76it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.3976159051211185  FDE: 0.5482151815170856\n**************************************************\n******************************\nEpoch: social-tag : 71\ntrain_loss -0.009377062051179181\nval_loss -0.010849787381069719\n{'min_val_epoch': 71, 'min_val_loss': -0.010849787381069719}\n******************************\nTRAIN: \t Epoch: 72 \t Loss: -0.011269232258200645\nTRAIN: \t Epoch: 72 \t Loss: -0.011079257354140282\nTRAIN: \t Epoch: 72 \t Loss: -0.010636031938095888\nTRAIN: \t Epoch: 72 \t Loss: -0.010219704592600465\nTRAIN: \t Epoch: 72 \t Loss: -0.00994659960269928\nTRAIN: \t Epoch: 72 \t Loss: -0.009890224939833084\nTRAIN: \t Epoch: 72 \t Loss: -0.00972423077161823\nTRAIN: \t Epoch: 72 \t Loss: -0.009760445798747241\nTRAIN: \t Epoch: 72 \t Loss: -0.009788238029513095\nTRAIN: \t Epoch: 72 \t Loss: -0.009670040477067232\nTRAIN: \t Epoch: 72 \t Loss: -0.009546618569980968\nTRAIN: \t Epoch: 72 \t Loss: -0.009559851527834931\nTRAIN: \t Epoch: 72 \t Loss: -0.009531255381611677\nTRAIN: \t Epoch: 72 \t Loss: -0.009543565168444599\nTRAIN: \t Epoch: 72 \t Loss: -0.009610118779043357\nTRAIN: \t Epoch: 72 \t Loss: -0.009654662746470422\nTRAIN: \t Epoch: 72 \t Loss: -0.00956793556756833\nTRAIN: \t Epoch: 72 \t Loss: -0.009547368157655\nTRAIN: \t Epoch: 72 \t Loss: -0.0095548837555574\nVALD: \t Epoch: 72 \t Loss: -0.011180764995515347\nVALD: \t Epoch: 72 \t Loss: -0.009675415698438883\nVALD: \t Epoch: 72 \t Loss: -0.01087008323520422\nVALD: \t Epoch: 72 \t Loss: -0.011037542251870036\nVALD: \t Epoch: 72 \t Loss: -0.010331074758009477\n******************************\nEpoch: social-tag : 72\ntrain_loss -0.0095548837555574\nval_loss -0.010331074758009477\n{'min_val_epoch': 71, 'min_val_loss': -0.010849787381069719}\n******************************\nTRAIN: \t Epoch: 73 \t Loss: -0.010493617504835129\nTRAIN: \t Epoch: 73 \t Loss: -0.010494915302842855\nTRAIN: \t Epoch: 73 \t Loss: -0.01015060736487309\nTRAIN: \t Epoch: 73 \t Loss: -0.009710930055007339\nTRAIN: \t Epoch: 73 \t Loss: -0.00927271107211709\nTRAIN: \t Epoch: 73 \t Loss: -0.00924359611235559\nTRAIN: \t Epoch: 73 \t Loss: -0.009390301004584347\nTRAIN: \t Epoch: 73 \t Loss: -0.009506417496595532\nTRAIN: \t Epoch: 73 \t Loss: -0.00947915395307872\nTRAIN: \t Epoch: 73 \t Loss: -0.009406108176335692\nTRAIN: \t Epoch: 73 \t Loss: -0.009277590338818052\nTRAIN: \t Epoch: 73 \t Loss: -0.00937952206004411\nTRAIN: \t Epoch: 73 \t Loss: -0.009389739017933607\nTRAIN: \t Epoch: 73 \t Loss: -0.00948054386701967\nTRAIN: \t Epoch: 73 \t Loss: -0.009548639971762895\nTRAIN: \t Epoch: 73 \t Loss: -0.00946519334684126\nTRAIN: \t Epoch: 73 \t Loss: -0.009376140557886922\nTRAIN: \t Epoch: 73 \t Loss: -0.009388261190098193\nTRAIN: \t Epoch: 73 \t Loss: -0.00939677982983355\nVALD: \t Epoch: 73 \t Loss: -0.012790895998477936\nVALD: \t Epoch: 73 \t Loss: -0.011030398309230804\nVALD: \t Epoch: 73 \t Loss: -0.011954698711633682\nVALD: \t Epoch: 73 \t Loss: -0.011942873243242502\nVALD: \t Epoch: 73 \t Loss: -0.01140977292021444\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.69it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.38608812393054454  FDE: 0.5450986955635987\n**************************************************\n******************************\nEpoch: social-tag : 73\ntrain_loss -0.00939677982983355\nval_loss -0.01140977292021444\n{'min_val_epoch': 73, 'min_val_loss': -0.01140977292021444}\n******************************\nTRAIN: \t Epoch: 74 \t Loss: -0.010650362819433212\nTRAIN: \t Epoch: 74 \t Loss: -0.010299933142960072\nTRAIN: \t Epoch: 74 \t Loss: -0.01030011847615242\nTRAIN: \t Epoch: 74 \t Loss: -0.01006325799971819\nTRAIN: \t Epoch: 74 \t Loss: -0.009634554386138916\nTRAIN: \t Epoch: 74 \t Loss: -0.009470178900907436\nTRAIN: \t Epoch: 74 \t Loss: -0.009653655413006033\nTRAIN: \t Epoch: 74 \t Loss: -0.009536825469695032\nTRAIN: \t Epoch: 74 \t Loss: -0.009434535271591611\nTRAIN: \t Epoch: 74 \t Loss: -0.009594413079321384\nTRAIN: \t Epoch: 74 \t Loss: -0.009644736197184433\nTRAIN: \t Epoch: 74 \t Loss: -0.009555837061877051\nTRAIN: \t Epoch: 74 \t Loss: -0.009554566242373906\nTRAIN: \t Epoch: 74 \t Loss: -0.009568789973855019\nTRAIN: \t Epoch: 74 \t Loss: -0.009605956512192886\nTRAIN: \t Epoch: 74 \t Loss: -0.009595570561941713\nTRAIN: \t Epoch: 74 \t Loss: -0.00955749275710653\nTRAIN: \t Epoch: 74 \t Loss: -0.00951677979901433\nTRAIN: \t Epoch: 74 \t Loss: -0.009500536166009566\nVALD: \t Epoch: 74 \t Loss: -0.011646482162177563\nVALD: \t Epoch: 74 \t Loss: -0.010167502798140049\nVALD: \t Epoch: 74 \t Loss: -0.011164720791081587\nVALD: \t Epoch: 74 \t Loss: -0.011188255622982979\nVALD: \t Epoch: 74 \t Loss: -0.010687745898223121\n******************************\nEpoch: social-tag : 74\ntrain_loss -0.009500536166009566\nval_loss -0.010687745898223121\n{'min_val_epoch': 73, 'min_val_loss': -0.01140977292021444}\n******************************\nTRAIN: \t Epoch: 75 \t Loss: -0.009585489518940449\nTRAIN: \t Epoch: 75 \t Loss: -0.009957716334611177\nTRAIN: \t Epoch: 75 \t Loss: -0.009830767599244913\nTRAIN: \t Epoch: 75 \t Loss: -0.009855384938418865\nTRAIN: \t Epoch: 75 \t Loss: -0.009739629924297333\nTRAIN: \t Epoch: 75 \t Loss: -0.009548750550796589\nTRAIN: \t Epoch: 75 \t Loss: -0.009552641505641597\nTRAIN: \t Epoch: 75 \t Loss: -0.009570557507686317\nTRAIN: \t Epoch: 75 \t Loss: -0.00952854524883959\nTRAIN: \t Epoch: 75 \t Loss: -0.009640880208462477\nTRAIN: \t Epoch: 75 \t Loss: -0.009565731171857227\nTRAIN: \t Epoch: 75 \t Loss: -0.009469661628827453\nTRAIN: \t Epoch: 75 \t Loss: -0.009439271516524829\nTRAIN: \t Epoch: 75 \t Loss: -0.009532321177955185\nTRAIN: \t Epoch: 75 \t Loss: -0.009531636039415996\nTRAIN: \t Epoch: 75 \t Loss: -0.009566422319039702\nTRAIN: \t Epoch: 75 \t Loss: -0.009555163135861649\nTRAIN: \t Epoch: 75 \t Loss: -0.009570936103247933\nTRAIN: \t Epoch: 75 \t Loss: -0.00957153172173652\nVALD: \t Epoch: 75 \t Loss: -0.011015779338777065\nVALD: \t Epoch: 75 \t Loss: -0.009952239692211151\nVALD: \t Epoch: 75 \t Loss: -0.01104554875443379\nVALD: \t Epoch: 75 \t Loss: -0.011086066486313939\nVALD: \t Epoch: 75 \t Loss: -0.010571435069249681\n******************************\nEpoch: social-tag : 75\ntrain_loss -0.00957153172173652\nval_loss -0.010571435069249681\n{'min_val_epoch': 73, 'min_val_loss': -0.01140977292021444}\n******************************\nTRAIN: \t Epoch: 76 \t Loss: -0.009005469270050526\nTRAIN: \t Epoch: 76 \t Loss: -0.009425386786460876\nTRAIN: \t Epoch: 76 \t Loss: -0.009535971097648144\nTRAIN: \t Epoch: 76 \t Loss: -0.009557079523801804\nTRAIN: \t Epoch: 76 \t Loss: -0.009413515962660313\nTRAIN: \t Epoch: 76 \t Loss: -0.009551823139190674\nTRAIN: \t Epoch: 76 \t Loss: -0.009602140901344163\nTRAIN: \t Epoch: 76 \t Loss: -0.009529800387099385\nTRAIN: \t Epoch: 76 \t Loss: -0.009538567211065028\nTRAIN: \t Epoch: 76 \t Loss: -0.009499289374798537\nTRAIN: \t Epoch: 76 \t Loss: -0.009534470821646128\nTRAIN: \t Epoch: 76 \t Loss: -0.009563305027162036\nTRAIN: \t Epoch: 76 \t Loss: -0.009540543461648317\nTRAIN: \t Epoch: 76 \t Loss: -0.009449494736535209\nTRAIN: \t Epoch: 76 \t Loss: -0.009419771904746691\nTRAIN: \t Epoch: 76 \t Loss: -0.00951519695809111\nTRAIN: \t Epoch: 76 \t Loss: -0.009547427899258979\nTRAIN: \t Epoch: 76 \t Loss: -0.009598354395065043\nTRAIN: \t Epoch: 76 \t Loss: -0.009604786711197081\nVALD: \t Epoch: 76 \t Loss: -0.012095165438950062\nVALD: \t Epoch: 76 \t Loss: -0.010289947036653757\nVALD: \t Epoch: 76 \t Loss: -0.011489981785416603\nVALD: \t Epoch: 76 \t Loss: -0.011484733317047358\nVALD: \t Epoch: 76 \t Loss: -0.010867570155908253\n******************************\nEpoch: social-tag : 76\ntrain_loss -0.009604786711197081\nval_loss -0.010867570155908253\n{'min_val_epoch': 73, 'min_val_loss': -0.01140977292021444}\n******************************\nTRAIN: \t Epoch: 77 \t Loss: -0.009653926827013493\nTRAIN: \t Epoch: 77 \t Loss: -0.00939676957204938\nTRAIN: \t Epoch: 77 \t Loss: -0.009592042615016302\nTRAIN: \t Epoch: 77 \t Loss: -0.009708259953185916\nTRAIN: \t Epoch: 77 \t Loss: -0.009766613692045211\nTRAIN: \t Epoch: 77 \t Loss: -0.009588458264867464\nTRAIN: \t Epoch: 77 \t Loss: -0.009474088437855244\nTRAIN: \t Epoch: 77 \t Loss: -0.009575304808095098\nTRAIN: \t Epoch: 77 \t Loss: -0.0097576765757468\nTRAIN: \t Epoch: 77 \t Loss: -0.009747381135821343\nTRAIN: \t Epoch: 77 \t Loss: -0.009693746793676506\nTRAIN: \t Epoch: 77 \t Loss: -0.009629758540540934\nTRAIN: \t Epoch: 77 \t Loss: -0.009524106549528929\nTRAIN: \t Epoch: 77 \t Loss: -0.009572312368878297\nTRAIN: \t Epoch: 77 \t Loss: -0.009609806599716346\nTRAIN: \t Epoch: 77 \t Loss: -0.009637684386689216\nTRAIN: \t Epoch: 77 \t Loss: -0.009551143766764332\nTRAIN: \t Epoch: 77 \t Loss: -0.009517702739685774\nTRAIN: \t Epoch: 77 \t Loss: -0.009534026750401112\nVALD: \t Epoch: 77 \t Loss: -0.011879734694957733\nVALD: \t Epoch: 77 \t Loss: -0.01039194455370307\nVALD: \t Epoch: 77 \t Loss: -0.011469163311024507\nVALD: \t Epoch: 77 \t Loss: -0.011575634824112058\nVALD: \t Epoch: 77 \t Loss: -0.011047475791174518\n******************************\nEpoch: social-tag : 77\ntrain_loss -0.009534026750401112\nval_loss -0.011047475791174518\n{'min_val_epoch': 73, 'min_val_loss': -0.01140977292021444}\n******************************\nTRAIN: \t Epoch: 78 \t Loss: -0.010742231272161007\nTRAIN: \t Epoch: 78 \t Loss: -0.010414808057248592\nTRAIN: \t Epoch: 78 \t Loss: -0.009983461971084276\nTRAIN: \t Epoch: 78 \t Loss: -0.00976584805175662\nTRAIN: \t Epoch: 78 \t Loss: -0.009907819144427777\nTRAIN: \t Epoch: 78 \t Loss: -0.01006506523117423\nTRAIN: \t Epoch: 78 \t Loss: -0.009731539591614689\nTRAIN: \t Epoch: 78 \t Loss: -0.009567550208885223\nTRAIN: \t Epoch: 78 \t Loss: -0.009585412995268902\nTRAIN: \t Epoch: 78 \t Loss: -0.00965248174034059\nTRAIN: \t Epoch: 78 \t Loss: -0.00970913761888038\nTRAIN: \t Epoch: 78 \t Loss: -0.009705135715194046\nTRAIN: \t Epoch: 78 \t Loss: -0.009693417375764022\nTRAIN: \t Epoch: 78 \t Loss: -0.00974555782574628\nTRAIN: \t Epoch: 78 \t Loss: -0.00981523614997665\nTRAIN: \t Epoch: 78 \t Loss: -0.009639378258725628\nTRAIN: \t Epoch: 78 \t Loss: -0.009505338282050455\nTRAIN: \t Epoch: 78 \t Loss: -0.009501989232376218\nTRAIN: \t Epoch: 78 \t Loss: -0.009509590732753533\nVALD: \t Epoch: 78 \t Loss: -0.012237461283802986\nVALD: \t Epoch: 78 \t Loss: -0.011082709766924381\nVALD: \t Epoch: 78 \t Loss: -0.012151960904399553\nVALD: \t Epoch: 78 \t Loss: -0.012244746088981628\nVALD: \t Epoch: 78 \t Loss: -0.011641685430668602\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.70it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.40109464789744403  FDE: 0.6220338221897855\n**************************************************\n******************************\nEpoch: social-tag : 78\ntrain_loss -0.009509590732753533\nval_loss -0.011641685430668602\n{'min_val_epoch': 78, 'min_val_loss': -0.011641685430668602}\n******************************\nTRAIN: \t Epoch: 79 \t Loss: -0.010094644501805305\nTRAIN: \t Epoch: 79 \t Loss: -0.010337337385863066\nTRAIN: \t Epoch: 79 \t Loss: -0.010439128304521242\nTRAIN: \t Epoch: 79 \t Loss: -0.01069214683957398\nTRAIN: \t Epoch: 79 \t Loss: -0.010332989878952504\nTRAIN: \t Epoch: 79 \t Loss: -0.009934189884612957\nTRAIN: \t Epoch: 79 \t Loss: -0.00997006573847362\nTRAIN: \t Epoch: 79 \t Loss: -0.01004841027315706\nTRAIN: \t Epoch: 79 \t Loss: -0.01007325171182553\nTRAIN: \t Epoch: 79 \t Loss: -0.010084275715053081\nTRAIN: \t Epoch: 79 \t Loss: -0.010001137683337385\nTRAIN: \t Epoch: 79 \t Loss: -0.009821451269090176\nTRAIN: \t Epoch: 79 \t Loss: -0.009838354057417465\nTRAIN: \t Epoch: 79 \t Loss: -0.009859154599585704\nTRAIN: \t Epoch: 79 \t Loss: -0.00987968302021424\nTRAIN: \t Epoch: 79 \t Loss: -0.009870021662209183\nTRAIN: \t Epoch: 79 \t Loss: -0.009834722725345808\nTRAIN: \t Epoch: 79 \t Loss: -0.00978295137691829\nTRAIN: \t Epoch: 79 \t Loss: -0.009787233916980043\nVALD: \t Epoch: 79 \t Loss: -0.012157968245446682\nVALD: \t Epoch: 79 \t Loss: -0.010418585035949945\nVALD: \t Epoch: 79 \t Loss: -0.011267299763858318\nVALD: \t Epoch: 79 \t Loss: -0.011299652978777885\nVALD: \t Epoch: 79 \t Loss: -0.010803632401237804\n******************************\nEpoch: social-tag : 79\ntrain_loss -0.009787233916980043\nval_loss -0.010803632401237804\n{'min_val_epoch': 78, 'min_val_loss': -0.011641685430668602}\n******************************\nTRAIN: \t Epoch: 80 \t Loss: -0.010392255149781704\nTRAIN: \t Epoch: 80 \t Loss: -0.01067540468648076\nTRAIN: \t Epoch: 80 \t Loss: -0.010329033869008223\nTRAIN: \t Epoch: 80 \t Loss: -0.009608774562366307\nTRAIN: \t Epoch: 80 \t Loss: -0.009226746950298547\nTRAIN: \t Epoch: 80 \t Loss: -0.009374820549661914\nTRAIN: \t Epoch: 80 \t Loss: -0.00965539292831506\nTRAIN: \t Epoch: 80 \t Loss: -0.009695566666778177\nTRAIN: \t Epoch: 80 \t Loss: -0.009711886186980538\nTRAIN: \t Epoch: 80 \t Loss: -0.00968223842792213\nTRAIN: \t Epoch: 80 \t Loss: -0.009517823642289097\nTRAIN: \t Epoch: 80 \t Loss: -0.009508028975687921\nTRAIN: \t Epoch: 80 \t Loss: -0.009578110972562661\nTRAIN: \t Epoch: 80 \t Loss: -0.00965553305910102\nTRAIN: \t Epoch: 80 \t Loss: -0.00956087401136756\nTRAIN: \t Epoch: 80 \t Loss: -0.00948249633074738\nTRAIN: \t Epoch: 80 \t Loss: -0.009540943675400579\nTRAIN: \t Epoch: 80 \t Loss: -0.009634045030300816\nTRAIN: \t Epoch: 80 \t Loss: -0.009655080626587946\nVALD: \t Epoch: 80 \t Loss: -0.011382753029465675\nVALD: \t Epoch: 80 \t Loss: -0.010253006126731634\nVALD: \t Epoch: 80 \t Loss: -0.011688143946230412\nVALD: \t Epoch: 80 \t Loss: -0.011784112080931664\nVALD: \t Epoch: 80 \t Loss: -0.011031969224125885\n******************************\nEpoch: social-tag : 80\ntrain_loss -0.009655080626587946\nval_loss -0.011031969224125885\n{'min_val_epoch': 78, 'min_val_loss': -0.011641685430668602}\n******************************\nTRAIN: \t Epoch: 81 \t Loss: -0.010634951293468475\nTRAIN: \t Epoch: 81 \t Loss: -0.010855942498892546\nTRAIN: \t Epoch: 81 \t Loss: -0.010086053361495336\nTRAIN: \t Epoch: 81 \t Loss: -0.009398011723533273\nTRAIN: \t Epoch: 81 \t Loss: -0.009479723870754242\nTRAIN: \t Epoch: 81 \t Loss: -0.009458025762190422\nTRAIN: \t Epoch: 81 \t Loss: -0.00969707580017192\nTRAIN: \t Epoch: 81 \t Loss: -0.009659540373831987\nTRAIN: \t Epoch: 81 \t Loss: -0.00973187004112535\nTRAIN: \t Epoch: 81 \t Loss: -0.009842347633093596\nTRAIN: \t Epoch: 81 \t Loss: -0.009832988239147446\nTRAIN: \t Epoch: 81 \t Loss: -0.009695574563617507\nTRAIN: \t Epoch: 81 \t Loss: -0.009673762923249831\nTRAIN: \t Epoch: 81 \t Loss: -0.009757782997829574\nTRAIN: \t Epoch: 81 \t Loss: -0.009815607282022635\nTRAIN: \t Epoch: 81 \t Loss: -0.009821661282330751\nTRAIN: \t Epoch: 81 \t Loss: -0.009815978981992778\nTRAIN: \t Epoch: 81 \t Loss: -0.009759338210440345\nTRAIN: \t Epoch: 81 \t Loss: -0.009740827247359238\nVALD: \t Epoch: 81 \t Loss: -0.011665706522762775\nVALD: \t Epoch: 81 \t Loss: -0.009789434727281332\nVALD: \t Epoch: 81 \t Loss: -0.010450137158234915\nVALD: \t Epoch: 81 \t Loss: -0.010405150009319186\nVALD: \t Epoch: 81 \t Loss: -0.009880562754701977\n******************************\nEpoch: social-tag : 81\ntrain_loss -0.009740827247359238\nval_loss -0.009880562754701977\n{'min_val_epoch': 78, 'min_val_loss': -0.011641685430668602}\n******************************\nTRAIN: \t Epoch: 82 \t Loss: -0.009865975007414818\nTRAIN: \t Epoch: 82 \t Loss: -0.010095917154103518\nTRAIN: \t Epoch: 82 \t Loss: -0.009930006849269072\nTRAIN: \t Epoch: 82 \t Loss: -0.009840907528996468\nTRAIN: \t Epoch: 82 \t Loss: -0.009701339341700078\nTRAIN: \t Epoch: 82 \t Loss: -0.0096635683439672\nTRAIN: \t Epoch: 82 \t Loss: -0.00975764383162771\nTRAIN: \t Epoch: 82 \t Loss: -0.00978396343998611\nTRAIN: \t Epoch: 82 \t Loss: -0.009791923686861992\nTRAIN: \t Epoch: 82 \t Loss: -0.009873069915920496\nTRAIN: \t Epoch: 82 \t Loss: -0.009882591326128353\nTRAIN: \t Epoch: 82 \t Loss: -0.00991607915299634\nTRAIN: \t Epoch: 82 \t Loss: -0.009965487684194859\nTRAIN: \t Epoch: 82 \t Loss: -0.00991934584453702\nTRAIN: \t Epoch: 82 \t Loss: -0.009882296373446783\nTRAIN: \t Epoch: 82 \t Loss: -0.009926087339408696\nTRAIN: \t Epoch: 82 \t Loss: -0.009926749941180734\nTRAIN: \t Epoch: 82 \t Loss: -0.009869914967566729\nTRAIN: \t Epoch: 82 \t Loss: -0.009866945535380917\nVALD: \t Epoch: 82 \t Loss: -0.011774641461670399\nVALD: \t Epoch: 82 \t Loss: -0.010163193568587303\nVALD: \t Epoch: 82 \t Loss: -0.011090917202333609\nVALD: \t Epoch: 82 \t Loss: -0.011012305272743106\nVALD: \t Epoch: 82 \t Loss: -0.010448661816021628\n******************************\nEpoch: social-tag : 82\ntrain_loss -0.009866945535380917\nval_loss -0.010448661816021628\n{'min_val_epoch': 78, 'min_val_loss': -0.011641685430668602}\n******************************\nTRAIN: \t Epoch: 83 \t Loss: -0.010065579786896706\nTRAIN: \t Epoch: 83 \t Loss: -0.010631593875586987\nTRAIN: \t Epoch: 83 \t Loss: -0.010397445410490036\nTRAIN: \t Epoch: 83 \t Loss: -0.010144468629732728\nTRAIN: \t Epoch: 83 \t Loss: -0.009817266836762428\nTRAIN: \t Epoch: 83 \t Loss: -0.009842516388744116\nTRAIN: \t Epoch: 83 \t Loss: -0.009926490086529936\nTRAIN: \t Epoch: 83 \t Loss: -0.009981693234294653\nTRAIN: \t Epoch: 83 \t Loss: -0.009833542112674978\nTRAIN: \t Epoch: 83 \t Loss: -0.00974158514291048\nTRAIN: \t Epoch: 83 \t Loss: -0.009827996434813196\nTRAIN: \t Epoch: 83 \t Loss: -0.009874606194595495\nTRAIN: \t Epoch: 83 \t Loss: -0.00989454540495689\nTRAIN: \t Epoch: 83 \t Loss: -0.00994504349572318\nTRAIN: \t Epoch: 83 \t Loss: -0.009886606782674789\nTRAIN: \t Epoch: 83 \t Loss: -0.009838612691964954\nTRAIN: \t Epoch: 83 \t Loss: -0.009832404992159675\nTRAIN: \t Epoch: 83 \t Loss: -0.009882724326517846\nTRAIN: \t Epoch: 83 \t Loss: -0.009891174861986405\nVALD: \t Epoch: 83 \t Loss: -0.012353607453405857\nVALD: \t Epoch: 83 \t Loss: -0.010993818286806345\nVALD: \t Epoch: 83 \t Loss: -0.012100937776267529\nVALD: \t Epoch: 83 \t Loss: -0.012058216845616698\nVALD: \t Epoch: 83 \t Loss: -0.011529339837633873\n******************************\nEpoch: social-tag : 83\ntrain_loss -0.009891174861986405\nval_loss -0.011529339837633873\n{'min_val_epoch': 78, 'min_val_loss': -0.011641685430668602}\n******************************\nTRAIN: \t Epoch: 84 \t Loss: -0.011075352318584919\nTRAIN: \t Epoch: 84 \t Loss: -0.010337531566619873\nTRAIN: \t Epoch: 84 \t Loss: -0.009682236549754938\nTRAIN: \t Epoch: 84 \t Loss: -0.009458814049139619\nTRAIN: \t Epoch: 84 \t Loss: -0.009810812771320343\nTRAIN: \t Epoch: 84 \t Loss: -0.00994102175657948\nTRAIN: \t Epoch: 84 \t Loss: -0.009983378861631666\nTRAIN: \t Epoch: 84 \t Loss: -0.009982065297663212\nTRAIN: \t Epoch: 84 \t Loss: -0.009989881681071388\nTRAIN: \t Epoch: 84 \t Loss: -0.009956622589379549\nTRAIN: \t Epoch: 84 \t Loss: -0.009902151047506115\nTRAIN: \t Epoch: 84 \t Loss: -0.009928828338161111\nTRAIN: \t Epoch: 84 \t Loss: -0.009963620239152359\nTRAIN: \t Epoch: 84 \t Loss: -0.009896479480500733\nTRAIN: \t Epoch: 84 \t Loss: -0.009843219878772895\nTRAIN: \t Epoch: 84 \t Loss: -0.009901852288749069\nTRAIN: \t Epoch: 84 \t Loss: -0.009945636122104\nTRAIN: \t Epoch: 84 \t Loss: -0.009969243055416478\nTRAIN: \t Epoch: 84 \t Loss: -0.00996464783523126\nVALD: \t Epoch: 84 \t Loss: -0.012674315832555294\nVALD: \t Epoch: 84 \t Loss: -0.011320841498672962\nVALD: \t Epoch: 84 \t Loss: -0.012340519887705645\nVALD: \t Epoch: 84 \t Loss: -0.012367756105959415\nVALD: \t Epoch: 84 \t Loss: -0.011697411241610188\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.63it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.39153154077669233  FDE: 0.6040163426652168\n**************************************************\n******************************\nEpoch: social-tag : 84\ntrain_loss -0.00996464783523126\nval_loss -0.011697411241610188\n{'min_val_epoch': 84, 'min_val_loss': -0.011697411241610188}\n******************************\nTRAIN: \t Epoch: 85 \t Loss: -0.01006011851131916\nTRAIN: \t Epoch: 85 \t Loss: -0.010702068451792002\nTRAIN: \t Epoch: 85 \t Loss: -0.01032158825546503\nTRAIN: \t Epoch: 85 \t Loss: -0.009971474530175328\nTRAIN: \t Epoch: 85 \t Loss: -0.01002101730555296\nTRAIN: \t Epoch: 85 \t Loss: -0.010007297775397698\nTRAIN: \t Epoch: 85 \t Loss: -0.009908225919519151\nTRAIN: \t Epoch: 85 \t Loss: -0.009789415751583874\nTRAIN: \t Epoch: 85 \t Loss: -0.009563577361404896\nTRAIN: \t Epoch: 85 \t Loss: -0.009606528095901012\nTRAIN: \t Epoch: 85 \t Loss: -0.009696809574961662\nTRAIN: \t Epoch: 85 \t Loss: -0.009827837813645601\nTRAIN: \t Epoch: 85 \t Loss: -0.009948942810297012\nTRAIN: \t Epoch: 85 \t Loss: -0.009935267535703523\nTRAIN: \t Epoch: 85 \t Loss: -0.00984289770325025\nTRAIN: \t Epoch: 85 \t Loss: -0.009833579184487462\nTRAIN: \t Epoch: 85 \t Loss: -0.009891118854284286\nTRAIN: \t Epoch: 85 \t Loss: -0.009955575741413567\nTRAIN: \t Epoch: 85 \t Loss: -0.009965710053148195\nVALD: \t Epoch: 85 \t Loss: -0.012875125743448734\nVALD: \t Epoch: 85 \t Loss: -0.011094803921878338\nVALD: \t Epoch: 85 \t Loss: -0.012282792168358961\nVALD: \t Epoch: 85 \t Loss: -0.012396616162732244\nVALD: \t Epoch: 85 \t Loss: -0.011680575638763176\n******************************\nEpoch: social-tag : 85\ntrain_loss -0.009965710053148195\nval_loss -0.011680575638763176\n{'min_val_epoch': 84, 'min_val_loss': -0.011697411241610188}\n******************************\nTRAIN: \t Epoch: 86 \t Loss: -0.011606181971728802\nTRAIN: \t Epoch: 86 \t Loss: -0.011054735630750656\nTRAIN: \t Epoch: 86 \t Loss: -0.010153259771565596\nTRAIN: \t Epoch: 86 \t Loss: -0.00997199583798647\nTRAIN: \t Epoch: 86 \t Loss: -0.01016650777310133\nTRAIN: \t Epoch: 86 \t Loss: -0.010286136219898859\nTRAIN: \t Epoch: 86 \t Loss: -0.01037694833108357\nTRAIN: \t Epoch: 86 \t Loss: -0.010446610394865274\nTRAIN: \t Epoch: 86 \t Loss: -0.010213619511988428\nTRAIN: \t Epoch: 86 \t Loss: -0.010013357736170292\nTRAIN: \t Epoch: 86 \t Loss: -0.010081083344464952\nTRAIN: \t Epoch: 86 \t Loss: -0.010087062604725361\nTRAIN: \t Epoch: 86 \t Loss: -0.01005257572978735\nTRAIN: \t Epoch: 86 \t Loss: -0.010104506609163113\nTRAIN: \t Epoch: 86 \t Loss: -0.010158436745405197\nTRAIN: \t Epoch: 86 \t Loss: -0.01010665885405615\nTRAIN: \t Epoch: 86 \t Loss: -0.009984613615362084\nTRAIN: \t Epoch: 86 \t Loss: -0.009924734994355176\nTRAIN: \t Epoch: 86 \t Loss: -0.009942641746474174\nVALD: \t Epoch: 86 \t Loss: -0.012109814211726189\nVALD: \t Epoch: 86 \t Loss: -0.010552785359323025\nVALD: \t Epoch: 86 \t Loss: -0.011829685730238756\nVALD: \t Epoch: 86 \t Loss: -0.011976035544648767\nVALD: \t Epoch: 86 \t Loss: -0.011295244122339674\n******************************\nEpoch: social-tag : 86\ntrain_loss -0.009942641746474174\nval_loss -0.011295244122339674\n{'min_val_epoch': 84, 'min_val_loss': -0.011697411241610188}\n******************************\nTRAIN: \t Epoch: 87 \t Loss: -0.011099745519459248\nTRAIN: \t Epoch: 87 \t Loss: -0.01064843637868762\nTRAIN: \t Epoch: 87 \t Loss: -0.011066999907294909\nTRAIN: \t Epoch: 87 \t Loss: -0.011230214731767774\nTRAIN: \t Epoch: 87 \t Loss: -0.010968246683478355\nTRAIN: \t Epoch: 87 \t Loss: -0.01088210055604577\nTRAIN: \t Epoch: 87 \t Loss: -0.01070611591317824\nTRAIN: \t Epoch: 87 \t Loss: -0.010529619525186718\nTRAIN: \t Epoch: 87 \t Loss: -0.010555333354406886\nTRAIN: \t Epoch: 87 \t Loss: -0.010519072413444519\nTRAIN: \t Epoch: 87 \t Loss: -0.010474175384098833\nTRAIN: \t Epoch: 87 \t Loss: -0.010504539667939147\nTRAIN: \t Epoch: 87 \t Loss: -0.010568408008951407\nTRAIN: \t Epoch: 87 \t Loss: -0.010527482123247214\nTRAIN: \t Epoch: 87 \t Loss: -0.010364724633594355\nTRAIN: \t Epoch: 87 \t Loss: -0.010281627473887056\nTRAIN: \t Epoch: 87 \t Loss: -0.010271360056803507\nTRAIN: \t Epoch: 87 \t Loss: -0.010253996277848879\nTRAIN: \t Epoch: 87 \t Loss: -0.010255373282796037\nVALD: \t Epoch: 87 \t Loss: -0.013317039236426353\nVALD: \t Epoch: 87 \t Loss: -0.01139402249827981\nVALD: \t Epoch: 87 \t Loss: -0.012421903821329275\nVALD: \t Epoch: 87 \t Loss: -0.01240594987757504\nVALD: \t Epoch: 87 \t Loss: -0.011819003731751244\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.70it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.37083515694127483  FDE: 0.5334514578469448\n**************************************************\n******************************\nEpoch: social-tag : 87\ntrain_loss -0.010255373282796037\nval_loss -0.011819003731751244\n{'min_val_epoch': 87, 'min_val_loss': -0.011819003731751244}\n******************************\nTRAIN: \t Epoch: 88 \t Loss: -0.011241186410188675\nTRAIN: \t Epoch: 88 \t Loss: -0.011252228170633316\nTRAIN: \t Epoch: 88 \t Loss: -0.011282138836880526\nTRAIN: \t Epoch: 88 \t Loss: -0.010984297841787338\nTRAIN: \t Epoch: 88 \t Loss: -0.010042680613696574\nTRAIN: \t Epoch: 88 \t Loss: -0.009636716994767388\nTRAIN: \t Epoch: 88 \t Loss: -0.00982327074078577\nTRAIN: \t Epoch: 88 \t Loss: -0.010005222458858043\nTRAIN: \t Epoch: 88 \t Loss: -0.010109714905007018\nTRAIN: \t Epoch: 88 \t Loss: -0.010220372444018721\nTRAIN: \t Epoch: 88 \t Loss: -0.010190420741723343\nTRAIN: \t Epoch: 88 \t Loss: -0.010116861706289152\nTRAIN: \t Epoch: 88 \t Loss: -0.010114199589364804\nTRAIN: \t Epoch: 88 \t Loss: -0.01009459949896804\nTRAIN: \t Epoch: 88 \t Loss: -0.010043205600231886\nTRAIN: \t Epoch: 88 \t Loss: -0.010066126385936514\nTRAIN: \t Epoch: 88 \t Loss: -0.010126782718169339\nTRAIN: \t Epoch: 88 \t Loss: -0.010045428734479679\nTRAIN: \t Epoch: 88 \t Loss: -0.010025413641120523\nVALD: \t Epoch: 88 \t Loss: -0.008624755777418613\nVALD: \t Epoch: 88 \t Loss: -0.006678866688162088\nVALD: \t Epoch: 88 \t Loss: -0.007256620563566685\nVALD: \t Epoch: 88 \t Loss: -0.007458823034539819\nVALD: \t Epoch: 88 \t Loss: -0.007054771076549183\n******************************\nEpoch: social-tag : 88\ntrain_loss -0.010025413641120523\nval_loss -0.007054771076549183\n{'min_val_epoch': 87, 'min_val_loss': -0.011819003731751244}\n******************************\nTRAIN: \t Epoch: 89 \t Loss: -0.007713099475950003\nTRAIN: \t Epoch: 89 \t Loss: -0.008664983557537198\nTRAIN: \t Epoch: 89 \t Loss: -0.009142639581114054\nTRAIN: \t Epoch: 89 \t Loss: -0.009542481624521315\nTRAIN: \t Epoch: 89 \t Loss: -0.009848688449710607\nTRAIN: \t Epoch: 89 \t Loss: -0.010060075748090943\nTRAIN: \t Epoch: 89 \t Loss: -0.010165216600788491\nTRAIN: \t Epoch: 89 \t Loss: -0.0104117970331572\nTRAIN: \t Epoch: 89 \t Loss: -0.010228345222357247\nTRAIN: \t Epoch: 89 \t Loss: -0.009941952582448722\nTRAIN: \t Epoch: 89 \t Loss: -0.00997779696163806\nTRAIN: \t Epoch: 89 \t Loss: -0.010005939907083908\nTRAIN: \t Epoch: 89 \t Loss: -0.01007792496910462\nTRAIN: \t Epoch: 89 \t Loss: -0.010220005110438381\nTRAIN: \t Epoch: 89 \t Loss: -0.010178800175587336\nTRAIN: \t Epoch: 89 \t Loss: -0.010042170295491815\nTRAIN: \t Epoch: 89 \t Loss: -0.010053926147520542\nTRAIN: \t Epoch: 89 \t Loss: -0.010124730929318402\nTRAIN: \t Epoch: 89 \t Loss: -0.01010919128713887\nVALD: \t Epoch: 89 \t Loss: -0.012401972897350788\nVALD: \t Epoch: 89 \t Loss: -0.011074728798121214\nVALD: \t Epoch: 89 \t Loss: -0.012212811037898064\nVALD: \t Epoch: 89 \t Loss: -0.012264016084372997\nVALD: \t Epoch: 89 \t Loss: -0.011574105684422265\n******************************\nEpoch: social-tag : 89\ntrain_loss -0.01010919128713887\nval_loss -0.011574105684422265\n{'min_val_epoch': 87, 'min_val_loss': -0.011819003731751244}\n******************************\nTRAIN: \t Epoch: 90 \t Loss: -0.011947089806199074\nTRAIN: \t Epoch: 90 \t Loss: -0.011614478193223476\nTRAIN: \t Epoch: 90 \t Loss: -0.011119877298672995\nTRAIN: \t Epoch: 90 \t Loss: -0.010599108878523111\nTRAIN: \t Epoch: 90 \t Loss: -0.010172059945762157\nTRAIN: \t Epoch: 90 \t Loss: -0.010144568824519714\nTRAIN: \t Epoch: 90 \t Loss: -0.01031535757439477\nTRAIN: \t Epoch: 90 \t Loss: -0.010512793553061783\nTRAIN: \t Epoch: 90 \t Loss: -0.01059221134831508\nTRAIN: \t Epoch: 90 \t Loss: -0.010426924657076597\nTRAIN: \t Epoch: 90 \t Loss: -0.01022256961600347\nTRAIN: \t Epoch: 90 \t Loss: -0.01024968409910798\nTRAIN: \t Epoch: 90 \t Loss: -0.010320507276516695\nTRAIN: \t Epoch: 90 \t Loss: -0.01029669972402709\nTRAIN: \t Epoch: 90 \t Loss: -0.010240018119414647\nTRAIN: \t Epoch: 90 \t Loss: -0.010186054976657033\nTRAIN: \t Epoch: 90 \t Loss: -0.010212321875288206\nTRAIN: \t Epoch: 90 \t Loss: -0.010234311171289947\nTRAIN: \t Epoch: 90 \t Loss: -0.01023931115942716\nVALD: \t Epoch: 90 \t Loss: -0.01179796177893877\nVALD: \t Epoch: 90 \t Loss: -0.010806719306856394\nVALD: \t Epoch: 90 \t Loss: -0.012157591370244821\nVALD: \t Epoch: 90 \t Loss: -0.01226987293921411\nVALD: \t Epoch: 90 \t Loss: -0.011611594050383764\n******************************\nEpoch: social-tag : 90\ntrain_loss -0.01023931115942716\nval_loss -0.011611594050383764\n{'min_val_epoch': 87, 'min_val_loss': -0.011819003731751244}\n******************************\nTRAIN: \t Epoch: 91 \t Loss: -0.010400037281215191\nTRAIN: \t Epoch: 91 \t Loss: -0.010217173025012016\nTRAIN: \t Epoch: 91 \t Loss: -0.010054417264958223\nTRAIN: \t Epoch: 91 \t Loss: -0.009956206660717726\nTRAIN: \t Epoch: 91 \t Loss: -0.010096739791333676\nTRAIN: \t Epoch: 91 \t Loss: -0.010129701811820269\nTRAIN: \t Epoch: 91 \t Loss: -0.010100933324013437\nTRAIN: \t Epoch: 91 \t Loss: -0.010150683927349746\nTRAIN: \t Epoch: 91 \t Loss: -0.010203472235136561\nTRAIN: \t Epoch: 91 \t Loss: -0.010035776440054179\nTRAIN: \t Epoch: 91 \t Loss: -0.009997486644847826\nTRAIN: \t Epoch: 91 \t Loss: -0.010025660196940104\nTRAIN: \t Epoch: 91 \t Loss: -0.010087646830540437\nTRAIN: \t Epoch: 91 \t Loss: -0.01015531990144934\nTRAIN: \t Epoch: 91 \t Loss: -0.010078192998965582\nTRAIN: \t Epoch: 91 \t Loss: -0.00998571515083313\nTRAIN: \t Epoch: 91 \t Loss: -0.010038649827680168\nTRAIN: \t Epoch: 91 \t Loss: -0.010095039279096656\nTRAIN: \t Epoch: 91 \t Loss: -0.010104942973783363\nVALD: \t Epoch: 91 \t Loss: -0.012547693215310574\nVALD: \t Epoch: 91 \t Loss: -0.010691226460039616\nVALD: \t Epoch: 91 \t Loss: -0.012175076641142368\nVALD: \t Epoch: 91 \t Loss: -0.012252119602635503\nVALD: \t Epoch: 91 \t Loss: -0.011578635243344898\n******************************\nEpoch: social-tag : 91\ntrain_loss -0.010104942973783363\nval_loss -0.011578635243344898\n{'min_val_epoch': 87, 'min_val_loss': -0.011819003731751244}\n******************************\nTRAIN: \t Epoch: 92 \t Loss: -0.011555954813957214\nTRAIN: \t Epoch: 92 \t Loss: -0.011090434156358242\nTRAIN: \t Epoch: 92 \t Loss: -0.01072602408627669\nTRAIN: \t Epoch: 92 \t Loss: -0.010246037039905787\nTRAIN: \t Epoch: 92 \t Loss: -0.010159733705222607\nTRAIN: \t Epoch: 92 \t Loss: -0.010279832252611717\nTRAIN: \t Epoch: 92 \t Loss: -0.010451818816363811\nTRAIN: \t Epoch: 92 \t Loss: -0.01049159315880388\nTRAIN: \t Epoch: 92 \t Loss: -0.010305148963299062\nTRAIN: \t Epoch: 92 \t Loss: -0.010264846868813038\nTRAIN: \t Epoch: 92 \t Loss: -0.010242339477620342\nTRAIN: \t Epoch: 92 \t Loss: -0.010307835803056756\nTRAIN: \t Epoch: 92 \t Loss: -0.010287540033459663\nTRAIN: \t Epoch: 92 \t Loss: -0.01028388093358704\nTRAIN: \t Epoch: 92 \t Loss: -0.010253335225085418\nTRAIN: \t Epoch: 92 \t Loss: -0.01028247398789972\nTRAIN: \t Epoch: 92 \t Loss: -0.010277549681418082\nTRAIN: \t Epoch: 92 \t Loss: -0.010218205810007121\nTRAIN: \t Epoch: 92 \t Loss: -0.010195192517544256\nVALD: \t Epoch: 92 \t Loss: -0.010601944290101528\nVALD: \t Epoch: 92 \t Loss: -0.009087084559723735\nVALD: \t Epoch: 92 \t Loss: -0.009981937240809202\nVALD: \t Epoch: 92 \t Loss: -0.009930402389727533\nVALD: \t Epoch: 92 \t Loss: -0.009417768943408305\n******************************\nEpoch: social-tag : 92\ntrain_loss -0.010195192517544256\nval_loss -0.009417768943408305\n{'min_val_epoch': 87, 'min_val_loss': -0.011819003731751244}\n******************************\nTRAIN: \t Epoch: 93 \t Loss: -0.009247190319001675\nTRAIN: \t Epoch: 93 \t Loss: -0.009903986938297749\nTRAIN: \t Epoch: 93 \t Loss: -0.010529814598460993\nTRAIN: \t Epoch: 93 \t Loss: -0.010553281754255295\nTRAIN: \t Epoch: 93 \t Loss: -0.010145871527493\nTRAIN: \t Epoch: 93 \t Loss: -0.009863225898394981\nTRAIN: \t Epoch: 93 \t Loss: -0.009979038499295712\nTRAIN: \t Epoch: 93 \t Loss: -0.010163678554818034\nTRAIN: \t Epoch: 93 \t Loss: -0.01036091935303476\nTRAIN: \t Epoch: 93 \t Loss: -0.010339646879583598\nTRAIN: \t Epoch: 93 \t Loss: -0.010229946283454245\nTRAIN: \t Epoch: 93 \t Loss: -0.010192904388532043\nTRAIN: \t Epoch: 93 \t Loss: -0.010148349003149914\nTRAIN: \t Epoch: 93 \t Loss: -0.010205931163259916\nTRAIN: \t Epoch: 93 \t Loss: -0.010220952828725179\nTRAIN: \t Epoch: 93 \t Loss: -0.010124911670573056\nTRAIN: \t Epoch: 93 \t Loss: -0.010059721767902374\nTRAIN: \t Epoch: 93 \t Loss: -0.010105715754131476\nTRAIN: \t Epoch: 93 \t Loss: -0.010117401653687364\nVALD: \t Epoch: 93 \t Loss: -0.012633245438337326\nVALD: \t Epoch: 93 \t Loss: -0.010924134869128466\nVALD: \t Epoch: 93 \t Loss: -0.012407144842048487\nVALD: \t Epoch: 93 \t Loss: -0.012312207603827119\nVALD: \t Epoch: 93 \t Loss: -0.011541413374183592\n******************************\nEpoch: social-tag : 93\ntrain_loss -0.010117401653687364\nval_loss -0.011541413374183592\n{'min_val_epoch': 87, 'min_val_loss': -0.011819003731751244}\n******************************\nTRAIN: \t Epoch: 94 \t Loss: -0.011659124866127968\nTRAIN: \t Epoch: 94 \t Loss: -0.011311704758554697\nTRAIN: \t Epoch: 94 \t Loss: -0.010994964589675268\nTRAIN: \t Epoch: 94 \t Loss: -0.010883556213229895\nTRAIN: \t Epoch: 94 \t Loss: -0.010682547464966774\nTRAIN: \t Epoch: 94 \t Loss: -0.010579091341545185\nTRAIN: \t Epoch: 94 \t Loss: -0.010541604033538274\nTRAIN: \t Epoch: 94 \t Loss: -0.010425977292470634\nTRAIN: \t Epoch: 94 \t Loss: -0.010378645319077704\nTRAIN: \t Epoch: 94 \t Loss: -0.010334299877285957\nTRAIN: \t Epoch: 94 \t Loss: -0.0102965808050199\nTRAIN: \t Epoch: 94 \t Loss: -0.01034809237656494\nTRAIN: \t Epoch: 94 \t Loss: -0.010291523729952482\nTRAIN: \t Epoch: 94 \t Loss: -0.010220283110226904\nTRAIN: \t Epoch: 94 \t Loss: -0.010240411820511023\nTRAIN: \t Epoch: 94 \t Loss: -0.010295498184859753\nTRAIN: \t Epoch: 94 \t Loss: -0.010272484601420514\nTRAIN: \t Epoch: 94 \t Loss: -0.010280345463090472\nTRAIN: \t Epoch: 94 \t Loss: -0.010278559521700575\nVALD: \t Epoch: 94 \t Loss: -0.012389215640723705\nVALD: \t Epoch: 94 \t Loss: -0.010426204651594162\nVALD: \t Epoch: 94 \t Loss: -0.011666316539049149\nVALD: \t Epoch: 94 \t Loss: -0.011540036648511887\nVALD: \t Epoch: 94 \t Loss: -0.010965105423257371\n******************************\nEpoch: social-tag : 94\ntrain_loss -0.010278559521700575\nval_loss -0.010965105423257371\n{'min_val_epoch': 87, 'min_val_loss': -0.011819003731751244}\n******************************\nTRAIN: \t Epoch: 95 \t Loss: -0.010522264055907726\nTRAIN: \t Epoch: 95 \t Loss: -0.011068391613662243\nTRAIN: \t Epoch: 95 \t Loss: -0.010991202356914679\nTRAIN: \t Epoch: 95 \t Loss: -0.010904517723247409\nTRAIN: \t Epoch: 95 \t Loss: -0.010767868719995022\nTRAIN: \t Epoch: 95 \t Loss: -0.010666280519217253\nTRAIN: \t Epoch: 95 \t Loss: -0.010602452659181185\nTRAIN: \t Epoch: 95 \t Loss: -0.010553796426393092\nTRAIN: \t Epoch: 95 \t Loss: -0.010481569812529616\nTRAIN: \t Epoch: 95 \t Loss: -0.010467391647398471\nTRAIN: \t Epoch: 95 \t Loss: -0.010433580218390985\nTRAIN: \t Epoch: 95 \t Loss: -0.010442448391889533\nTRAIN: \t Epoch: 95 \t Loss: -0.010451776620287161\nTRAIN: \t Epoch: 95 \t Loss: -0.010363542740898473\nTRAIN: \t Epoch: 95 \t Loss: -0.010352133587002755\nTRAIN: \t Epoch: 95 \t Loss: -0.010323924419935793\nTRAIN: \t Epoch: 95 \t Loss: -0.01030173574519508\nTRAIN: \t Epoch: 95 \t Loss: -0.010344919386423297\nTRAIN: \t Epoch: 95 \t Loss: -0.010355582269843953\nVALD: \t Epoch: 95 \t Loss: -0.012769627384841442\nVALD: \t Epoch: 95 \t Loss: -0.010549074970185757\nVALD: \t Epoch: 95 \t Loss: -0.011976638808846474\nVALD: \t Epoch: 95 \t Loss: -0.012053257320076227\nVALD: \t Epoch: 95 \t Loss: -0.011312906308607621\n******************************\nEpoch: social-tag : 95\ntrain_loss -0.010355582269843953\nval_loss -0.011312906308607621\n{'min_val_epoch': 87, 'min_val_loss': -0.011819003731751244}\n******************************\nTRAIN: \t Epoch: 96 \t Loss: -0.012349426746368408\nTRAIN: \t Epoch: 96 \t Loss: -0.011839855927973986\nTRAIN: \t Epoch: 96 \t Loss: -0.010383033193647861\nTRAIN: \t Epoch: 96 \t Loss: -0.009697167668491602\nTRAIN: \t Epoch: 96 \t Loss: -0.009778779186308383\nTRAIN: \t Epoch: 96 \t Loss: -0.00999659082541863\nTRAIN: \t Epoch: 96 \t Loss: -0.010246040698673044\nTRAIN: \t Epoch: 96 \t Loss: -0.010407267371192575\nTRAIN: \t Epoch: 96 \t Loss: -0.010375891708665423\nTRAIN: \t Epoch: 96 \t Loss: -0.010529737267643214\nTRAIN: \t Epoch: 96 \t Loss: -0.010628638535060665\nTRAIN: \t Epoch: 96 \t Loss: -0.010470106266438961\nTRAIN: \t Epoch: 96 \t Loss: -0.010325920911362538\nTRAIN: \t Epoch: 96 \t Loss: -0.010405599339199918\nTRAIN: \t Epoch: 96 \t Loss: -0.010447743038336435\nTRAIN: \t Epoch: 96 \t Loss: -0.010424402367789298\nTRAIN: \t Epoch: 96 \t Loss: -0.010413156986674842\nTRAIN: \t Epoch: 96 \t Loss: -0.010368979535996914\nTRAIN: \t Epoch: 96 \t Loss: -0.010376775122274109\nVALD: \t Epoch: 96 \t Loss: -0.01209329068660736\nVALD: \t Epoch: 96 \t Loss: -0.010194637347012758\nVALD: \t Epoch: 96 \t Loss: -0.011309507302939892\nVALD: \t Epoch: 96 \t Loss: -0.011424997355788946\nVALD: \t Epoch: 96 \t Loss: -0.010783115103225077\n******************************\nEpoch: social-tag : 96\ntrain_loss -0.010376775122274109\nval_loss -0.010783115103225077\n{'min_val_epoch': 87, 'min_val_loss': -0.011819003731751244}\n******************************\nTRAIN: \t Epoch: 97 \t Loss: -0.011763853952288628\nTRAIN: \t Epoch: 97 \t Loss: -0.011268360540270805\nTRAIN: \t Epoch: 97 \t Loss: -0.010039457430442175\nTRAIN: \t Epoch: 97 \t Loss: -0.009844958549365401\nTRAIN: \t Epoch: 97 \t Loss: -0.009982994757592678\nTRAIN: \t Epoch: 97 \t Loss: -0.00998893721650044\nTRAIN: \t Epoch: 97 \t Loss: -0.010175453365913458\nTRAIN: \t Epoch: 97 \t Loss: -0.010312577825970948\nTRAIN: \t Epoch: 97 \t Loss: -0.010312279065450033\nTRAIN: \t Epoch: 97 \t Loss: -0.010176870319992305\nTRAIN: \t Epoch: 97 \t Loss: -0.010070017200301994\nTRAIN: \t Epoch: 97 \t Loss: -0.010146451648324728\nTRAIN: \t Epoch: 97 \t Loss: -0.010294559913185926\nTRAIN: \t Epoch: 97 \t Loss: -0.010372224796031202\nTRAIN: \t Epoch: 97 \t Loss: -0.01032902921239535\nTRAIN: \t Epoch: 97 \t Loss: -0.010219705989584327\nTRAIN: \t Epoch: 97 \t Loss: -0.010234974817756344\nTRAIN: \t Epoch: 97 \t Loss: -0.010324454245467981\nTRAIN: \t Epoch: 97 \t Loss: -0.010311181770122637\nVALD: \t Epoch: 97 \t Loss: -0.013325016014277935\nVALD: \t Epoch: 97 \t Loss: -0.011459631845355034\nVALD: \t Epoch: 97 \t Loss: -0.012392555984357992\nVALD: \t Epoch: 97 \t Loss: -0.012190751964226365\nVALD: \t Epoch: 97 \t Loss: -0.011475062862900664\n******************************\nEpoch: social-tag : 97\ntrain_loss -0.010311181770122637\nval_loss -0.011475062862900664\n{'min_val_epoch': 87, 'min_val_loss': -0.011819003731751244}\n******************************\nTRAIN: \t Epoch: 98 \t Loss: -0.01202214602380991\nTRAIN: \t Epoch: 98 \t Loss: -0.01163105433806777\nTRAIN: \t Epoch: 98 \t Loss: -0.011522239074110985\nTRAIN: \t Epoch: 98 \t Loss: -0.011467842385172844\nTRAIN: \t Epoch: 98 \t Loss: -0.01100467536598444\nTRAIN: \t Epoch: 98 \t Loss: -0.010483515293647846\nTRAIN: \t Epoch: 98 \t Loss: -0.010427064528422696\nTRAIN: \t Epoch: 98 \t Loss: -0.010519248782657087\nTRAIN: \t Epoch: 98 \t Loss: -0.010646865599685244\nTRAIN: \t Epoch: 98 \t Loss: -0.010674506891518831\nTRAIN: \t Epoch: 98 \t Loss: -0.010777215812016617\nTRAIN: \t Epoch: 98 \t Loss: -0.010872966687505444\nTRAIN: \t Epoch: 98 \t Loss: -0.01082992797287611\nTRAIN: \t Epoch: 98 \t Loss: -0.010834439058921166\nTRAIN: \t Epoch: 98 \t Loss: -0.010901900567114353\nTRAIN: \t Epoch: 98 \t Loss: -0.010854479449335486\nTRAIN: \t Epoch: 98 \t Loss: -0.0105875286492793\nTRAIN: \t Epoch: 98 \t Loss: -0.010495372918537922\nTRAIN: \t Epoch: 98 \t Loss: -0.010496900043900316\nVALD: \t Epoch: 98 \t Loss: -0.012971109710633755\nVALD: \t Epoch: 98 \t Loss: -0.011398387607187033\nVALD: \t Epoch: 98 \t Loss: -0.012855357490479946\nVALD: \t Epoch: 98 \t Loss: -0.012861620401963592\nVALD: \t Epoch: 98 \t Loss: -0.012217071036661952\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.59it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.4050175613418685  FDE: 0.6431278559768395\n**************************************************\n******************************\nEpoch: social-tag : 98\ntrain_loss -0.010496900043900316\nval_loss -0.012217071036661952\n{'min_val_epoch': 98, 'min_val_loss': -0.012217071036661952}\n******************************\nTRAIN: \t Epoch: 99 \t Loss: -0.011251648887991905\nTRAIN: \t Epoch: 99 \t Loss: -0.011408149264752865\nTRAIN: \t Epoch: 99 \t Loss: -0.011401824032266935\nTRAIN: \t Epoch: 99 \t Loss: -0.011363952187821269\nTRAIN: \t Epoch: 99 \t Loss: -0.011145325936377048\nTRAIN: \t Epoch: 99 \t Loss: -0.010558223584666848\nTRAIN: \t Epoch: 99 \t Loss: -0.010314673917101962\nTRAIN: \t Epoch: 99 \t Loss: -0.010521438729483634\nTRAIN: \t Epoch: 99 \t Loss: -0.010526741906586621\nTRAIN: \t Epoch: 99 \t Loss: -0.010514853661879897\nTRAIN: \t Epoch: 99 \t Loss: -0.010584750424393198\nTRAIN: \t Epoch: 99 \t Loss: -0.010634849740502736\nTRAIN: \t Epoch: 99 \t Loss: -0.010582471087288398\nTRAIN: \t Epoch: 99 \t Loss: -0.010562417263697301\nTRAIN: \t Epoch: 99 \t Loss: -0.010588872898370027\nTRAIN: \t Epoch: 99 \t Loss: -0.010618096071993932\nTRAIN: \t Epoch: 99 \t Loss: -0.010563935892766012\nTRAIN: \t Epoch: 99 \t Loss: -0.010475165029573772\nTRAIN: \t Epoch: 99 \t Loss: -0.010462984410623965\nVALD: \t Epoch: 99 \t Loss: -0.011794565245509148\nVALD: \t Epoch: 99 \t Loss: -0.010548466816544533\nVALD: \t Epoch: 99 \t Loss: -0.011756388780971369\nVALD: \t Epoch: 99 \t Loss: -0.011783382156863809\nVALD: \t Epoch: 99 \t Loss: -0.011234956043810885\n******************************\nEpoch: social-tag : 99\ntrain_loss -0.010462984410623965\nval_loss -0.011234956043810885\n{'min_val_epoch': 98, 'min_val_loss': -0.012217071036661952}\n******************************\nTRAIN: \t Epoch: 100 \t Loss: -0.010845258831977844\nTRAIN: \t Epoch: 100 \t Loss: -0.011400958988815546\nTRAIN: \t Epoch: 100 \t Loss: -0.011052442404131094\nTRAIN: \t Epoch: 100 \t Loss: -0.0106830142904073\nTRAIN: \t Epoch: 100 \t Loss: -0.010487055405974388\nTRAIN: \t Epoch: 100 \t Loss: -0.010382812159756819\nTRAIN: \t Epoch: 100 \t Loss: -0.01043023741138833\nTRAIN: \t Epoch: 100 \t Loss: -0.01044292002916336\nTRAIN: \t Epoch: 100 \t Loss: -0.01037238124344084\nTRAIN: \t Epoch: 100 \t Loss: -0.010338499676436186\nTRAIN: \t Epoch: 100 \t Loss: -0.010412065481597727\nTRAIN: \t Epoch: 100 \t Loss: -0.010497311285386482\nTRAIN: \t Epoch: 100 \t Loss: -0.01056462898850441\nTRAIN: \t Epoch: 100 \t Loss: -0.010612516597445523\nTRAIN: \t Epoch: 100 \t Loss: -0.010482137960692247\nTRAIN: \t Epoch: 100 \t Loss: -0.01035531476372853\nTRAIN: \t Epoch: 100 \t Loss: -0.010377908125519753\nTRAIN: \t Epoch: 100 \t Loss: -0.010463512916531827\nTRAIN: \t Epoch: 100 \t Loss: -0.010469360676538728\nVALD: \t Epoch: 100 \t Loss: -0.01342347078025341\nVALD: \t Epoch: 100 \t Loss: -0.011703462805598974\nVALD: \t Epoch: 100 \t Loss: -0.013103078119456768\nVALD: \t Epoch: 100 \t Loss: -0.013205436756834388\nVALD: \t Epoch: 100 \t Loss: -0.012403698893617993\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.72it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\nADE: 0.41476730003939005  FDE: 0.7010723127698044\n**************************************************\n******************************\nEpoch: social-tag : 100\ntrain_loss -0.010469360676538728\nval_loss -0.012403698893617993\n{'min_val_epoch': 100, 'min_val_loss': -0.012403698893617993}\n******************************\nTRAIN: \t Epoch: 101 \t Loss: -0.011900641955435276\nTRAIN: \t Epoch: 101 \t Loss: -0.012121692299842834\nTRAIN: \t Epoch: 101 \t Loss: -0.011947011885543665\nTRAIN: \t Epoch: 101 \t Loss: -0.011551881674677134\nTRAIN: \t Epoch: 101 \t Loss: -0.011288321390748025\nTRAIN: \t Epoch: 101 \t Loss: -0.011109018853555122\nTRAIN: \t Epoch: 101 \t Loss: -0.010929311359567302\nTRAIN: \t Epoch: 101 \t Loss: -0.010771337198093534\nTRAIN: \t Epoch: 101 \t Loss: -0.010840540234413411\nTRAIN: \t Epoch: 101 \t Loss: -0.010826178174465895\nTRAIN: \t Epoch: 101 \t Loss: -0.010662476532161236\nTRAIN: \t Epoch: 101 \t Loss: -0.010571384414409598\nTRAIN: \t Epoch: 101 \t Loss: -0.010639137015319787\nTRAIN: \t Epoch: 101 \t Loss: -0.010678026559097427\nTRAIN: \t Epoch: 101 \t Loss: -0.010697969732185205\nTRAIN: \t Epoch: 101 \t Loss: -0.010670991730876267\nTRAIN: \t Epoch: 101 \t Loss: -0.010599256164449103\nTRAIN: \t Epoch: 101 \t Loss: -0.01053935568779707\nTRAIN: \t Epoch: 101 \t Loss: -0.010537586522246106\nVALD: \t Epoch: 101 \t Loss: -0.01234879344701767\nVALD: \t Epoch: 101 \t Loss: -0.010456677991896868\nVALD: \t Epoch: 101 \t Loss: -0.011711136437952518\nVALD: \t Epoch: 101 \t Loss: -0.01182209444232285\nVALD: \t Epoch: 101 \t Loss: -0.011158921501853249\n******************************\nEpoch: social-tag : 101\ntrain_loss -0.010537586522246106\nval_loss -0.011158921501853249\n{'min_val_epoch': 100, 'min_val_loss': -0.012403698893617993}\n******************************\nTRAIN: \t Epoch: 102 \t Loss: -0.011644328013062477\nTRAIN: \t Epoch: 102 \t Loss: -0.011615914292633533\nTRAIN: \t Epoch: 102 \t Loss: -0.011349118004242579\nTRAIN: \t Epoch: 102 \t Loss: -0.01085295295342803\nTRAIN: \t Epoch: 102 \t Loss: -0.010598954372107982\nTRAIN: \t Epoch: 102 \t Loss: -0.010724748795231184\nTRAIN: \t Epoch: 102 \t Loss: -0.010768884792923927\nTRAIN: \t Epoch: 102 \t Loss: -0.010834814980626106\nTRAIN: \t Epoch: 102 \t Loss: -0.01085367084791263\nTRAIN: \t Epoch: 102 \t Loss: -0.010714745614677668\nTRAIN: \t Epoch: 102 \t Loss: -0.01047744940627705\nTRAIN: \t Epoch: 102 \t Loss: -0.01058582728728652\nTRAIN: \t Epoch: 102 \t Loss: -0.010641466682920089\nTRAIN: \t Epoch: 102 \t Loss: -0.010639862756111793\nTRAIN: \t Epoch: 102 \t Loss: -0.010634297070403894\nTRAIN: \t Epoch: 102 \t Loss: -0.010657337785232812\nTRAIN: \t Epoch: 102 \t Loss: -0.010615897419698098\nTRAIN: \t Epoch: 102 \t Loss: -0.01056491035140223\nTRAIN: \t Epoch: 102 \t Loss: -0.010552706837038045\nVALD: \t Epoch: 102 \t Loss: -0.012216229923069477\nVALD: \t Epoch: 102 \t Loss: -0.010692857671529055\nVALD: \t Epoch: 102 \t Loss: -0.011830018212397894\nVALD: \t Epoch: 102 \t Loss: -0.011941335629671812\nVALD: \t Epoch: 102 \t Loss: -0.011398502814868264\n******************************\nEpoch: social-tag : 102\ntrain_loss -0.010552706837038045\nval_loss -0.011398502814868264\n{'min_val_epoch': 100, 'min_val_loss': -0.012403698893617993}\n******************************\nTRAIN: \t Epoch: 103 \t Loss: -0.010857122018933296\nTRAIN: \t Epoch: 103 \t Loss: -0.011534960009157658\nTRAIN: \t Epoch: 103 \t Loss: -0.011164075384537378\nTRAIN: \t Epoch: 103 \t Loss: -0.010810169856995344\nTRAIN: \t Epoch: 103 \t Loss: -0.010886156372725964\nTRAIN: \t Epoch: 103 \t Loss: -0.010922964041431745\nTRAIN: \t Epoch: 103 \t Loss: -0.010840934568217822\nTRAIN: \t Epoch: 103 \t Loss: -0.010698514175601304\nTRAIN: \t Epoch: 103 \t Loss: -0.01068471134122875\nTRAIN: \t Epoch: 103 \t Loss: -0.010721032600849867\nTRAIN: \t Epoch: 103 \t Loss: -0.010701542042873123\nTRAIN: \t Epoch: 103 \t Loss: -0.010637710569426417\nTRAIN: \t Epoch: 103 \t Loss: -0.010588104765002545\nTRAIN: \t Epoch: 103 \t Loss: -0.010665922691779477\nTRAIN: \t Epoch: 103 \t Loss: -0.010630080414315065\nTRAIN: \t Epoch: 103 \t Loss: -0.010623980138916522\nTRAIN: \t Epoch: 103 \t Loss: -0.010613880284568843\nTRAIN: \t Epoch: 103 \t Loss: -0.01060072063571877\nTRAIN: \t Epoch: 103 \t Loss: -0.010577702861115605\nVALD: \t Epoch: 103 \t Loss: -0.011761787347495556\nVALD: \t Epoch: 103 \t Loss: -0.010655301623046398\nVALD: \t Epoch: 103 \t Loss: -0.01188105313728253\nVALD: \t Epoch: 103 \t Loss: -0.011923930142074823\nVALD: \t Epoch: 103 \t Loss: -0.011425152396367601\n******************************\nEpoch: social-tag : 103\ntrain_loss -0.010577702861115605\nval_loss -0.011425152396367601\n{'min_val_epoch': 100, 'min_val_loss': -0.012403698893617993}\n******************************\nTRAIN: \t Epoch: 104 \t Loss: -0.010697016492486\nTRAIN: \t Epoch: 104 \t Loss: -0.010591693222522736\nTRAIN: \t Epoch: 104 \t Loss: -0.010091539472341537\nTRAIN: \t Epoch: 104 \t Loss: -0.010362710105255246\nTRAIN: \t Epoch: 104 \t Loss: -0.01064971461892128\nTRAIN: \t Epoch: 104 \t Loss: -0.010778557509183884\nTRAIN: \t Epoch: 104 \t Loss: -0.01073253939726523\nTRAIN: \t Epoch: 104 \t Loss: -0.01073341234587133\nTRAIN: \t Epoch: 104 \t Loss: -0.010812674131658342\nTRAIN: \t Epoch: 104 \t Loss: -0.01076487060636282\nTRAIN: \t Epoch: 104 \t Loss: -0.010605553673072294\nTRAIN: \t Epoch: 104 \t Loss: -0.010560723021626472\nTRAIN: \t Epoch: 104 \t Loss: -0.010611984950418655\nTRAIN: \t Epoch: 104 \t Loss: -0.010673603841236659\nTRAIN: \t Epoch: 104 \t Loss: -0.01048520461966594\nTRAIN: \t Epoch: 104 \t Loss: -0.010321722482331097\nTRAIN: \t Epoch: 104 \t Loss: -0.010368069907759918\nTRAIN: \t Epoch: 104 \t Loss: -0.010541146310667196\nTRAIN: \t Epoch: 104 \t Loss: -0.010541032797606417\nVALD: \t Epoch: 104 \t Loss: -0.014017045497894287\nVALD: \t Epoch: 104 \t Loss: -0.011737137101590633\nVALD: \t Epoch: 104 \t Loss: -0.013204635431369146\nVALD: \t Epoch: 104 \t Loss: -0.013228291412815452\nVALD: \t Epoch: 104 \t Loss: -0.012391305856468264\n******************************\nEpoch: social-tag : 104\ntrain_loss -0.010541032797606417\nval_loss -0.012391305856468264\n{'min_val_epoch': 100, 'min_val_loss': -0.012403698893617993}\n******************************\nTRAIN: \t Epoch: 105 \t Loss: -0.012456144206225872\nTRAIN: \t Epoch: 105 \t Loss: -0.011605116538703442\nTRAIN: \t Epoch: 105 \t Loss: -0.010793328595658144\nTRAIN: \t Epoch: 105 \t Loss: -0.010813585249707103\nTRAIN: \t Epoch: 105 \t Loss: -0.010655108839273453\nTRAIN: \t Epoch: 105 \t Loss: -0.01047853489095966\nTRAIN: \t Epoch: 105 \t Loss: -0.010631593875586987\nTRAIN: \t Epoch: 105 \t Loss: -0.01078454521484673\nTRAIN: \t Epoch: 105 \t Loss: -0.010493117074171701\nTRAIN: \t Epoch: 105 \t Loss: -0.01026244992390275\nTRAIN: \t Epoch: 105 \t Loss: -0.01025147261944684\nTRAIN: \t Epoch: 105 \t Loss: -0.010391903265068928\nTRAIN: \t Epoch: 105 \t Loss: -0.010449648834764957\nTRAIN: \t Epoch: 105 \t Loss: -0.010564668569713831\nTRAIN: \t Epoch: 105 \t Loss: -0.010613544409473737\nTRAIN: \t Epoch: 105 \t Loss: -0.010550017119385302\nTRAIN: \t Epoch: 105 \t Loss: -0.010442421442883857\nTRAIN: \t Epoch: 105 \t Loss: -0.010474776538709799\nTRAIN: \t Epoch: 105 \t Loss: -0.010495469256478276\nVALD: \t Epoch: 105 \t Loss: -0.013678519986569881\nVALD: \t Epoch: 105 \t Loss: -0.01186956325545907\nVALD: \t Epoch: 105 \t Loss: -0.013397681526839733\nVALD: \t Epoch: 105 \t Loss: -0.013395294081419706\nVALD: \t Epoch: 105 \t Loss: -0.012577920511734387\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.70it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.3840072195778556  FDE: 0.6029902173687633\n**************************************************\n******************************\nEpoch: social-tag : 105\ntrain_loss -0.010495469256478276\nval_loss -0.012577920511734387\n{'min_val_epoch': 105, 'min_val_loss': -0.012577920511734387}\n******************************\nTRAIN: \t Epoch: 106 \t Loss: -0.012475613504648209\nTRAIN: \t Epoch: 106 \t Loss: -0.011915752198547125\nTRAIN: \t Epoch: 106 \t Loss: -0.0117079708725214\nTRAIN: \t Epoch: 106 \t Loss: -0.01171873020939529\nTRAIN: \t Epoch: 106 \t Loss: -0.011804288998246194\nTRAIN: \t Epoch: 106 \t Loss: -0.011498464737087488\nTRAIN: \t Epoch: 106 \t Loss: -0.011276561234678541\nTRAIN: \t Epoch: 106 \t Loss: -0.011239661718718708\nTRAIN: \t Epoch: 106 \t Loss: -0.011348865098423429\nTRAIN: \t Epoch: 106 \t Loss: -0.011279534269124269\nTRAIN: \t Epoch: 106 \t Loss: -0.01111094035546888\nTRAIN: \t Epoch: 106 \t Loss: -0.010895071473593513\nTRAIN: \t Epoch: 106 \t Loss: -0.010969445754129153\nTRAIN: \t Epoch: 106 \t Loss: -0.011065144357936723\nTRAIN: \t Epoch: 106 \t Loss: -0.011060285133620102\nTRAIN: \t Epoch: 106 \t Loss: -0.011038341850508004\nTRAIN: \t Epoch: 106 \t Loss: -0.010972196126685423\nTRAIN: \t Epoch: 106 \t Loss: -0.010958707529223628\nTRAIN: \t Epoch: 106 \t Loss: -0.010954048705963509\nVALD: \t Epoch: 106 \t Loss: -0.01361511554569006\nVALD: \t Epoch: 106 \t Loss: -0.011726218741387129\nVALD: \t Epoch: 106 \t Loss: -0.013079207700987657\nVALD: \t Epoch: 106 \t Loss: -0.013012896059080958\nVALD: \t Epoch: 106 \t Loss: -0.012378773708974035\n******************************\nEpoch: social-tag : 106\ntrain_loss -0.010954048705963509\nval_loss -0.012378773708974035\n{'min_val_epoch': 105, 'min_val_loss': -0.012577920511734387}\n******************************\nTRAIN: \t Epoch: 107 \t Loss: -0.01095308642834425\nTRAIN: \t Epoch: 107 \t Loss: -0.011470792349427938\nTRAIN: \t Epoch: 107 \t Loss: -0.011576896843810877\nTRAIN: \t Epoch: 107 \t Loss: -0.011489361990243196\nTRAIN: \t Epoch: 107 \t Loss: -0.011281945928931236\nTRAIN: \t Epoch: 107 \t Loss: -0.011056126716236273\nTRAIN: \t Epoch: 107 \t Loss: -0.0109108014564429\nTRAIN: \t Epoch: 107 \t Loss: -0.011005245731212199\nTRAIN: \t Epoch: 107 \t Loss: -0.01112370017088122\nTRAIN: \t Epoch: 107 \t Loss: -0.01114169042557478\nTRAIN: \t Epoch: 107 \t Loss: -0.010994211601262743\nTRAIN: \t Epoch: 107 \t Loss: -0.010722884869513413\nTRAIN: \t Epoch: 107 \t Loss: -0.010634289875340004\nTRAIN: \t Epoch: 107 \t Loss: -0.010727831994050316\nTRAIN: \t Epoch: 107 \t Loss: -0.010815704458703597\nTRAIN: \t Epoch: 107 \t Loss: -0.010783855657791719\nTRAIN: \t Epoch: 107 \t Loss: -0.010739732122815707\nTRAIN: \t Epoch: 107 \t Loss: -0.010744737901000513\nTRAIN: \t Epoch: 107 \t Loss: -0.010748087872111725\nVALD: \t Epoch: 107 \t Loss: -0.013756603933870792\nVALD: \t Epoch: 107 \t Loss: -0.011459834408015013\nVALD: \t Epoch: 107 \t Loss: -0.01284884630391995\nVALD: \t Epoch: 107 \t Loss: -0.012862532865256071\nVALD: \t Epoch: 107 \t Loss: -0.012147828567126566\n******************************\nEpoch: social-tag : 107\ntrain_loss -0.010748087872111725\nval_loss -0.012147828567126566\n{'min_val_epoch': 105, 'min_val_loss': -0.012577920511734387}\n******************************\nTRAIN: \t Epoch: 108 \t Loss: -0.011053940281271935\nTRAIN: \t Epoch: 108 \t Loss: -0.011261222884058952\nTRAIN: \t Epoch: 108 \t Loss: -0.011402683022121588\nTRAIN: \t Epoch: 108 \t Loss: -0.010980518301948905\nTRAIN: \t Epoch: 108 \t Loss: -0.010507765412330627\nTRAIN: \t Epoch: 108 \t Loss: -0.010558107712616524\nTRAIN: \t Epoch: 108 \t Loss: -0.010742745229176112\nTRAIN: \t Epoch: 108 \t Loss: -0.010841587092727423\nTRAIN: \t Epoch: 108 \t Loss: -0.010948560511072477\nTRAIN: \t Epoch: 108 \t Loss: -0.010852411948144436\nTRAIN: \t Epoch: 108 \t Loss: -0.010816635563969612\nTRAIN: \t Epoch: 108 \t Loss: -0.010758904817824563\nTRAIN: \t Epoch: 108 \t Loss: -0.010875971414721929\nTRAIN: \t Epoch: 108 \t Loss: -0.010957425726311547\nTRAIN: \t Epoch: 108 \t Loss: -0.010957816056907177\nTRAIN: \t Epoch: 108 \t Loss: -0.01093245402444154\nTRAIN: \t Epoch: 108 \t Loss: -0.010858000803957968\nTRAIN: \t Epoch: 108 \t Loss: -0.010807028040289879\nTRAIN: \t Epoch: 108 \t Loss: -0.010803322125672267\nVALD: \t Epoch: 108 \t Loss: -0.01199869904667139\nVALD: \t Epoch: 108 \t Loss: -0.010436468292027712\nVALD: \t Epoch: 108 \t Loss: -0.011460480901102224\nVALD: \t Epoch: 108 \t Loss: -0.011541304411366582\nVALD: \t Epoch: 108 \t Loss: -0.010992837740370064\n******************************\nEpoch: social-tag : 108\ntrain_loss -0.010803322125672267\nval_loss -0.010992837740370064\n{'min_val_epoch': 105, 'min_val_loss': -0.012577920511734387}\n******************************\nTRAIN: \t Epoch: 109 \t Loss: -0.011814102530479431\nTRAIN: \t Epoch: 109 \t Loss: -0.011362782679498196\nTRAIN: \t Epoch: 109 \t Loss: -0.01092584989964962\nTRAIN: \t Epoch: 109 \t Loss: -0.010803094832226634\nTRAIN: \t Epoch: 109 \t Loss: -0.010763424634933471\nTRAIN: \t Epoch: 109 \t Loss: -0.010901853442192078\nTRAIN: \t Epoch: 109 \t Loss: -0.010853790012853486\nTRAIN: \t Epoch: 109 \t Loss: -0.010697684017941356\nTRAIN: \t Epoch: 109 \t Loss: -0.010654790326952934\nTRAIN: \t Epoch: 109 \t Loss: -0.010713427886366844\nTRAIN: \t Epoch: 109 \t Loss: -0.010727477513931015\nTRAIN: \t Epoch: 109 \t Loss: -0.010725495172664523\nTRAIN: \t Epoch: 109 \t Loss: -0.010672622790130286\nTRAIN: \t Epoch: 109 \t Loss: -0.010649841145745345\nTRAIN: \t Epoch: 109 \t Loss: -0.010593970368305842\nTRAIN: \t Epoch: 109 \t Loss: -0.010614015394821763\nTRAIN: \t Epoch: 109 \t Loss: -0.010643417180022773\nTRAIN: \t Epoch: 109 \t Loss: -0.010679495624370046\nTRAIN: \t Epoch: 109 \t Loss: -0.010696294490475782\nVALD: \t Epoch: 109 \t Loss: -0.011730301193892956\nVALD: \t Epoch: 109 \t Loss: -0.009567026514559984\nVALD: \t Epoch: 109 \t Loss: -0.010984459891915321\nVALD: \t Epoch: 109 \t Loss: -0.01114002033136785\nVALD: \t Epoch: 109 \t Loss: -0.010477190175332314\n******************************\nEpoch: social-tag : 109\ntrain_loss -0.010696294490475782\nval_loss -0.010477190175332314\n{'min_val_epoch': 105, 'min_val_loss': -0.012577920511734387}\n******************************\nTRAIN: \t Epoch: 110 \t Loss: -0.011112119071185589\nTRAIN: \t Epoch: 110 \t Loss: -0.010806924663484097\nTRAIN: \t Epoch: 110 \t Loss: -0.01026748027652502\nTRAIN: \t Epoch: 110 \t Loss: -0.01021472318097949\nTRAIN: \t Epoch: 110 \t Loss: -0.010127176158130169\nTRAIN: \t Epoch: 110 \t Loss: -0.010371761706968149\nTRAIN: \t Epoch: 110 \t Loss: -0.010444854918335165\nTRAIN: \t Epoch: 110 \t Loss: -0.010498781106434762\nTRAIN: \t Epoch: 110 \t Loss: -0.010531828842229314\nTRAIN: \t Epoch: 110 \t Loss: -0.010394287668168545\nTRAIN: \t Epoch: 110 \t Loss: -0.010330271737819368\nTRAIN: \t Epoch: 110 \t Loss: -0.010418600790823499\nTRAIN: \t Epoch: 110 \t Loss: -0.010534371392658124\nTRAIN: \t Epoch: 110 \t Loss: -0.010626628529280424\nTRAIN: \t Epoch: 110 \t Loss: -0.01056258666018645\nTRAIN: \t Epoch: 110 \t Loss: -0.010500075004529208\nTRAIN: \t Epoch: 110 \t Loss: -0.01051644904210287\nTRAIN: \t Epoch: 110 \t Loss: -0.010518697193927236\nTRAIN: \t Epoch: 110 \t Loss: -0.010522430565314083\nVALD: \t Epoch: 110 \t Loss: -0.012654965743422508\nVALD: \t Epoch: 110 \t Loss: -0.011179899796843529\nVALD: \t Epoch: 110 \t Loss: -0.012258462297419706\nVALD: \t Epoch: 110 \t Loss: -0.012363152811303735\nVALD: \t Epoch: 110 \t Loss: -0.011766875085751872\n******************************\nEpoch: social-tag : 110\ntrain_loss -0.010522430565314083\nval_loss -0.011766875085751872\n{'min_val_epoch': 105, 'min_val_loss': -0.012577920511734387}\n******************************\nTRAIN: \t Epoch: 111 \t Loss: -0.011141631752252579\nTRAIN: \t Epoch: 111 \t Loss: -0.011061605997383595\nTRAIN: \t Epoch: 111 \t Loss: -0.010494045292337736\nTRAIN: \t Epoch: 111 \t Loss: -0.01030353270471096\nTRAIN: \t Epoch: 111 \t Loss: -0.010466411337256431\nTRAIN: \t Epoch: 111 \t Loss: -0.010643621130536\nTRAIN: \t Epoch: 111 \t Loss: -0.010630540948893343\nTRAIN: \t Epoch: 111 \t Loss: -0.010764239937998354\nTRAIN: \t Epoch: 111 \t Loss: -0.010710499663319852\nTRAIN: \t Epoch: 111 \t Loss: -0.010550051927566528\nTRAIN: \t Epoch: 111 \t Loss: -0.010546369139443745\nTRAIN: \t Epoch: 111 \t Loss: -0.0106636598551025\nTRAIN: \t Epoch: 111 \t Loss: -0.010712956364911336\nTRAIN: \t Epoch: 111 \t Loss: -0.010761310878608907\nTRAIN: \t Epoch: 111 \t Loss: -0.010724835035701593\nTRAIN: \t Epoch: 111 \t Loss: -0.01070833666017279\nTRAIN: \t Epoch: 111 \t Loss: -0.01076077313765007\nTRAIN: \t Epoch: 111 \t Loss: -0.010800474426812597\nTRAIN: \t Epoch: 111 \t Loss: -0.010784995802305157\nVALD: \t Epoch: 111 \t Loss: -0.012485964223742485\nVALD: \t Epoch: 111 \t Loss: -0.010173856746405363\nVALD: \t Epoch: 111 \t Loss: -0.0112224988018473\nVALD: \t Epoch: 111 \t Loss: -0.011218324769288301\nVALD: \t Epoch: 111 \t Loss: -0.010589573304515239\n******************************\nEpoch: social-tag : 111\ntrain_loss -0.010784995802305157\nval_loss -0.010589573304515239\n{'min_val_epoch': 105, 'min_val_loss': -0.012577920511734387}\n******************************\nTRAIN: \t Epoch: 112 \t Loss: -0.010291773825883865\nTRAIN: \t Epoch: 112 \t Loss: -0.010743049439042807\nTRAIN: \t Epoch: 112 \t Loss: -0.010907946154475212\nTRAIN: \t Epoch: 112 \t Loss: -0.010784820420667529\nTRAIN: \t Epoch: 112 \t Loss: -0.010678987205028533\nTRAIN: \t Epoch: 112 \t Loss: -0.010731279384344816\nTRAIN: \t Epoch: 112 \t Loss: -0.010803624854556151\nTRAIN: \t Epoch: 112 \t Loss: -0.010908967815339565\nTRAIN: \t Epoch: 112 \t Loss: -0.01079360105925136\nTRAIN: \t Epoch: 112 \t Loss: -0.010623177234083413\nTRAIN: \t Epoch: 112 \t Loss: -0.01062017780813304\nTRAIN: \t Epoch: 112 \t Loss: -0.01083559178126355\nTRAIN: \t Epoch: 112 \t Loss: -0.010915277310861992\nTRAIN: \t Epoch: 112 \t Loss: -0.010772079894585269\nTRAIN: \t Epoch: 112 \t Loss: -0.01065931674093008\nTRAIN: \t Epoch: 112 \t Loss: -0.010701087478082627\nTRAIN: \t Epoch: 112 \t Loss: -0.010853157269165796\nTRAIN: \t Epoch: 112 \t Loss: -0.010912472692628702\nTRAIN: \t Epoch: 112 \t Loss: -0.010902663206715301\nVALD: \t Epoch: 112 \t Loss: -0.011066688224673271\nVALD: \t Epoch: 112 \t Loss: -0.009410525672137737\nVALD: \t Epoch: 112 \t Loss: -0.010934567078948021\nVALD: \t Epoch: 112 \t Loss: -0.010962458094581962\nVALD: \t Epoch: 112 \t Loss: -0.010311383944897612\n******************************\nEpoch: social-tag : 112\ntrain_loss -0.010902663206715301\nval_loss -0.010311383944897612\n{'min_val_epoch': 105, 'min_val_loss': -0.012577920511734387}\n******************************\nTRAIN: \t Epoch: 113 \t Loss: -0.010640806518495083\nTRAIN: \t Epoch: 113 \t Loss: -0.01102826977148652\nTRAIN: \t Epoch: 113 \t Loss: -0.010946631121138731\nTRAIN: \t Epoch: 113 \t Loss: -0.01092225918546319\nTRAIN: \t Epoch: 113 \t Loss: -0.010862683691084386\nTRAIN: \t Epoch: 113 \t Loss: -0.010730402388920387\nTRAIN: \t Epoch: 113 \t Loss: -0.010767158520008837\nTRAIN: \t Epoch: 113 \t Loss: -0.010870027355849743\nTRAIN: \t Epoch: 113 \t Loss: -0.01097035739156935\nTRAIN: \t Epoch: 113 \t Loss: -0.010918679181486368\nTRAIN: \t Epoch: 113 \t Loss: -0.010877959514883432\nTRAIN: \t Epoch: 113 \t Loss: -0.010823098011314869\nTRAIN: \t Epoch: 113 \t Loss: -0.01085823280020402\nTRAIN: \t Epoch: 113 \t Loss: -0.010922987014055252\nTRAIN: \t Epoch: 113 \t Loss: -0.010936093827088674\nTRAIN: \t Epoch: 113 \t Loss: -0.01081750652519986\nTRAIN: \t Epoch: 113 \t Loss: -0.010820465819800602\nTRAIN: \t Epoch: 113 \t Loss: -0.010859705352534851\nTRAIN: \t Epoch: 113 \t Loss: -0.010877874239470607\nVALD: \t Epoch: 113 \t Loss: -0.014018495567142963\nVALD: \t Epoch: 113 \t Loss: -0.012262911070138216\nVALD: \t Epoch: 113 \t Loss: -0.01374299731105566\nVALD: \t Epoch: 113 \t Loss: -0.013588751433417201\nVALD: \t Epoch: 113 \t Loss: -0.012882212075320157\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.77it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.3719329615179839  FDE: 0.5644832859780314\n**************************************************\n******************************\nEpoch: social-tag : 113\ntrain_loss -0.010877874239470607\nval_loss -0.012882212075320157\n{'min_val_epoch': 113, 'min_val_loss': -0.012882212075320157}\n******************************\nTRAIN: \t Epoch: 114 \t Loss: -0.012196522206068039\nTRAIN: \t Epoch: 114 \t Loss: -0.011937827803194523\nTRAIN: \t Epoch: 114 \t Loss: -0.011231958866119385\nTRAIN: \t Epoch: 114 \t Loss: -0.010763156227767467\nTRAIN: \t Epoch: 114 \t Loss: -0.010810712911188602\nTRAIN: \t Epoch: 114 \t Loss: -0.010944208906342586\nTRAIN: \t Epoch: 114 \t Loss: -0.01072119056646313\nTRAIN: \t Epoch: 114 \t Loss: -0.010632896213792264\nTRAIN: \t Epoch: 114 \t Loss: -0.010645981567601362\nTRAIN: \t Epoch: 114 \t Loss: -0.010684381611645222\nTRAIN: \t Epoch: 114 \t Loss: -0.010772395286370407\nTRAIN: \t Epoch: 114 \t Loss: -0.010818534065037966\nTRAIN: \t Epoch: 114 \t Loss: -0.010616144881798672\nTRAIN: \t Epoch: 114 \t Loss: -0.010642871393689088\nTRAIN: \t Epoch: 114 \t Loss: -0.010700482750932376\nTRAIN: \t Epoch: 114 \t Loss: -0.010775166971143335\nTRAIN: \t Epoch: 114 \t Loss: -0.010792878401630065\nTRAIN: \t Epoch: 114 \t Loss: -0.010833087842911482\nTRAIN: \t Epoch: 114 \t Loss: -0.010828684967309621\nVALD: \t Epoch: 114 \t Loss: -0.012884264811873436\nVALD: \t Epoch: 114 \t Loss: -0.010933128651231527\nVALD: \t Epoch: 114 \t Loss: -0.012505115009844303\nVALD: \t Epoch: 114 \t Loss: -0.012602182570844889\nVALD: \t Epoch: 114 \t Loss: -0.01182427524535124\n******************************\nEpoch: social-tag : 114\ntrain_loss -0.010828684967309621\nval_loss -0.01182427524535124\n{'min_val_epoch': 113, 'min_val_loss': -0.012882212075320157}\n******************************\nTRAIN: \t Epoch: 115 \t Loss: -0.012245666235685349\nTRAIN: \t Epoch: 115 \t Loss: -0.011785530019551516\nTRAIN: \t Epoch: 115 \t Loss: -0.011883105772236982\nTRAIN: \t Epoch: 115 \t Loss: -0.011680482421070337\nTRAIN: \t Epoch: 115 \t Loss: -0.010973438806831836\nTRAIN: \t Epoch: 115 \t Loss: -0.010677068183819452\nTRAIN: \t Epoch: 115 \t Loss: -0.01092324179730245\nTRAIN: \t Epoch: 115 \t Loss: -0.011016911128535867\nTRAIN: \t Epoch: 115 \t Loss: -0.01092741752250327\nTRAIN: \t Epoch: 115 \t Loss: -0.011058992892503738\nTRAIN: \t Epoch: 115 \t Loss: -0.01113864000548016\nTRAIN: \t Epoch: 115 \t Loss: -0.011157861057048043\nTRAIN: \t Epoch: 115 \t Loss: -0.011068064647798356\nTRAIN: \t Epoch: 115 \t Loss: -0.011018549025590931\nTRAIN: \t Epoch: 115 \t Loss: -0.010947167190412681\nTRAIN: \t Epoch: 115 \t Loss: -0.010965246183332056\nTRAIN: \t Epoch: 115 \t Loss: -0.01099485021961086\nTRAIN: \t Epoch: 115 \t Loss: -0.010915185945729414\nTRAIN: \t Epoch: 115 \t Loss: -0.010888338384188834\nVALD: \t Epoch: 115 \t Loss: -0.009440924040973186\nVALD: \t Epoch: 115 \t Loss: -0.0076758177019655704\nVALD: \t Epoch: 115 \t Loss: -0.008377057500183582\nVALD: \t Epoch: 115 \t Loss: -0.008452467620372772\nVALD: \t Epoch: 115 \t Loss: -0.008087085495310382\n******************************\nEpoch: social-tag : 115\ntrain_loss -0.010888338384188834\nval_loss -0.008087085495310382\n{'min_val_epoch': 113, 'min_val_loss': -0.012882212075320157}\n******************************\nTRAIN: \t Epoch: 116 \t Loss: -0.009136428125202656\nTRAIN: \t Epoch: 116 \t Loss: -0.010049563366919756\nTRAIN: \t Epoch: 116 \t Loss: -0.010818676402171453\nTRAIN: \t Epoch: 116 \t Loss: -0.010963339358568192\nTRAIN: \t Epoch: 116 \t Loss: -0.011196448281407357\nTRAIN: \t Epoch: 116 \t Loss: -0.011161851851890484\nTRAIN: \t Epoch: 116 \t Loss: -0.011161917820572853\nTRAIN: \t Epoch: 116 \t Loss: -0.011185588780790567\nTRAIN: \t Epoch: 116 \t Loss: -0.011219046492543485\nTRAIN: \t Epoch: 116 \t Loss: -0.011116668116301298\nTRAIN: \t Epoch: 116 \t Loss: -0.011108108372850851\nTRAIN: \t Epoch: 116 \t Loss: -0.011106786473343769\nTRAIN: \t Epoch: 116 \t Loss: -0.01109355680931073\nTRAIN: \t Epoch: 116 \t Loss: -0.011064224543848209\nTRAIN: \t Epoch: 116 \t Loss: -0.011087871094544729\nTRAIN: \t Epoch: 116 \t Loss: -0.01117571140639484\nTRAIN: \t Epoch: 116 \t Loss: -0.01107888767386184\nTRAIN: \t Epoch: 116 \t Loss: -0.010964277459101545\nTRAIN: \t Epoch: 116 \t Loss: -0.01095288031246003\nVALD: \t Epoch: 116 \t Loss: -0.011883504688739777\nVALD: \t Epoch: 116 \t Loss: -0.010449351742863655\nVALD: \t Epoch: 116 \t Loss: -0.011856915118793646\nVALD: \t Epoch: 116 \t Loss: -0.012009416706860065\nVALD: \t Epoch: 116 \t Loss: -0.011410786100655547\n******************************\nEpoch: social-tag : 116\ntrain_loss -0.01095288031246003\nval_loss -0.011410786100655547\n{'min_val_epoch': 113, 'min_val_loss': -0.012882212075320157}\n******************************\nTRAIN: \t Epoch: 117 \t Loss: -0.010884272865951061\nTRAIN: \t Epoch: 117 \t Loss: -0.011046285275369883\nTRAIN: \t Epoch: 117 \t Loss: -0.011219485041995844\nTRAIN: \t Epoch: 117 \t Loss: -0.011321131605654955\nTRAIN: \t Epoch: 117 \t Loss: -0.01104749869555235\nTRAIN: \t Epoch: 117 \t Loss: -0.011006345506757498\nTRAIN: \t Epoch: 117 \t Loss: -0.011080678951527392\nTRAIN: \t Epoch: 117 \t Loss: -0.01102432666812092\nTRAIN: \t Epoch: 117 \t Loss: -0.010778641431695886\nTRAIN: \t Epoch: 117 \t Loss: -0.010759587399661541\nTRAIN: \t Epoch: 117 \t Loss: -0.010865250179036097\nTRAIN: \t Epoch: 117 \t Loss: -0.010983894268671671\nTRAIN: \t Epoch: 117 \t Loss: -0.011067043345134992\nTRAIN: \t Epoch: 117 \t Loss: -0.011041119096002408\nTRAIN: \t Epoch: 117 \t Loss: -0.010978332658608755\nTRAIN: \t Epoch: 117 \t Loss: -0.010884307324886322\nTRAIN: \t Epoch: 117 \t Loss: -0.010917299427092075\nTRAIN: \t Epoch: 117 \t Loss: -0.010990285366359685\nTRAIN: \t Epoch: 117 \t Loss: -0.01100101933512166\nVALD: \t Epoch: 117 \t Loss: -0.013500946573913097\nVALD: \t Epoch: 117 \t Loss: -0.011699456255882978\nVALD: \t Epoch: 117 \t Loss: -0.013337940288086733\nVALD: \t Epoch: 117 \t Loss: -0.013348749605938792\nVALD: \t Epoch: 117 \t Loss: -0.0125546409078866\n******************************\nEpoch: social-tag : 117\ntrain_loss -0.01100101933512166\nval_loss -0.0125546409078866\n{'min_val_epoch': 113, 'min_val_loss': -0.012882212075320157}\n******************************\nTRAIN: \t Epoch: 118 \t Loss: -0.011614194139838219\nTRAIN: \t Epoch: 118 \t Loss: -0.012140325270593166\nTRAIN: \t Epoch: 118 \t Loss: -0.011544091627001762\nTRAIN: \t Epoch: 118 \t Loss: -0.011192359728738666\nTRAIN: \t Epoch: 118 \t Loss: -0.011074665561318397\nTRAIN: \t Epoch: 118 \t Loss: -0.010994068657358488\nTRAIN: \t Epoch: 118 \t Loss: -0.011071460986776012\nTRAIN: \t Epoch: 118 \t Loss: -0.01096778002101928\nTRAIN: \t Epoch: 118 \t Loss: -0.01074582390073273\nTRAIN: \t Epoch: 118 \t Loss: -0.010767191741615533\nTRAIN: \t Epoch: 118 \t Loss: -0.01082752018489621\nTRAIN: \t Epoch: 118 \t Loss: -0.010927113859603802\nTRAIN: \t Epoch: 118 \t Loss: -0.010984443199749176\nTRAIN: \t Epoch: 118 \t Loss: -0.01100160866709692\nTRAIN: \t Epoch: 118 \t Loss: -0.010952540983756384\nTRAIN: \t Epoch: 118 \t Loss: -0.010892206220887601\nTRAIN: \t Epoch: 118 \t Loss: -0.0109171696993358\nTRAIN: \t Epoch: 118 \t Loss: -0.010974603549887737\nTRAIN: \t Epoch: 118 \t Loss: -0.010973757006144544\nVALD: \t Epoch: 118 \t Loss: -0.012989761307835579\nVALD: \t Epoch: 118 \t Loss: -0.01112186349928379\nVALD: \t Epoch: 118 \t Loss: -0.012614799352983633\nVALD: \t Epoch: 118 \t Loss: -0.012663223315030336\nVALD: \t Epoch: 118 \t Loss: -0.011959958470557346\n******************************\nEpoch: social-tag : 118\ntrain_loss -0.010973757006144544\nval_loss -0.011959958470557346\n{'min_val_epoch': 113, 'min_val_loss': -0.012882212075320157}\n******************************\nTRAIN: \t Epoch: 119 \t Loss: -0.011968457140028477\nTRAIN: \t Epoch: 119 \t Loss: -0.01187847275286913\nTRAIN: \t Epoch: 119 \t Loss: -0.011793207998077074\nTRAIN: \t Epoch: 119 \t Loss: -0.011665676021948457\nTRAIN: \t Epoch: 119 \t Loss: -0.011501538567245007\nTRAIN: \t Epoch: 119 \t Loss: -0.011461901323248943\nTRAIN: \t Epoch: 119 \t Loss: -0.011352895093815667\nTRAIN: \t Epoch: 119 \t Loss: -0.011187378782778978\nTRAIN: \t Epoch: 119 \t Loss: -0.01122758723795414\nTRAIN: \t Epoch: 119 \t Loss: -0.011268623638898134\nTRAIN: \t Epoch: 119 \t Loss: -0.011054932901805098\nTRAIN: \t Epoch: 119 \t Loss: -0.011058580130338669\nTRAIN: \t Epoch: 119 \t Loss: -0.011040507600857662\nTRAIN: \t Epoch: 119 \t Loss: -0.011075746401080064\nTRAIN: \t Epoch: 119 \t Loss: -0.011129572490851085\nTRAIN: \t Epoch: 119 \t Loss: -0.011145990283694118\nTRAIN: \t Epoch: 119 \t Loss: -0.011113111422780682\nTRAIN: \t Epoch: 119 \t Loss: -0.011065463483747508\nTRAIN: \t Epoch: 119 \t Loss: -0.011065621965409968\nVALD: \t Epoch: 119 \t Loss: -0.0126087237149477\nVALD: \t Epoch: 119 \t Loss: -0.010739631950855255\nVALD: \t Epoch: 119 \t Loss: -0.012211792481442293\nVALD: \t Epoch: 119 \t Loss: -0.012299500638619065\nVALD: \t Epoch: 119 \t Loss: -0.011571701400536151\n******************************\nEpoch: social-tag : 119\ntrain_loss -0.011065621965409968\nval_loss -0.011571701400536151\n{'min_val_epoch': 113, 'min_val_loss': -0.012882212075320157}\n******************************\nTRAIN: \t Epoch: 120 \t Loss: -0.011530149728059769\nTRAIN: \t Epoch: 120 \t Loss: -0.011439262889325619\nTRAIN: \t Epoch: 120 \t Loss: -0.010999166406691074\nTRAIN: \t Epoch: 120 \t Loss: -0.010734458453953266\nTRAIN: \t Epoch: 120 \t Loss: -0.010797531343996525\nTRAIN: \t Epoch: 120 \t Loss: -0.011038414823512236\nTRAIN: \t Epoch: 120 \t Loss: -0.011046790650912694\nTRAIN: \t Epoch: 120 \t Loss: -0.01088912389241159\nTRAIN: \t Epoch: 120 \t Loss: -0.010756253885726133\nTRAIN: \t Epoch: 120 \t Loss: -0.010820631962269544\nTRAIN: \t Epoch: 120 \t Loss: -0.010878214358606121\nTRAIN: \t Epoch: 120 \t Loss: -0.010910682147368789\nTRAIN: \t Epoch: 120 \t Loss: -0.010919715779332014\nTRAIN: \t Epoch: 120 \t Loss: -0.010876130113112075\nTRAIN: \t Epoch: 120 \t Loss: -0.010868762681881586\nTRAIN: \t Epoch: 120 \t Loss: -0.01095750683452934\nTRAIN: \t Epoch: 120 \t Loss: -0.010970688425004482\nTRAIN: \t Epoch: 120 \t Loss: -0.010954531427058909\nTRAIN: \t Epoch: 120 \t Loss: -0.010955542306721364\nVALD: \t Epoch: 120 \t Loss: -0.01222358550876379\nVALD: \t Epoch: 120 \t Loss: -0.010473017115145922\nVALD: \t Epoch: 120 \t Loss: -0.01161828264594078\nVALD: \t Epoch: 120 \t Loss: -0.011642482597380877\nVALD: \t Epoch: 120 \t Loss: -0.011088952545292121\n******************************\nEpoch: social-tag : 120\ntrain_loss -0.010955542306721364\nval_loss -0.011088952545292121\n{'min_val_epoch': 113, 'min_val_loss': -0.012882212075320157}\n******************************\nTRAIN: \t Epoch: 121 \t Loss: -0.011218050494790077\nTRAIN: \t Epoch: 121 \t Loss: -0.011532902717590332\nTRAIN: \t Epoch: 121 \t Loss: -0.011279727953175703\nTRAIN: \t Epoch: 121 \t Loss: -0.010972463060170412\nTRAIN: \t Epoch: 121 \t Loss: -0.010713404789566994\nTRAIN: \t Epoch: 121 \t Loss: -0.010730946126083532\nTRAIN: \t Epoch: 121 \t Loss: -0.010945649285401617\nTRAIN: \t Epoch: 121 \t Loss: -0.011034496361389756\nTRAIN: \t Epoch: 121 \t Loss: -0.010979395359754562\nTRAIN: \t Epoch: 121 \t Loss: -0.010940367728471756\nTRAIN: \t Epoch: 121 \t Loss: -0.010983026501807299\nTRAIN: \t Epoch: 121 \t Loss: -0.010972039851670464\nTRAIN: \t Epoch: 121 \t Loss: -0.0109885844330375\nTRAIN: \t Epoch: 121 \t Loss: -0.01086283940821886\nTRAIN: \t Epoch: 121 \t Loss: -0.010822820849716664\nTRAIN: \t Epoch: 121 \t Loss: -0.010913115052971989\nTRAIN: \t Epoch: 121 \t Loss: -0.010996365864925525\nTRAIN: \t Epoch: 121 \t Loss: -0.010981275286111567\nTRAIN: \t Epoch: 121 \t Loss: -0.010951655831037155\nVALD: \t Epoch: 121 \t Loss: -0.01066264696419239\nVALD: \t Epoch: 121 \t Loss: -0.00882504670880735\nVALD: \t Epoch: 121 \t Loss: -0.009538220707327127\nVALD: \t Epoch: 121 \t Loss: -0.009601740282960236\nVALD: \t Epoch: 121 \t Loss: -0.009193156277837832\n******************************\nEpoch: social-tag : 121\ntrain_loss -0.010951655831037155\nval_loss -0.009193156277837832\n{'min_val_epoch': 113, 'min_val_loss': -0.012882212075320157}\n******************************\nTRAIN: \t Epoch: 122 \t Loss: -0.009822768159210682\nTRAIN: \t Epoch: 122 \t Loss: -0.010734010487794876\nTRAIN: \t Epoch: 122 \t Loss: -0.010871250182390213\nTRAIN: \t Epoch: 122 \t Loss: -0.010733984410762787\nTRAIN: \t Epoch: 122 \t Loss: -0.010776396654546262\nTRAIN: \t Epoch: 122 \t Loss: -0.010925136972218752\nTRAIN: \t Epoch: 122 \t Loss: -0.01119045021810702\nTRAIN: \t Epoch: 122 \t Loss: -0.011083489167504013\nTRAIN: \t Epoch: 122 \t Loss: -0.010684990427560277\nTRAIN: \t Epoch: 122 \t Loss: -0.010568488761782647\nTRAIN: \t Epoch: 122 \t Loss: -0.010758557526225393\nTRAIN: \t Epoch: 122 \t Loss: -0.01090486852141718\nTRAIN: \t Epoch: 122 \t Loss: -0.011007892326093636\nTRAIN: \t Epoch: 122 \t Loss: -0.01103242626413703\nTRAIN: \t Epoch: 122 \t Loss: -0.011047501054902872\nTRAIN: \t Epoch: 122 \t Loss: -0.01102383597753942\nTRAIN: \t Epoch: 122 \t Loss: -0.011029202874530764\nTRAIN: \t Epoch: 122 \t Loss: -0.01107217956127392\nTRAIN: \t Epoch: 122 \t Loss: -0.011078625853260872\nVALD: \t Epoch: 122 \t Loss: -0.01273314282298088\nVALD: \t Epoch: 122 \t Loss: -0.01149373734369874\nVALD: \t Epoch: 122 \t Loss: -0.012844625239570936\nVALD: \t Epoch: 122 \t Loss: -0.012880933471024036\nVALD: \t Epoch: 122 \t Loss: -0.012215435307873182\n******************************\nEpoch: social-tag : 122\ntrain_loss -0.011078625853260872\nval_loss -0.012215435307873182\n{'min_val_epoch': 113, 'min_val_loss': -0.012882212075320157}\n******************************\nTRAIN: \t Epoch: 123 \t Loss: -0.012135830707848072\nTRAIN: \t Epoch: 123 \t Loss: -0.012140451930463314\nTRAIN: \t Epoch: 123 \t Loss: -0.010976380358139673\nTRAIN: \t Epoch: 123 \t Loss: -0.010438495548442006\nTRAIN: \t Epoch: 123 \t Loss: -0.010653456859290599\nTRAIN: \t Epoch: 123 \t Loss: -0.010948332802702984\nTRAIN: \t Epoch: 123 \t Loss: -0.011127767419176442\nTRAIN: \t Epoch: 123 \t Loss: -0.011176184401847422\nTRAIN: \t Epoch: 123 \t Loss: -0.010975213307473395\nTRAIN: \t Epoch: 123 \t Loss: -0.010689774621278047\nTRAIN: \t Epoch: 123 \t Loss: -0.01069596375931393\nTRAIN: \t Epoch: 123 \t Loss: -0.010743293988828858\nTRAIN: \t Epoch: 123 \t Loss: -0.01087360786130795\nTRAIN: \t Epoch: 123 \t Loss: -0.010961405068103756\nTRAIN: \t Epoch: 123 \t Loss: -0.010992653978367647\nTRAIN: \t Epoch: 123 \t Loss: -0.010987329646013677\nTRAIN: \t Epoch: 123 \t Loss: -0.010974849135998417\nTRAIN: \t Epoch: 123 \t Loss: -0.01096439708231224\nTRAIN: \t Epoch: 123 \t Loss: -0.010974757886723543\nVALD: \t Epoch: 123 \t Loss: -0.012866992503404617\nVALD: \t Epoch: 123 \t Loss: -0.011389971245080233\nVALD: \t Epoch: 123 \t Loss: -0.0127954405422012\nVALD: \t Epoch: 123 \t Loss: -0.012729166774079204\nVALD: \t Epoch: 123 \t Loss: -0.012062259942046866\n******************************\nEpoch: social-tag : 123\ntrain_loss -0.010974757886723543\nval_loss -0.012062259942046866\n{'min_val_epoch': 113, 'min_val_loss': -0.012882212075320157}\n******************************\nTRAIN: \t Epoch: 124 \t Loss: -0.011227324604988098\nTRAIN: \t Epoch: 124 \t Loss: -0.011700143106281757\nTRAIN: \t Epoch: 124 \t Loss: -0.011868754091362158\nTRAIN: \t Epoch: 124 \t Loss: -0.011454309802502394\nTRAIN: \t Epoch: 124 \t Loss: -0.011000423505902291\nTRAIN: \t Epoch: 124 \t Loss: -0.011088343958059946\nTRAIN: \t Epoch: 124 \t Loss: -0.011164956326995577\nTRAIN: \t Epoch: 124 \t Loss: -0.011360704666003585\nTRAIN: \t Epoch: 124 \t Loss: -0.011242373742991023\nTRAIN: \t Epoch: 124 \t Loss: -0.011016176175326109\nTRAIN: \t Epoch: 124 \t Loss: -0.010772311890667135\nTRAIN: \t Epoch: 124 \t Loss: -0.01084098561356465\nTRAIN: \t Epoch: 124 \t Loss: -0.01095233542414812\nTRAIN: \t Epoch: 124 \t Loss: -0.011049165124339717\nTRAIN: \t Epoch: 124 \t Loss: -0.011055731711288293\nTRAIN: \t Epoch: 124 \t Loss: -0.011128423328045756\nTRAIN: \t Epoch: 124 \t Loss: -0.011253692319287974\nTRAIN: \t Epoch: 124 \t Loss: -0.011184375629656844\nTRAIN: \t Epoch: 124 \t Loss: -0.011175480635129204\nVALD: \t Epoch: 124 \t Loss: -0.011468406766653061\nVALD: \t Epoch: 124 \t Loss: -0.009797290898859501\nVALD: \t Epoch: 124 \t Loss: -0.011071000869075457\nVALD: \t Epoch: 124 \t Loss: -0.011103642173111439\nVALD: \t Epoch: 124 \t Loss: -0.010570487306137716\n******************************\nEpoch: social-tag : 124\ntrain_loss -0.011175480635129204\nval_loss -0.010570487306137716\n{'min_val_epoch': 113, 'min_val_loss': -0.012882212075320157}\n******************************\nTRAIN: \t Epoch: 125 \t Loss: -0.010916337370872498\nTRAIN: \t Epoch: 125 \t Loss: -0.010956272948533297\nTRAIN: \t Epoch: 125 \t Loss: -0.011267496272921562\nTRAIN: \t Epoch: 125 \t Loss: -0.011289558606222272\nTRAIN: \t Epoch: 125 \t Loss: -0.011367504298686982\nTRAIN: \t Epoch: 125 \t Loss: -0.011155740823596716\nTRAIN: \t Epoch: 125 \t Loss: -0.010928064318639892\nTRAIN: \t Epoch: 125 \t Loss: -0.010931384982541203\nTRAIN: \t Epoch: 125 \t Loss: -0.011073553106851049\nTRAIN: \t Epoch: 125 \t Loss: -0.01110403034836054\nTRAIN: \t Epoch: 125 \t Loss: -0.01105396593497558\nTRAIN: \t Epoch: 125 \t Loss: -0.011033532597745458\nTRAIN: \t Epoch: 125 \t Loss: -0.01098583359271288\nTRAIN: \t Epoch: 125 \t Loss: -0.010997604977871691\nTRAIN: \t Epoch: 125 \t Loss: -0.011089567591746648\nTRAIN: \t Epoch: 125 \t Loss: -0.011184915201738477\nTRAIN: \t Epoch: 125 \t Loss: -0.011137975872877766\nTRAIN: \t Epoch: 125 \t Loss: -0.010942374925232597\nTRAIN: \t Epoch: 125 \t Loss: -0.010936552843176836\nVALD: \t Epoch: 125 \t Loss: -0.011257805861532688\nVALD: \t Epoch: 125 \t Loss: -0.010125541593879461\nVALD: \t Epoch: 125 \t Loss: -0.011531344304482142\nVALD: \t Epoch: 125 \t Loss: -0.011628934880718589\nVALD: \t Epoch: 125 \t Loss: -0.011062829849148584\n******************************\nEpoch: social-tag : 125\ntrain_loss -0.010936552843176836\nval_loss -0.011062829849148584\n{'min_val_epoch': 113, 'min_val_loss': -0.012882212075320157}\n******************************\nTRAIN: \t Epoch: 126 \t Loss: -0.009867183864116669\nTRAIN: \t Epoch: 126 \t Loss: -0.011379457544535398\nTRAIN: \t Epoch: 126 \t Loss: -0.011514071375131607\nTRAIN: \t Epoch: 126 \t Loss: -0.011402169009670615\nTRAIN: \t Epoch: 126 \t Loss: -0.011392507888376712\nTRAIN: \t Epoch: 126 \t Loss: -0.011088610937198004\nTRAIN: \t Epoch: 126 \t Loss: -0.010956014240426677\nTRAIN: \t Epoch: 126 \t Loss: -0.011061349418014288\nTRAIN: \t Epoch: 126 \t Loss: -0.011209651724331908\nTRAIN: \t Epoch: 126 \t Loss: -0.011331229750066996\nTRAIN: \t Epoch: 126 \t Loss: -0.011176515031944622\nTRAIN: \t Epoch: 126 \t Loss: -0.01095375643732647\nTRAIN: \t Epoch: 126 \t Loss: -0.010989465607473483\nTRAIN: \t Epoch: 126 \t Loss: -0.011101433741194862\nTRAIN: \t Epoch: 126 \t Loss: -0.011109591896335284\nTRAIN: \t Epoch: 126 \t Loss: -0.01112351706251502\nTRAIN: \t Epoch: 126 \t Loss: -0.011142212130567607\nTRAIN: \t Epoch: 126 \t Loss: -0.011204479727894068\nTRAIN: \t Epoch: 126 \t Loss: -0.011206108791164855\nVALD: \t Epoch: 126 \t Loss: -0.01423842553049326\nVALD: \t Epoch: 126 \t Loss: -0.01273123687133193\nVALD: \t Epoch: 126 \t Loss: -0.014251984345416227\nVALD: \t Epoch: 126 \t Loss: -0.014009485021233559\nVALD: \t Epoch: 126 \t Loss: -0.013209172910895229\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.74it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\nADE: 0.37444938394954  FDE: 0.5969401469108833\n**************************************************\n******************************\nEpoch: social-tag : 126\ntrain_loss -0.011206108791164855\nval_loss -0.013209172910895229\n{'min_val_epoch': 126, 'min_val_loss': -0.013209172910895229}\n******************************\nTRAIN: \t Epoch: 127 \t Loss: -0.011549628339707851\nTRAIN: \t Epoch: 127 \t Loss: -0.011205675546079874\nTRAIN: \t Epoch: 127 \t Loss: -0.011388114343086878\nTRAIN: \t Epoch: 127 \t Loss: -0.011249293107539415\nTRAIN: \t Epoch: 127 \t Loss: -0.011136186122894288\nTRAIN: \t Epoch: 127 \t Loss: -0.01121155358850956\nTRAIN: \t Epoch: 127 \t Loss: -0.011193132826260157\nTRAIN: \t Epoch: 127 \t Loss: -0.01105763134546578\nTRAIN: \t Epoch: 127 \t Loss: -0.011041008867323399\nTRAIN: \t Epoch: 127 \t Loss: -0.011008191201835871\nTRAIN: \t Epoch: 127 \t Loss: -0.011114281856200912\nTRAIN: \t Epoch: 127 \t Loss: -0.01105609885416925\nTRAIN: \t Epoch: 127 \t Loss: -0.010986830107867718\nTRAIN: \t Epoch: 127 \t Loss: -0.011012689010905368\nTRAIN: \t Epoch: 127 \t Loss: -0.011086363904178142\nTRAIN: \t Epoch: 127 \t Loss: -0.011129755061119795\nTRAIN: \t Epoch: 127 \t Loss: -0.011155825930044931\nTRAIN: \t Epoch: 127 \t Loss: -0.011129295401689079\nTRAIN: \t Epoch: 127 \t Loss: -0.01111197033311456\nVALD: \t Epoch: 127 \t Loss: -0.013260695151984692\nVALD: \t Epoch: 127 \t Loss: -0.011448641773313284\nVALD: \t Epoch: 127 \t Loss: -0.01299375637123982\nVALD: \t Epoch: 127 \t Loss: -0.012908734614029527\nVALD: \t Epoch: 127 \t Loss: -0.01230821826241233\n******************************\nEpoch: social-tag : 127\ntrain_loss -0.01111197033311456\nval_loss -0.01230821826241233\n{'min_val_epoch': 126, 'min_val_loss': -0.013209172910895229}\n******************************\nTRAIN: \t Epoch: 128 \t Loss: -0.010540522634983063\nTRAIN: \t Epoch: 128 \t Loss: -0.011271500028669834\nTRAIN: \t Epoch: 128 \t Loss: -0.011094078111151854\nTRAIN: \t Epoch: 128 \t Loss: -0.011143062263727188\nTRAIN: \t Epoch: 128 \t Loss: -0.011319714598357677\nTRAIN: \t Epoch: 128 \t Loss: -0.011333571126063665\nTRAIN: \t Epoch: 128 \t Loss: -0.01133250751133476\nTRAIN: \t Epoch: 128 \t Loss: -0.011181203066371381\nTRAIN: \t Epoch: 128 \t Loss: -0.011244838229484029\nTRAIN: \t Epoch: 128 \t Loss: -0.011247107479721307\nTRAIN: \t Epoch: 128 \t Loss: -0.01125923747366125\nTRAIN: \t Epoch: 128 \t Loss: -0.01132842618972063\nTRAIN: \t Epoch: 128 \t Loss: -0.011342500981230002\nTRAIN: \t Epoch: 128 \t Loss: -0.011360987156097378\nTRAIN: \t Epoch: 128 \t Loss: -0.011403562501072884\nTRAIN: \t Epoch: 128 \t Loss: -0.011365981423296034\nTRAIN: \t Epoch: 128 \t Loss: -0.011248720809817314\nTRAIN: \t Epoch: 128 \t Loss: -0.011261985585507419\nTRAIN: \t Epoch: 128 \t Loss: -0.011266158673909078\nVALD: \t Epoch: 128 \t Loss: -0.013322217389941216\nVALD: \t Epoch: 128 \t Loss: -0.011954417452216148\nVALD: \t Epoch: 128 \t Loss: -0.013603566214442253\nVALD: \t Epoch: 128 \t Loss: -0.013630670728161931\nVALD: \t Epoch: 128 \t Loss: -0.01279736254826065\n******************************\nEpoch: social-tag : 128\ntrain_loss -0.011266158673909078\nval_loss -0.01279736254826065\n{'min_val_epoch': 126, 'min_val_loss': -0.013209172910895229}\n******************************\nTRAIN: \t Epoch: 129 \t Loss: -0.011845752596855164\nTRAIN: \t Epoch: 129 \t Loss: -0.012054783757776022\nTRAIN: \t Epoch: 129 \t Loss: -0.012088815681636333\nTRAIN: \t Epoch: 129 \t Loss: -0.011758555192500353\nTRAIN: \t Epoch: 129 \t Loss: -0.01117173135280609\nTRAIN: \t Epoch: 129 \t Loss: -0.011100881888220707\nTRAIN: \t Epoch: 129 \t Loss: -0.011356629298201628\nTRAIN: \t Epoch: 129 \t Loss: -0.011393952067010105\nTRAIN: \t Epoch: 129 \t Loss: -0.011426498492558798\nTRAIN: \t Epoch: 129 \t Loss: -0.011418118979781866\nTRAIN: \t Epoch: 129 \t Loss: -0.011487970805980942\nTRAIN: \t Epoch: 129 \t Loss: -0.011321438010782003\nTRAIN: \t Epoch: 129 \t Loss: -0.011220935373925246\nTRAIN: \t Epoch: 129 \t Loss: -0.011189638104821955\nTRAIN: \t Epoch: 129 \t Loss: -0.011252465409537156\nTRAIN: \t Epoch: 129 \t Loss: -0.011287340195849538\nTRAIN: \t Epoch: 129 \t Loss: -0.011249063207822688\nTRAIN: \t Epoch: 129 \t Loss: -0.011281405679053731\nTRAIN: \t Epoch: 129 \t Loss: -0.011297152851749145\nVALD: \t Epoch: 129 \t Loss: -0.01280165370553732\nVALD: \t Epoch: 129 \t Loss: -0.010597495827823877\nVALD: \t Epoch: 129 \t Loss: -0.012359013470510641\nVALD: \t Epoch: 129 \t Loss: -0.012349365977570415\nVALD: \t Epoch: 129 \t Loss: -0.011477302224182886\n******************************\nEpoch: social-tag : 129\ntrain_loss -0.011297152851749145\nval_loss -0.011477302224182886\n{'min_val_epoch': 126, 'min_val_loss': -0.013209172910895229}\n******************************\nTRAIN: \t Epoch: 130 \t Loss: -0.012984937988221645\nTRAIN: \t Epoch: 130 \t Loss: -0.012001816183328629\nTRAIN: \t Epoch: 130 \t Loss: -0.012171949880818525\nTRAIN: \t Epoch: 130 \t Loss: -0.012222848366945982\nTRAIN: \t Epoch: 130 \t Loss: -0.011993744410574436\nTRAIN: \t Epoch: 130 \t Loss: -0.011761412024497986\nTRAIN: \t Epoch: 130 \t Loss: -0.011702698655426502\nTRAIN: \t Epoch: 130 \t Loss: -0.011760907946154475\nTRAIN: \t Epoch: 130 \t Loss: -0.011859254911541939\nTRAIN: \t Epoch: 130 \t Loss: -0.01169892456382513\nTRAIN: \t Epoch: 130 \t Loss: -0.0115436813370748\nTRAIN: \t Epoch: 130 \t Loss: -0.011554924150307974\nTRAIN: \t Epoch: 130 \t Loss: -0.01157955896969025\nTRAIN: \t Epoch: 130 \t Loss: -0.011532492603042297\nTRAIN: \t Epoch: 130 \t Loss: -0.011482811781267326\nTRAIN: \t Epoch: 130 \t Loss: -0.011448504461441189\nTRAIN: \t Epoch: 130 \t Loss: -0.01148301291772548\nTRAIN: \t Epoch: 130 \t Loss: -0.011477592815127637\nTRAIN: \t Epoch: 130 \t Loss: -0.011471307946370655\nVALD: \t Epoch: 130 \t Loss: -0.013286475092172623\nVALD: \t Epoch: 130 \t Loss: -0.01175159215927124\nVALD: \t Epoch: 130 \t Loss: -0.013136894131700197\nVALD: \t Epoch: 130 \t Loss: -0.012775769922882318\nVALD: \t Epoch: 130 \t Loss: -0.012133100899783048\n******************************\nEpoch: social-tag : 130\ntrain_loss -0.011471307946370655\nval_loss -0.012133100899783048\n{'min_val_epoch': 126, 'min_val_loss': -0.013209172910895229}\n******************************\nTRAIN: \t Epoch: 131 \t Loss: -0.011629057116806507\nTRAIN: \t Epoch: 131 \t Loss: -0.011810461059212685\nTRAIN: \t Epoch: 131 \t Loss: -0.011491359832386175\nTRAIN: \t Epoch: 131 \t Loss: -0.011352581903338432\nTRAIN: \t Epoch: 131 \t Loss: -0.01161585059016943\nTRAIN: \t Epoch: 131 \t Loss: -0.011565797807027897\nTRAIN: \t Epoch: 131 \t Loss: -0.011442434042692184\nTRAIN: \t Epoch: 131 \t Loss: -0.011344681261107326\nTRAIN: \t Epoch: 131 \t Loss: -0.011413417653077178\nTRAIN: \t Epoch: 131 \t Loss: -0.011402538139373064\nTRAIN: \t Epoch: 131 \t Loss: -0.011382285674864595\nTRAIN: \t Epoch: 131 \t Loss: -0.011395638032505909\nTRAIN: \t Epoch: 131 \t Loss: -0.011399407512866534\nTRAIN: \t Epoch: 131 \t Loss: -0.011364239866712264\nTRAIN: \t Epoch: 131 \t Loss: -0.011394725553691388\nTRAIN: \t Epoch: 131 \t Loss: -0.011430222424678504\nTRAIN: \t Epoch: 131 \t Loss: -0.011411335209713262\nTRAIN: \t Epoch: 131 \t Loss: -0.011348990878711144\nTRAIN: \t Epoch: 131 \t Loss: -0.011350697376520648\nVALD: \t Epoch: 131 \t Loss: -0.013031568378210068\nVALD: \t Epoch: 131 \t Loss: -0.01173056522384286\nVALD: \t Epoch: 131 \t Loss: -0.013052118010818958\nVALD: \t Epoch: 131 \t Loss: -0.012905663577839732\nVALD: \t Epoch: 131 \t Loss: -0.012264650715284112\n******************************\nEpoch: social-tag : 131\ntrain_loss -0.011350697376520648\nval_loss -0.012264650715284112\n{'min_val_epoch': 126, 'min_val_loss': -0.013209172910895229}\n******************************\nTRAIN: \t Epoch: 132 \t Loss: -0.012392042204737663\nTRAIN: \t Epoch: 132 \t Loss: -0.012728323228657246\nTRAIN: \t Epoch: 132 \t Loss: -0.012463299557566643\nTRAIN: \t Epoch: 132 \t Loss: -0.012013546889647841\nTRAIN: \t Epoch: 132 \t Loss: -0.011691975966095924\nTRAIN: \t Epoch: 132 \t Loss: -0.011701091813544432\nTRAIN: \t Epoch: 132 \t Loss: -0.011538358804370676\nTRAIN: \t Epoch: 132 \t Loss: -0.011373541434295475\nTRAIN: \t Epoch: 132 \t Loss: -0.011415624059736729\nTRAIN: \t Epoch: 132 \t Loss: -0.011359392758458852\nTRAIN: \t Epoch: 132 \t Loss: -0.011415234174240719\nTRAIN: \t Epoch: 132 \t Loss: -0.011395884134496251\nTRAIN: \t Epoch: 132 \t Loss: -0.011371771160226602\nTRAIN: \t Epoch: 132 \t Loss: -0.011353548948786088\nTRAIN: \t Epoch: 132 \t Loss: -0.011317326066394648\nTRAIN: \t Epoch: 132 \t Loss: -0.011353438079822809\nTRAIN: \t Epoch: 132 \t Loss: -0.011340636814780095\nTRAIN: \t Epoch: 132 \t Loss: -0.01127405609521601\nTRAIN: \t Epoch: 132 \t Loss: -0.011264691268904881\nVALD: \t Epoch: 132 \t Loss: -0.012333730235695839\nVALD: \t Epoch: 132 \t Loss: -0.010550900362432003\nVALD: \t Epoch: 132 \t Loss: -0.011941084017356237\nVALD: \t Epoch: 132 \t Loss: -0.011954942950978875\nVALD: \t Epoch: 132 \t Loss: -0.011296466756458125\n******************************\nEpoch: social-tag : 132\ntrain_loss -0.011264691268904881\nval_loss -0.011296466756458125\n{'min_val_epoch': 126, 'min_val_loss': -0.013209172910895229}\n******************************\nTRAIN: \t Epoch: 133 \t Loss: -0.011092666536569595\nTRAIN: \t Epoch: 133 \t Loss: -0.012164675164967775\nTRAIN: \t Epoch: 133 \t Loss: -0.012017893915375074\nTRAIN: \t Epoch: 133 \t Loss: -0.011351488996297121\nTRAIN: \t Epoch: 133 \t Loss: -0.01089811623096466\nTRAIN: \t Epoch: 133 \t Loss: -0.010940098203718662\nTRAIN: \t Epoch: 133 \t Loss: -0.01097034995577165\nTRAIN: \t Epoch: 133 \t Loss: -0.011160589987412095\nTRAIN: \t Epoch: 133 \t Loss: -0.011261355959706836\nTRAIN: \t Epoch: 133 \t Loss: -0.011331405024975538\nTRAIN: \t Epoch: 133 \t Loss: -0.011281670663844456\nTRAIN: \t Epoch: 133 \t Loss: -0.011275164860611161\nTRAIN: \t Epoch: 133 \t Loss: -0.011209648389082689\nTRAIN: \t Epoch: 133 \t Loss: -0.011252858210355043\nTRAIN: \t Epoch: 133 \t Loss: -0.011247707841296991\nTRAIN: \t Epoch: 133 \t Loss: -0.011263612774200737\nTRAIN: \t Epoch: 133 \t Loss: -0.011216290842960863\nTRAIN: \t Epoch: 133 \t Loss: -0.01124798356451922\nTRAIN: \t Epoch: 133 \t Loss: -0.011246234281221203\nVALD: \t Epoch: 133 \t Loss: -0.012600880116224289\nVALD: \t Epoch: 133 \t Loss: -0.01059222174808383\nVALD: \t Epoch: 133 \t Loss: -0.012041660336156687\nVALD: \t Epoch: 133 \t Loss: -0.012091603130102158\nVALD: \t Epoch: 133 \t Loss: -0.011395681298468723\n******************************\nEpoch: social-tag : 133\ntrain_loss -0.011246234281221203\nval_loss -0.011395681298468723\n{'min_val_epoch': 126, 'min_val_loss': -0.013209172910895229}\n******************************\nTRAIN: \t Epoch: 134 \t Loss: -0.012408764101564884\nTRAIN: \t Epoch: 134 \t Loss: -0.012255391106009483\nTRAIN: \t Epoch: 134 \t Loss: -0.010935172438621521\nTRAIN: \t Epoch: 134 \t Loss: -0.01060387003235519\nTRAIN: \t Epoch: 134 \t Loss: -0.011019447259604931\nTRAIN: \t Epoch: 134 \t Loss: -0.011181333102285862\nTRAIN: \t Epoch: 134 \t Loss: -0.01126383657434157\nTRAIN: \t Epoch: 134 \t Loss: -0.011351990746334195\nTRAIN: \t Epoch: 134 \t Loss: -0.011448142636153433\nTRAIN: \t Epoch: 134 \t Loss: -0.011227028165012597\nTRAIN: \t Epoch: 134 \t Loss: -0.011179303750395775\nTRAIN: \t Epoch: 134 \t Loss: -0.011248401831835508\nTRAIN: \t Epoch: 134 \t Loss: -0.011344615513315568\nTRAIN: \t Epoch: 134 \t Loss: -0.011382714845240116\nTRAIN: \t Epoch: 134 \t Loss: -0.011509498705466588\nTRAIN: \t Epoch: 134 \t Loss: -0.011560279119294137\nTRAIN: \t Epoch: 134 \t Loss: -0.011506187181700678\nTRAIN: \t Epoch: 134 \t Loss: -0.011433733730680413\nTRAIN: \t Epoch: 134 \t Loss: -0.011430479342263902\nVALD: \t Epoch: 134 \t Loss: -0.013153952546417713\nVALD: \t Epoch: 134 \t Loss: -0.011844765394926071\nVALD: \t Epoch: 134 \t Loss: -0.013548934211333593\nVALD: \t Epoch: 134 \t Loss: -0.013465489260852337\nVALD: \t Epoch: 134 \t Loss: -0.01274796231719088\n******************************\nEpoch: social-tag : 134\ntrain_loss -0.011430479342263902\nval_loss -0.01274796231719088\n{'min_val_epoch': 126, 'min_val_loss': -0.013209172910895229}\n******************************\nTRAIN: \t Epoch: 135 \t Loss: -0.011745501309633255\nTRAIN: \t Epoch: 135 \t Loss: -0.012073987629264593\nTRAIN: \t Epoch: 135 \t Loss: -0.011705843731760979\nTRAIN: \t Epoch: 135 \t Loss: -0.010975586017593741\nTRAIN: \t Epoch: 135 \t Loss: -0.011019018106162548\nTRAIN: \t Epoch: 135 \t Loss: -0.011080105478564898\nTRAIN: \t Epoch: 135 \t Loss: -0.011181468277105264\nTRAIN: \t Epoch: 135 \t Loss: -0.011296863085590303\nTRAIN: \t Epoch: 135 \t Loss: -0.011345239356160164\nTRAIN: \t Epoch: 135 \t Loss: -0.011242076195776463\nTRAIN: \t Epoch: 135 \t Loss: -0.011284066025506367\nTRAIN: \t Epoch: 135 \t Loss: -0.011336574330925941\nTRAIN: \t Epoch: 135 \t Loss: -0.011430189085121337\nTRAIN: \t Epoch: 135 \t Loss: -0.011372437101921864\nTRAIN: \t Epoch: 135 \t Loss: -0.011214360781013965\nTRAIN: \t Epoch: 135 \t Loss: -0.011216900951694697\nTRAIN: \t Epoch: 135 \t Loss: -0.011256955509238383\nTRAIN: \t Epoch: 135 \t Loss: -0.011363065232419305\nTRAIN: \t Epoch: 135 \t Loss: -0.01137603107682597\nVALD: \t Epoch: 135 \t Loss: -0.014326502569019794\nVALD: \t Epoch: 135 \t Loss: -0.012611714657396078\nVALD: \t Epoch: 135 \t Loss: -0.014393420579532782\nVALD: \t Epoch: 135 \t Loss: -0.014125831192359328\nVALD: \t Epoch: 135 \t Loss: -0.013215723215055859\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.72it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.40840905899315544  FDE: 0.7064753683608003\n**************************************************\n******************************\nEpoch: social-tag : 135\ntrain_loss -0.01137603107682597\nval_loss -0.013215723215055859\n{'min_val_epoch': 135, 'min_val_loss': -0.013215723215055859}\n******************************\nTRAIN: \t Epoch: 136 \t Loss: -0.012927771545946598\nTRAIN: \t Epoch: 136 \t Loss: -0.012659152504056692\nTRAIN: \t Epoch: 136 \t Loss: -0.012495587579905987\nTRAIN: \t Epoch: 136 \t Loss: -0.011971110245212913\nTRAIN: \t Epoch: 136 \t Loss: -0.012093967013061046\nTRAIN: \t Epoch: 136 \t Loss: -0.01196959444011251\nTRAIN: \t Epoch: 136 \t Loss: -0.011822906189731188\nTRAIN: \t Epoch: 136 \t Loss: -0.01179398933891207\nTRAIN: \t Epoch: 136 \t Loss: -0.01166304728637139\nTRAIN: \t Epoch: 136 \t Loss: -0.01152402488514781\nTRAIN: \t Epoch: 136 \t Loss: -0.011486058462072502\nTRAIN: \t Epoch: 136 \t Loss: -0.011573687583828965\nTRAIN: \t Epoch: 136 \t Loss: -0.01158976311293932\nTRAIN: \t Epoch: 136 \t Loss: -0.011524014108415161\nTRAIN: \t Epoch: 136 \t Loss: -0.011500042801102003\nTRAIN: \t Epoch: 136 \t Loss: -0.011477487743832171\nTRAIN: \t Epoch: 136 \t Loss: -0.011481544660294758\nTRAIN: \t Epoch: 136 \t Loss: -0.011470113156570328\nTRAIN: \t Epoch: 136 \t Loss: -0.011464668458695046\nVALD: \t Epoch: 136 \t Loss: -0.013935297727584839\nVALD: \t Epoch: 136 \t Loss: -0.012091802433133125\nVALD: \t Epoch: 136 \t Loss: -0.01376663272579511\nVALD: \t Epoch: 136 \t Loss: -0.013666214421391487\nVALD: \t Epoch: 136 \t Loss: -0.012930088102324934\n******************************\nEpoch: social-tag : 136\ntrain_loss -0.011464668458695046\nval_loss -0.012930088102324934\n{'min_val_epoch': 135, 'min_val_loss': -0.013215723215055859}\n******************************\nTRAIN: \t Epoch: 137 \t Loss: -0.011817988008260727\nTRAIN: \t Epoch: 137 \t Loss: -0.012041748967021704\nTRAIN: \t Epoch: 137 \t Loss: -0.011861306304732958\nTRAIN: \t Epoch: 137 \t Loss: -0.011256507830694318\nTRAIN: \t Epoch: 137 \t Loss: -0.011446823365986347\nTRAIN: \t Epoch: 137 \t Loss: -0.011581428038577238\nTRAIN: \t Epoch: 137 \t Loss: -0.011614739229636533\nTRAIN: \t Epoch: 137 \t Loss: -0.011530239717103541\nTRAIN: \t Epoch: 137 \t Loss: -0.011457514535221789\nTRAIN: \t Epoch: 137 \t Loss: -0.011633097287267446\nTRAIN: \t Epoch: 137 \t Loss: -0.011626314371824265\nTRAIN: \t Epoch: 137 \t Loss: -0.011396089335903525\nTRAIN: \t Epoch: 137 \t Loss: -0.011383670238921275\nTRAIN: \t Epoch: 137 \t Loss: -0.01145740837923118\nTRAIN: \t Epoch: 137 \t Loss: -0.011491455137729645\nTRAIN: \t Epoch: 137 \t Loss: -0.011516532918903977\nTRAIN: \t Epoch: 137 \t Loss: -0.011531429255709928\nTRAIN: \t Epoch: 137 \t Loss: -0.0114988145004544\nTRAIN: \t Epoch: 137 \t Loss: -0.011472747300997774\nVALD: \t Epoch: 137 \t Loss: -0.0120786651968956\nVALD: \t Epoch: 137 \t Loss: -0.009821530664339662\nVALD: \t Epoch: 137 \t Loss: -0.010982226114720106\nVALD: \t Epoch: 137 \t Loss: -0.010906882933340967\nVALD: \t Epoch: 137 \t Loss: -0.010276771379896433\n******************************\nEpoch: social-tag : 137\ntrain_loss -0.011472747300997774\nval_loss -0.010276771379896433\n{'min_val_epoch': 135, 'min_val_loss': -0.013215723215055859}\n******************************\nTRAIN: \t Epoch: 138 \t Loss: -0.009759459644556046\nTRAIN: \t Epoch: 138 \t Loss: -0.010935255326330662\nTRAIN: \t Epoch: 138 \t Loss: -0.011594233103096485\nTRAIN: \t Epoch: 138 \t Loss: -0.01162515883333981\nTRAIN: \t Epoch: 138 \t Loss: -0.011736024916172028\nTRAIN: \t Epoch: 138 \t Loss: -0.011797321339448294\nTRAIN: \t Epoch: 138 \t Loss: -0.011507577395864896\nTRAIN: \t Epoch: 138 \t Loss: -0.011441467562690377\nTRAIN: \t Epoch: 138 \t Loss: -0.011538664913839765\nTRAIN: \t Epoch: 138 \t Loss: -0.011554510798305273\nTRAIN: \t Epoch: 138 \t Loss: -0.011482984674247828\nTRAIN: \t Epoch: 138 \t Loss: -0.011484576001142463\nTRAIN: \t Epoch: 138 \t Loss: -0.011555394802529078\nTRAIN: \t Epoch: 138 \t Loss: -0.011544706233377968\nTRAIN: \t Epoch: 138 \t Loss: -0.011487750274439653\nTRAIN: \t Epoch: 138 \t Loss: -0.011487577867228538\nTRAIN: \t Epoch: 138 \t Loss: -0.011494247790645151\nTRAIN: \t Epoch: 138 \t Loss: -0.011486374773085117\nTRAIN: \t Epoch: 138 \t Loss: -0.011481307414869723\nVALD: \t Epoch: 138 \t Loss: -0.01284815277904272\nVALD: \t Epoch: 138 \t Loss: -0.011537921614944935\nVALD: \t Epoch: 138 \t Loss: -0.013013516863187155\nVALD: \t Epoch: 138 \t Loss: -0.01308084325864911\nVALD: \t Epoch: 138 \t Loss: -0.012399705776498337\n******************************\nEpoch: social-tag : 138\ntrain_loss -0.011481307414869723\nval_loss -0.012399705776498337\n{'min_val_epoch': 135, 'min_val_loss': -0.013215723215055859}\n******************************\nTRAIN: \t Epoch: 139 \t Loss: -0.011848639696836472\nTRAIN: \t Epoch: 139 \t Loss: -0.011800037231296301\nTRAIN: \t Epoch: 139 \t Loss: -0.01195155115177234\nTRAIN: \t Epoch: 139 \t Loss: -0.011847337707877159\nTRAIN: \t Epoch: 139 \t Loss: -0.01144101209938526\nTRAIN: \t Epoch: 139 \t Loss: -0.011254019103944302\nTRAIN: \t Epoch: 139 \t Loss: -0.011409094690212182\nTRAIN: \t Epoch: 139 \t Loss: -0.011467678938060999\nTRAIN: \t Epoch: 139 \t Loss: -0.011475529418223433\nTRAIN: \t Epoch: 139 \t Loss: -0.011536917183548212\nTRAIN: \t Epoch: 139 \t Loss: -0.011491205204616894\nTRAIN: \t Epoch: 139 \t Loss: -0.011476750563209256\nTRAIN: \t Epoch: 139 \t Loss: -0.011527594823676806\nTRAIN: \t Epoch: 139 \t Loss: -0.011452865454235248\nTRAIN: \t Epoch: 139 \t Loss: -0.011374523242314657\nTRAIN: \t Epoch: 139 \t Loss: -0.011381830903701484\nTRAIN: \t Epoch: 139 \t Loss: -0.011483454035923761\nTRAIN: \t Epoch: 139 \t Loss: -0.011499761396812068\nTRAIN: \t Epoch: 139 \t Loss: -0.011491319345924796\nVALD: \t Epoch: 139 \t Loss: -0.013122452422976494\nVALD: \t Epoch: 139 \t Loss: -0.011926846578717232\nVALD: \t Epoch: 139 \t Loss: -0.013467015077670416\nVALD: \t Epoch: 139 \t Loss: -0.013426891528069973\nVALD: \t Epoch: 139 \t Loss: -0.012720555608922785\n******************************\nEpoch: social-tag : 139\ntrain_loss -0.011491319345924796\nval_loss -0.012720555608922785\n{'min_val_epoch': 135, 'min_val_loss': -0.013215723215055859}\n******************************\nTRAIN: \t Epoch: 140 \t Loss: -0.011275322176516056\nTRAIN: \t Epoch: 140 \t Loss: -0.011302992701530457\nTRAIN: \t Epoch: 140 \t Loss: -0.011634061733881632\nTRAIN: \t Epoch: 140 \t Loss: -0.011594616109505296\nTRAIN: \t Epoch: 140 \t Loss: -0.011038688197731971\nTRAIN: \t Epoch: 140 \t Loss: -0.01098657383893927\nTRAIN: \t Epoch: 140 \t Loss: -0.01132944572184767\nTRAIN: \t Epoch: 140 \t Loss: -0.011371129541657865\nTRAIN: \t Epoch: 140 \t Loss: -0.011512419933246242\nTRAIN: \t Epoch: 140 \t Loss: -0.011686736531555653\nTRAIN: \t Epoch: 140 \t Loss: -0.01169377997178923\nTRAIN: \t Epoch: 140 \t Loss: -0.011675668259461721\nTRAIN: \t Epoch: 140 \t Loss: -0.011463437372675309\nTRAIN: \t Epoch: 140 \t Loss: -0.011402466999632972\nTRAIN: \t Epoch: 140 \t Loss: -0.011421957425773144\nTRAIN: \t Epoch: 140 \t Loss: -0.01151232048869133\nTRAIN: \t Epoch: 140 \t Loss: -0.011527516714790288\nTRAIN: \t Epoch: 140 \t Loss: -0.011537547533710798\nTRAIN: \t Epoch: 140 \t Loss: -0.011537476912025333\nVALD: \t Epoch: 140 \t Loss: -0.013678424060344696\nVALD: \t Epoch: 140 \t Loss: -0.011863503605127335\nVALD: \t Epoch: 140 \t Loss: -0.013288040334979693\nVALD: \t Epoch: 140 \t Loss: -0.013188330456614494\nVALD: \t Epoch: 140 \t Loss: -0.012432445967492978\n******************************\nEpoch: social-tag : 140\ntrain_loss -0.011537476912025333\nval_loss -0.012432445967492978\n{'min_val_epoch': 135, 'min_val_loss': -0.013215723215055859}\n******************************\nTRAIN: \t Epoch: 141 \t Loss: -0.012298635207116604\nTRAIN: \t Epoch: 141 \t Loss: -0.011969267390668392\nTRAIN: \t Epoch: 141 \t Loss: -0.011614639312028885\nTRAIN: \t Epoch: 141 \t Loss: -0.011402344796806574\nTRAIN: \t Epoch: 141 \t Loss: -0.011068212240934372\nTRAIN: \t Epoch: 141 \t Loss: -0.01108969080572327\nTRAIN: \t Epoch: 141 \t Loss: -0.011311746468501431\nTRAIN: \t Epoch: 141 \t Loss: -0.011457829736173153\nTRAIN: \t Epoch: 141 \t Loss: -0.011225891196065478\nTRAIN: \t Epoch: 141 \t Loss: -0.010953135415911674\nTRAIN: \t Epoch: 141 \t Loss: -0.010973588902164589\nTRAIN: \t Epoch: 141 \t Loss: -0.011121737227464715\nTRAIN: \t Epoch: 141 \t Loss: -0.011273324632873902\nTRAIN: \t Epoch: 141 \t Loss: -0.011397687252610922\nTRAIN: \t Epoch: 141 \t Loss: -0.01145096899320682\nTRAIN: \t Epoch: 141 \t Loss: -0.011448038334492594\nTRAIN: \t Epoch: 141 \t Loss: -0.011290491065558265\nTRAIN: \t Epoch: 141 \t Loss: -0.011313406698819663\nTRAIN: \t Epoch: 141 \t Loss: -0.011314775444997168\nVALD: \t Epoch: 141 \t Loss: -0.014479381963610649\nVALD: \t Epoch: 141 \t Loss: -0.01290882844477892\nVALD: \t Epoch: 141 \t Loss: -0.014553303519884745\nVALD: \t Epoch: 141 \t Loss: -0.014367929194122553\nVALD: \t Epoch: 141 \t Loss: -0.013516417022578973\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 602/602 [00:32<00:00, 18.75it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.3818638188755609  FDE: 0.606985072941723\n**************************************************\n******************************\nEpoch: social-tag : 141\ntrain_loss -0.011314775444997168\nval_loss -0.013516417022578973\n{'min_val_epoch': 141, 'min_val_loss': -0.013516417022578973}\n******************************\nTRAIN: \t Epoch: 142 \t Loss: -0.011996381916105747\nTRAIN: \t Epoch: 142 \t Loss: -0.01252239290624857\nTRAIN: \t Epoch: 142 \t Loss: -0.012779214108983675\nTRAIN: \t Epoch: 142 \t Loss: -0.012641215464100242\nTRAIN: \t Epoch: 142 \t Loss: -0.012040808238089085\nTRAIN: \t Epoch: 142 \t Loss: -0.01177009909103314\nTRAIN: \t Epoch: 142 \t Loss: -0.011793331908328193\nTRAIN: \t Epoch: 142 \t Loss: -0.011905087623745203\nTRAIN: \t Epoch: 142 \t Loss: -0.01196788458360566\nTRAIN: \t Epoch: 142 \t Loss: -0.01200602762401104\nTRAIN: \t Epoch: 142 \t Loss: -0.011744501407850872\nTRAIN: \t Epoch: 142 \t Loss: -0.01164160684371988\nTRAIN: \t Epoch: 142 \t Loss: -0.011602583341300488\nTRAIN: \t Epoch: 142 \t Loss: -0.011707049129264695\nTRAIN: \t Epoch: 142 \t Loss: -0.011769415065646171\nTRAIN: \t Epoch: 142 \t Loss: -0.011844687920529395\nTRAIN: \t Epoch: 142 \t Loss: -0.011778072454035282\nTRAIN: \t Epoch: 142 \t Loss: -0.011732914691997899\nTRAIN: \t Epoch: 142 \t Loss: -0.01173858624994498\nVALD: \t Epoch: 142 \t Loss: -0.013794191181659698\nVALD: \t Epoch: 142 \t Loss: -0.011848673224449158\nVALD: \t Epoch: 142 \t Loss: -0.013389588023225466\nVALD: \t Epoch: 142 \t Loss: -0.013295789249241352\nVALD: \t Epoch: 142 \t Loss: -0.012561594060629852\n******************************\nEpoch: social-tag : 142\ntrain_loss -0.01173858624994498\nval_loss -0.012561594060629852\n{'min_val_epoch': 141, 'min_val_loss': -0.013516417022578973}\n******************************\nTRAIN: \t Epoch: 143 \t Loss: -0.013401498086750507\nTRAIN: \t Epoch: 143 \t Loss: -0.012622977141290903\nTRAIN: \t Epoch: 143 \t Loss: -0.01219812904795011\nTRAIN: \t Epoch: 143 \t Loss: -0.012240492505952716\nTRAIN: \t Epoch: 143 \t Loss: -0.012161982804536819\nTRAIN: \t Epoch: 143 \t Loss: -0.012006174152096113\nTRAIN: \t Epoch: 143 \t Loss: -0.011793490233165877\nTRAIN: \t Epoch: 143 \t Loss: -0.011790343443863094\nTRAIN: \t Epoch: 143 \t Loss: -0.011684523791902594\nTRAIN: \t Epoch: 143 \t Loss: -0.011665136460214854\nTRAIN: \t Epoch: 143 \t Loss: -0.01166302211243998\nTRAIN: \t Epoch: 143 \t Loss: -0.011679249194761118\nTRAIN: \t Epoch: 143 \t Loss: -0.01156050325013124\nTRAIN: \t Epoch: 143 \t Loss: -0.011539355253002473\nTRAIN: \t Epoch: 143 \t Loss: -0.01152615708609422\nTRAIN: \t Epoch: 143 \t Loss: -0.011571977869607508\nTRAIN: \t Epoch: 143 \t Loss: -0.011584993397050044\nTRAIN: \t Epoch: 143 \t Loss: -0.011534655880596902\nTRAIN: \t Epoch: 143 \t Loss: -0.011522054062589913\nVALD: \t Epoch: 143 \t Loss: -0.011974080465734005\nVALD: \t Epoch: 143 \t Loss: -0.010541411582380533\nVALD: \t Epoch: 143 \t Loss: -0.01177450641989708\nVALD: \t Epoch: 143 \t Loss: -0.011819422943517566\nVALD: \t Epoch: 143 \t Loss: -0.011236397688053856\n******************************\nEpoch: social-tag : 143\ntrain_loss -0.011522054062589913\nval_loss -0.011236397688053856\n{'min_val_epoch': 141, 'min_val_loss': -0.013516417022578973}\n******************************\nTRAIN: \t Epoch: 144 \t Loss: -0.010704964399337769\nTRAIN: \t Epoch: 144 \t Loss: -0.01143303606659174\nTRAIN: \t Epoch: 144 \t Loss: -0.01169007271528244\nTRAIN: \t Epoch: 144 \t Loss: -0.011829966679215431\nTRAIN: \t Epoch: 144 \t Loss: -0.01173082012683153\nTRAIN: \t Epoch: 144 \t Loss: -0.011571250390261412\nTRAIN: \t Epoch: 144 \t Loss: -0.011344259870903832\nTRAIN: \t Epoch: 144 \t Loss: -0.011458531953394413\nTRAIN: \t Epoch: 144 \t Loss: -0.011524119931790564\nTRAIN: \t Epoch: 144 \t Loss: -0.011564095783978701\nTRAIN: \t Epoch: 144 \t Loss: -0.011607252654026855\nTRAIN: \t Epoch: 144 \t Loss: -0.011569147231057286\nTRAIN: \t Epoch: 144 \t Loss: -0.01152049656957388\nTRAIN: \t Epoch: 144 \t Loss: -0.011528217300240482\nTRAIN: \t Epoch: 144 \t Loss: -0.011528236108521621\nTRAIN: \t Epoch: 144 \t Loss: -0.011532856442499906\nTRAIN: \t Epoch: 144 \t Loss: -0.01157124515841989\nTRAIN: \t Epoch: 144 \t Loss: -0.011638665778769387\nTRAIN: \t Epoch: 144 \t Loss: -0.011650232946328517\nVALD: \t Epoch: 144 \t Loss: -0.013758235611021519\nVALD: \t Epoch: 144 \t Loss: -0.0124394572339952\nVALD: \t Epoch: 144 \t Loss: -0.014248671941459179\nVALD: \t Epoch: 144 \t Loss: -0.013995036948472261\nVALD: \t Epoch: 144 \t Loss: -0.013204453405269906\n******************************\nEpoch: social-tag : 144\ntrain_loss -0.011650232946328517\nval_loss -0.013204453405269906\n{'min_val_epoch': 141, 'min_val_loss': -0.013516417022578973}\n******************************\nTRAIN: \t Epoch: 145 \t Loss: -0.012433518655598164\nTRAIN: \t Epoch: 145 \t Loss: -0.011771802324801683\nTRAIN: \t Epoch: 145 \t Loss: -0.011242504231631756\nTRAIN: \t Epoch: 145 \t Loss: -0.010949389077723026\nTRAIN: \t Epoch: 145 \t Loss: -0.011112198047339917\nTRAIN: \t Epoch: 145 \t Loss: -0.011146530508995056\nTRAIN: \t Epoch: 145 \t Loss: -0.011229997101638998\nTRAIN: \t Epoch: 145 \t Loss: -0.01146936311852187\nTRAIN: \t Epoch: 145 \t Loss: -0.011585794699688753\nTRAIN: \t Epoch: 145 \t Loss: -0.011632069479674101\nTRAIN: \t Epoch: 145 \t Loss: -0.011555121618915688\nTRAIN: \t Epoch: 145 \t Loss: -0.011390251495564977\nTRAIN: \t Epoch: 145 \t Loss: -0.011409402036896119\nTRAIN: \t Epoch: 145 \t Loss: -0.011528035825384515\nTRAIN: \t Epoch: 145 \t Loss: -0.011618987781306107\nTRAIN: \t Epoch: 145 \t Loss: -0.011580520134884864\nTRAIN: \t Epoch: 145 \t Loss: -0.01156301555388114\nTRAIN: \t Epoch: 145 \t Loss: -0.01160636885712544\nTRAIN: \t Epoch: 145 \t Loss: -0.01161580750338483\nVALD: \t Epoch: 145 \t Loss: -0.011762221343815327\nVALD: \t Epoch: 145 \t Loss: -0.009855877608060837\nVALD: \t Epoch: 145 \t Loss: -0.011264847591519356\nVALD: \t Epoch: 145 \t Loss: -0.011240498628467321\nVALD: \t Epoch: 145 \t Loss: -0.010566027597947555\n******************************\nEpoch: social-tag : 145\ntrain_loss -0.01161580750338483\nval_loss -0.010566027597947555\n{'min_val_epoch': 141, 'min_val_loss': -0.013516417022578973}\n******************************\nTRAIN: \t Epoch: 146 \t Loss: -0.012189234606921673\nTRAIN: \t Epoch: 146 \t Loss: -0.012290083803236485\nTRAIN: \t Epoch: 146 \t Loss: -0.011650165853401026\nTRAIN: \t Epoch: 146 \t Loss: -0.011711335508152843\nTRAIN: \t Epoch: 146 \t Loss: -0.011914926208555698\nTRAIN: \t Epoch: 146 \t Loss: -0.011709594478209814\nTRAIN: \t Epoch: 146 \t Loss: -0.011338551661797933\nTRAIN: \t Epoch: 146 \t Loss: -0.011399682960473001\nTRAIN: \t Epoch: 146 \t Loss: -0.0115529154944751\nTRAIN: \t Epoch: 146 \t Loss: -0.011626332625746727\nTRAIN: \t Epoch: 146 \t Loss: -0.011689124544235792\nTRAIN: \t Epoch: 146 \t Loss: -0.01169407918738822\nTRAIN: \t Epoch: 146 \t Loss: -0.011531889366988953\nTRAIN: \t Epoch: 146 \t Loss: -0.011397119877593858\nTRAIN: \t Epoch: 146 \t Loss: -0.011473564803600312\nTRAIN: \t Epoch: 146 \t Loss: -0.011518860759679228\nTRAIN: \t Epoch: 146 \t Loss: -0.011541006484014146\nTRAIN: \t Epoch: 146 \t Loss: -0.011594018071062036\nTRAIN: \t Epoch: 146 \t Loss: -0.011589714132640816\nVALD: \t Epoch: 146 \t Loss: -0.014266720041632652\nVALD: \t Epoch: 146 \t Loss: -0.012326284311711788\nVALD: \t Epoch: 146 \t Loss: -0.013814741745591164\nVALD: \t Epoch: 146 \t Loss: -0.013597972691059113\nVALD: \t Epoch: 146 \t Loss: -0.012740310263042608\n******************************\nEpoch: social-tag : 146\ntrain_loss -0.011589714132640816\nval_loss -0.012740310263042608\n{'min_val_epoch': 141, 'min_val_loss': -0.013516417022578973}\n******************************\nTRAIN: \t Epoch: 147 \t Loss: -0.012317332439124584\nTRAIN: \t Epoch: 147 \t Loss: -0.011971567291766405\nTRAIN: \t Epoch: 147 \t Loss: -0.011246086098253727\nTRAIN: \t Epoch: 147 \t Loss: -0.011414084350690246\nTRAIN: \t Epoch: 147 \t Loss: -0.011715363897383212\nTRAIN: \t Epoch: 147 \t Loss: -0.011827193200588226\nTRAIN: \t Epoch: 147 \t Loss: -0.01171443544860397\nTRAIN: \t Epoch: 147 \t Loss: -0.01172416063491255\nTRAIN: \t Epoch: 147 \t Loss: -0.011711998532215754\nTRAIN: \t Epoch: 147 \t Loss: -0.01168937999755144\nTRAIN: \t Epoch: 147 \t Loss: -0.011730204251679506\nTRAIN: \t Epoch: 147 \t Loss: -0.011763705406337976\nTRAIN: \t Epoch: 147 \t Loss: -0.011796938470349861\nTRAIN: \t Epoch: 147 \t Loss: -0.011637791126434292\nTRAIN: \t Epoch: 147 \t Loss: -0.011602487539251645\nTRAIN: \t Epoch: 147 \t Loss: -0.011568037618417293\nTRAIN: \t Epoch: 147 \t Loss: -0.011627251446685371\nTRAIN: \t Epoch: 147 \t Loss: -0.011601755240311226\nTRAIN: \t Epoch: 147 \t Loss: -0.011607611036885925\nVALD: \t Epoch: 147 \t Loss: -0.012854238972067833\nVALD: \t Epoch: 147 \t Loss: -0.011536682955920696\nVALD: \t Epoch: 147 \t Loss: -0.013182281826933226\nVALD: \t Epoch: 147 \t Loss: -0.013161784037947655\nVALD: \t Epoch: 147 \t Loss: -0.012426740177406753\n******************************\nEpoch: social-tag : 147\ntrain_loss -0.011607611036885925\nval_loss -0.012426740177406753\n{'min_val_epoch': 141, 'min_val_loss': -0.013516417022578973}\n******************************\nTRAIN: \t Epoch: 148 \t Loss: -0.012019235640764236\nTRAIN: \t Epoch: 148 \t Loss: -0.011927036568522453\nTRAIN: \t Epoch: 148 \t Loss: -0.012138564450045427\nTRAIN: \t Epoch: 148 \t Loss: -0.012020838912576437\nTRAIN: \t Epoch: 148 \t Loss: -0.011771680228412152\nTRAIN: \t Epoch: 148 \t Loss: -0.011643454587707916\nTRAIN: \t Epoch: 148 \t Loss: -0.01162826722221715\nTRAIN: \t Epoch: 148 \t Loss: -0.011630922090262175\nTRAIN: \t Epoch: 148 \t Loss: -0.011707117781043053\nTRAIN: \t Epoch: 148 \t Loss: -0.011726273130625487\nTRAIN: \t Epoch: 148 \t Loss: -0.011645793152803724\nTRAIN: \t Epoch: 148 \t Loss: -0.01151724842687448\nTRAIN: \t Epoch: 148 \t Loss: -0.011544579926591653\nTRAIN: \t Epoch: 148 \t Loss: -0.011540528719446488\nTRAIN: \t Epoch: 148 \t Loss: -0.011549704583982627\nTRAIN: \t Epoch: 148 \t Loss: -0.011611476424150169\nTRAIN: \t Epoch: 148 \t Loss: -0.011521819705034004\nTRAIN: \t Epoch: 148 \t Loss: -0.011426829836434789\nTRAIN: \t Epoch: 148 \t Loss: -0.01143307522465503\nVALD: \t Epoch: 148 \t Loss: -0.01209796778857708\nVALD: \t Epoch: 148 \t Loss: -0.010670955758541822\nVALD: \t Epoch: 148 \t Loss: -0.012340445381899675\nVALD: \t Epoch: 148 \t Loss: -0.012415208853781223\nVALD: \t Epoch: 148 \t Loss: -0.011730195569597986\n******************************\nEpoch: social-tag : 148\ntrain_loss -0.01143307522465503\nval_loss -0.011730195569597986\n{'min_val_epoch': 141, 'min_val_loss': -0.013516417022578973}\n******************************\nTRAIN: \t Epoch: 149 \t Loss: -0.012043078429996967\nTRAIN: \t Epoch: 149 \t Loss: -0.012478318065404892\nTRAIN: \t Epoch: 149 \t Loss: -0.012311283809443315\nTRAIN: \t Epoch: 149 \t Loss: -0.011917414143681526\nTRAIN: \t Epoch: 149 \t Loss: -0.011836274713277816\nTRAIN: \t Epoch: 149 \t Loss: -0.011972347429643074\nTRAIN: \t Epoch: 149 \t Loss: -0.011889768365238394\nTRAIN: \t Epoch: 149 \t Loss: -0.011950229411013424\nTRAIN: \t Epoch: 149 \t Loss: -0.011991496922241317\nTRAIN: \t Epoch: 149 \t Loss: -0.012015757523477078\nTRAIN: \t Epoch: 149 \t Loss: -0.011889036067507484\nTRAIN: \t Epoch: 149 \t Loss: -0.011798588481421271\nTRAIN: \t Epoch: 149 \t Loss: -0.011736202985048294\nTRAIN: \t Epoch: 149 \t Loss: -0.011760702556265252\nTRAIN: \t Epoch: 149 \t Loss: -0.011780511463681857\nTRAIN: \t Epoch: 149 \t Loss: -0.011811626143753529\nTRAIN: \t Epoch: 149 \t Loss: -0.011840620023362777\nTRAIN: \t Epoch: 149 \t Loss: -0.011818015378796391\nTRAIN: \t Epoch: 149 \t Loss: -0.011801400009308469\nVALD: \t Epoch: 149 \t Loss: -0.012650646269321442\nVALD: \t Epoch: 149 \t Loss: -0.010783400386571884\nVALD: \t Epoch: 149 \t Loss: -0.011741994259258112\nVALD: \t Epoch: 149 \t Loss: -0.011644352227449417\nVALD: \t Epoch: 149 \t Loss: -0.011034894974763728\n******************************\nEpoch: social-tag : 149\ntrain_loss -0.011801400009308469\nval_loss -0.011034894974763728\n{'min_val_epoch': 141, 'min_val_loss': -0.013516417022578973}\n******************************\nTRAIN: \t Epoch: 150 \t Loss: -0.01130190771073103\nTRAIN: \t Epoch: 150 \t Loss: -0.011350215878337622\nTRAIN: \t Epoch: 150 \t Loss: -0.0116770314052701\nTRAIN: \t Epoch: 150 \t Loss: -0.011675706831738353\nTRAIN: \t Epoch: 150 \t Loss: -0.011741825006902218\nTRAIN: \t Epoch: 150 \t Loss: -0.011865246264884869\nTRAIN: \t Epoch: 150 \t Loss: -0.011894299249563898\nTRAIN: \t Epoch: 150 \t Loss: -0.012019668123684824\nTRAIN: \t Epoch: 150 \t Loss: -0.01210378110408783\nTRAIN: \t Epoch: 150 \t Loss: -0.012113912217319011\nTRAIN: \t Epoch: 150 \t Loss: -0.0121922567486763\nTRAIN: \t Epoch: 150 \t Loss: -0.012230138604839643\nTRAIN: \t Epoch: 150 \t Loss: -0.012259739666030956\nTRAIN: \t Epoch: 150 \t Loss: -0.012286614493599959\nTRAIN: \t Epoch: 150 \t Loss: -0.01229096595197916\nTRAIN: \t Epoch: 150 \t Loss: -0.01234405708964914\nTRAIN: \t Epoch: 150 \t Loss: -0.01234324079226045\nTRAIN: \t Epoch: 150 \t Loss: -0.012380854071428379\nTRAIN: \t Epoch: 150 \t Loss: -0.012369604574309455\nVALD: \t Epoch: 150 \t Loss: -0.0137985460460186\nVALD: \t Epoch: 150 \t Loss: -0.01190637843683362\nVALD: \t Epoch: 150 \t Loss: -0.014025360656281313\nVALD: \t Epoch: 150 \t Loss: -0.013960819458588958\nVALD: \t Epoch: 150 \t Loss: -0.013116201093374205\n******************************\nEpoch: social-tag : 150\ntrain_loss -0.012369604574309455\nval_loss -0.013116201093374205\n{'min_val_epoch': 141, 'min_val_loss': -0.013516417022578973}\n******************************\nTRAIN: \t Epoch: 151 \t Loss: -0.012858106754720211\nTRAIN: \t Epoch: 151 \t Loss: -0.01236506411805749\nTRAIN: \t Epoch: 151 \t Loss: -0.012807872456808886\nTRAIN: \t Epoch: 151 \t Loss: -0.012813046341761947\nTRAIN: \t Epoch: 151 \t Loss: -0.01293633971363306\nTRAIN: \t Epoch: 151 \t Loss: -0.0130391422038277\nTRAIN: \t Epoch: 151 \t Loss: -0.01300985845071929\nTRAIN: \t Epoch: 151 \t Loss: -0.012969491886906326\nTRAIN: \t Epoch: 151 \t Loss: -0.013011250955363115\nTRAIN: \t Epoch: 151 \t Loss: -0.013039151951670647\nTRAIN: \t Epoch: 151 \t Loss: -0.013000464998185635\nTRAIN: \t Epoch: 151 \t Loss: -0.012975285373007258\nTRAIN: \t Epoch: 151 \t Loss: -0.01289430153197967\nTRAIN: \t Epoch: 151 \t Loss: -0.012892428852085556\nTRAIN: \t Epoch: 151 \t Loss: -0.012850292213261127\nTRAIN: \t Epoch: 151 \t Loss: -0.012867859099060297\nTRAIN: \t Epoch: 151 \t Loss: -0.01290046818116132\nTRAIN: \t Epoch: 151 \t Loss: -0.012851513560033508\nTRAIN: \t Epoch: 151 \t Loss: -0.012834508270410296\nVALD: \t Epoch: 151 \t Loss: -0.01478620246052742\nVALD: \t Epoch: 151 \t Loss: -0.01302527030929923\nVALD: \t Epoch: 151 \t Loss: -0.014853923581540585\nVALD: \t Epoch: 151 \t Loss: -0.014642984606325626\nVALD: \t Epoch: 151 \t Loss: -0.013747511619378712\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": " 76%|███████▌  | 456/602 [00:24<00:05, 24.73it/s]",
          "output_type": "stream"
        }
      ]
    }
  ]
}