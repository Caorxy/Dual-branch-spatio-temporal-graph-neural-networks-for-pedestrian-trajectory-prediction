{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8544056,
          "sourceType": "datasetVersion",
          "datasetId": 5104693
        }
      ],
      "dockerImageVersionId": 30715,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Defining the model\n",
        "\n",
        "model = social_stgcnn(n_stgcnn =args.n_stgcnn,n_txpcnn=args.n_txpcnn,\n",
        "output_feat=args.output_size,seq_len=args.obs_seq_len,\n",
        "kernel_size=args.kernel_size,pred_seq_len=args.pred_seq_len)\n",
        "\n",
        "\n",
        "#Training settings\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(),lr=args.lr)\n",
        "\n",
        "if args.use_lrschd:\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_sh_rate, gamma=0.1)\n",
        "\n",
        "\n",
        "\n",
        "checkpoint_dir = './checkpoint/'+args.tag+'/'\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "with open(checkpoint_dir+'args.pkl', 'wb') as fp:\n",
        "    pickle.dump(args, fp)\n",
        "\n",
        "\n",
        "\n",
        "print('Data and model loaded')\n",
        "print('Checkpoint dir:', checkpoint_dir)\n",
        "\n",
        "#Training\n",
        "metrics = {'train_loss':[],  'val_loss':[]}\n",
        "constant_metrics = {'min_val_epoch':-1, 'min_val_loss':9999999999999999}\n",
        "\n",
        "def train(epoch):\n",
        "    global metrics,loader_train\n",
        "    model.train()\n",
        "    loss_batch = 0\n",
        "    batch_count = 0\n",
        "    is_fst_loss = True\n",
        "    loader_len = len(loader_train)\n",
        "    turn_point =int(loader_len/args.batch_size)*args.batch_size+ loader_len%args.batch_size -1\n",
        "\n",
        "\n",
        "    for cnt,batch in enumerate(loader_train):\n",
        "        batch_count+=1\n",
        "\n",
        "        #Get data\n",
        "        batch = [tensor for tensor in batch]\n",
        "        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, non_linear_ped,\\\n",
        "         loss_mask,V_obs,A_obs,A_dir_obs, V_tr,A_tr,A_dir_tr = batch\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        #Forward\n",
        "        #V_obs = batch,seq,node,feat\n",
        "        #V_obs_tmp = batch,feat,seq,node\n",
        "        V_obs_tmp =V_obs.permute(0,3,1,2)\n",
        "\n",
        "        V_pred,_,_ = model(V_obs_tmp,A_obs.squeeze(), A_dir_obs.squeeze())\n",
        "\n",
        "        V_pred = V_pred.permute(0,2,3,1)\n",
        "\n",
        "\n",
        "\n",
        "        V_tr = V_tr.squeeze()\n",
        "        A_tr = A_tr.squeeze()\n",
        "        A_dir_tr = A_dir_tr.squeeze()\n",
        "        V_pred = V_pred.squeeze()\n",
        "\n",
        "        if batch_count%args.batch_size !=0 and cnt != turn_point :\n",
        "            l = graph_loss(V_pred,V_tr)\n",
        "            if is_fst_loss :\n",
        "                loss = l\n",
        "                is_fst_loss = False\n",
        "            else:\n",
        "                loss += l\n",
        "\n",
        "        else:\n",
        "            loss = loss/args.batch_size\n",
        "            is_fst_loss = True\n",
        "            loss.backward()\n",
        "\n",
        "            if args.clip_grad is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(),args.clip_grad)\n",
        "\n",
        "\n",
        "            optimizer.step()\n",
        "            #Metrics\n",
        "            loss_batch += loss.item()\n",
        "            print('TRAIN:','\\t Epoch:', epoch,'\\t Loss:',loss_batch/batch_count)\n",
        "\n",
        "    metrics['train_loss'].append(loss_batch/batch_count)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def vald(epoch):\n",
        "    global metrics,loader_val,constant_metrics\n",
        "    model.eval()\n",
        "    loss_batch = 0\n",
        "    batch_count = 0\n",
        "    is_fst_loss = True\n",
        "    loader_len = len(loader_val)\n",
        "    turn_point =int(loader_len/args.batch_size)*args.batch_size+ loader_len%args.batch_size -1\n",
        "\n",
        "    for cnt,batch in enumerate(loader_val):\n",
        "        batch_count+=1\n",
        "\n",
        "        #Get data\n",
        "        batch = [tensor for tensor in batch]\n",
        "        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, non_linear_ped,\\\n",
        "         loss_mask,V_obs,A_obs,A_dir_obs, V_tr,A_tr,A_dir_tr = batch\n",
        "\n",
        "\n",
        "        V_obs_tmp =V_obs.permute(0,3,1,2)\n",
        "\n",
        "        V_pred,_,_ = model(V_obs_tmp,A_obs.squeeze(), A_dir_obs.squeeze())\n",
        "\n",
        "        V_pred = V_pred.permute(0,2,3,1)\n",
        "\n",
        "        V_tr = V_tr.squeeze()\n",
        "        A_tr = A_tr.squeeze()\n",
        "        A_dir_tr = A_dir_tr.squeeze()\n",
        "        V_pred = V_pred.squeeze()\n",
        "\n",
        "        if batch_count%args.batch_size !=0 and cnt != turn_point :\n",
        "            l = graph_loss(V_pred,V_tr)\n",
        "            if is_fst_loss :\n",
        "                loss = l\n",
        "                is_fst_loss = False\n",
        "            else:\n",
        "                loss += l\n",
        "\n",
        "        else:\n",
        "            loss = loss/args.batch_size\n",
        "            is_fst_loss = True\n",
        "            #Metrics\n",
        "            loss_batch += loss.item()\n",
        "            print('VALD:','\\t Epoch:', epoch,'\\t Loss:',loss_batch/batch_count)\n",
        "\n",
        "    metrics['val_loss'].append(loss_batch/batch_count)\n",
        "\n",
        "    if  metrics['val_loss'][-1]< constant_metrics['min_val_loss']:\n",
        "        constant_metrics['min_val_loss'] =  metrics['val_loss'][-1]\n",
        "        constant_metrics['min_val_epoch'] = epoch\n",
        "        torch.save(model.state_dict(),checkpoint_dir+'val_best.pth')  # OK\n",
        "        check_test_performance()\n",
        "\n",
        "print('Training started ...')\n",
        "for epoch in range(args.num_epochs):\n",
        "    train(epoch)\n",
        "    vald(epoch)\n",
        "    if args.use_lrschd:\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "    print('*'*30)\n",
        "    print('Epoch:',args.tag,\":\", epoch)\n",
        "    for k,v in metrics.items():\n",
        "        if len(v)>0:\n",
        "            print(k,v[-1])\n",
        "\n",
        "\n",
        "    print(constant_metrics)\n",
        "    print('*'*30)\n",
        "\n",
        "    with open(checkpoint_dir+'metrics.pkl', 'wb') as fp:\n",
        "        pickle.dump(metrics, fp)\n",
        "\n",
        "    with open(checkpoint_dir+'constant_metrics.pkl', 'wb') as fp:\n",
        "        pickle.dump(constant_metrics, fp)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T19:51:06.502846Z",
          "iopub.execute_input": "2024-06-03T19:51:06.503205Z",
          "iopub.status.idle": "2024-06-03T22:58:14.088957Z",
          "shell.execute_reply.started": "2024-06-03T19:51:06.503179Z",
          "shell.execute_reply": "2024-06-03T22:58:14.087667Z"
        },
        "trusted": true,
        "id": "4hfhMIxvfLwG",
        "outputId": "1595ed80-5dea-4461-9b67-10e0d591e17a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Data and model loaded\nCheckpoint dir: ./checkpoint/social-tag/\nTraining started ...\nTRAIN: \t Epoch: 0 \t Loss: 0.016137098893523216\nTRAIN: \t Epoch: 0 \t Loss: 0.015971467830240726\nTRAIN: \t Epoch: 0 \t Loss: 0.01582263099650542\nTRAIN: \t Epoch: 0 \t Loss: 0.01569222123362124\nTRAIN: \t Epoch: 0 \t Loss: 0.01556869801133871\nTRAIN: \t Epoch: 0 \t Loss: 0.015435969922691584\nTRAIN: \t Epoch: 0 \t Loss: 0.015314615198544093\nTRAIN: \t Epoch: 0 \t Loss: 0.015195480664260685\nTRAIN: \t Epoch: 0 \t Loss: 0.015073743441866504\nTRAIN: \t Epoch: 0 \t Loss: 0.014958398323506117\nTRAIN: \t Epoch: 0 \t Loss: 0.014832561158321121\nTRAIN: \t Epoch: 0 \t Loss: 0.014712462977816662\nTRAIN: \t Epoch: 0 \t Loss: 0.014586966413144883\nTRAIN: \t Epoch: 0 \t Loss: 0.014463134109973907\nTRAIN: \t Epoch: 0 \t Loss: 0.014330958699186642\nTRAIN: \t Epoch: 0 \t Loss: 0.014194586256053299\nTRAIN: \t Epoch: 0 \t Loss: 0.014058986569152158\nTRAIN: \t Epoch: 0 \t Loss: 0.013917815768056445\nTRAIN: \t Epoch: 0 \t Loss: 0.013773563170903608\nTRAIN: \t Epoch: 0 \t Loss: 0.013618041016161441\nTRAIN: \t Epoch: 0 \t Loss: 0.013448961966094516\nTRAIN: \t Epoch: 0 \t Loss: 0.013322172451704152\nVALD: \t Epoch: 0 \t Loss: 0.00962913315743208\nVALD: \t Epoch: 0 \t Loss: 0.009761645458638668\nVALD: \t Epoch: 0 \t Loss: 0.012069761753082275\nVALD: \t Epoch: 0 \t Loss: 0.012725163949653506\nVALD: \t Epoch: 0 \t Loss: 0.012118774093687534\nVALD: \t Epoch: 0 \t Loss: 0.012009402612845103\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 70/70 [00:02<00:00, 25.85it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 1.9133237653693322  FDE: 2.7102087273278452\n**************************************************\n******************************\nEpoch: social-tag : 0\ntrain_loss 0.013322172451704152\nval_loss 0.012009402612845103\n{'min_val_epoch': 0, 'min_val_loss': 0.012009402612845103}\n******************************\nTRAIN: \t Epoch: 1 \t Loss: 0.009136193431913853\nTRAIN: \t Epoch: 1 \t Loss: 0.00898823793977499\nTRAIN: \t Epoch: 1 \t Loss: 0.008789907830456892\nTRAIN: \t Epoch: 1 \t Loss: 0.008572125108912587\nTRAIN: \t Epoch: 1 \t Loss: 0.008344466891139745\nTRAIN: \t Epoch: 1 \t Loss: 0.008179783743495742\nTRAIN: \t Epoch: 1 \t Loss: 0.008020659309944935\nTRAIN: \t Epoch: 1 \t Loss: 0.007809473667293787\nTRAIN: \t Epoch: 1 \t Loss: 0.007620842920409309\nTRAIN: \t Epoch: 1 \t Loss: 0.007457277784124017\nTRAIN: \t Epoch: 1 \t Loss: 0.007289383348754861\nTRAIN: \t Epoch: 1 \t Loss: 0.007147022445375721\nTRAIN: \t Epoch: 1 \t Loss: 0.0070416820951952385\nTRAIN: \t Epoch: 1 \t Loss: 0.007041029365999358\nTRAIN: \t Epoch: 1 \t Loss: 0.007008424773812294\nTRAIN: \t Epoch: 1 \t Loss: 0.006905796763021499\nTRAIN: \t Epoch: 1 \t Loss: 0.006777419626493664\nTRAIN: \t Epoch: 1 \t Loss: 0.0066253982950001955\nTRAIN: \t Epoch: 1 \t Loss: 0.006529211875443396\nTRAIN: \t Epoch: 1 \t Loss: 0.006466670380905271\nTRAIN: \t Epoch: 1 \t Loss: 0.006401742919392529\nTRAIN: \t Epoch: 1 \t Loss: 0.0063257849944130095\nVALD: \t Epoch: 1 \t Loss: 0.003911411389708519\nVALD: \t Epoch: 1 \t Loss: 0.005020234966650605\nVALD: \t Epoch: 1 \t Loss: 0.008408764532456795\nVALD: \t Epoch: 1 \t Loss: 0.009556514560244977\nVALD: \t Epoch: 1 \t Loss: 0.008529247995465995\nVALD: \t Epoch: 1 \t Loss: 0.008431297766439843\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 70/70 [00:02<00:00, 25.54it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 1.716184230197125  FDE: 2.5759416993879114\n**************************************************\n******************************\nEpoch: social-tag : 1\ntrain_loss 0.0063257849944130095\nval_loss 0.008431297766439843\n{'min_val_epoch': 1, 'min_val_loss': 0.008431297766439843}\n******************************\nTRAIN: \t Epoch: 2 \t Loss: 0.0036069550551474094\nTRAIN: \t Epoch: 2 \t Loss: 0.0038039223290979862\nTRAIN: \t Epoch: 2 \t Loss: 0.003968314733356237\nTRAIN: \t Epoch: 2 \t Loss: 0.004904887289740145\nTRAIN: \t Epoch: 2 \t Loss: 0.005210123769938946\nTRAIN: \t Epoch: 2 \t Loss: 0.005227515163520972\nTRAIN: \t Epoch: 2 \t Loss: 0.005049059034458229\nTRAIN: \t Epoch: 2 \t Loss: 0.0048292987630702555\nTRAIN: \t Epoch: 2 \t Loss: 0.00459916361918052\nTRAIN: \t Epoch: 2 \t Loss: 0.004426805372349918\nTRAIN: \t Epoch: 2 \t Loss: 0.004285929216579957\nTRAIN: \t Epoch: 2 \t Loss: 0.0042633513221517205\nTRAIN: \t Epoch: 2 \t Loss: 0.00446029813387073\nTRAIN: \t Epoch: 2 \t Loss: 0.0045443137641996145\nTRAIN: \t Epoch: 2 \t Loss: 0.004494009151433905\nTRAIN: \t Epoch: 2 \t Loss: 0.004388893532450311\nTRAIN: \t Epoch: 2 \t Loss: 0.0042749126030899144\nTRAIN: \t Epoch: 2 \t Loss: 0.0041318161270788144\nTRAIN: \t Epoch: 2 \t Loss: 0.0040138658675316135\nTRAIN: \t Epoch: 2 \t Loss: 0.0039000015531200916\nTRAIN: \t Epoch: 2 \t Loss: 0.003806270160047071\nTRAIN: \t Epoch: 2 \t Loss: 0.0038006698742803166\nVALD: \t Epoch: 2 \t Loss: 0.005511438939720392\nVALD: \t Epoch: 2 \t Loss: 0.0058487593196332455\nVALD: \t Epoch: 2 \t Loss: 0.00994686254610618\nVALD: \t Epoch: 2 \t Loss: 0.014640941517427564\nVALD: \t Epoch: 2 \t Loss: 0.013121709786355495\nVALD: \t Epoch: 2 \t Loss: 0.012935919924215836\n******************************\nEpoch: social-tag : 2\ntrain_loss 0.0038006698742803166\nval_loss 0.012935919924215836\n{'min_val_epoch': 1, 'min_val_loss': 0.008431297766439843}\n******************************\nTRAIN: \t Epoch: 3 \t Loss: 0.004346733912825584\nTRAIN: \t Epoch: 3 \t Loss: 0.0035617989487946033\nTRAIN: \t Epoch: 3 \t Loss: 0.0029277439462020993\nTRAIN: \t Epoch: 3 \t Loss: 0.0026255368429701775\nTRAIN: \t Epoch: 3 \t Loss: 0.0024616541806608437\nTRAIN: \t Epoch: 3 \t Loss: 0.0026094188603262105\nTRAIN: \t Epoch: 3 \t Loss: 0.003073250874876976\nTRAIN: \t Epoch: 3 \t Loss: 0.0031482656486332417\nTRAIN: \t Epoch: 3 \t Loss: 0.0030667646788060665\nTRAIN: \t Epoch: 3 \t Loss: 0.002884465840179473\nTRAIN: \t Epoch: 3 \t Loss: 0.002668779009995474\nTRAIN: \t Epoch: 3 \t Loss: 0.0027160424360772595\nTRAIN: \t Epoch: 3 \t Loss: 0.0029764194981768155\nTRAIN: \t Epoch: 3 \t Loss: 0.0030659600701515694\nTRAIN: \t Epoch: 3 \t Loss: 0.0030558556667529047\nTRAIN: \t Epoch: 3 \t Loss: 0.0029695417288166936\nTRAIN: \t Epoch: 3 \t Loss: 0.002828824128407766\nTRAIN: \t Epoch: 3 \t Loss: 0.00271047181255805\nTRAIN: \t Epoch: 3 \t Loss: 0.0026166596339623396\nTRAIN: \t Epoch: 3 \t Loss: 0.0025167299900203943\nTRAIN: \t Epoch: 3 \t Loss: 0.0025635419989980405\nTRAIN: \t Epoch: 3 \t Loss: 0.002628444052256202\nVALD: \t Epoch: 3 \t Loss: 0.003174553159624338\nVALD: \t Epoch: 3 \t Loss: 0.002951285568997264\nVALD: \t Epoch: 3 \t Loss: 0.0043791950059433775\nVALD: \t Epoch: 3 \t Loss: 0.004818372544832528\nVALD: \t Epoch: 3 \t Loss: 0.0044984699226915835\nVALD: \t Epoch: 3 \t Loss: 0.004485917091369629\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 70/70 [00:02<00:00, 25.95it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 1.4300111251230292  FDE: 2.033452722153643\n**************************************************\n******************************\nEpoch: social-tag : 3\ntrain_loss 0.002628444052256202\nval_loss 0.004485917091369629\n{'min_val_epoch': 3, 'min_val_loss': 0.004485917091369629}\n******************************\nTRAIN: \t Epoch: 4 \t Loss: 0.00266238278709352\nTRAIN: \t Epoch: 4 \t Loss: 0.0021564409253187478\nTRAIN: \t Epoch: 4 \t Loss: 0.0019027217058464885\nTRAIN: \t Epoch: 4 \t Loss: 0.0015694172325311229\nTRAIN: \t Epoch: 4 \t Loss: 0.001366744504775852\nTRAIN: \t Epoch: 4 \t Loss: 0.0017621332760124158\nTRAIN: \t Epoch: 4 \t Loss: 0.0020221403184612946\nTRAIN: \t Epoch: 4 \t Loss: 0.002074067590001505\nTRAIN: \t Epoch: 4 \t Loss: 0.0019666478619910777\nTRAIN: \t Epoch: 4 \t Loss: 0.0017710549938783516\nTRAIN: \t Epoch: 4 \t Loss: 0.0016594308445664037\nTRAIN: \t Epoch: 4 \t Loss: 0.0015338043067458784\nTRAIN: \t Epoch: 4 \t Loss: 0.001469699523183338\nTRAIN: \t Epoch: 4 \t Loss: 0.001491431097487553\nTRAIN: \t Epoch: 4 \t Loss: 0.0014882426248126043\nTRAIN: \t Epoch: 4 \t Loss: 0.001379541636651993\nTRAIN: \t Epoch: 4 \t Loss: 0.0012525951079784565\nTRAIN: \t Epoch: 4 \t Loss: 0.0012612993001918464\nTRAIN: \t Epoch: 4 \t Loss: 0.0014842836432140565\nTRAIN: \t Epoch: 4 \t Loss: 0.00159226564464916\nTRAIN: \t Epoch: 4 \t Loss: 0.0016045582669903524\nTRAIN: \t Epoch: 4 \t Loss: 0.0015652248328233964\nVALD: \t Epoch: 4 \t Loss: -1.0178764568991028e-05\nVALD: \t Epoch: 4 \t Loss: -0.00035175177163182525\nVALD: \t Epoch: 4 \t Loss: 0.0031683267473757346\nVALD: \t Epoch: 4 \t Loss: 0.004239306278122967\nVALD: \t Epoch: 4 \t Loss: 0.0035107391438941706\nVALD: \t Epoch: 4 \t Loss: 0.003467425276793427\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 70/70 [00:02<00:00, 25.92it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 1.2555149809323325  FDE: 1.889430661557245\n**************************************************\n******************************\nEpoch: social-tag : 4\ntrain_loss 0.0015652248328233964\nval_loss 0.003467425276793427\n{'min_val_epoch': 4, 'min_val_loss': 0.003467425276793427}\n******************************\nTRAIN: \t Epoch: 5 \t Loss: -0.0002096564567182213\nTRAIN: \t Epoch: 5 \t Loss: -0.00032755783468019217\nTRAIN: \t Epoch: 5 \t Loss: -0.0006299367620764921\nTRAIN: \t Epoch: 5 \t Loss: -0.00033350640296703205\nTRAIN: \t Epoch: 5 \t Loss: 0.000284265197115019\nTRAIN: \t Epoch: 5 \t Loss: 0.0005789750575786456\nTRAIN: \t Epoch: 5 \t Loss: 0.0005395045736804605\nTRAIN: \t Epoch: 5 \t Loss: 0.0004380340797069948\nTRAIN: \t Epoch: 5 \t Loss: 0.00026612856567630335\nTRAIN: \t Epoch: 5 \t Loss: 0.00022661725233774632\nTRAIN: \t Epoch: 5 \t Loss: 0.0004961937427817082\nTRAIN: \t Epoch: 5 \t Loss: 0.0006742628344606297\nTRAIN: \t Epoch: 5 \t Loss: 0.0007369791206796295\nTRAIN: \t Epoch: 5 \t Loss: 0.0006720785219970692\nTRAIN: \t Epoch: 5 \t Loss: 0.0005388608638895676\nTRAIN: \t Epoch: 5 \t Loss: 0.0004408666400195216\nTRAIN: \t Epoch: 5 \t Loss: 0.0003642961254466654\nTRAIN: \t Epoch: 5 \t Loss: 0.0002546401430865646\nTRAIN: \t Epoch: 5 \t Loss: 0.00023135631023538545\nTRAIN: \t Epoch: 5 \t Loss: 0.0004096186712558847\nTRAIN: \t Epoch: 5 \t Loss: 0.0004799927666594851\nTRAIN: \t Epoch: 5 \t Loss: 0.0004700766946565313\nVALD: \t Epoch: 5 \t Loss: -0.00027497115661390126\nVALD: \t Epoch: 5 \t Loss: 0.0002704987273318693\nVALD: \t Epoch: 5 \t Loss: 0.0036147198309966675\nVALD: \t Epoch: 5 \t Loss: 0.009351221284305211\nVALD: \t Epoch: 5 \t Loss: 0.007572341785999015\nVALD: \t Epoch: 5 \t Loss: 0.007399474846368486\n******************************\nEpoch: social-tag : 5\ntrain_loss 0.0004700766946565313\nval_loss 0.007399474846368486\n{'min_val_epoch': 4, 'min_val_loss': 0.003467425276793427}\n******************************\nTRAIN: \t Epoch: 6 \t Loss: -0.000632606737781316\nTRAIN: \t Epoch: 6 \t Loss: -0.0012161978229414672\nTRAIN: \t Epoch: 6 \t Loss: -0.0012380863190628588\nTRAIN: \t Epoch: 6 \t Loss: -0.0012348281888989732\nTRAIN: \t Epoch: 6 \t Loss: -0.0009399565547937528\nTRAIN: \t Epoch: 6 \t Loss: -0.000588780242348245\nTRAIN: \t Epoch: 6 \t Loss: -0.000492994931326913\nTRAIN: \t Epoch: 6 \t Loss: -0.0005823899045935832\nTRAIN: \t Epoch: 6 \t Loss: -0.0006747893769190543\nTRAIN: \t Epoch: 6 \t Loss: -0.0007800916617270559\nTRAIN: \t Epoch: 6 \t Loss: -0.0008769803733395582\nTRAIN: \t Epoch: 6 \t Loss: -0.0006669552579599743\nTRAIN: \t Epoch: 6 \t Loss: -0.0005455373768479779\nTRAIN: \t Epoch: 6 \t Loss: -0.0005369327222329698\nTRAIN: \t Epoch: 6 \t Loss: -0.0006221191220295926\nTRAIN: \t Epoch: 6 \t Loss: -0.0006988884670136031\nTRAIN: \t Epoch: 6 \t Loss: -0.0007594616241369615\nTRAIN: \t Epoch: 6 \t Loss: -0.000723499309970066\nTRAIN: \t Epoch: 6 \t Loss: -0.0006354732843311993\nTRAIN: \t Epoch: 6 \t Loss: -0.0006303417147137224\nTRAIN: \t Epoch: 6 \t Loss: -0.0006512518517584318\nTRAIN: \t Epoch: 6 \t Loss: -0.000701658596256263\nVALD: \t Epoch: 6 \t Loss: -0.0012860525166615844\nVALD: \t Epoch: 6 \t Loss: -0.000531140249222517\nVALD: \t Epoch: 6 \t Loss: 0.0029144824172059693\nVALD: \t Epoch: 6 \t Loss: 0.004546039737761021\nVALD: \t Epoch: 6 \t Loss: 0.003622776387783233\nVALD: \t Epoch: 6 \t Loss: 0.003572203908962282\n******************************\nEpoch: social-tag : 6\ntrain_loss -0.000701658596256263\nval_loss 0.003572203908962282\n{'min_val_epoch': 4, 'min_val_loss': 0.003467425276793427}\n******************************\nTRAIN: \t Epoch: 7 \t Loss: -0.0020020348019897938\nTRAIN: \t Epoch: 7 \t Loss: -0.0017930230824276805\nTRAIN: \t Epoch: 7 \t Loss: -0.0015660908926899235\nTRAIN: \t Epoch: 7 \t Loss: -0.0006990943802520633\nTRAIN: \t Epoch: 7 \t Loss: -0.0005954675347311422\nTRAIN: \t Epoch: 7 \t Loss: -0.000826023671834264\nTRAIN: \t Epoch: 7 \t Loss: -0.001061548210730377\nTRAIN: \t Epoch: 7 \t Loss: -0.0013039129244134529\nTRAIN: \t Epoch: 7 \t Loss: -0.001330968728224333\nTRAIN: \t Epoch: 7 \t Loss: -0.0010951685937470757\nTRAIN: \t Epoch: 7 \t Loss: -0.0008244799987137826\nTRAIN: \t Epoch: 7 \t Loss: -0.0007606848133339857\nTRAIN: \t Epoch: 7 \t Loss: -0.0008052336275935746\nTRAIN: \t Epoch: 7 \t Loss: -0.0008901935943868011\nTRAIN: \t Epoch: 7 \t Loss: -0.000946226060235252\nTRAIN: \t Epoch: 7 \t Loss: -0.001029294708132511\nTRAIN: \t Epoch: 7 \t Loss: -0.0009852978058488054\nTRAIN: \t Epoch: 7 \t Loss: -0.0008916464770057549\nTRAIN: \t Epoch: 7 \t Loss: -0.0008816344320381942\nTRAIN: \t Epoch: 7 \t Loss: -0.000939342100173235\nTRAIN: \t Epoch: 7 \t Loss: -0.0010323874386293547\nTRAIN: \t Epoch: 7 \t Loss: -0.001067727451050517\nVALD: \t Epoch: 7 \t Loss: -5.8983234339393675e-05\nVALD: \t Epoch: 7 \t Loss: 0.0018587612939882092\nVALD: \t Epoch: 7 \t Loss: 0.0033695531650058306\nVALD: \t Epoch: 7 \t Loss: 0.004226050103170564\nVALD: \t Epoch: 7 \t Loss: 0.0034656641975743694\nVALD: \t Epoch: 7 \t Loss: 0.003433960983811906\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 70/70 [00:02<00:00, 26.01it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 1.0364834262745244  FDE: 1.4465154618953604\n**************************************************\n******************************\nEpoch: social-tag : 7\ntrain_loss -0.001067727451050517\nval_loss 0.003433960983811906\n{'min_val_epoch': 7, 'min_val_loss': 0.003433960983811906}\n******************************\nTRAIN: \t Epoch: 8 \t Loss: -0.002464808290824294\nTRAIN: \t Epoch: 8 \t Loss: -0.0024648169055581093\nTRAIN: \t Epoch: 8 \t Loss: -0.001820997354419281\nTRAIN: \t Epoch: 8 \t Loss: -0.0016913772415136918\nTRAIN: \t Epoch: 8 \t Loss: -0.0018360753427259624\nTRAIN: \t Epoch: 8 \t Loss: -0.002060282655293122\nTRAIN: \t Epoch: 8 \t Loss: -0.00219684568998803\nTRAIN: \t Epoch: 8 \t Loss: -0.0020007025595987216\nTRAIN: \t Epoch: 8 \t Loss: -0.00159153925617122\nTRAIN: \t Epoch: 8 \t Loss: -0.0015124097350053488\nTRAIN: \t Epoch: 8 \t Loss: -0.0015268461588262157\nTRAIN: \t Epoch: 8 \t Loss: -0.0016234395540474604\nTRAIN: \t Epoch: 8 \t Loss: -0.0018128609696689707\nTRAIN: \t Epoch: 8 \t Loss: -0.0019100242311001889\nTRAIN: \t Epoch: 8 \t Loss: -0.001797506275276343\nTRAIN: \t Epoch: 8 \t Loss: -0.0016041482813307084\nTRAIN: \t Epoch: 8 \t Loss: -0.001558957905645537\nTRAIN: \t Epoch: 8 \t Loss: -0.0015820669003814044\nTRAIN: \t Epoch: 8 \t Loss: -0.0016547077868476901\nTRAIN: \t Epoch: 8 \t Loss: -0.0017559803643962368\nTRAIN: \t Epoch: 8 \t Loss: -0.0017562115126999007\nTRAIN: \t Epoch: 8 \t Loss: -0.0017849130759975426\nVALD: \t Epoch: 8 \t Loss: -0.003564876038581133\nVALD: \t Epoch: 8 \t Loss: -0.0014625399489887059\nVALD: \t Epoch: 8 \t Loss: 0.0009051652547592918\nVALD: \t Epoch: 8 \t Loss: 0.0021816483640577644\nVALD: \t Epoch: 8 \t Loss: 0.0013996174791827797\nVALD: \t Epoch: 8 \t Loss: 0.0013709912598697525\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 70/70 [00:02<00:00, 25.95it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.9863268169440483  FDE: 1.3463543805668168\n**************************************************\n******************************\nEpoch: social-tag : 8\ntrain_loss -0.0017849130759975426\nval_loss 0.0013709912598697525\n{'min_val_epoch': 8, 'min_val_loss': 0.0013709912598697525}\n******************************\nTRAIN: \t Epoch: 9 \t Loss: -0.002121726516634226\nTRAIN: \t Epoch: 9 \t Loss: -0.0018606901867315173\nTRAIN: \t Epoch: 9 \t Loss: -0.0013967796597474564\nTRAIN: \t Epoch: 9 \t Loss: -0.0015202188733383082\nTRAIN: \t Epoch: 9 \t Loss: -0.0018832190136890859\nTRAIN: \t Epoch: 9 \t Loss: -0.0021894288705273843\nTRAIN: \t Epoch: 9 \t Loss: -0.0023057183841176864\nTRAIN: \t Epoch: 9 \t Loss: -0.002340185088542057\nTRAIN: \t Epoch: 9 \t Loss: -0.0022645920948384125\nTRAIN: \t Epoch: 9 \t Loss: -0.0021211947459960356\nTRAIN: \t Epoch: 9 \t Loss: -0.0021267298063982957\nTRAIN: \t Epoch: 9 \t Loss: -0.002180928163094601\nTRAIN: \t Epoch: 9 \t Loss: -0.0022666913819893333\nTRAIN: \t Epoch: 9 \t Loss: -0.002224092315632983\nTRAIN: \t Epoch: 9 \t Loss: -0.002086689595792753\nTRAIN: \t Epoch: 9 \t Loss: -0.0020187923446428613\nTRAIN: \t Epoch: 9 \t Loss: -0.002070379026597092\nTRAIN: \t Epoch: 9 \t Loss: -0.002132678748037304\nTRAIN: \t Epoch: 9 \t Loss: -0.0022260716597022685\nTRAIN: \t Epoch: 9 \t Loss: -0.0022988817836449016\nTRAIN: \t Epoch: 9 \t Loss: -0.0023500398750738462\nTRAIN: \t Epoch: 9 \t Loss: -0.002314077021430382\nVALD: \t Epoch: 9 \t Loss: -8.04982555564493e-05\nVALD: \t Epoch: 9 \t Loss: 0.0009920416196109727\nVALD: \t Epoch: 9 \t Loss: 0.00214541211607866\nVALD: \t Epoch: 9 \t Loss: 0.0017945266517926939\nVALD: \t Epoch: 9 \t Loss: 0.001562786620343104\nVALD: \t Epoch: 9 \t Loss: 0.0015592191509450927\n******************************\nEpoch: social-tag : 9\ntrain_loss -0.002314077021430382\nval_loss 0.0015592191509450927\n{'min_val_epoch': 8, 'min_val_loss': 0.0013709912598697525}\n******************************\nTRAIN: \t Epoch: 10 \t Loss: -0.0014674824196845293\nTRAIN: \t Epoch: 10 \t Loss: -0.002042727777734399\nTRAIN: \t Epoch: 10 \t Loss: -0.0025922102698435387\nTRAIN: \t Epoch: 10 \t Loss: -0.003027735569048673\nTRAIN: \t Epoch: 10 \t Loss: -0.0031385004986077547\nTRAIN: \t Epoch: 10 \t Loss: -0.0023472003328303495\nTRAIN: \t Epoch: 10 \t Loss: -0.001667390943371824\nTRAIN: \t Epoch: 10 \t Loss: -0.0013876370430807583\nTRAIN: \t Epoch: 10 \t Loss: -0.0013278234594811995\nTRAIN: \t Epoch: 10 \t Loss: -0.001400200556963682\nTRAIN: \t Epoch: 10 \t Loss: -0.001557052664628083\nTRAIN: \t Epoch: 10 \t Loss: -0.0017770554598731299\nTRAIN: \t Epoch: 10 \t Loss: -0.001970707241875621\nTRAIN: \t Epoch: 10 \t Loss: -0.0021361486620402764\nTRAIN: \t Epoch: 10 \t Loss: -0.002166378280768792\nTRAIN: \t Epoch: 10 \t Loss: -0.0020243536760062852\nTRAIN: \t Epoch: 10 \t Loss: -0.001989160511584487\nTRAIN: \t Epoch: 10 \t Loss: -0.002046776118529831\nTRAIN: \t Epoch: 10 \t Loss: -0.0021200312625488118\nTRAIN: \t Epoch: 10 \t Loss: -0.0022383383922715437\nTRAIN: \t Epoch: 10 \t Loss: -0.002313605153176468\nTRAIN: \t Epoch: 10 \t Loss: -0.0022470950769324064\nVALD: \t Epoch: 10 \t Loss: -0.002848618431016803\nVALD: \t Epoch: 10 \t Loss: -0.0024369722232222557\nVALD: \t Epoch: 10 \t Loss: -0.0002942724774281184\nVALD: \t Epoch: 10 \t Loss: -3.0597628210671246e-05\nVALD: \t Epoch: 10 \t Loss: -0.0004327200469560921\nVALD: \t Epoch: 10 \t Loss: -0.00041319159367545085\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 70/70 [00:02<00:00, 25.80it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.9474488307091528  FDE: 1.2261486778186257\n**************************************************\n******************************\nEpoch: social-tag : 10\ntrain_loss -0.0022470950769324064\nval_loss -0.00041319159367545085\n{'min_val_epoch': 10, 'min_val_loss': -0.00041319159367545085}\n******************************\nTRAIN: \t Epoch: 11 \t Loss: -0.0024016331881284714\nTRAIN: \t Epoch: 11 \t Loss: -0.002837168052792549\nTRAIN: \t Epoch: 11 \t Loss: -0.003219843376427889\nTRAIN: \t Epoch: 11 \t Loss: -0.003219298319891095\nTRAIN: \t Epoch: 11 \t Loss: -0.003361177071928978\nTRAIN: \t Epoch: 11 \t Loss: -0.0034738656443854174\nTRAIN: \t Epoch: 11 \t Loss: -0.0035089682787656784\nTRAIN: \t Epoch: 11 \t Loss: -0.0034299918625038117\nTRAIN: \t Epoch: 11 \t Loss: -0.0031288759394859276\nTRAIN: \t Epoch: 11 \t Loss: -0.003086185303982347\nTRAIN: \t Epoch: 11 \t Loss: -0.0031700170154429293\nTRAIN: \t Epoch: 11 \t Loss: -0.00332820364080059\nTRAIN: \t Epoch: 11 \t Loss: -0.0034774078253226783\nTRAIN: \t Epoch: 11 \t Loss: -0.003519066168727087\nTRAIN: \t Epoch: 11 \t Loss: -0.003455590278220673\nTRAIN: \t Epoch: 11 \t Loss: -0.0033748021596693434\nTRAIN: \t Epoch: 11 \t Loss: -0.00340914140756735\nTRAIN: \t Epoch: 11 \t Loss: -0.0034470467685928773\nTRAIN: \t Epoch: 11 \t Loss: -0.0035046372672935065\nTRAIN: \t Epoch: 11 \t Loss: -0.0034231394587550313\nTRAIN: \t Epoch: 11 \t Loss: -0.003177843788372619\nTRAIN: \t Epoch: 11 \t Loss: -0.00310230119491608\nVALD: \t Epoch: 11 \t Loss: -0.0020150169730186462\nVALD: \t Epoch: 11 \t Loss: -0.0015910070505924523\nVALD: \t Epoch: 11 \t Loss: -0.00013073157363881668\nVALD: \t Epoch: 11 \t Loss: 0.0024492123920936137\nVALD: \t Epoch: 11 \t Loss: 0.0017039337661117315\nVALD: \t Epoch: 11 \t Loss: 0.0016739641146903688\n******************************\nEpoch: social-tag : 11\ntrain_loss -0.00310230119491608\nval_loss 0.0016739641146903688\n{'min_val_epoch': 10, 'min_val_loss': -0.00041319159367545085}\n******************************\nTRAIN: \t Epoch: 12 \t Loss: -0.002953276736661792\nTRAIN: \t Epoch: 12 \t Loss: -0.0034272826742380857\nTRAIN: \t Epoch: 12 \t Loss: -0.0036280492010215917\nTRAIN: \t Epoch: 12 \t Loss: -0.003942092880606651\nTRAIN: \t Epoch: 12 \t Loss: -0.004005985520780087\nTRAIN: \t Epoch: 12 \t Loss: -0.003927506312417488\nTRAIN: \t Epoch: 12 \t Loss: -0.003790252442870821\nTRAIN: \t Epoch: 12 \t Loss: -0.003307573819256504\nTRAIN: \t Epoch: 12 \t Loss: -0.003173352091430893\nTRAIN: \t Epoch: 12 \t Loss: -0.0032813618672662415\nTRAIN: \t Epoch: 12 \t Loss: -0.003344183979524215\nTRAIN: \t Epoch: 12 \t Loss: -0.003399581358583722\nTRAIN: \t Epoch: 12 \t Loss: -0.0034120705061538434\nTRAIN: \t Epoch: 12 \t Loss: -0.0034627709548138747\nTRAIN: \t Epoch: 12 \t Loss: -0.003520203755275967\nTRAIN: \t Epoch: 12 \t Loss: -0.0035113047206323245\nTRAIN: \t Epoch: 12 \t Loss: -0.0034942290726843674\nTRAIN: \t Epoch: 12 \t Loss: -0.003560166949076423\nTRAIN: \t Epoch: 12 \t Loss: -0.003629067386531173\nTRAIN: \t Epoch: 12 \t Loss: -0.0036824670511123258\nTRAIN: \t Epoch: 12 \t Loss: -0.0037206224993237163\nTRAIN: \t Epoch: 12 \t Loss: -0.003725385131511697\nVALD: \t Epoch: 12 \t Loss: -0.003496639896184206\nVALD: \t Epoch: 12 \t Loss: -0.001223226950969547\nVALD: \t Epoch: 12 \t Loss: 0.001501012632312874\nVALD: \t Epoch: 12 \t Loss: 0.006443836755352095\nVALD: \t Epoch: 12 \t Loss: 0.004851150675676763\nVALD: \t Epoch: 12 \t Loss: 0.004678879285965002\n******************************\nEpoch: social-tag : 12\ntrain_loss -0.003725385131511697\nval_loss 0.004678879285965002\n{'min_val_epoch': 10, 'min_val_loss': -0.00041319159367545085}\n******************************\nTRAIN: \t Epoch: 13 \t Loss: -0.004183817654848099\nTRAIN: \t Epoch: 13 \t Loss: -0.003218308906070888\nTRAIN: \t Epoch: 13 \t Loss: -0.0025525826883191862\nTRAIN: \t Epoch: 13 \t Loss: -0.0024472921795677394\nTRAIN: \t Epoch: 13 \t Loss: -0.002778399153612554\nTRAIN: \t Epoch: 13 \t Loss: -0.0031430181697942317\nTRAIN: \t Epoch: 13 \t Loss: -0.003374695495170142\nTRAIN: \t Epoch: 13 \t Loss: -0.00332326402713079\nTRAIN: \t Epoch: 13 \t Loss: -0.00297590164700523\nTRAIN: \t Epoch: 13 \t Loss: -0.0028146204131189734\nTRAIN: \t Epoch: 13 \t Loss: -0.0028872165924192154\nTRAIN: \t Epoch: 13 \t Loss: -0.0030532075955610103\nTRAIN: \t Epoch: 13 \t Loss: -0.003175395670740937\nTRAIN: \t Epoch: 13 \t Loss: -0.003304695964158912\nTRAIN: \t Epoch: 13 \t Loss: -0.0034099516342394054\nTRAIN: \t Epoch: 13 \t Loss: -0.0034850200354412664\nTRAIN: \t Epoch: 13 \t Loss: -0.0034815516658401225\nTRAIN: \t Epoch: 13 \t Loss: -0.003371485764445323\nTRAIN: \t Epoch: 13 \t Loss: -0.0033150824172863444\nTRAIN: \t Epoch: 13 \t Loss: -0.003376216287142597\nTRAIN: \t Epoch: 13 \t Loss: -0.0034448218038527385\nTRAIN: \t Epoch: 13 \t Loss: -0.003496357940364783\nVALD: \t Epoch: 13 \t Loss: -0.005259009078145027\nVALD: \t Epoch: 13 \t Loss: -0.0037681274116039276\nVALD: \t Epoch: 13 \t Loss: -0.0001421069415907065\nVALD: \t Epoch: 13 \t Loss: 0.002937552868388593\nVALD: \t Epoch: 13 \t Loss: 0.0018581302370876073\nVALD: \t Epoch: 13 \t Loss: 0.001750333821683219\n******************************\nEpoch: social-tag : 13\ntrain_loss -0.003496357940364783\nval_loss 0.001750333821683219\n{'min_val_epoch': 10, 'min_val_loss': -0.00041319159367545085}\n******************************\nTRAIN: \t Epoch: 14 \t Loss: -0.0049508847296237946\nTRAIN: \t Epoch: 14 \t Loss: -0.00484324200078845\nTRAIN: \t Epoch: 14 \t Loss: -0.00410855080311497\nTRAIN: \t Epoch: 14 \t Loss: -0.003260518205934204\nTRAIN: \t Epoch: 14 \t Loss: -0.0032214745064266027\nTRAIN: \t Epoch: 14 \t Loss: -0.0034131777259365967\nTRAIN: \t Epoch: 14 \t Loss: -0.003575602288557483\nTRAIN: \t Epoch: 14 \t Loss: -0.0038111786780063994\nTRAIN: \t Epoch: 14 \t Loss: -0.003908990940544754\nTRAIN: \t Epoch: 14 \t Loss: -0.004005280887940899\nTRAIN: \t Epoch: 14 \t Loss: -0.004064837433609434\nTRAIN: \t Epoch: 14 \t Loss: -0.004007989914195302\nTRAIN: \t Epoch: 14 \t Loss: -0.0037732747150585055\nTRAIN: \t Epoch: 14 \t Loss: -0.0036265383408005747\nTRAIN: \t Epoch: 14 \t Loss: -0.003647942841053009\nTRAIN: \t Epoch: 14 \t Loss: -0.003740762738743797\nTRAIN: \t Epoch: 14 \t Loss: -0.003830467597307528\nTRAIN: \t Epoch: 14 \t Loss: -0.003906387525300185\nTRAIN: \t Epoch: 14 \t Loss: -0.003923568139342885\nTRAIN: \t Epoch: 14 \t Loss: -0.003879067848902196\nTRAIN: \t Epoch: 14 \t Loss: -0.0038505216639134147\nTRAIN: \t Epoch: 14 \t Loss: -0.003872434497949679\nVALD: \t Epoch: 14 \t Loss: -0.005042324308305979\nVALD: \t Epoch: 14 \t Loss: -0.0037288586609065533\nVALD: \t Epoch: 14 \t Loss: -0.0005781613290309906\nVALD: \t Epoch: 14 \t Loss: 0.0012940026354044676\nVALD: \t Epoch: 14 \t Loss: 0.00038700355216860773\nVALD: \t Epoch: 14 \t Loss: 0.0003532530725792502\n******************************\nEpoch: social-tag : 14\ntrain_loss -0.003872434497949679\nval_loss 0.0003532530725792502\n{'min_val_epoch': 10, 'min_val_loss': -0.00041319159367545085}\n******************************\nTRAIN: \t Epoch: 15 \t Loss: -0.005668331868946552\nTRAIN: \t Epoch: 15 \t Loss: -0.005405383417382836\nTRAIN: \t Epoch: 15 \t Loss: -0.005238197278231382\nTRAIN: \t Epoch: 15 \t Loss: -0.004779268754646182\nTRAIN: \t Epoch: 15 \t Loss: -0.003970499616116285\nTRAIN: \t Epoch: 15 \t Loss: -0.0036856700122977295\nTRAIN: \t Epoch: 15 \t Loss: -0.003807702866782035\nTRAIN: \t Epoch: 15 \t Loss: -0.003982176101999357\nTRAIN: \t Epoch: 15 \t Loss: -0.004090313618588779\nTRAIN: \t Epoch: 15 \t Loss: -0.004301691870205104\nTRAIN: \t Epoch: 15 \t Loss: -0.004423673836175691\nTRAIN: \t Epoch: 15 \t Loss: -0.004501538797436903\nTRAIN: \t Epoch: 15 \t Loss: -0.004403233904248247\nTRAIN: \t Epoch: 15 \t Loss: -0.004126249213836023\nTRAIN: \t Epoch: 15 \t Loss: -0.004032432505240043\nTRAIN: \t Epoch: 15 \t Loss: -0.004039868479594588\nTRAIN: \t Epoch: 15 \t Loss: -0.0041060410713886514\nTRAIN: \t Epoch: 15 \t Loss: -0.004235773968199889\nTRAIN: \t Epoch: 15 \t Loss: -0.00434523168951273\nTRAIN: \t Epoch: 15 \t Loss: -0.004372536926530302\nTRAIN: \t Epoch: 15 \t Loss: -0.004397289250932988\nTRAIN: \t Epoch: 15 \t Loss: -0.004335051817782692\nVALD: \t Epoch: 15 \t Loss: -0.004074449185281992\nVALD: \t Epoch: 15 \t Loss: -0.003882144927047193\nVALD: \t Epoch: 15 \t Loss: -0.0035008343402296305\nVALD: \t Epoch: 15 \t Loss: -0.003222998755518347\nVALD: \t Epoch: 15 \t Loss: -0.0032529917545616625\nVALD: \t Epoch: 15 \t Loss: -0.003182736033517303\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 70/70 [00:02<00:00, 25.42it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.828551654248747  FDE: 1.0743531573836225\n**************************************************\n******************************\nEpoch: social-tag : 15\ntrain_loss -0.004335051817782692\nval_loss -0.003182736033517303\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 16 \t Loss: -0.004535517189651728\nTRAIN: \t Epoch: 16 \t Loss: -0.004951075650751591\nTRAIN: \t Epoch: 16 \t Loss: -0.005105444851020972\nTRAIN: \t Epoch: 16 \t Loss: -0.004940431099385023\nTRAIN: \t Epoch: 16 \t Loss: -0.0047953838482499124\nTRAIN: \t Epoch: 16 \t Loss: -0.004474440822377801\nTRAIN: \t Epoch: 16 \t Loss: -0.004382789733686617\nTRAIN: \t Epoch: 16 \t Loss: -0.004473291337490082\nTRAIN: \t Epoch: 16 \t Loss: -0.004571671856360303\nTRAIN: \t Epoch: 16 \t Loss: -0.004537262581288814\nTRAIN: \t Epoch: 16 \t Loss: -0.004412315701219169\nTRAIN: \t Epoch: 16 \t Loss: -0.004306511060955624\nTRAIN: \t Epoch: 16 \t Loss: -0.004371308255940676\nTRAIN: \t Epoch: 16 \t Loss: -0.004520893130185348\nTRAIN: \t Epoch: 16 \t Loss: -0.004567662471284469\nTRAIN: \t Epoch: 16 \t Loss: -0.004481852956814691\nTRAIN: \t Epoch: 16 \t Loss: -0.004406845950357178\nTRAIN: \t Epoch: 16 \t Loss: -0.004439612035639584\nTRAIN: \t Epoch: 16 \t Loss: -0.0045170283655783065\nTRAIN: \t Epoch: 16 \t Loss: -0.004603811667766422\nTRAIN: \t Epoch: 16 \t Loss: -0.00465654325671494\nTRAIN: \t Epoch: 16 \t Loss: -0.004673188689367134\nVALD: \t Epoch: 16 \t Loss: -0.0024877379182726145\nVALD: \t Epoch: 16 \t Loss: -0.00016320543363690376\nVALD: \t Epoch: 16 \t Loss: 0.002256456296890974\nVALD: \t Epoch: 16 \t Loss: 0.004334521829150617\nVALD: \t Epoch: 16 \t Loss: 0.0032446432858705522\nVALD: \t Epoch: 16 \t Loss: 0.0030867659458608337\n******************************\nEpoch: social-tag : 16\ntrain_loss -0.004673188689367134\nval_loss 0.0030867659458608337\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 17 \t Loss: -0.005712432786822319\nTRAIN: \t Epoch: 17 \t Loss: -0.004956797230988741\nTRAIN: \t Epoch: 17 \t Loss: -0.003951884340494871\nTRAIN: \t Epoch: 17 \t Loss: -0.003765735775232315\nTRAIN: \t Epoch: 17 \t Loss: -0.004125289898365736\nTRAIN: \t Epoch: 17 \t Loss: -0.0042271430138498545\nTRAIN: \t Epoch: 17 \t Loss: -0.004526752047240734\nTRAIN: \t Epoch: 17 \t Loss: -0.00469687778968364\nTRAIN: \t Epoch: 17 \t Loss: -0.004760612216260698\nTRAIN: \t Epoch: 17 \t Loss: -0.004607811756432057\nTRAIN: \t Epoch: 17 \t Loss: -0.004433721473271196\nTRAIN: \t Epoch: 17 \t Loss: -0.0043879183164487285\nTRAIN: \t Epoch: 17 \t Loss: -0.004485463078778524\nTRAIN: \t Epoch: 17 \t Loss: -0.004644810288612332\nTRAIN: \t Epoch: 17 \t Loss: -0.0047529541576902075\nTRAIN: \t Epoch: 17 \t Loss: -0.004767139471368864\nTRAIN: \t Epoch: 17 \t Loss: -0.004646675647510325\nTRAIN: \t Epoch: 17 \t Loss: -0.004527314087479479\nTRAIN: \t Epoch: 17 \t Loss: -0.004563289339114961\nTRAIN: \t Epoch: 17 \t Loss: -0.004681576101575047\nTRAIN: \t Epoch: 17 \t Loss: -0.004786497525249918\nTRAIN: \t Epoch: 17 \t Loss: -0.004819611255313593\nVALD: \t Epoch: 17 \t Loss: -0.004976957105100155\nVALD: \t Epoch: 17 \t Loss: -0.0005997613770887256\nVALD: \t Epoch: 17 \t Loss: 0.0008875263544420401\nVALD: \t Epoch: 17 \t Loss: 0.00517160224262625\nVALD: \t Epoch: 17 \t Loss: 0.0036316738929599523\nVALD: \t Epoch: 17 \t Loss: 0.0034983346224621388\n******************************\nEpoch: social-tag : 17\ntrain_loss -0.004819611255313593\nval_loss 0.0034983346224621388\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 18 \t Loss: -0.004683829378336668\nTRAIN: \t Epoch: 18 \t Loss: -0.0034784222953021526\nTRAIN: \t Epoch: 18 \t Loss: -0.0030224968989690146\nTRAIN: \t Epoch: 18 \t Loss: -0.003409851808100939\nTRAIN: \t Epoch: 18 \t Loss: -0.003957860730588436\nTRAIN: \t Epoch: 18 \t Loss: -0.004405516354988019\nTRAIN: \t Epoch: 18 \t Loss: -0.004751066477703196\nTRAIN: \t Epoch: 18 \t Loss: -0.004894275974947959\nTRAIN: \t Epoch: 18 \t Loss: -0.004579863045364618\nTRAIN: \t Epoch: 18 \t Loss: -0.0043176187900826335\nTRAIN: \t Epoch: 18 \t Loss: -0.004287003349004822\nTRAIN: \t Epoch: 18 \t Loss: -0.004397301915256928\nTRAIN: \t Epoch: 18 \t Loss: -0.004533526541378636\nTRAIN: \t Epoch: 18 \t Loss: -0.004598650100108769\nTRAIN: \t Epoch: 18 \t Loss: -0.004683873321240147\nTRAIN: \t Epoch: 18 \t Loss: -0.004742926350445487\nTRAIN: \t Epoch: 18 \t Loss: -0.004735675571924623\nTRAIN: \t Epoch: 18 \t Loss: -0.004674027854990628\nTRAIN: \t Epoch: 18 \t Loss: -0.004700670459944951\nTRAIN: \t Epoch: 18 \t Loss: -0.004752921778708697\nTRAIN: \t Epoch: 18 \t Loss: -0.0048007396315889695\nTRAIN: \t Epoch: 18 \t Loss: -0.004874957528105758\nVALD: \t Epoch: 18 \t Loss: -0.0037235519848763943\nVALD: \t Epoch: 18 \t Loss: 0.0037389362696558237\nVALD: \t Epoch: 18 \t Loss: 0.004021632174650828\nVALD: \t Epoch: 18 \t Loss: 0.007409620564430952\nVALD: \t Epoch: 18 \t Loss: 0.005609945743344724\nVALD: \t Epoch: 18 \t Loss: 0.0053724251179532575\n******************************\nEpoch: social-tag : 18\ntrain_loss -0.004874957528105758\nval_loss 0.0053724251179532575\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 19 \t Loss: -0.005147220566868782\nTRAIN: \t Epoch: 19 \t Loss: -0.005635529523715377\nTRAIN: \t Epoch: 19 \t Loss: -0.00580029080932339\nTRAIN: \t Epoch: 19 \t Loss: -0.005706338561140001\nTRAIN: \t Epoch: 19 \t Loss: -0.005469896923750639\nTRAIN: \t Epoch: 19 \t Loss: -0.005198846144291262\nTRAIN: \t Epoch: 19 \t Loss: -0.005224809482959765\nTRAIN: \t Epoch: 19 \t Loss: -0.005284416285576299\nTRAIN: \t Epoch: 19 \t Loss: -0.005372870998043153\nTRAIN: \t Epoch: 19 \t Loss: -0.005413364036940038\nTRAIN: \t Epoch: 19 \t Loss: -0.005405429602515968\nTRAIN: \t Epoch: 19 \t Loss: -0.005308870206742237\nTRAIN: \t Epoch: 19 \t Loss: -0.005179403725868234\nTRAIN: \t Epoch: 19 \t Loss: -0.0052064912493473715\nTRAIN: \t Epoch: 19 \t Loss: -0.005355051609997948\nTRAIN: \t Epoch: 19 \t Loss: -0.00542201085772831\nTRAIN: \t Epoch: 19 \t Loss: -0.005483704304103465\nTRAIN: \t Epoch: 19 \t Loss: -0.0053974946883196635\nTRAIN: \t Epoch: 19 \t Loss: -0.005231801199873811\nTRAIN: \t Epoch: 19 \t Loss: -0.005221574660390615\nTRAIN: \t Epoch: 19 \t Loss: -0.005279676966546546\nTRAIN: \t Epoch: 19 \t Loss: -0.005328315057703151\nVALD: \t Epoch: 19 \t Loss: -0.0064476593397557735\nVALD: \t Epoch: 19 \t Loss: -0.00297669880092144\nVALD: \t Epoch: 19 \t Loss: -0.002016246250908201\nVALD: \t Epoch: 19 \t Loss: 0.0008871285754139535\nVALD: \t Epoch: 19 \t Loss: -0.0001882950367871672\nVALD: \t Epoch: 19 \t Loss: -0.00025652030087781674\n******************************\nEpoch: social-tag : 19\ntrain_loss -0.005328315057703151\nval_loss -0.00025652030087781674\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 20 \t Loss: -0.0070868064649403095\nTRAIN: \t Epoch: 20 \t Loss: -0.00671949191018939\nTRAIN: \t Epoch: 20 \t Loss: -0.006399054545909166\nTRAIN: \t Epoch: 20 \t Loss: -0.006143850740045309\nTRAIN: \t Epoch: 20 \t Loss: -0.005722554679960012\nTRAIN: \t Epoch: 20 \t Loss: -0.005645420402288437\nTRAIN: \t Epoch: 20 \t Loss: -0.005826281250587532\nTRAIN: \t Epoch: 20 \t Loss: -0.005921960750129074\nTRAIN: \t Epoch: 20 \t Loss: -0.006101989549481207\nTRAIN: \t Epoch: 20 \t Loss: -0.006156797148287296\nTRAIN: \t Epoch: 20 \t Loss: -0.0058962841602888975\nTRAIN: \t Epoch: 20 \t Loss: -0.005627277520640443\nTRAIN: \t Epoch: 20 \t Loss: -0.0055997272272809194\nTRAIN: \t Epoch: 20 \t Loss: -0.00565441300360752\nTRAIN: \t Epoch: 20 \t Loss: -0.005743422436838349\nTRAIN: \t Epoch: 20 \t Loss: -0.005805772190797143\nTRAIN: \t Epoch: 20 \t Loss: -0.005883633638458217\nTRAIN: \t Epoch: 20 \t Loss: -0.0058395810709852315\nTRAIN: \t Epoch: 20 \t Loss: -0.005653024013889463\nTRAIN: \t Epoch: 20 \t Loss: -0.0055734333349391815\nTRAIN: \t Epoch: 20 \t Loss: -0.005607046408667451\nTRAIN: \t Epoch: 20 \t Loss: -0.005630004341769261\nVALD: \t Epoch: 20 \t Loss: -0.007559849880635738\nVALD: \t Epoch: 20 \t Loss: -0.0022893865825608373\nVALD: \t Epoch: 20 \t Loss: -0.0009117215328539411\nVALD: \t Epoch: 20 \t Loss: 0.002719133481150493\nVALD: \t Epoch: 20 \t Loss: 0.0015651499619707466\nVALD: \t Epoch: 20 \t Loss: 0.001469081019361814\n******************************\nEpoch: social-tag : 20\ntrain_loss -0.005630004341769261\nval_loss 0.001469081019361814\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 21 \t Loss: -0.007906254380941391\nTRAIN: \t Epoch: 21 \t Loss: -0.007507254369556904\nTRAIN: \t Epoch: 21 \t Loss: -0.006641369468222062\nTRAIN: \t Epoch: 21 \t Loss: -0.0062349161598831415\nTRAIN: \t Epoch: 21 \t Loss: -0.005961139220744371\nTRAIN: \t Epoch: 21 \t Loss: -0.005970899093275269\nTRAIN: \t Epoch: 21 \t Loss: -0.006057384157819408\nTRAIN: \t Epoch: 21 \t Loss: -0.006072773947380483\nTRAIN: \t Epoch: 21 \t Loss: -0.006067028062211143\nTRAIN: \t Epoch: 21 \t Loss: -0.00569745113607496\nTRAIN: \t Epoch: 21 \t Loss: -0.005534492221406915\nTRAIN: \t Epoch: 21 \t Loss: -0.005538439727388322\nTRAIN: \t Epoch: 21 \t Loss: -0.0057005586030964665\nTRAIN: \t Epoch: 21 \t Loss: -0.005769538693130016\nTRAIN: \t Epoch: 21 \t Loss: -0.005884785888095697\nTRAIN: \t Epoch: 21 \t Loss: -0.005910826032049954\nTRAIN: \t Epoch: 21 \t Loss: -0.005705792881438837\nTRAIN: \t Epoch: 21 \t Loss: -0.005557008449816042\nTRAIN: \t Epoch: 21 \t Loss: -0.0055261548412473575\nTRAIN: \t Epoch: 21 \t Loss: -0.005556049733422696\nTRAIN: \t Epoch: 21 \t Loss: -0.005615138116159609\nTRAIN: \t Epoch: 21 \t Loss: -0.005649554579647482\nVALD: \t Epoch: 21 \t Loss: -0.007272384595125914\nVALD: \t Epoch: 21 \t Loss: 0.0005397542845457792\nVALD: \t Epoch: 21 \t Loss: -0.00024000252597033978\nVALD: \t Epoch: 21 \t Loss: 0.002347154717426747\nVALD: \t Epoch: 21 \t Loss: 0.001189758349210024\nVALD: \t Epoch: 21 \t Loss: 0.0011089255807526185\n******************************\nEpoch: social-tag : 21\ntrain_loss -0.005649554579647482\nval_loss 0.0011089255807526185\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 22 \t Loss: -0.006554374936968088\nTRAIN: \t Epoch: 22 \t Loss: -0.006394147872924805\nTRAIN: \t Epoch: 22 \t Loss: -0.006293673533946276\nTRAIN: \t Epoch: 22 \t Loss: -0.005966863827779889\nTRAIN: \t Epoch: 22 \t Loss: -0.005829187389463186\nTRAIN: \t Epoch: 22 \t Loss: -0.005930714774876833\nTRAIN: \t Epoch: 22 \t Loss: -0.0061220659076103145\nTRAIN: \t Epoch: 22 \t Loss: -0.0062073186854831874\nTRAIN: \t Epoch: 22 \t Loss: -0.006301637842423386\nTRAIN: \t Epoch: 22 \t Loss: -0.006166949821636081\nTRAIN: \t Epoch: 22 \t Loss: -0.005753892473876476\nTRAIN: \t Epoch: 22 \t Loss: -0.005569368018768728\nTRAIN: \t Epoch: 22 \t Loss: -0.0055802890027944856\nTRAIN: \t Epoch: 22 \t Loss: -0.00566229151029672\nTRAIN: \t Epoch: 22 \t Loss: -0.0058040855141977465\nTRAIN: \t Epoch: 22 \t Loss: -0.0058545835490804166\nTRAIN: \t Epoch: 22 \t Loss: -0.005858421353075434\nTRAIN: \t Epoch: 22 \t Loss: -0.005755768079931538\nTRAIN: \t Epoch: 22 \t Loss: -0.005743830930441618\nTRAIN: \t Epoch: 22 \t Loss: -0.005748004093766213\nTRAIN: \t Epoch: 22 \t Loss: -0.005812320481276228\nTRAIN: \t Epoch: 22 \t Loss: -0.005873260014361068\nVALD: \t Epoch: 22 \t Loss: -0.0029694498516619205\nVALD: \t Epoch: 22 \t Loss: 0.0006886688061058521\nVALD: \t Epoch: 22 \t Loss: 0.002750867356856664\nVALD: \t Epoch: 22 \t Loss: 0.003267045016400516\nVALD: \t Epoch: 22 \t Loss: 0.0019407372921705246\nVALD: \t Epoch: 22 \t Loss: 0.0018072236780867432\n******************************\nEpoch: social-tag : 22\ntrain_loss -0.005873260014361068\nval_loss 0.0018072236780867432\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 23 \t Loss: -0.0074021341279149055\nTRAIN: \t Epoch: 23 \t Loss: -0.005976284388452768\nTRAIN: \t Epoch: 23 \t Loss: -0.0053497618064284325\nTRAIN: \t Epoch: 23 \t Loss: -0.005453428719192743\nTRAIN: \t Epoch: 23 \t Loss: -0.005810551904141903\nTRAIN: \t Epoch: 23 \t Loss: -0.006144059511522452\nTRAIN: \t Epoch: 23 \t Loss: -0.006274818442761898\nTRAIN: \t Epoch: 23 \t Loss: -0.006336238409858197\nTRAIN: \t Epoch: 23 \t Loss: -0.006349438646187385\nTRAIN: \t Epoch: 23 \t Loss: -0.0062285222113132475\nTRAIN: \t Epoch: 23 \t Loss: -0.00617780972441489\nTRAIN: \t Epoch: 23 \t Loss: -0.006258139697213967\nTRAIN: \t Epoch: 23 \t Loss: -0.006336246044016802\nTRAIN: \t Epoch: 23 \t Loss: -0.0064230068985904965\nTRAIN: \t Epoch: 23 \t Loss: -0.006278648786246776\nTRAIN: \t Epoch: 23 \t Loss: -0.006007907351886388\nTRAIN: \t Epoch: 23 \t Loss: -0.005916917114518583\nTRAIN: \t Epoch: 23 \t Loss: -0.005954996909066621\nTRAIN: \t Epoch: 23 \t Loss: -0.006080465556710567\nTRAIN: \t Epoch: 23 \t Loss: -0.006115805375156924\nTRAIN: \t Epoch: 23 \t Loss: -0.006147866020910442\nTRAIN: \t Epoch: 23 \t Loss: -0.006145407839994242\nVALD: \t Epoch: 23 \t Loss: -0.005187891889363527\nVALD: \t Epoch: 23 \t Loss: -3.858865238726139e-05\nVALD: \t Epoch: 23 \t Loss: -0.0007031935577591261\nVALD: \t Epoch: 23 \t Loss: 0.00018956500571221113\nVALD: \t Epoch: 23 \t Loss: -0.000602586381137371\nVALD: \t Epoch: 23 \t Loss: -0.0006371894681995565\n******************************\nEpoch: social-tag : 23\ntrain_loss -0.006145407839994242\nval_loss -0.0006371894681995565\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 24 \t Loss: -0.0067900195717811584\nTRAIN: \t Epoch: 24 \t Loss: -0.006600398337468505\nTRAIN: \t Epoch: 24 \t Loss: -0.006665883585810661\nTRAIN: \t Epoch: 24 \t Loss: -0.006591207347810268\nTRAIN: \t Epoch: 24 \t Loss: -0.006342814210802317\nTRAIN: \t Epoch: 24 \t Loss: -0.006253085487211744\nTRAIN: \t Epoch: 24 \t Loss: -0.006274226853357894\nTRAIN: \t Epoch: 24 \t Loss: -0.006289940269198269\nTRAIN: \t Epoch: 24 \t Loss: -0.006313967363288005\nTRAIN: \t Epoch: 24 \t Loss: -0.006281242100521922\nTRAIN: \t Epoch: 24 \t Loss: -0.006216840649192984\nTRAIN: \t Epoch: 24 \t Loss: -0.006140854869348307\nTRAIN: \t Epoch: 24 \t Loss: -0.00620837789028883\nTRAIN: \t Epoch: 24 \t Loss: -0.006268645703260388\nTRAIN: \t Epoch: 24 \t Loss: -0.006245647619167963\nTRAIN: \t Epoch: 24 \t Loss: -0.00621858547674492\nTRAIN: \t Epoch: 24 \t Loss: -0.006227377993876443\nTRAIN: \t Epoch: 24 \t Loss: -0.006290149124753144\nTRAIN: \t Epoch: 24 \t Loss: -0.006305792417965438\nTRAIN: \t Epoch: 24 \t Loss: -0.0062105579301714895\nTRAIN: \t Epoch: 24 \t Loss: -0.006199360204239686\nTRAIN: \t Epoch: 24 \t Loss: -0.00622781487298825\nVALD: \t Epoch: 24 \t Loss: -0.006709382403641939\nVALD: \t Epoch: 24 \t Loss: -0.002138325246050954\nVALD: \t Epoch: 24 \t Loss: -0.001808137516491115\nVALD: \t Epoch: 24 \t Loss: -0.0009978378366213292\nVALD: \t Epoch: 24 \t Loss: -0.0018417619867250323\nVALD: \t Epoch: 24 \t Loss: -0.0018608001371224721\n******************************\nEpoch: social-tag : 24\ntrain_loss -0.00622781487298825\nval_loss -0.0018608001371224721\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 25 \t Loss: -0.007283741142600775\nTRAIN: \t Epoch: 25 \t Loss: -0.0072676255367696285\nTRAIN: \t Epoch: 25 \t Loss: -0.007337281014770269\nTRAIN: \t Epoch: 25 \t Loss: -0.006986644468270242\nTRAIN: \t Epoch: 25 \t Loss: -0.0063195046037435535\nTRAIN: \t Epoch: 25 \t Loss: -0.006203177540252606\nTRAIN: \t Epoch: 25 \t Loss: -0.006387980654835701\nTRAIN: \t Epoch: 25 \t Loss: -0.006609816802665591\nTRAIN: \t Epoch: 25 \t Loss: -0.006661120439983076\nTRAIN: \t Epoch: 25 \t Loss: -0.006664131442084909\nTRAIN: \t Epoch: 25 \t Loss: -0.0064709152687679634\nTRAIN: \t Epoch: 25 \t Loss: -0.006345985573716462\nTRAIN: \t Epoch: 25 \t Loss: -0.006375201798688907\nTRAIN: \t Epoch: 25 \t Loss: -0.006416075696636524\nTRAIN: \t Epoch: 25 \t Loss: -0.006498383078724146\nTRAIN: \t Epoch: 25 \t Loss: -0.00649972097016871\nTRAIN: \t Epoch: 25 \t Loss: -0.006551357095732409\nTRAIN: \t Epoch: 25 \t Loss: -0.006560176817907227\nTRAIN: \t Epoch: 25 \t Loss: -0.006540732531759299\nTRAIN: \t Epoch: 25 \t Loss: -0.006567509472370147\nTRAIN: \t Epoch: 25 \t Loss: -0.006584495505584138\nTRAIN: \t Epoch: 25 \t Loss: -0.006576657723267588\nVALD: \t Epoch: 25 \t Loss: -0.0022027441300451756\nVALD: \t Epoch: 25 \t Loss: 0.005055637331679463\nVALD: \t Epoch: 25 \t Loss: 0.00377385636481146\nVALD: \t Epoch: 25 \t Loss: 0.0039716961910016835\nVALD: \t Epoch: 25 \t Loss: 0.0023458768147975205\nVALD: \t Epoch: 25 \t Loss: 0.002228787572433551\n******************************\nEpoch: social-tag : 25\ntrain_loss -0.006576657723267588\nval_loss 0.002228787572433551\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 26 \t Loss: -0.006423282437026501\nTRAIN: \t Epoch: 26 \t Loss: -0.006878566928207874\nTRAIN: \t Epoch: 26 \t Loss: -0.006790739018470049\nTRAIN: \t Epoch: 26 \t Loss: -0.006459196680225432\nTRAIN: \t Epoch: 26 \t Loss: -0.006259778328239918\nTRAIN: \t Epoch: 26 \t Loss: -0.006352349339673917\nTRAIN: \t Epoch: 26 \t Loss: -0.006612377640392099\nTRAIN: \t Epoch: 26 \t Loss: -0.0068252444034442306\nTRAIN: \t Epoch: 26 \t Loss: -0.006913990454955233\nTRAIN: \t Epoch: 26 \t Loss: -0.006617343332618475\nTRAIN: \t Epoch: 26 \t Loss: -0.006362465633587403\nTRAIN: \t Epoch: 26 \t Loss: -0.006366014053734641\nTRAIN: \t Epoch: 26 \t Loss: -0.006476802178300344\nTRAIN: \t Epoch: 26 \t Loss: -0.006571599415370396\nTRAIN: \t Epoch: 26 \t Loss: -0.006626838104178508\nTRAIN: \t Epoch: 26 \t Loss: -0.006532118655741215\nTRAIN: \t Epoch: 26 \t Loss: -0.006320744081783821\nTRAIN: \t Epoch: 26 \t Loss: -0.006292311798056794\nTRAIN: \t Epoch: 26 \t Loss: -0.006352760914811178\nTRAIN: \t Epoch: 26 \t Loss: -0.006462500093039125\nTRAIN: \t Epoch: 26 \t Loss: -0.006522798057024677\nTRAIN: \t Epoch: 26 \t Loss: -0.006562688695250123\nVALD: \t Epoch: 26 \t Loss: -0.00544473621994257\nVALD: \t Epoch: 26 \t Loss: -0.0017037155339494348\nVALD: \t Epoch: 26 \t Loss: -0.0029814562294632196\nVALD: \t Epoch: 26 \t Loss: -0.0022486324596684426\nVALD: \t Epoch: 26 \t Loss: -0.0029140127589926124\nVALD: \t Epoch: 26 \t Loss: -0.00287223875127507\n******************************\nEpoch: social-tag : 26\ntrain_loss -0.006562688695250123\nval_loss -0.00287223875127507\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 27 \t Loss: -0.0075586112216115\nTRAIN: \t Epoch: 27 \t Loss: -0.006819200236350298\nTRAIN: \t Epoch: 27 \t Loss: -0.006191986923416455\nTRAIN: \t Epoch: 27 \t Loss: -0.006050549913197756\nTRAIN: \t Epoch: 27 \t Loss: -0.006324036605656147\nTRAIN: \t Epoch: 27 \t Loss: -0.006657874522109826\nTRAIN: \t Epoch: 27 \t Loss: -0.006970209601734366\nTRAIN: \t Epoch: 27 \t Loss: -0.00690274074440822\nTRAIN: \t Epoch: 27 \t Loss: -0.0065388717792100376\nTRAIN: \t Epoch: 27 \t Loss: -0.006393903214484453\nTRAIN: \t Epoch: 27 \t Loss: -0.006448290734128518\nTRAIN: \t Epoch: 27 \t Loss: -0.0065907220511386795\nTRAIN: \t Epoch: 27 \t Loss: -0.006718967539759783\nTRAIN: \t Epoch: 27 \t Loss: -0.006751809435497437\nTRAIN: \t Epoch: 27 \t Loss: -0.006612836786856254\nTRAIN: \t Epoch: 27 \t Loss: -0.00646154279820621\nTRAIN: \t Epoch: 27 \t Loss: -0.0064687937333741605\nTRAIN: \t Epoch: 27 \t Loss: -0.006569868445189463\nTRAIN: \t Epoch: 27 \t Loss: -0.006662903329063403\nTRAIN: \t Epoch: 27 \t Loss: -0.006728998315520584\nTRAIN: \t Epoch: 27 \t Loss: -0.006702356395267305\nTRAIN: \t Epoch: 27 \t Loss: -0.006682185232104262\nVALD: \t Epoch: 27 \t Loss: -0.002593566430732608\nVALD: \t Epoch: 27 \t Loss: -0.00029263366013765335\nVALD: \t Epoch: 27 \t Loss: 0.001341751931856076\nVALD: \t Epoch: 27 \t Loss: 0.011154476902447641\nVALD: \t Epoch: 27 \t Loss: 0.008511957665905356\nVALD: \t Epoch: 27 \t Loss: 0.00825681284980171\n******************************\nEpoch: social-tag : 27\ntrain_loss -0.006682185232104262\nval_loss 0.00825681284980171\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 28 \t Loss: -0.007278545759618282\nTRAIN: \t Epoch: 28 \t Loss: -0.007831962313503027\nTRAIN: \t Epoch: 28 \t Loss: -0.007855156436562538\nTRAIN: \t Epoch: 28 \t Loss: -0.007754449616186321\nTRAIN: \t Epoch: 28 \t Loss: -0.00747919213026762\nTRAIN: \t Epoch: 28 \t Loss: -0.007231690920889378\nTRAIN: \t Epoch: 28 \t Loss: -0.007228107856852668\nTRAIN: \t Epoch: 28 \t Loss: -0.0072968426393345\nTRAIN: \t Epoch: 28 \t Loss: -0.007293065544217825\nTRAIN: \t Epoch: 28 \t Loss: -0.007365504326298833\nTRAIN: \t Epoch: 28 \t Loss: -0.007416217206892642\nTRAIN: \t Epoch: 28 \t Loss: -0.007322545823020239\nTRAIN: \t Epoch: 28 \t Loss: -0.007178877265407489\nTRAIN: \t Epoch: 28 \t Loss: -0.007043426490521857\nTRAIN: \t Epoch: 28 \t Loss: -0.0070761416107416155\nTRAIN: \t Epoch: 28 \t Loss: -0.007100948540028185\nTRAIN: \t Epoch: 28 \t Loss: -0.0071731704451582015\nTRAIN: \t Epoch: 28 \t Loss: -0.007235309212572045\nTRAIN: \t Epoch: 28 \t Loss: -0.007267782366589496\nTRAIN: \t Epoch: 28 \t Loss: -0.007118527009151876\nTRAIN: \t Epoch: 28 \t Loss: -0.0069683884004397055\nTRAIN: \t Epoch: 28 \t Loss: -0.00696113334309905\nVALD: \t Epoch: 28 \t Loss: -0.004400297533720732\nVALD: \t Epoch: 28 \t Loss: -0.0032094926573336124\nVALD: \t Epoch: 28 \t Loss: -0.0018049502201999228\nVALD: \t Epoch: 28 \t Loss: 0.0036590738163795322\nVALD: \t Epoch: 28 \t Loss: 0.0022106961579993365\nVALD: \t Epoch: 28 \t Loss: 0.0021929519646095505\n******************************\nEpoch: social-tag : 28\ntrain_loss -0.00696113334309905\nval_loss 0.0021929519646095505\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 29 \t Loss: -0.00781999621540308\nTRAIN: \t Epoch: 29 \t Loss: -0.00839032419025898\nTRAIN: \t Epoch: 29 \t Loss: -0.007832542837907871\nTRAIN: \t Epoch: 29 \t Loss: -0.007603295263834298\nTRAIN: \t Epoch: 29 \t Loss: -0.007289491780102253\nTRAIN: \t Epoch: 29 \t Loss: -0.0073021647209922476\nTRAIN: \t Epoch: 29 \t Loss: -0.007279796592359032\nTRAIN: \t Epoch: 29 \t Loss: -0.007201162574347109\nTRAIN: \t Epoch: 29 \t Loss: -0.007069233960161607\nTRAIN: \t Epoch: 29 \t Loss: -0.0068666676990687845\nTRAIN: \t Epoch: 29 \t Loss: -0.006897883235730908\nTRAIN: \t Epoch: 29 \t Loss: -0.007027819287031889\nTRAIN: \t Epoch: 29 \t Loss: -0.007055655766565066\nTRAIN: \t Epoch: 29 \t Loss: -0.007005180564842054\nTRAIN: \t Epoch: 29 \t Loss: -0.006942001543939114\nTRAIN: \t Epoch: 29 \t Loss: -0.0069474294723477215\nTRAIN: \t Epoch: 29 \t Loss: -0.006969610092175358\nTRAIN: \t Epoch: 29 \t Loss: -0.00703460358393689\nTRAIN: \t Epoch: 29 \t Loss: -0.0071181185102384345\nTRAIN: \t Epoch: 29 \t Loss: -0.007057625520974398\nTRAIN: \t Epoch: 29 \t Loss: -0.006944284153481324\nTRAIN: \t Epoch: 29 \t Loss: -0.006952920229687511\nVALD: \t Epoch: 29 \t Loss: -0.005614669527858496\nVALD: \t Epoch: 29 \t Loss: 0.0053509792778640985\nVALD: \t Epoch: 29 \t Loss: 0.004480976999426882\nVALD: \t Epoch: 29 \t Loss: 0.007000970013905317\nVALD: \t Epoch: 29 \t Loss: 0.004716489417478442\nVALD: \t Epoch: 29 \t Loss: 0.004510333270511844\n******************************\nEpoch: social-tag : 29\ntrain_loss -0.006952920229687511\nval_loss 0.004510333270511844\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 30 \t Loss: -0.008398253470659256\nTRAIN: \t Epoch: 30 \t Loss: -0.0077727308962494135\nTRAIN: \t Epoch: 30 \t Loss: -0.007559680379927158\nTRAIN: \t Epoch: 30 \t Loss: -0.007463973481208086\nTRAIN: \t Epoch: 30 \t Loss: -0.007221729028970003\nTRAIN: \t Epoch: 30 \t Loss: -0.007147709994266431\nTRAIN: \t Epoch: 30 \t Loss: -0.007203876040875912\nTRAIN: \t Epoch: 30 \t Loss: -0.007256917480845004\nTRAIN: \t Epoch: 30 \t Loss: -0.007330156914475892\nTRAIN: \t Epoch: 30 \t Loss: -0.007344525260850787\nTRAIN: \t Epoch: 30 \t Loss: -0.007280734663998539\nTRAIN: \t Epoch: 30 \t Loss: -0.007203799556009471\nTRAIN: \t Epoch: 30 \t Loss: -0.007284404100993505\nTRAIN: \t Epoch: 30 \t Loss: -0.007388413717438068\nTRAIN: \t Epoch: 30 \t Loss: -0.007275652668128411\nTRAIN: \t Epoch: 30 \t Loss: -0.007002186859608628\nTRAIN: \t Epoch: 30 \t Loss: -0.0069185886669027455\nTRAIN: \t Epoch: 30 \t Loss: -0.0069901999215491945\nTRAIN: \t Epoch: 30 \t Loss: -0.007114422372787406\nTRAIN: \t Epoch: 30 \t Loss: -0.007154061982873827\nTRAIN: \t Epoch: 30 \t Loss: -0.007250451139130053\nTRAIN: \t Epoch: 30 \t Loss: -0.007282604039036393\nVALD: \t Epoch: 30 \t Loss: -0.004799598827958107\nVALD: \t Epoch: 30 \t Loss: -0.003478339873254299\nVALD: \t Epoch: 30 \t Loss: -0.003296169607589642\nVALD: \t Epoch: 30 \t Loss: 0.0020087986486032605\nVALD: \t Epoch: 30 \t Loss: 0.0006355684250593186\nVALD: \t Epoch: 30 \t Loss: 0.0005665946638945378\n******************************\nEpoch: social-tag : 30\ntrain_loss -0.007282604039036393\nval_loss 0.0005665946638945378\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 31 \t Loss: -0.007618470583111048\nTRAIN: \t Epoch: 31 \t Loss: -0.007744502974674106\nTRAIN: \t Epoch: 31 \t Loss: -0.007325106921295325\nTRAIN: \t Epoch: 31 \t Loss: -0.006911816308274865\nTRAIN: \t Epoch: 31 \t Loss: -0.006945865787565708\nTRAIN: \t Epoch: 31 \t Loss: -0.007115172998358806\nTRAIN: \t Epoch: 31 \t Loss: -0.007247351642165866\nTRAIN: \t Epoch: 31 \t Loss: -0.007428073673509061\nTRAIN: \t Epoch: 31 \t Loss: -0.007544156060450607\nTRAIN: \t Epoch: 31 \t Loss: -0.007610395364463329\nTRAIN: \t Epoch: 31 \t Loss: -0.0075822267681360245\nTRAIN: \t Epoch: 31 \t Loss: -0.007476500313108166\nTRAIN: \t Epoch: 31 \t Loss: -0.0074526559060009625\nTRAIN: \t Epoch: 31 \t Loss: -0.007450440632445472\nTRAIN: \t Epoch: 31 \t Loss: -0.007535137298206488\nTRAIN: \t Epoch: 31 \t Loss: -0.007538394682342187\nTRAIN: \t Epoch: 31 \t Loss: -0.007394004394026364\nTRAIN: \t Epoch: 31 \t Loss: -0.007282125660114818\nTRAIN: \t Epoch: 31 \t Loss: -0.007307710469161209\nTRAIN: \t Epoch: 31 \t Loss: -0.007420602114871144\nTRAIN: \t Epoch: 31 \t Loss: -0.007450479437552747\nTRAIN: \t Epoch: 31 \t Loss: -0.0074526313819405846\nVALD: \t Epoch: 31 \t Loss: -0.005795028526335955\nVALD: \t Epoch: 31 \t Loss: -0.0030771785241086036\nVALD: \t Epoch: 31 \t Loss: -0.0035200397057148316\nVALD: \t Epoch: 31 \t Loss: -0.0021727872226620093\nVALD: \t Epoch: 31 \t Loss: -0.0027430758927948773\nVALD: \t Epoch: 31 \t Loss: -0.0026630103570465563\n******************************\nEpoch: social-tag : 31\ntrain_loss -0.0074526313819405846\nval_loss -0.0026630103570465563\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 32 \t Loss: -0.007912058383226395\nTRAIN: \t Epoch: 32 \t Loss: -0.007665113778784871\nTRAIN: \t Epoch: 32 \t Loss: -0.007070018909871578\nTRAIN: \t Epoch: 32 \t Loss: -0.006835707696154714\nTRAIN: \t Epoch: 32 \t Loss: -0.006957358401268721\nTRAIN: \t Epoch: 32 \t Loss: -0.007050499552860856\nTRAIN: \t Epoch: 32 \t Loss: -0.006991126441529819\nTRAIN: \t Epoch: 32 \t Loss: -0.006924746849108487\nTRAIN: \t Epoch: 32 \t Loss: -0.00706288047755758\nTRAIN: \t Epoch: 32 \t Loss: -0.007290394091978669\nTRAIN: \t Epoch: 32 \t Loss: -0.007386780132285573\nTRAIN: \t Epoch: 32 \t Loss: -0.007424610783345997\nTRAIN: \t Epoch: 32 \t Loss: -0.007310363607337842\nTRAIN: \t Epoch: 32 \t Loss: -0.0071468720811286145\nTRAIN: \t Epoch: 32 \t Loss: -0.00719300132865707\nTRAIN: \t Epoch: 32 \t Loss: -0.0072813806182239205\nTRAIN: \t Epoch: 32 \t Loss: -0.007356241318013738\nTRAIN: \t Epoch: 32 \t Loss: -0.0074462786886013215\nTRAIN: \t Epoch: 32 \t Loss: -0.007559974800403181\nTRAIN: \t Epoch: 32 \t Loss: -0.007609817362390458\nTRAIN: \t Epoch: 32 \t Loss: -0.007631198037415743\nTRAIN: \t Epoch: 32 \t Loss: -0.007609141300995859\nVALD: \t Epoch: 32 \t Loss: -0.005966939963400364\nVALD: \t Epoch: 32 \t Loss: -0.004335254430770874\nVALD: \t Epoch: 32 \t Loss: -0.0034273066945994892\nVALD: \t Epoch: 32 \t Loss: -0.0003449368814472109\nVALD: \t Epoch: 32 \t Loss: -0.0012027184711769223\nVALD: \t Epoch: 32 \t Loss: -0.0012013961476358501\n******************************\nEpoch: social-tag : 32\ntrain_loss -0.007609141300995859\nval_loss -0.0012013961476358501\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 33 \t Loss: -0.007978230714797974\nTRAIN: \t Epoch: 33 \t Loss: -0.00792835233733058\nTRAIN: \t Epoch: 33 \t Loss: -0.007585455818722646\nTRAIN: \t Epoch: 33 \t Loss: -0.007316578063182533\nTRAIN: \t Epoch: 33 \t Loss: -0.007309316005557775\nTRAIN: \t Epoch: 33 \t Loss: -0.007483615772798657\nTRAIN: \t Epoch: 33 \t Loss: -0.00753156967195017\nTRAIN: \t Epoch: 33 \t Loss: -0.007667742494959384\nTRAIN: \t Epoch: 33 \t Loss: -0.007769505441602733\nTRAIN: \t Epoch: 33 \t Loss: -0.00781891872175038\nTRAIN: \t Epoch: 33 \t Loss: -0.007827949329194698\nTRAIN: \t Epoch: 33 \t Loss: -0.007829657018495103\nTRAIN: \t Epoch: 33 \t Loss: -0.007710532679294164\nTRAIN: \t Epoch: 33 \t Loss: -0.007597881269508174\nTRAIN: \t Epoch: 33 \t Loss: -0.007606516561160485\nTRAIN: \t Epoch: 33 \t Loss: -0.007575570314656943\nTRAIN: \t Epoch: 33 \t Loss: -0.007629506837795763\nTRAIN: \t Epoch: 33 \t Loss: -0.007660470488998625\nTRAIN: \t Epoch: 33 \t Loss: -0.00762009885358183\nTRAIN: \t Epoch: 33 \t Loss: -0.007574288756586611\nTRAIN: \t Epoch: 33 \t Loss: -0.007586693324680839\nTRAIN: \t Epoch: 33 \t Loss: -0.00766806523290528\nVALD: \t Epoch: 33 \t Loss: -0.002238135552033782\nVALD: \t Epoch: 33 \t Loss: 0.003741545253433287\nVALD: \t Epoch: 33 \t Loss: 0.0025802418046320477\nVALD: \t Epoch: 33 \t Loss: 0.009156728308880702\nVALD: \t Epoch: 33 \t Loss: 0.006743977521546185\nVALD: \t Epoch: 33 \t Loss: 0.006552886124700308\n******************************\nEpoch: social-tag : 33\ntrain_loss -0.00766806523290528\nval_loss 0.006552886124700308\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 34 \t Loss: -0.008848484605550766\nTRAIN: \t Epoch: 34 \t Loss: -0.008869689423590899\nTRAIN: \t Epoch: 34 \t Loss: -0.00886366299043099\nTRAIN: \t Epoch: 34 \t Loss: -0.008997895987704396\nTRAIN: \t Epoch: 34 \t Loss: -0.008576297946274281\nTRAIN: \t Epoch: 34 \t Loss: -0.008071009380122026\nTRAIN: \t Epoch: 34 \t Loss: -0.008101968759936946\nTRAIN: \t Epoch: 34 \t Loss: -0.00823968683835119\nTRAIN: \t Epoch: 34 \t Loss: -0.008329909708764818\nTRAIN: \t Epoch: 34 \t Loss: -0.00835818201303482\nTRAIN: \t Epoch: 34 \t Loss: -0.008043266748162832\nTRAIN: \t Epoch: 34 \t Loss: -0.007864862214773893\nTRAIN: \t Epoch: 34 \t Loss: -0.007901578281934444\nTRAIN: \t Epoch: 34 \t Loss: -0.007965861087931054\nTRAIN: \t Epoch: 34 \t Loss: -0.008014004615445932\nTRAIN: \t Epoch: 34 \t Loss: -0.007957934372825548\nTRAIN: \t Epoch: 34 \t Loss: -0.007744568783570738\nTRAIN: \t Epoch: 34 \t Loss: -0.007661300581983394\nTRAIN: \t Epoch: 34 \t Loss: -0.007718175183981657\nTRAIN: \t Epoch: 34 \t Loss: -0.0077606923645362254\nTRAIN: \t Epoch: 34 \t Loss: -0.007848354743882305\nTRAIN: \t Epoch: 34 \t Loss: -0.007901929910135868\nVALD: \t Epoch: 34 \t Loss: 0.0004932956653647125\nVALD: \t Epoch: 34 \t Loss: 0.00655952058150433\nVALD: \t Epoch: 34 \t Loss: 0.005643843110495557\nVALD: \t Epoch: 34 \t Loss: 0.008437227734248154\nVALD: \t Epoch: 34 \t Loss: 0.006316220376174897\nVALD: \t Epoch: 34 \t Loss: 0.006033404882658612\n******************************\nEpoch: social-tag : 34\ntrain_loss -0.007901929910135868\nval_loss 0.006033404882658612\n{'min_val_epoch': 15, 'min_val_loss': -0.003182736033517303}\n******************************\nTRAIN: \t Epoch: 35 \t Loss: -0.008358598686754704\nTRAIN: \t Epoch: 35 \t Loss: -0.007698713568970561\nTRAIN: \t Epoch: 35 \t Loss: -0.007443846669048071\nTRAIN: \t Epoch: 35 \t Loss: -0.007484114612452686\nTRAIN: \t Epoch: 35 \t Loss: -0.007824149820953608\nTRAIN: \t Epoch: 35 \t Loss: -0.007993685314431787\nTRAIN: \t Epoch: 35 \t Loss: -0.007803646381944418\nTRAIN: \t Epoch: 35 \t Loss: -0.007311761495657265\nTRAIN: \t Epoch: 35 \t Loss: -0.007240692670974467\nTRAIN: \t Epoch: 35 \t Loss: -0.00739730903878808\nTRAIN: \t Epoch: 35 \t Loss: -0.007602573965083469\nTRAIN: \t Epoch: 35 \t Loss: -0.007705412805080414\nTRAIN: \t Epoch: 35 \t Loss: -0.007744990933973055\nTRAIN: \t Epoch: 35 \t Loss: -0.007630949307765279\nTRAIN: \t Epoch: 35 \t Loss: -0.0075744041552146275\nTRAIN: \t Epoch: 35 \t Loss: -0.007626652542967349\nTRAIN: \t Epoch: 35 \t Loss: -0.007640471846303519\nTRAIN: \t Epoch: 35 \t Loss: -0.0077360291551384665\nTRAIN: \t Epoch: 35 \t Loss: -0.0077894158955467375\nTRAIN: \t Epoch: 35 \t Loss: -0.007865461660549045\nTRAIN: \t Epoch: 35 \t Loss: -0.007861834729001635\nTRAIN: \t Epoch: 35 \t Loss: -0.007783441141220997\nVALD: \t Epoch: 35 \t Loss: -0.0061071463860571384\nVALD: \t Epoch: 35 \t Loss: -0.004847830394282937\nVALD: \t Epoch: 35 \t Loss: -0.004171344839657347\nVALD: \t Epoch: 35 \t Loss: -0.003214865289919544\nVALD: \t Epoch: 35 \t Loss: -0.0036588536167982966\nVALD: \t Epoch: 35 \t Loss: -0.0035986260877865733\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 70/70 [00:02<00:00, 25.99it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.7571972458632835  FDE: 1.0323289405027707\n**************************************************\n******************************\nEpoch: social-tag : 35\ntrain_loss -0.007783441141220997\nval_loss -0.0035986260877865733\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 36 \t Loss: -0.008187077939510345\nTRAIN: \t Epoch: 36 \t Loss: -0.009008629713207483\nTRAIN: \t Epoch: 36 \t Loss: -0.00893100971976916\nTRAIN: \t Epoch: 36 \t Loss: -0.00866425852291286\nTRAIN: \t Epoch: 36 \t Loss: -0.008529812283813953\nTRAIN: \t Epoch: 36 \t Loss: -0.008253764283532897\nTRAIN: \t Epoch: 36 \t Loss: -0.008090884757361241\nTRAIN: \t Epoch: 36 \t Loss: -0.008038094965741038\nTRAIN: \t Epoch: 36 \t Loss: -0.008060884868933095\nTRAIN: \t Epoch: 36 \t Loss: -0.008141392935067415\nTRAIN: \t Epoch: 36 \t Loss: -0.00819303078407591\nTRAIN: \t Epoch: 36 \t Loss: -0.008064840105362236\nTRAIN: \t Epoch: 36 \t Loss: -0.007918529105014525\nTRAIN: \t Epoch: 36 \t Loss: -0.00785489410295018\nTRAIN: \t Epoch: 36 \t Loss: -0.00781616869693001\nTRAIN: \t Epoch: 36 \t Loss: -0.007860250218072906\nTRAIN: \t Epoch: 36 \t Loss: -0.00788601118085139\nTRAIN: \t Epoch: 36 \t Loss: -0.007956323084524937\nTRAIN: \t Epoch: 36 \t Loss: -0.007938054267709193\nTRAIN: \t Epoch: 36 \t Loss: -0.007864934322424234\nTRAIN: \t Epoch: 36 \t Loss: -0.007812584550785167\nTRAIN: \t Epoch: 36 \t Loss: -0.007785286102945543\nVALD: \t Epoch: 36 \t Loss: -0.002328116213902831\nVALD: \t Epoch: 36 \t Loss: -0.0006736131617799401\nVALD: \t Epoch: 36 \t Loss: 0.0022076364451398454\nVALD: \t Epoch: 36 \t Loss: 0.010867520875763148\nVALD: \t Epoch: 36 \t Loss: 0.00849668332375586\nVALD: \t Epoch: 36 \t Loss: 0.008241792085975634\n******************************\nEpoch: social-tag : 36\ntrain_loss -0.007785286102945543\nval_loss 0.008241792085975634\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 37 \t Loss: -0.008136247284710407\nTRAIN: \t Epoch: 37 \t Loss: -0.00809624744579196\nTRAIN: \t Epoch: 37 \t Loss: -0.008376089545587698\nTRAIN: \t Epoch: 37 \t Loss: -0.008415285497903824\nTRAIN: \t Epoch: 37 \t Loss: -0.007669576723128557\nTRAIN: \t Epoch: 37 \t Loss: -0.007309982009852926\nTRAIN: \t Epoch: 37 \t Loss: -0.00741037427048598\nTRAIN: \t Epoch: 37 \t Loss: -0.007564230530988425\nTRAIN: \t Epoch: 37 \t Loss: -0.007676150805006425\nTRAIN: \t Epoch: 37 \t Loss: -0.007804357213899493\nTRAIN: \t Epoch: 37 \t Loss: -0.007773356088860469\nTRAIN: \t Epoch: 37 \t Loss: -0.007638887851499021\nTRAIN: \t Epoch: 37 \t Loss: -0.007617922571416085\nTRAIN: \t Epoch: 37 \t Loss: -0.007730621711484024\nTRAIN: \t Epoch: 37 \t Loss: -0.007828335339824358\nTRAIN: \t Epoch: 37 \t Loss: -0.00791327195474878\nTRAIN: \t Epoch: 37 \t Loss: -0.007891919902142356\nTRAIN: \t Epoch: 37 \t Loss: -0.007846166586710347\nTRAIN: \t Epoch: 37 \t Loss: -0.007869984954595566\nTRAIN: \t Epoch: 37 \t Loss: -0.007904027495533228\nTRAIN: \t Epoch: 37 \t Loss: -0.007940132747448626\nTRAIN: \t Epoch: 37 \t Loss: -0.00793246214864926\nVALD: \t Epoch: 37 \t Loss: -0.00728292902931571\nVALD: \t Epoch: 37 \t Loss: -0.00461204070597887\nVALD: \t Epoch: 37 \t Loss: -0.0034305872783685722\nVALD: \t Epoch: 37 \t Loss: 0.0024978967558126897\nVALD: \t Epoch: 37 \t Loss: 0.0010784933110699058\nVALD: \t Epoch: 37 \t Loss: 0.0010595672199446144\n******************************\nEpoch: social-tag : 37\ntrain_loss -0.00793246214864926\nval_loss 0.0010595672199446144\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 38 \t Loss: -0.009075538255274296\nTRAIN: \t Epoch: 38 \t Loss: -0.009155334904789925\nTRAIN: \t Epoch: 38 \t Loss: -0.00834444398060441\nTRAIN: \t Epoch: 38 \t Loss: -0.007746065617538989\nTRAIN: \t Epoch: 38 \t Loss: -0.007729678135365248\nTRAIN: \t Epoch: 38 \t Loss: -0.007939239265397191\nTRAIN: \t Epoch: 38 \t Loss: -0.008060719485261611\nTRAIN: \t Epoch: 38 \t Loss: -0.007992509345058352\nTRAIN: \t Epoch: 38 \t Loss: -0.007845320376671024\nTRAIN: \t Epoch: 38 \t Loss: -0.00783506389707327\nTRAIN: \t Epoch: 38 \t Loss: -0.007929247211326252\nTRAIN: \t Epoch: 38 \t Loss: -0.00800790723102788\nTRAIN: \t Epoch: 38 \t Loss: -0.008005048864736007\nTRAIN: \t Epoch: 38 \t Loss: -0.008002233092806168\nTRAIN: \t Epoch: 38 \t Loss: -0.008096121003230412\nTRAIN: \t Epoch: 38 \t Loss: -0.00815274950582534\nTRAIN: \t Epoch: 38 \t Loss: -0.008124973405809963\nTRAIN: \t Epoch: 38 \t Loss: -0.008105102848882476\nTRAIN: \t Epoch: 38 \t Loss: -0.008149380526064258\nTRAIN: \t Epoch: 38 \t Loss: -0.008191308449022472\nTRAIN: \t Epoch: 38 \t Loss: -0.008141844422512111\nTRAIN: \t Epoch: 38 \t Loss: -0.008070363583213764\nVALD: \t Epoch: 38 \t Loss: -0.005075691267848015\nVALD: \t Epoch: 38 \t Loss: -0.0032669728389009833\nVALD: \t Epoch: 38 \t Loss: -0.002806594556507965\nVALD: \t Epoch: 38 \t Loss: 0.002813976287143305\nVALD: \t Epoch: 38 \t Loss: 0.001297105592675507\nVALD: \t Epoch: 38 \t Loss: 0.0011946223055322965\n******************************\nEpoch: social-tag : 38\ntrain_loss -0.008070363583213764\nval_loss 0.0011946223055322965\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 39 \t Loss: -0.008459027856588364\nTRAIN: \t Epoch: 39 \t Loss: -0.009244532324373722\nTRAIN: \t Epoch: 39 \t Loss: -0.009351952001452446\nTRAIN: \t Epoch: 39 \t Loss: -0.00905113504268229\nTRAIN: \t Epoch: 39 \t Loss: -0.008619110193103552\nTRAIN: \t Epoch: 39 \t Loss: -0.008650602384780845\nTRAIN: \t Epoch: 39 \t Loss: -0.008770038573337453\nTRAIN: \t Epoch: 39 \t Loss: -0.008889118384104222\nTRAIN: \t Epoch: 39 \t Loss: -0.00863896305155423\nTRAIN: \t Epoch: 39 \t Loss: -0.008296792395412922\nTRAIN: \t Epoch: 39 \t Loss: -0.008195166010409594\nTRAIN: \t Epoch: 39 \t Loss: -0.008267056856614849\nTRAIN: \t Epoch: 39 \t Loss: -0.00838422434977614\nTRAIN: \t Epoch: 39 \t Loss: -0.008413378581670778\nTRAIN: \t Epoch: 39 \t Loss: -0.008371693765123684\nTRAIN: \t Epoch: 39 \t Loss: -0.008291222795378417\nTRAIN: \t Epoch: 39 \t Loss: -0.008176847388420035\nTRAIN: \t Epoch: 39 \t Loss: -0.008186111976909969\nTRAIN: \t Epoch: 39 \t Loss: -0.008219058159738779\nTRAIN: \t Epoch: 39 \t Loss: -0.008236082899384201\nTRAIN: \t Epoch: 39 \t Loss: -0.008214792680172693\nTRAIN: \t Epoch: 39 \t Loss: -0.008162018704885213\nVALD: \t Epoch: 39 \t Loss: -0.006193961016833782\nVALD: \t Epoch: 39 \t Loss: -0.004635977209545672\nVALD: \t Epoch: 39 \t Loss: -0.0035405607195571065\nVALD: \t Epoch: 39 \t Loss: 0.0013849119713995606\nVALD: \t Epoch: 39 \t Loss: -1.340711023658514e-05\nVALD: \t Epoch: 39 \t Loss: -0.0001095503004211368\n******************************\nEpoch: social-tag : 39\ntrain_loss -0.008162018704885213\nval_loss -0.0001095503004211368\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 40 \t Loss: -0.008799071423709393\nTRAIN: \t Epoch: 40 \t Loss: -0.009084062650799751\nTRAIN: \t Epoch: 40 \t Loss: -0.009408805829783281\nTRAIN: \t Epoch: 40 \t Loss: -0.009281542850658298\nTRAIN: \t Epoch: 40 \t Loss: -0.009189285151660443\nTRAIN: \t Epoch: 40 \t Loss: -0.009291269350796938\nTRAIN: \t Epoch: 40 \t Loss: -0.009269093828541892\nTRAIN: \t Epoch: 40 \t Loss: -0.008979862148407847\nTRAIN: \t Epoch: 40 \t Loss: -0.008684367200152742\nTRAIN: \t Epoch: 40 \t Loss: -0.008630247553810477\nTRAIN: \t Epoch: 40 \t Loss: -0.008703944230960175\nTRAIN: \t Epoch: 40 \t Loss: -0.008788528890969852\nTRAIN: \t Epoch: 40 \t Loss: -0.008725961276258413\nTRAIN: \t Epoch: 40 \t Loss: -0.008603749896532722\nTRAIN: \t Epoch: 40 \t Loss: -0.008587567166735728\nTRAIN: \t Epoch: 40 \t Loss: -0.00861641691881232\nTRAIN: \t Epoch: 40 \t Loss: -0.008658890530247898\nTRAIN: \t Epoch: 40 \t Loss: -0.008658040444263153\nTRAIN: \t Epoch: 40 \t Loss: -0.008650858960065403\nTRAIN: \t Epoch: 40 \t Loss: -0.008558270381763577\nTRAIN: \t Epoch: 40 \t Loss: -0.00849942738811175\nTRAIN: \t Epoch: 40 \t Loss: -0.008518446069645497\nVALD: \t Epoch: 40 \t Loss: -0.0008331630378961563\nVALD: \t Epoch: 40 \t Loss: 0.004585988353937864\nVALD: \t Epoch: 40 \t Loss: 0.003379164438229054\nVALD: \t Epoch: 40 \t Loss: 0.0073756991332629696\nVALD: \t Epoch: 40 \t Loss: 0.005239454715047031\nVALD: \t Epoch: 40 \t Loss: 0.005121785718383211\n******************************\nEpoch: social-tag : 40\ntrain_loss -0.008518446069645497\nval_loss 0.005121785718383211\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 41 \t Loss: -0.010055000893771648\nTRAIN: \t Epoch: 41 \t Loss: -0.009461663197726011\nTRAIN: \t Epoch: 41 \t Loss: -0.009215960279107094\nTRAIN: \t Epoch: 41 \t Loss: -0.008640471380203962\nTRAIN: \t Epoch: 41 \t Loss: -0.00837913854047656\nTRAIN: \t Epoch: 41 \t Loss: -0.008307622047141194\nTRAIN: \t Epoch: 41 \t Loss: -0.00835984033931579\nTRAIN: \t Epoch: 41 \t Loss: -0.008343336696270853\nTRAIN: \t Epoch: 41 \t Loss: -0.008237858406371541\nTRAIN: \t Epoch: 41 \t Loss: -0.008188470685854553\nTRAIN: \t Epoch: 41 \t Loss: -0.00831752410158515\nTRAIN: \t Epoch: 41 \t Loss: -0.008432870459121963\nTRAIN: \t Epoch: 41 \t Loss: -0.008518984362196464\nTRAIN: \t Epoch: 41 \t Loss: -0.008449288750333446\nTRAIN: \t Epoch: 41 \t Loss: -0.008335519240548213\nTRAIN: \t Epoch: 41 \t Loss: -0.00830864833551459\nTRAIN: \t Epoch: 41 \t Loss: -0.008416658952174819\nTRAIN: \t Epoch: 41 \t Loss: -0.008482416755416326\nTRAIN: \t Epoch: 41 \t Loss: -0.008349499910285598\nTRAIN: \t Epoch: 41 \t Loss: -0.008279388560913504\nTRAIN: \t Epoch: 41 \t Loss: -0.008340092265002784\nTRAIN: \t Epoch: 41 \t Loss: -0.00835470449132594\nVALD: \t Epoch: 41 \t Loss: 0.0050815618596971035\nVALD: \t Epoch: 41 \t Loss: 0.018600630341097713\nVALD: \t Epoch: 41 \t Loss: 0.015039574820548296\nVALD: \t Epoch: 41 \t Loss: 0.021383566199801862\nVALD: \t Epoch: 41 \t Loss: 0.016852428391575813\nVALD: \t Epoch: 41 \t Loss: 0.016329577937722207\n******************************\nEpoch: social-tag : 41\ntrain_loss -0.00835470449132594\nval_loss 0.016329577937722207\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 42 \t Loss: -0.009106597863137722\nTRAIN: \t Epoch: 42 \t Loss: -0.008925135713070631\nTRAIN: \t Epoch: 42 \t Loss: -0.00837982880572478\nTRAIN: \t Epoch: 42 \t Loss: -0.00772239058278501\nTRAIN: \t Epoch: 42 \t Loss: -0.007835016585886478\nTRAIN: \t Epoch: 42 \t Loss: -0.008000317340095838\nTRAIN: \t Epoch: 42 \t Loss: -0.008185762512896742\nTRAIN: \t Epoch: 42 \t Loss: -0.008256083587184548\nTRAIN: \t Epoch: 42 \t Loss: -0.008278945150474707\nTRAIN: \t Epoch: 42 \t Loss: -0.00817330265417695\nTRAIN: \t Epoch: 42 \t Loss: -0.00802875670011748\nTRAIN: \t Epoch: 42 \t Loss: -0.008097026303100089\nTRAIN: \t Epoch: 42 \t Loss: -0.008315514808950515\nTRAIN: \t Epoch: 42 \t Loss: -0.008431806062747325\nTRAIN: \t Epoch: 42 \t Loss: -0.008424073861291011\nTRAIN: \t Epoch: 42 \t Loss: -0.008246004988905042\nTRAIN: \t Epoch: 42 \t Loss: -0.008189750643556608\nTRAIN: \t Epoch: 42 \t Loss: -0.00821460630848176\nTRAIN: \t Epoch: 42 \t Loss: -0.008290051262041456\nTRAIN: \t Epoch: 42 \t Loss: -0.008352812216617166\nTRAIN: \t Epoch: 42 \t Loss: -0.008439012780962955\nTRAIN: \t Epoch: 42 \t Loss: -0.00845322493376809\nVALD: \t Epoch: 42 \t Loss: 0.0030571185052394867\nVALD: \t Epoch: 42 \t Loss: 0.002524740295484662\nVALD: \t Epoch: 42 \t Loss: 0.0016879218953060142\nVALD: \t Epoch: 42 \t Loss: 0.010887520621508884\nVALD: \t Epoch: 42 \t Loss: 0.008397571021487239\nVALD: \t Epoch: 42 \t Loss: 0.008094269963630447\n******************************\nEpoch: social-tag : 42\ntrain_loss -0.00845322493376809\nval_loss 0.008094269963630447\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 43 \t Loss: -0.008511867374181747\nTRAIN: \t Epoch: 43 \t Loss: -0.008152102353051305\nTRAIN: \t Epoch: 43 \t Loss: -0.008163509424775839\nTRAIN: \t Epoch: 43 \t Loss: -0.008543781354092062\nTRAIN: \t Epoch: 43 \t Loss: -0.008593619894236326\nTRAIN: \t Epoch: 43 \t Loss: -0.008567863221590718\nTRAIN: \t Epoch: 43 \t Loss: -0.008288445150745767\nTRAIN: \t Epoch: 43 \t Loss: -0.008231357613112777\nTRAIN: \t Epoch: 43 \t Loss: -0.008417245569742389\nTRAIN: \t Epoch: 43 \t Loss: -0.008588381623849273\nTRAIN: \t Epoch: 43 \t Loss: -0.008624989327720621\nTRAIN: \t Epoch: 43 \t Loss: -0.008634814565690855\nTRAIN: \t Epoch: 43 \t Loss: -0.008607729230649196\nTRAIN: \t Epoch: 43 \t Loss: -0.00863530031139297\nTRAIN: \t Epoch: 43 \t Loss: -0.008645955380052328\nTRAIN: \t Epoch: 43 \t Loss: -0.00858140186755918\nTRAIN: \t Epoch: 43 \t Loss: -0.008593564332627198\nTRAIN: \t Epoch: 43 \t Loss: -0.008583969928117262\nTRAIN: \t Epoch: 43 \t Loss: -0.008594740451754708\nTRAIN: \t Epoch: 43 \t Loss: -0.008609028928913177\nTRAIN: \t Epoch: 43 \t Loss: -0.008660002633751858\nTRAIN: \t Epoch: 43 \t Loss: -0.00866016727375599\nVALD: \t Epoch: 43 \t Loss: -0.004153992980718613\nVALD: \t Epoch: 43 \t Loss: -0.0031258001690730453\nVALD: \t Epoch: 43 \t Loss: -0.003175638926525911\nVALD: \t Epoch: 43 \t Loss: 0.003589926054701209\nVALD: \t Epoch: 43 \t Loss: 0.0023462381679564713\nVALD: \t Epoch: 43 \t Loss: 0.0022701098389613134\n******************************\nEpoch: social-tag : 43\ntrain_loss -0.00866016727375599\nval_loss 0.0022701098389613134\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 44 \t Loss: -0.0091972965747118\nTRAIN: \t Epoch: 44 \t Loss: -0.008819082286208868\nTRAIN: \t Epoch: 44 \t Loss: -0.008206146614005169\nTRAIN: \t Epoch: 44 \t Loss: -0.008326633949764073\nTRAIN: \t Epoch: 44 \t Loss: -0.008715125452727079\nTRAIN: \t Epoch: 44 \t Loss: -0.00883328130779167\nTRAIN: \t Epoch: 44 \t Loss: -0.009010534533964736\nTRAIN: \t Epoch: 44 \t Loss: -0.00893175887176767\nTRAIN: \t Epoch: 44 \t Loss: -0.008733987446046539\nTRAIN: \t Epoch: 44 \t Loss: -0.00847277482971549\nTRAIN: \t Epoch: 44 \t Loss: -0.008532055060971867\nTRAIN: \t Epoch: 44 \t Loss: -0.008657977993910512\nTRAIN: \t Epoch: 44 \t Loss: -0.008792339609219478\nTRAIN: \t Epoch: 44 \t Loss: -0.008851895840572459\nTRAIN: \t Epoch: 44 \t Loss: -0.008845542247096697\nTRAIN: \t Epoch: 44 \t Loss: -0.00875004442059435\nTRAIN: \t Epoch: 44 \t Loss: -0.008700444445233135\nTRAIN: \t Epoch: 44 \t Loss: -0.008730549132451415\nTRAIN: \t Epoch: 44 \t Loss: -0.008815770454116557\nTRAIN: \t Epoch: 44 \t Loss: -0.008866346045397221\nTRAIN: \t Epoch: 44 \t Loss: -0.00881079879278938\nTRAIN: \t Epoch: 44 \t Loss: -0.00870770665430723\nVALD: \t Epoch: 44 \t Loss: -0.005383426323533058\nVALD: \t Epoch: 44 \t Loss: -0.0031022908515296876\nVALD: \t Epoch: 44 \t Loss: -0.003380390931852162\nVALD: \t Epoch: 44 \t Loss: 0.0027812399493996054\nVALD: \t Epoch: 44 \t Loss: 0.001302624517120421\nVALD: \t Epoch: 44 \t Loss: 0.0012433602404075138\n******************************\nEpoch: social-tag : 44\ntrain_loss -0.00870770665430723\nval_loss 0.0012433602404075138\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 45 \t Loss: -0.008845599368214607\nTRAIN: \t Epoch: 45 \t Loss: -0.009274694602936506\nTRAIN: \t Epoch: 45 \t Loss: -0.009471864129106203\nTRAIN: \t Epoch: 45 \t Loss: -0.009509833762422204\nTRAIN: \t Epoch: 45 \t Loss: -0.009741544537246228\nTRAIN: \t Epoch: 45 \t Loss: -0.009420629125088453\nTRAIN: \t Epoch: 45 \t Loss: -0.008906956030321973\nTRAIN: \t Epoch: 45 \t Loss: -0.00862120243255049\nTRAIN: \t Epoch: 45 \t Loss: -0.008707440354757838\nTRAIN: \t Epoch: 45 \t Loss: -0.00886527644470334\nTRAIN: \t Epoch: 45 \t Loss: -0.008923974565484306\nTRAIN: \t Epoch: 45 \t Loss: -0.008682930880847076\nTRAIN: \t Epoch: 45 \t Loss: -0.008475885475770785\nTRAIN: \t Epoch: 45 \t Loss: -0.0084740811892386\nTRAIN: \t Epoch: 45 \t Loss: -0.00855408866579334\nTRAIN: \t Epoch: 45 \t Loss: -0.008651982148876414\nTRAIN: \t Epoch: 45 \t Loss: -0.008752585854381323\nTRAIN: \t Epoch: 45 \t Loss: -0.00878589877134396\nTRAIN: \t Epoch: 45 \t Loss: -0.008706604939346252\nTRAIN: \t Epoch: 45 \t Loss: -0.008593349601142108\nTRAIN: \t Epoch: 45 \t Loss: -0.008619440874705711\nTRAIN: \t Epoch: 45 \t Loss: -0.008677902320129011\nVALD: \t Epoch: 45 \t Loss: -0.0024941449519246817\nVALD: \t Epoch: 45 \t Loss: -0.0008678774174768478\nVALD: \t Epoch: 45 \t Loss: -0.0012244844304708142\nVALD: \t Epoch: 45 \t Loss: 0.0033719166094670072\nVALD: \t Epoch: 45 \t Loss: 0.0017732275766320527\nVALD: \t Epoch: 45 \t Loss: 0.0016409017597184036\n******************************\nEpoch: social-tag : 45\ntrain_loss -0.008677902320129011\nval_loss 0.0016409017597184036\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 46 \t Loss: -0.009974435903131962\nTRAIN: \t Epoch: 46 \t Loss: -0.00918044988065958\nTRAIN: \t Epoch: 46 \t Loss: -0.00874496903270483\nTRAIN: \t Epoch: 46 \t Loss: -0.00876282318495214\nTRAIN: \t Epoch: 46 \t Loss: -0.008725113049149514\nTRAIN: \t Epoch: 46 \t Loss: -0.008802120573818684\nTRAIN: \t Epoch: 46 \t Loss: -0.009032584327672209\nTRAIN: \t Epoch: 46 \t Loss: -0.009010576992295682\nTRAIN: \t Epoch: 46 \t Loss: -0.008894445788529184\nTRAIN: \t Epoch: 46 \t Loss: -0.008867813087999821\nTRAIN: \t Epoch: 46 \t Loss: -0.0089687252078544\nTRAIN: \t Epoch: 46 \t Loss: -0.009037222558011612\nTRAIN: \t Epoch: 46 \t Loss: -0.009011973340350848\nTRAIN: \t Epoch: 46 \t Loss: -0.008961566923452275\nTRAIN: \t Epoch: 46 \t Loss: -0.008850146954258283\nTRAIN: \t Epoch: 46 \t Loss: -0.008901313878595829\nTRAIN: \t Epoch: 46 \t Loss: -0.0089768311008811\nTRAIN: \t Epoch: 46 \t Loss: -0.009008664172142744\nTRAIN: \t Epoch: 46 \t Loss: -0.008864862133601778\nTRAIN: \t Epoch: 46 \t Loss: -0.008708251430653036\nTRAIN: \t Epoch: 46 \t Loss: -0.00871689495674911\nTRAIN: \t Epoch: 46 \t Loss: -0.008747541754635275\nVALD: \t Epoch: 46 \t Loss: 0.0004208579193800688\nVALD: \t Epoch: 46 \t Loss: -0.001588498125784099\nVALD: \t Epoch: 46 \t Loss: -0.002712563844397664\nVALD: \t Epoch: 46 \t Loss: 0.0011418574140407145\nVALD: \t Epoch: 46 \t Loss: -0.00016470202244818212\nVALD: \t Epoch: 46 \t Loss: -0.00025339037970159995\n******************************\nEpoch: social-tag : 46\ntrain_loss -0.008747541754635275\nval_loss -0.00025339037970159995\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 47 \t Loss: -0.010501750744879246\nTRAIN: \t Epoch: 47 \t Loss: -0.00962842395529151\nTRAIN: \t Epoch: 47 \t Loss: -0.009588882327079773\nTRAIN: \t Epoch: 47 \t Loss: -0.009732837788760662\nTRAIN: \t Epoch: 47 \t Loss: -0.009641486033797265\nTRAIN: \t Epoch: 47 \t Loss: -0.009552595671266317\nTRAIN: \t Epoch: 47 \t Loss: -0.00946490652859211\nTRAIN: \t Epoch: 47 \t Loss: -0.00923840084578842\nTRAIN: \t Epoch: 47 \t Loss: -0.00924714758164353\nTRAIN: \t Epoch: 47 \t Loss: -0.00932187745347619\nTRAIN: \t Epoch: 47 \t Loss: -0.009366334415972233\nTRAIN: \t Epoch: 47 \t Loss: -0.009252628544345498\nTRAIN: \t Epoch: 47 \t Loss: -0.009163202789540473\nTRAIN: \t Epoch: 47 \t Loss: -0.00918389278064881\nTRAIN: \t Epoch: 47 \t Loss: -0.009235629873971144\nTRAIN: \t Epoch: 47 \t Loss: -0.009243499604053795\nTRAIN: \t Epoch: 47 \t Loss: -0.009105620823581429\nTRAIN: \t Epoch: 47 \t Loss: -0.00898073986172676\nTRAIN: \t Epoch: 47 \t Loss: -0.008994103850502717\nTRAIN: \t Epoch: 47 \t Loss: -0.009051433997228742\nTRAIN: \t Epoch: 47 \t Loss: -0.009097783959337644\nTRAIN: \t Epoch: 47 \t Loss: -0.00913977901212189\nVALD: \t Epoch: 47 \t Loss: -0.002389094792306423\nVALD: \t Epoch: 47 \t Loss: -0.00028899742756038904\nVALD: \t Epoch: 47 \t Loss: -0.00018769769667414948\nVALD: \t Epoch: 47 \t Loss: 0.006557283317306428\nVALD: \t Epoch: 47 \t Loss: 0.004423641115135979\nVALD: \t Epoch: 47 \t Loss: 0.004218669984997673\n******************************\nEpoch: social-tag : 47\ntrain_loss -0.00913977901212189\nval_loss 0.004218669984997673\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 48 \t Loss: -0.009750641882419586\nTRAIN: \t Epoch: 48 \t Loss: -0.009229830466210842\nTRAIN: \t Epoch: 48 \t Loss: -0.008843212698896727\nTRAIN: \t Epoch: 48 \t Loss: -0.008647057460621\nTRAIN: \t Epoch: 48 \t Loss: -0.00871940590441227\nTRAIN: \t Epoch: 48 \t Loss: -0.008950919223328432\nTRAIN: \t Epoch: 48 \t Loss: -0.00898683669843844\nTRAIN: \t Epoch: 48 \t Loss: -0.008904773159883916\nTRAIN: \t Epoch: 48 \t Loss: -0.00874501932412386\nTRAIN: \t Epoch: 48 \t Loss: -0.008812947943806649\nTRAIN: \t Epoch: 48 \t Loss: -0.008937961252575571\nTRAIN: \t Epoch: 48 \t Loss: -0.008979369265337786\nTRAIN: \t Epoch: 48 \t Loss: -0.008795322921986762\nTRAIN: \t Epoch: 48 \t Loss: -0.00866558971548719\nTRAIN: \t Epoch: 48 \t Loss: -0.008688274429490168\nTRAIN: \t Epoch: 48 \t Loss: -0.008776953356573358\nTRAIN: \t Epoch: 48 \t Loss: -0.008883414324373007\nTRAIN: \t Epoch: 48 \t Loss: -0.008951835153210495\nTRAIN: \t Epoch: 48 \t Loss: -0.008954795445070454\nTRAIN: \t Epoch: 48 \t Loss: -0.008920520008541644\nTRAIN: \t Epoch: 48 \t Loss: -0.008925838462476219\nTRAIN: \t Epoch: 48 \t Loss: -0.008952299760958878\nVALD: \t Epoch: 48 \t Loss: 0.0026572903152555227\nVALD: \t Epoch: 48 \t Loss: 0.002374989679083228\nVALD: \t Epoch: 48 \t Loss: 0.0009310997556895018\nVALD: \t Epoch: 48 \t Loss: 0.008338788116816431\nVALD: \t Epoch: 48 \t Loss: 0.005645044567063451\nVALD: \t Epoch: 48 \t Loss: 0.005445628370525259\n******************************\nEpoch: social-tag : 48\ntrain_loss -0.008952299760958878\nval_loss 0.005445628370525259\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 49 \t Loss: -0.011044741608202457\nTRAIN: \t Epoch: 49 \t Loss: -0.010699196718633175\nTRAIN: \t Epoch: 49 \t Loss: -0.010013901628553867\nTRAIN: \t Epoch: 49 \t Loss: -0.008930133539251983\nTRAIN: \t Epoch: 49 \t Loss: -0.008674844913184643\nTRAIN: \t Epoch: 49 \t Loss: -0.008942729017386833\nTRAIN: \t Epoch: 49 \t Loss: -0.009244750919086593\nTRAIN: \t Epoch: 49 \t Loss: -0.009222666965797544\nTRAIN: \t Epoch: 49 \t Loss: -0.009187697122494379\nTRAIN: \t Epoch: 49 \t Loss: -0.009169178642332554\nTRAIN: \t Epoch: 49 \t Loss: -0.009160695343532345\nTRAIN: \t Epoch: 49 \t Loss: -0.009171224742506942\nTRAIN: \t Epoch: 49 \t Loss: -0.009183531841979576\nTRAIN: \t Epoch: 49 \t Loss: -0.00917340409276741\nTRAIN: \t Epoch: 49 \t Loss: -0.00914309819539388\nTRAIN: \t Epoch: 49 \t Loss: -0.009104075084906071\nTRAIN: \t Epoch: 49 \t Loss: -0.009131191243581912\nTRAIN: \t Epoch: 49 \t Loss: -0.00917246544526683\nTRAIN: \t Epoch: 49 \t Loss: -0.009205171054131106\nTRAIN: \t Epoch: 49 \t Loss: -0.009223334258422256\nTRAIN: \t Epoch: 49 \t Loss: -0.009131242565455892\nTRAIN: \t Epoch: 49 \t Loss: -0.009082142540539616\nVALD: \t Epoch: 49 \t Loss: 0.00688063632696867\nVALD: \t Epoch: 49 \t Loss: 0.012406186666339636\nVALD: \t Epoch: 49 \t Loss: 0.008701202925294638\nVALD: \t Epoch: 49 \t Loss: 0.014211254310794175\nVALD: \t Epoch: 49 \t Loss: 0.011209527612663805\nVALD: \t Epoch: 49 \t Loss: 0.010754989302068045\n******************************\nEpoch: social-tag : 49\ntrain_loss -0.009082142540539616\nval_loss 0.010754989302068045\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 50 \t Loss: -0.01002469751983881\nTRAIN: \t Epoch: 50 \t Loss: -0.010036862920969725\nTRAIN: \t Epoch: 50 \t Loss: -0.010012799563507238\nTRAIN: \t Epoch: 50 \t Loss: -0.009886689716950059\nTRAIN: \t Epoch: 50 \t Loss: -0.009944935515522956\nTRAIN: \t Epoch: 50 \t Loss: -0.009754325728863478\nTRAIN: \t Epoch: 50 \t Loss: -0.009488604962825775\nTRAIN: \t Epoch: 50 \t Loss: -0.009350610431283712\nTRAIN: \t Epoch: 50 \t Loss: -0.00937199302845531\nTRAIN: \t Epoch: 50 \t Loss: -0.009320880100131036\nTRAIN: \t Epoch: 50 \t Loss: -0.009346672757105394\nTRAIN: \t Epoch: 50 \t Loss: -0.009428633687396845\nTRAIN: \t Epoch: 50 \t Loss: -0.009518758680384893\nTRAIN: \t Epoch: 50 \t Loss: -0.009343960887885519\nTRAIN: \t Epoch: 50 \t Loss: -0.009099811936418215\nTRAIN: \t Epoch: 50 \t Loss: -0.009073595516383648\nTRAIN: \t Epoch: 50 \t Loss: -0.009145202279529151\nTRAIN: \t Epoch: 50 \t Loss: -0.009251890238374472\nTRAIN: \t Epoch: 50 \t Loss: -0.009300038561617074\nTRAIN: \t Epoch: 50 \t Loss: -0.009329835791140795\nTRAIN: \t Epoch: 50 \t Loss: -0.009314159640953654\nTRAIN: \t Epoch: 50 \t Loss: -0.009293491664865929\nVALD: \t Epoch: 50 \t Loss: -0.003926173318177462\nVALD: \t Epoch: 50 \t Loss: -0.003218246274627745\nVALD: \t Epoch: 50 \t Loss: -0.002754266761864225\nVALD: \t Epoch: 50 \t Loss: 0.0052166678360663354\nVALD: \t Epoch: 50 \t Loss: 0.003069569496437907\nVALD: \t Epoch: 50 \t Loss: 0.002859072477528543\n******************************\nEpoch: social-tag : 50\ntrain_loss -0.009293491664865929\nval_loss 0.002859072477528543\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 51 \t Loss: -0.009516231715679169\nTRAIN: \t Epoch: 51 \t Loss: -0.010149242356419563\nTRAIN: \t Epoch: 51 \t Loss: -0.010098444608350595\nTRAIN: \t Epoch: 51 \t Loss: -0.009510229690931737\nTRAIN: \t Epoch: 51 \t Loss: -0.009064295049756765\nTRAIN: \t Epoch: 51 \t Loss: -0.009022713250791034\nTRAIN: \t Epoch: 51 \t Loss: -0.009238524828106165\nTRAIN: \t Epoch: 51 \t Loss: -0.009490561729762703\nTRAIN: \t Epoch: 51 \t Loss: -0.00945186061370704\nTRAIN: \t Epoch: 51 \t Loss: -0.009170150570571422\nTRAIN: \t Epoch: 51 \t Loss: -0.009146049026061188\nTRAIN: \t Epoch: 51 \t Loss: -0.009312205094223222\nTRAIN: \t Epoch: 51 \t Loss: -0.009457053043521367\nTRAIN: \t Epoch: 51 \t Loss: -0.009466753540826696\nTRAIN: \t Epoch: 51 \t Loss: -0.009448293782770633\nTRAIN: \t Epoch: 51 \t Loss: -0.00941161235095933\nTRAIN: \t Epoch: 51 \t Loss: -0.009350739912513424\nTRAIN: \t Epoch: 51 \t Loss: -0.009337008723782169\nTRAIN: \t Epoch: 51 \t Loss: -0.009347057607220976\nTRAIN: \t Epoch: 51 \t Loss: -0.009299882361665368\nTRAIN: \t Epoch: 51 \t Loss: -0.009268196255323432\nTRAIN: \t Epoch: 51 \t Loss: -0.009240411534985692\nVALD: \t Epoch: 51 \t Loss: -0.009522845037281513\nVALD: \t Epoch: 51 \t Loss: -0.00796767114661634\nVALD: \t Epoch: 51 \t Loss: -0.006797321295986573\nVALD: \t Epoch: 51 \t Loss: 0.0004267360782250762\nVALD: \t Epoch: 51 \t Loss: -0.0010524634271860123\nVALD: \t Epoch: 51 \t Loss: -0.001138474859974601\n******************************\nEpoch: social-tag : 51\ntrain_loss -0.009240411534985692\nval_loss -0.001138474859974601\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 52 \t Loss: -0.010679289698600769\nTRAIN: \t Epoch: 52 \t Loss: -0.01049385080114007\nTRAIN: \t Epoch: 52 \t Loss: -0.010505400287608305\nTRAIN: \t Epoch: 52 \t Loss: -0.010228083934634924\nTRAIN: \t Epoch: 52 \t Loss: -0.00964350886642933\nTRAIN: \t Epoch: 52 \t Loss: -0.009529601937780777\nTRAIN: \t Epoch: 52 \t Loss: -0.009736167932195323\nTRAIN: \t Epoch: 52 \t Loss: -0.009835314471274614\nTRAIN: \t Epoch: 52 \t Loss: -0.009931132197380066\nTRAIN: \t Epoch: 52 \t Loss: -0.0096160972956568\nTRAIN: \t Epoch: 52 \t Loss: -0.009144028945064003\nTRAIN: \t Epoch: 52 \t Loss: -0.009032351818556586\nTRAIN: \t Epoch: 52 \t Loss: -0.009146955580665516\nTRAIN: \t Epoch: 52 \t Loss: -0.00925924549145358\nTRAIN: \t Epoch: 52 \t Loss: -0.009238343313336372\nTRAIN: \t Epoch: 52 \t Loss: -0.009268362075090408\nTRAIN: \t Epoch: 52 \t Loss: -0.009287860518430962\nTRAIN: \t Epoch: 52 \t Loss: -0.00925029700415002\nTRAIN: \t Epoch: 52 \t Loss: -0.009235077831698092\nTRAIN: \t Epoch: 52 \t Loss: -0.009244127618148922\nTRAIN: \t Epoch: 52 \t Loss: -0.009241049887523764\nTRAIN: \t Epoch: 52 \t Loss: -0.009255498358762456\nVALD: \t Epoch: 52 \t Loss: 0.0064170267432928085\nVALD: \t Epoch: 52 \t Loss: 0.003023459095857106\nVALD: \t Epoch: 52 \t Loss: -0.0007767708448227495\nVALD: \t Epoch: 52 \t Loss: 0.0063024428600328974\nVALD: \t Epoch: 52 \t Loss: 0.004862129263347015\nVALD: \t Epoch: 52 \t Loss: 0.005259479215425072\n******************************\nEpoch: social-tag : 52\ntrain_loss -0.009255498358762456\nval_loss 0.005259479215425072\n{'min_val_epoch': 35, 'min_val_loss': -0.0035986260877865733}\n******************************\nTRAIN: \t Epoch: 53 \t Loss: -0.010061795823276043\nTRAIN: \t Epoch: 53 \t Loss: -0.009565085172653198\nTRAIN: \t Epoch: 53 \t Loss: -0.009894605105121931\nTRAIN: \t Epoch: 53 \t Loss: -0.009789705509319901\nTRAIN: \t Epoch: 53 \t Loss: -0.009586919844150544\nTRAIN: \t Epoch: 53 \t Loss: -0.009373933542519808\nTRAIN: \t Epoch: 53 \t Loss: -0.0093308724462986\nTRAIN: \t Epoch: 53 \t Loss: -0.00944079039618373\nTRAIN: \t Epoch: 53 \t Loss: -0.009523645560774539\nTRAIN: \t Epoch: 53 \t Loss: -0.009556799195706844\nTRAIN: \t Epoch: 53 \t Loss: -0.009468254379250786\nTRAIN: \t Epoch: 53 \t Loss: -0.009401765263949832\nTRAIN: \t Epoch: 53 \t Loss: -0.009408487293582696\nTRAIN: \t Epoch: 53 \t Loss: -0.009470079559832811\nTRAIN: \t Epoch: 53 \t Loss: -0.009462402078012626\nTRAIN: \t Epoch: 53 \t Loss: -0.009447334101423621\nTRAIN: \t Epoch: 53 \t Loss: -0.009425280451336327\nTRAIN: \t Epoch: 53 \t Loss: -0.009448063145909045\nTRAIN: \t Epoch: 53 \t Loss: -0.009534704302878757\nTRAIN: \t Epoch: 53 \t Loss: -0.009519194206222891\nTRAIN: \t Epoch: 53 \t Loss: -0.009385731115582444\nTRAIN: \t Epoch: 53 \t Loss: -0.009345468905498566\nVALD: \t Epoch: 53 \t Loss: -0.008182959631085396\nVALD: \t Epoch: 53 \t Loss: -0.007545281667262316\nVALD: \t Epoch: 53 \t Loss: -0.007229120625803868\nVALD: \t Epoch: 53 \t Loss: -0.003734897472895682\nVALD: \t Epoch: 53 \t Loss: -0.004303890559822321\nVALD: \t Epoch: 53 \t Loss: -0.004279432784427296\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 70/70 [00:02<00:00, 25.68it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.763456390096164  FDE: 1.19462281552342\n**************************************************\n******************************\nEpoch: social-tag : 53\ntrain_loss -0.009345468905498566\nval_loss -0.004279432784427296\n{'min_val_epoch': 53, 'min_val_loss': -0.004279432784427296}\n******************************\nTRAIN: \t Epoch: 54 \t Loss: -0.01032225787639618\nTRAIN: \t Epoch: 54 \t Loss: -0.010807203128933907\nTRAIN: \t Epoch: 54 \t Loss: -0.010501846360663572\nTRAIN: \t Epoch: 54 \t Loss: -0.010268584126606584\nTRAIN: \t Epoch: 54 \t Loss: -0.010029209591448308\nTRAIN: \t Epoch: 54 \t Loss: -0.009871698915958405\nTRAIN: \t Epoch: 54 \t Loss: -0.009645129154835428\nTRAIN: \t Epoch: 54 \t Loss: -0.009588319808244705\nTRAIN: \t Epoch: 54 \t Loss: -0.009643250662419531\nTRAIN: \t Epoch: 54 \t Loss: -0.00972882341593504\nTRAIN: \t Epoch: 54 \t Loss: -0.009751388549127361\nTRAIN: \t Epoch: 54 \t Loss: -0.009620516172920665\nTRAIN: \t Epoch: 54 \t Loss: -0.009603702821410619\nTRAIN: \t Epoch: 54 \t Loss: -0.009711915627121925\nTRAIN: \t Epoch: 54 \t Loss: -0.009715023450553417\nTRAIN: \t Epoch: 54 \t Loss: -0.00961536809336394\nTRAIN: \t Epoch: 54 \t Loss: -0.009563088197918498\nTRAIN: \t Epoch: 54 \t Loss: -0.009616385420991315\nTRAIN: \t Epoch: 54 \t Loss: -0.009626686082858788\nTRAIN: \t Epoch: 54 \t Loss: -0.009649770287796854\nTRAIN: \t Epoch: 54 \t Loss: -0.009728186320336092\nTRAIN: \t Epoch: 54 \t Loss: -0.009750600349111231\nVALD: \t Epoch: 54 \t Loss: -0.005181153770536184\nVALD: \t Epoch: 54 \t Loss: -0.004264022572897375\nVALD: \t Epoch: 54 \t Loss: -0.004909701722984512\nVALD: \t Epoch: 54 \t Loss: 0.0037155411555431783\nVALD: \t Epoch: 54 \t Loss: 0.001951657747849822\nVALD: \t Epoch: 54 \t Loss: 0.001886556822467934\n******************************\nEpoch: social-tag : 54\ntrain_loss -0.009750600349111231\nval_loss 0.001886556822467934\n{'min_val_epoch': 53, 'min_val_loss': -0.004279432784427296}\n******************************\nTRAIN: \t Epoch: 55 \t Loss: -0.011001989245414734\nTRAIN: \t Epoch: 55 \t Loss: -0.01093620015308261\nTRAIN: \t Epoch: 55 \t Loss: -0.010804637335240841\nTRAIN: \t Epoch: 55 \t Loss: -0.010798490839079022\nTRAIN: \t Epoch: 55 \t Loss: -0.010765389166772365\nTRAIN: \t Epoch: 55 \t Loss: -0.01057561207562685\nTRAIN: \t Epoch: 55 \t Loss: -0.010553962817149503\nTRAIN: \t Epoch: 55 \t Loss: -0.010237047798000276\nTRAIN: \t Epoch: 55 \t Loss: -0.009963345972614156\nTRAIN: \t Epoch: 55 \t Loss: -0.009914855612441898\nTRAIN: \t Epoch: 55 \t Loss: -0.009936491408470001\nTRAIN: \t Epoch: 55 \t Loss: -0.00989046743294845\nTRAIN: \t Epoch: 55 \t Loss: -0.009812412962604027\nTRAIN: \t Epoch: 55 \t Loss: -0.00973989165920232\nTRAIN: \t Epoch: 55 \t Loss: -0.009779407984266679\nTRAIN: \t Epoch: 55 \t Loss: -0.0098421094880905\nTRAIN: \t Epoch: 55 \t Loss: -0.009867056848152596\nTRAIN: \t Epoch: 55 \t Loss: -0.009810336477433642\nTRAIN: \t Epoch: 55 \t Loss: -0.009705973813604368\nTRAIN: \t Epoch: 55 \t Loss: -0.00970171990338713\nTRAIN: \t Epoch: 55 \t Loss: -0.009741258164424272\nTRAIN: \t Epoch: 55 \t Loss: -0.00977233280185309\nVALD: \t Epoch: 55 \t Loss: -0.007829437032341957\nVALD: \t Epoch: 55 \t Loss: -0.007233708631247282\nVALD: \t Epoch: 55 \t Loss: -0.006915128789842129\nVALD: \t Epoch: 55 \t Loss: -0.004871015349635854\nVALD: \t Epoch: 55 \t Loss: -0.005300379754044115\nVALD: \t Epoch: 55 \t Loss: -0.005239669594800833\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 70/70 [00:02<00:00, 26.00it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.7455614495412283  FDE: 1.187009947614389\n**************************************************\n******************************\nEpoch: social-tag : 55\ntrain_loss -0.00977233280185309\nval_loss -0.005239669594800833\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 56 \t Loss: -0.01053666789084673\nTRAIN: \t Epoch: 56 \t Loss: -0.010446148924529552\nTRAIN: \t Epoch: 56 \t Loss: -0.01019344199448824\nTRAIN: \t Epoch: 56 \t Loss: -0.009850868256762624\nTRAIN: \t Epoch: 56 \t Loss: -0.009588285721838475\nTRAIN: \t Epoch: 56 \t Loss: -0.009520831673095623\nTRAIN: \t Epoch: 56 \t Loss: -0.009727978280612401\nTRAIN: \t Epoch: 56 \t Loss: -0.00977617793250829\nTRAIN: \t Epoch: 56 \t Loss: -0.00979364456401931\nTRAIN: \t Epoch: 56 \t Loss: -0.009833499230444432\nTRAIN: \t Epoch: 56 \t Loss: -0.009902984581210396\nTRAIN: \t Epoch: 56 \t Loss: -0.009984503965824842\nTRAIN: \t Epoch: 56 \t Loss: -0.010030372211566338\nTRAIN: \t Epoch: 56 \t Loss: -0.009906618589801448\nTRAIN: \t Epoch: 56 \t Loss: -0.00987243012835582\nTRAIN: \t Epoch: 56 \t Loss: -0.009907072351779789\nTRAIN: \t Epoch: 56 \t Loss: -0.010013667483101873\nTRAIN: \t Epoch: 56 \t Loss: -0.010042114286786981\nTRAIN: \t Epoch: 56 \t Loss: -0.009918189313458768\nTRAIN: \t Epoch: 56 \t Loss: -0.00971817031968385\nTRAIN: \t Epoch: 56 \t Loss: -0.009642695808517081\nTRAIN: \t Epoch: 56 \t Loss: -0.009684759046701812\nVALD: \t Epoch: 56 \t Loss: -0.0034033332485705614\nVALD: \t Epoch: 56 \t Loss: -0.0035906422417610884\nVALD: \t Epoch: 56 \t Loss: -0.003584821398059527\nVALD: \t Epoch: 56 \t Loss: 0.0034372429363429546\nVALD: \t Epoch: 56 \t Loss: 0.0017191048711538315\nVALD: \t Epoch: 56 \t Loss: 0.0016230887620512283\n******************************\nEpoch: social-tag : 56\ntrain_loss -0.009684759046701812\nval_loss 0.0016230887620512283\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 57 \t Loss: -0.011131897568702698\nTRAIN: \t Epoch: 57 \t Loss: -0.010858844965696335\nTRAIN: \t Epoch: 57 \t Loss: -0.010330968225995699\nTRAIN: \t Epoch: 57 \t Loss: -0.009513230645097792\nTRAIN: \t Epoch: 57 \t Loss: -0.009343162085860968\nTRAIN: \t Epoch: 57 \t Loss: -0.009613070404157043\nTRAIN: \t Epoch: 57 \t Loss: -0.009889982042035885\nTRAIN: \t Epoch: 57 \t Loss: -0.010061717417556792\nTRAIN: \t Epoch: 57 \t Loss: -0.010214407359146409\nTRAIN: \t Epoch: 57 \t Loss: -0.00996790211647749\nTRAIN: \t Epoch: 57 \t Loss: -0.009692464955151081\nTRAIN: \t Epoch: 57 \t Loss: -0.009664437811200818\nTRAIN: \t Epoch: 57 \t Loss: -0.009717149230150076\nTRAIN: \t Epoch: 57 \t Loss: -0.00972187053412199\nTRAIN: \t Epoch: 57 \t Loss: -0.009817369033892949\nTRAIN: \t Epoch: 57 \t Loss: -0.009885035688057542\nTRAIN: \t Epoch: 57 \t Loss: -0.00996871035107795\nTRAIN: \t Epoch: 57 \t Loss: -0.009914237384994825\nTRAIN: \t Epoch: 57 \t Loss: -0.009744246269723303\nTRAIN: \t Epoch: 57 \t Loss: -0.009697064128704368\nTRAIN: \t Epoch: 57 \t Loss: -0.009725979019311212\nTRAIN: \t Epoch: 57 \t Loss: -0.009766238959944227\nVALD: \t Epoch: 57 \t Loss: 0.03011210635304451\nVALD: \t Epoch: 57 \t Loss: 0.030277696438133717\nVALD: \t Epoch: 57 \t Loss: 0.023557283605138462\nVALD: \t Epoch: 57 \t Loss: 0.04334708163514733\nVALD: \t Epoch: 57 \t Loss: 0.03685828596353531\nVALD: \t Epoch: 57 \t Loss: 0.035736370949201623\n******************************\nEpoch: social-tag : 57\ntrain_loss -0.009766238959944227\nval_loss 0.035736370949201623\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 58 \t Loss: -0.011723531410098076\nTRAIN: \t Epoch: 58 \t Loss: -0.01026842463761568\nTRAIN: \t Epoch: 58 \t Loss: -0.009405116240183512\nTRAIN: \t Epoch: 58 \t Loss: -0.009417684050276875\nTRAIN: \t Epoch: 58 \t Loss: -0.0095990639179945\nTRAIN: \t Epoch: 58 \t Loss: -0.009680279220143953\nTRAIN: \t Epoch: 58 \t Loss: -0.009611541937504495\nTRAIN: \t Epoch: 58 \t Loss: -0.009544158005155623\nTRAIN: \t Epoch: 58 \t Loss: -0.009519409905705187\nTRAIN: \t Epoch: 58 \t Loss: -0.009549867361783981\nTRAIN: \t Epoch: 58 \t Loss: -0.009696822867474773\nTRAIN: \t Epoch: 58 \t Loss: -0.009807967658465108\nTRAIN: \t Epoch: 58 \t Loss: -0.009621945711282583\nTRAIN: \t Epoch: 58 \t Loss: -0.009374261002189346\nTRAIN: \t Epoch: 58 \t Loss: -0.00935935511564215\nTRAIN: \t Epoch: 58 \t Loss: -0.009419246081961319\nTRAIN: \t Epoch: 58 \t Loss: -0.009552622679620981\nTRAIN: \t Epoch: 58 \t Loss: -0.009576895558792684\nTRAIN: \t Epoch: 58 \t Loss: -0.009629496887914445\nTRAIN: \t Epoch: 58 \t Loss: -0.009620451205410063\nTRAIN: \t Epoch: 58 \t Loss: -0.009496331414473909\nTRAIN: \t Epoch: 58 \t Loss: -0.009429034103825019\nVALD: \t Epoch: 58 \t Loss: -0.0018203890649601817\nVALD: \t Epoch: 58 \t Loss: -0.002501367882359773\nVALD: \t Epoch: 58 \t Loss: -0.0036026216500128307\nVALD: \t Epoch: 58 \t Loss: 0.003161453962093219\nVALD: \t Epoch: 58 \t Loss: 0.0016534754773601889\nVALD: \t Epoch: 58 \t Loss: 0.0016293642506229155\n******************************\nEpoch: social-tag : 58\ntrain_loss -0.009429034103825019\nval_loss 0.0016293642506229155\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 59 \t Loss: -0.010639432817697525\nTRAIN: \t Epoch: 59 \t Loss: -0.010600600391626358\nTRAIN: \t Epoch: 59 \t Loss: -0.01080673219015201\nTRAIN: \t Epoch: 59 \t Loss: -0.010942614637315273\nTRAIN: \t Epoch: 59 \t Loss: -0.010904816538095474\nTRAIN: \t Epoch: 59 \t Loss: -0.01051269487167398\nTRAIN: \t Epoch: 59 \t Loss: -0.010021311297480549\nTRAIN: \t Epoch: 59 \t Loss: -0.009973308246117085\nTRAIN: \t Epoch: 59 \t Loss: -0.010092429351061583\nTRAIN: \t Epoch: 59 \t Loss: -0.010185985313728451\nTRAIN: \t Epoch: 59 \t Loss: -0.010308580854060974\nTRAIN: \t Epoch: 59 \t Loss: -0.010395098594017327\nTRAIN: \t Epoch: 59 \t Loss: -0.010452642571181059\nTRAIN: \t Epoch: 59 \t Loss: -0.010363847449687975\nTRAIN: \t Epoch: 59 \t Loss: -0.010220809063563744\nTRAIN: \t Epoch: 59 \t Loss: -0.010138228681171313\nTRAIN: \t Epoch: 59 \t Loss: -0.010178533136187232\nTRAIN: \t Epoch: 59 \t Loss: -0.010194033358453048\nTRAIN: \t Epoch: 59 \t Loss: -0.010218191475264336\nTRAIN: \t Epoch: 59 \t Loss: -0.01024561037775129\nTRAIN: \t Epoch: 59 \t Loss: -0.010205667044612624\nTRAIN: \t Epoch: 59 \t Loss: -0.010105845979557004\nVALD: \t Epoch: 59 \t Loss: -0.005137464497238398\nVALD: \t Epoch: 59 \t Loss: -0.004985525505617261\nVALD: \t Epoch: 59 \t Loss: -0.004643690151472886\nVALD: \t Epoch: 59 \t Loss: -0.0011483824346214533\nVALD: \t Epoch: 59 \t Loss: -0.002075441647320986\nVALD: \t Epoch: 59 \t Loss: -0.0021029586536866246\n******************************\nEpoch: social-tag : 59\ntrain_loss -0.010105845979557004\nval_loss -0.0021029586536866246\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 60 \t Loss: -0.009404029697179794\nTRAIN: \t Epoch: 60 \t Loss: -0.010167612694203854\nTRAIN: \t Epoch: 60 \t Loss: -0.010127791513999304\nTRAIN: \t Epoch: 60 \t Loss: -0.009756589541211724\nTRAIN: \t Epoch: 60 \t Loss: -0.00951393749564886\nTRAIN: \t Epoch: 60 \t Loss: -0.009528943647940954\nTRAIN: \t Epoch: 60 \t Loss: -0.009652171949190753\nTRAIN: \t Epoch: 60 \t Loss: -0.009943013545125723\nTRAIN: \t Epoch: 60 \t Loss: -0.01004930833975474\nTRAIN: \t Epoch: 60 \t Loss: -0.00975909004919231\nTRAIN: \t Epoch: 60 \t Loss: -0.00957920550453392\nTRAIN: \t Epoch: 60 \t Loss: -0.00965635443571955\nTRAIN: \t Epoch: 60 \t Loss: -0.009736744006379293\nTRAIN: \t Epoch: 60 \t Loss: -0.009815925965085626\nTRAIN: \t Epoch: 60 \t Loss: -0.00992429545149207\nTRAIN: \t Epoch: 60 \t Loss: -0.009985155338654295\nTRAIN: \t Epoch: 60 \t Loss: -0.00994579672046444\nTRAIN: \t Epoch: 60 \t Loss: -0.009845130947521992\nTRAIN: \t Epoch: 60 \t Loss: -0.0098410580846432\nTRAIN: \t Epoch: 60 \t Loss: -0.00989146560896188\nTRAIN: \t Epoch: 60 \t Loss: -0.009986891876906157\nTRAIN: \t Epoch: 60 \t Loss: -0.009981256070642111\nVALD: \t Epoch: 60 \t Loss: 0.0069716512225568295\nVALD: \t Epoch: 60 \t Loss: 0.008826737059280276\nVALD: \t Epoch: 60 \t Loss: 0.006775141538431247\nVALD: \t Epoch: 60 \t Loss: 0.015157374204136431\nVALD: \t Epoch: 60 \t Loss: 0.011949096550233662\nVALD: \t Epoch: 60 \t Loss: 0.01173533062365922\n******************************\nEpoch: social-tag : 60\ntrain_loss -0.009981256070642111\nval_loss 0.01173533062365922\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 61 \t Loss: -0.009973070584237576\nTRAIN: \t Epoch: 61 \t Loss: -0.009587897453457117\nTRAIN: \t Epoch: 61 \t Loss: -0.009633533967037996\nTRAIN: \t Epoch: 61 \t Loss: -0.00957984896376729\nTRAIN: \t Epoch: 61 \t Loss: -0.009736978635191917\nTRAIN: \t Epoch: 61 \t Loss: -0.009904985781759024\nTRAIN: \t Epoch: 61 \t Loss: -0.009908022092921394\nTRAIN: \t Epoch: 61 \t Loss: -0.009564826148562133\nTRAIN: \t Epoch: 61 \t Loss: -0.009524497720930312\nTRAIN: \t Epoch: 61 \t Loss: -0.009688013140112162\nTRAIN: \t Epoch: 61 \t Loss: -0.00977063560011712\nTRAIN: \t Epoch: 61 \t Loss: -0.0099253139924258\nTRAIN: \t Epoch: 61 \t Loss: -0.010006029158830643\nTRAIN: \t Epoch: 61 \t Loss: -0.010047753141926867\nTRAIN: \t Epoch: 61 \t Loss: -0.010055170208215714\nTRAIN: \t Epoch: 61 \t Loss: -0.009796187048777938\nTRAIN: \t Epoch: 61 \t Loss: -0.009595298180904458\nTRAIN: \t Epoch: 61 \t Loss: -0.009585159804878963\nTRAIN: \t Epoch: 61 \t Loss: -0.009672237731712429\nTRAIN: \t Epoch: 61 \t Loss: -0.009751915861852468\nTRAIN: \t Epoch: 61 \t Loss: -0.009817041700617188\nTRAIN: \t Epoch: 61 \t Loss: -0.009845555256684336\nVALD: \t Epoch: 61 \t Loss: -0.0032288730144500732\nVALD: \t Epoch: 61 \t Loss: -0.0022163427202031016\nVALD: \t Epoch: 61 \t Loss: -0.0024919205655654273\nVALD: \t Epoch: 61 \t Loss: 0.0023528174497187138\nVALD: \t Epoch: 61 \t Loss: 0.000998502504080534\nVALD: \t Epoch: 61 \t Loss: 0.0009145316244526342\n******************************\nEpoch: social-tag : 61\ntrain_loss -0.009845555256684336\nval_loss 0.0009145316244526342\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 62 \t Loss: -0.010924193076789379\nTRAIN: \t Epoch: 62 \t Loss: -0.00978696160018444\nTRAIN: \t Epoch: 62 \t Loss: -0.009710034976402918\nTRAIN: \t Epoch: 62 \t Loss: -0.00996998860500753\nTRAIN: \t Epoch: 62 \t Loss: -0.010211452469229698\nTRAIN: \t Epoch: 62 \t Loss: -0.0100689932393531\nTRAIN: \t Epoch: 62 \t Loss: -0.009760760968284947\nTRAIN: \t Epoch: 62 \t Loss: -0.009804931352846324\nTRAIN: \t Epoch: 62 \t Loss: -0.010008599195215438\nTRAIN: \t Epoch: 62 \t Loss: -0.010064121056348085\nTRAIN: \t Epoch: 62 \t Loss: -0.009961317039348862\nTRAIN: \t Epoch: 62 \t Loss: -0.00989192957058549\nTRAIN: \t Epoch: 62 \t Loss: -0.00994368613912509\nTRAIN: \t Epoch: 62 \t Loss: -0.009973751780177866\nTRAIN: \t Epoch: 62 \t Loss: -0.009955236874520778\nTRAIN: \t Epoch: 62 \t Loss: -0.009886512707453221\nTRAIN: \t Epoch: 62 \t Loss: -0.009823122773976886\nTRAIN: \t Epoch: 62 \t Loss: -0.009849730206446515\nTRAIN: \t Epoch: 62 \t Loss: -0.009910635993276773\nTRAIN: \t Epoch: 62 \t Loss: -0.009967719530686736\nTRAIN: \t Epoch: 62 \t Loss: -0.009990759282594635\nTRAIN: \t Epoch: 62 \t Loss: -0.009947138949827404\nVALD: \t Epoch: 62 \t Loss: -0.006269109435379505\nVALD: \t Epoch: 62 \t Loss: -0.005690608639270067\nVALD: \t Epoch: 62 \t Loss: -0.005298629092673461\nVALD: \t Epoch: 62 \t Loss: -0.004442556179128587\nVALD: \t Epoch: 62 \t Loss: -0.004709423333406448\nVALD: \t Epoch: 62 \t Loss: -0.004649085353947047\n******************************\nEpoch: social-tag : 62\ntrain_loss -0.009947138949827404\nval_loss -0.004649085353947047\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 63 \t Loss: -0.009298330172896385\nTRAIN: \t Epoch: 63 \t Loss: -0.010471245273947716\nTRAIN: \t Epoch: 63 \t Loss: -0.010728674630324045\nTRAIN: \t Epoch: 63 \t Loss: -0.010628072312101722\nTRAIN: \t Epoch: 63 \t Loss: -0.010505651496350766\nTRAIN: \t Epoch: 63 \t Loss: -0.010162037642051777\nTRAIN: \t Epoch: 63 \t Loss: -0.010265031297292029\nTRAIN: \t Epoch: 63 \t Loss: -0.010318761807866395\nTRAIN: \t Epoch: 63 \t Loss: -0.010408377585311731\nTRAIN: \t Epoch: 63 \t Loss: -0.010443458240479231\nTRAIN: \t Epoch: 63 \t Loss: -0.010480488057840954\nTRAIN: \t Epoch: 63 \t Loss: -0.010491609650974473\nTRAIN: \t Epoch: 63 \t Loss: -0.010489375760348944\nTRAIN: \t Epoch: 63 \t Loss: -0.010593946823584182\nTRAIN: \t Epoch: 63 \t Loss: -0.010541784018278122\nTRAIN: \t Epoch: 63 \t Loss: -0.010388293885625899\nTRAIN: \t Epoch: 63 \t Loss: -0.010340055834282847\nTRAIN: \t Epoch: 63 \t Loss: -0.01034044748586085\nTRAIN: \t Epoch: 63 \t Loss: -0.010396292776261506\nTRAIN: \t Epoch: 63 \t Loss: -0.010427632927894592\nTRAIN: \t Epoch: 63 \t Loss: -0.010433146242229711\nTRAIN: \t Epoch: 63 \t Loss: -0.010374166978325836\nVALD: \t Epoch: 63 \t Loss: -0.0052363211289048195\nVALD: \t Epoch: 63 \t Loss: -0.004239468835294247\nVALD: \t Epoch: 63 \t Loss: -0.004933222041775783\nVALD: \t Epoch: 63 \t Loss: 0.000276693026535213\nVALD: \t Epoch: 63 \t Loss: -0.0009716900065541267\nVALD: \t Epoch: 63 \t Loss: -0.0010298767606868888\n******************************\nEpoch: social-tag : 63\ntrain_loss -0.010374166978325836\nval_loss -0.0010298767606868888\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 64 \t Loss: -0.011232676915824413\nTRAIN: \t Epoch: 64 \t Loss: -0.011100610252469778\nTRAIN: \t Epoch: 64 \t Loss: -0.011020967115958532\nTRAIN: \t Epoch: 64 \t Loss: -0.010703387670218945\nTRAIN: \t Epoch: 64 \t Loss: -0.010509556718170643\nTRAIN: \t Epoch: 64 \t Loss: -0.010268156416714191\nTRAIN: \t Epoch: 64 \t Loss: -0.010254418876554285\nTRAIN: \t Epoch: 64 \t Loss: -0.010335784289054573\nTRAIN: \t Epoch: 64 \t Loss: -0.010419082620905505\nTRAIN: \t Epoch: 64 \t Loss: -0.010309967398643493\nTRAIN: \t Epoch: 64 \t Loss: -0.010118542323735628\nTRAIN: \t Epoch: 64 \t Loss: -0.010043171544869741\nTRAIN: \t Epoch: 64 \t Loss: -0.010146529342119511\nTRAIN: \t Epoch: 64 \t Loss: -0.010126360026853425\nTRAIN: \t Epoch: 64 \t Loss: -0.010083281435072422\nTRAIN: \t Epoch: 64 \t Loss: -0.010128245572559536\nTRAIN: \t Epoch: 64 \t Loss: -0.010129771140568396\nTRAIN: \t Epoch: 64 \t Loss: -0.010096128409107527\nTRAIN: \t Epoch: 64 \t Loss: -0.01017446668916627\nTRAIN: \t Epoch: 64 \t Loss: -0.01022348771803081\nTRAIN: \t Epoch: 64 \t Loss: -0.010201774803655488\nTRAIN: \t Epoch: 64 \t Loss: -0.01020042977598454\nVALD: \t Epoch: 64 \t Loss: 0.014144821092486382\nVALD: \t Epoch: 64 \t Loss: 0.012993273790925741\nVALD: \t Epoch: 64 \t Loss: 0.010431026574224234\nVALD: \t Epoch: 64 \t Loss: 0.020140108768828213\nVALD: \t Epoch: 64 \t Loss: 0.016363475657999515\nVALD: \t Epoch: 64 \t Loss: 0.015839088160657523\n******************************\nEpoch: social-tag : 64\ntrain_loss -0.01020042977598454\nval_loss 0.015839088160657523\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 65 \t Loss: -0.01206307951360941\nTRAIN: \t Epoch: 65 \t Loss: -0.011548725422471762\nTRAIN: \t Epoch: 65 \t Loss: -0.011054956664641699\nTRAIN: \t Epoch: 65 \t Loss: -0.010219023679383099\nTRAIN: \t Epoch: 65 \t Loss: -0.009876098949462175\nTRAIN: \t Epoch: 65 \t Loss: -0.010063749349986514\nTRAIN: \t Epoch: 65 \t Loss: -0.01035751129633614\nTRAIN: \t Epoch: 65 \t Loss: -0.010483940306585282\nTRAIN: \t Epoch: 65 \t Loss: -0.010379090956929658\nTRAIN: \t Epoch: 65 \t Loss: -0.010035904450342059\nTRAIN: \t Epoch: 65 \t Loss: -0.009857105641541157\nTRAIN: \t Epoch: 65 \t Loss: -0.009931733016856015\nTRAIN: \t Epoch: 65 \t Loss: -0.010036939647621833\nTRAIN: \t Epoch: 65 \t Loss: -0.010142812738195062\nTRAIN: \t Epoch: 65 \t Loss: -0.010154018271714449\nTRAIN: \t Epoch: 65 \t Loss: -0.009962104610167444\nTRAIN: \t Epoch: 65 \t Loss: -0.009824136874693282\nTRAIN: \t Epoch: 65 \t Loss: -0.009806748738305436\nTRAIN: \t Epoch: 65 \t Loss: -0.00990899578717194\nTRAIN: \t Epoch: 65 \t Loss: -0.00997900334186852\nTRAIN: \t Epoch: 65 \t Loss: -0.009960673616400786\nTRAIN: \t Epoch: 65 \t Loss: -0.009901876851943065\nVALD: \t Epoch: 65 \t Loss: -0.0010565376142039895\nVALD: \t Epoch: 65 \t Loss: -0.0013656889786943793\nVALD: \t Epoch: 65 \t Loss: -0.0029047764061639705\nVALD: \t Epoch: 65 \t Loss: 0.0014948812895454466\nVALD: \t Epoch: 65 \t Loss: -0.00014402619563043119\nVALD: \t Epoch: 65 \t Loss: -0.00024650143177220316\n******************************\nEpoch: social-tag : 65\ntrain_loss -0.009901876851943065\nval_loss -0.00024650143177220316\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 66 \t Loss: -0.01085385400801897\nTRAIN: \t Epoch: 66 \t Loss: -0.011625563260167837\nTRAIN: \t Epoch: 66 \t Loss: -0.011513674321273962\nTRAIN: \t Epoch: 66 \t Loss: -0.01122806197963655\nTRAIN: \t Epoch: 66 \t Loss: -0.01102877501398325\nTRAIN: \t Epoch: 66 \t Loss: -0.010970266070216894\nTRAIN: \t Epoch: 66 \t Loss: -0.010906137659081392\nTRAIN: \t Epoch: 66 \t Loss: -0.010718241799622774\nTRAIN: \t Epoch: 66 \t Loss: -0.01047011868407329\nTRAIN: \t Epoch: 66 \t Loss: -0.010390168800950051\nTRAIN: \t Epoch: 66 \t Loss: -0.01056733913719654\nTRAIN: \t Epoch: 66 \t Loss: -0.0105886934325099\nTRAIN: \t Epoch: 66 \t Loss: -0.010619206783863215\nTRAIN: \t Epoch: 66 \t Loss: -0.01054554000230772\nTRAIN: \t Epoch: 66 \t Loss: -0.01035157662505905\nTRAIN: \t Epoch: 66 \t Loss: -0.01032698733615689\nTRAIN: \t Epoch: 66 \t Loss: -0.010397561305366895\nTRAIN: \t Epoch: 66 \t Loss: -0.01040916028432548\nTRAIN: \t Epoch: 66 \t Loss: -0.01044278359040618\nTRAIN: \t Epoch: 66 \t Loss: -0.010487127327360212\nTRAIN: \t Epoch: 66 \t Loss: -0.010384230941001858\nTRAIN: \t Epoch: 66 \t Loss: -0.01024254106747612\nVALD: \t Epoch: 66 \t Loss: -0.005892155226320028\nVALD: \t Epoch: 66 \t Loss: -0.006336661521345377\nVALD: \t Epoch: 66 \t Loss: -0.006758206834395726\nVALD: \t Epoch: 66 \t Loss: -0.0016572796739637852\nVALD: \t Epoch: 66 \t Loss: -0.002715087588876486\nVALD: \t Epoch: 66 \t Loss: -0.0027584493950460896\n******************************\nEpoch: social-tag : 66\ntrain_loss -0.01024254106747612\nval_loss -0.0027584493950460896\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 67 \t Loss: -0.009928443469107151\nTRAIN: \t Epoch: 67 \t Loss: -0.010353106539696455\nTRAIN: \t Epoch: 67 \t Loss: -0.010866843784848848\nTRAIN: \t Epoch: 67 \t Loss: -0.011194244725629687\nTRAIN: \t Epoch: 67 \t Loss: -0.011207026802003384\nTRAIN: \t Epoch: 67 \t Loss: -0.011250734950105349\nTRAIN: \t Epoch: 67 \t Loss: -0.011202836542257242\nTRAIN: \t Epoch: 67 \t Loss: -0.011044813552871346\nTRAIN: \t Epoch: 67 \t Loss: -0.01087975139833159\nTRAIN: \t Epoch: 67 \t Loss: -0.010934165678918362\nTRAIN: \t Epoch: 67 \t Loss: -0.010878365740857342\nTRAIN: \t Epoch: 67 \t Loss: -0.010646506988753876\nTRAIN: \t Epoch: 67 \t Loss: -0.010549525801952068\nTRAIN: \t Epoch: 67 \t Loss: -0.010607328997658831\nTRAIN: \t Epoch: 67 \t Loss: -0.01063973680138588\nTRAIN: \t Epoch: 67 \t Loss: -0.010703289415687323\nTRAIN: \t Epoch: 67 \t Loss: -0.010702501982450485\nTRAIN: \t Epoch: 67 \t Loss: -0.010551938011000553\nTRAIN: \t Epoch: 67 \t Loss: -0.010488614243896384\nTRAIN: \t Epoch: 67 \t Loss: -0.010474188672378659\nTRAIN: \t Epoch: 67 \t Loss: -0.010499377069728715\nTRAIN: \t Epoch: 67 \t Loss: -0.010542514636657825\nVALD: \t Epoch: 67 \t Loss: -0.008467436768114567\nVALD: \t Epoch: 67 \t Loss: -0.007235782686620951\nVALD: \t Epoch: 67 \t Loss: -0.00785337636868159\nVALD: \t Epoch: 67 \t Loss: -0.002437192015349865\nVALD: \t Epoch: 67 \t Loss: -0.003291237074881792\nVALD: \t Epoch: 67 \t Loss: -0.003272642669352618\n******************************\nEpoch: social-tag : 67\ntrain_loss -0.010542514636657825\nval_loss -0.003272642669352618\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 68 \t Loss: -0.011731726117432117\nTRAIN: \t Epoch: 68 \t Loss: -0.012340372893959284\nTRAIN: \t Epoch: 68 \t Loss: -0.011588724019626776\nTRAIN: \t Epoch: 68 \t Loss: -0.011026339372619987\nTRAIN: \t Epoch: 68 \t Loss: -0.010692092962563038\nTRAIN: \t Epoch: 68 \t Loss: -0.010894317645579576\nTRAIN: \t Epoch: 68 \t Loss: -0.010976564272173814\nTRAIN: \t Epoch: 68 \t Loss: -0.010755214374512434\nTRAIN: \t Epoch: 68 \t Loss: -0.010547764495842986\nTRAIN: \t Epoch: 68 \t Loss: -0.010511857457458973\nTRAIN: \t Epoch: 68 \t Loss: -0.01057103851979429\nTRAIN: \t Epoch: 68 \t Loss: -0.010585722591107091\nTRAIN: \t Epoch: 68 \t Loss: -0.010645561923201267\nTRAIN: \t Epoch: 68 \t Loss: -0.010715651192835398\nTRAIN: \t Epoch: 68 \t Loss: -0.010635196852187315\nTRAIN: \t Epoch: 68 \t Loss: -0.010409501293906942\nTRAIN: \t Epoch: 68 \t Loss: -0.010342465686228345\nTRAIN: \t Epoch: 68 \t Loss: -0.01035259402770963\nTRAIN: \t Epoch: 68 \t Loss: -0.010423598688487945\nTRAIN: \t Epoch: 68 \t Loss: -0.010506906011141837\nTRAIN: \t Epoch: 68 \t Loss: -0.010513759417725461\nTRAIN: \t Epoch: 68 \t Loss: -0.010541879103470533\nVALD: \t Epoch: 68 \t Loss: 0.009315907023847103\nVALD: \t Epoch: 68 \t Loss: 0.007958046393468976\nVALD: \t Epoch: 68 \t Loss: 0.004569700298209985\nVALD: \t Epoch: 68 \t Loss: 0.011643878882750869\nVALD: \t Epoch: 68 \t Loss: 0.008836631523445248\nVALD: \t Epoch: 68 \t Loss: 0.008738339285958897\n******************************\nEpoch: social-tag : 68\ntrain_loss -0.010541879103470533\nval_loss 0.008738339285958897\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 69 \t Loss: -0.012296881526708603\nTRAIN: \t Epoch: 69 \t Loss: -0.010796185582876205\nTRAIN: \t Epoch: 69 \t Loss: -0.009712578573574623\nTRAIN: \t Epoch: 69 \t Loss: -0.009806786780245602\nTRAIN: \t Epoch: 69 \t Loss: -0.010105038154870271\nTRAIN: \t Epoch: 69 \t Loss: -0.010310242340589562\nTRAIN: \t Epoch: 69 \t Loss: -0.01034376909956336\nTRAIN: \t Epoch: 69 \t Loss: -0.01014705520356074\nTRAIN: \t Epoch: 69 \t Loss: -0.0101952297716505\nTRAIN: \t Epoch: 69 \t Loss: -0.010283869737759233\nTRAIN: \t Epoch: 69 \t Loss: -0.01043355240571228\nTRAIN: \t Epoch: 69 \t Loss: -0.01054682997831454\nTRAIN: \t Epoch: 69 \t Loss: -0.010358349814151343\nTRAIN: \t Epoch: 69 \t Loss: -0.010223970582176532\nTRAIN: \t Epoch: 69 \t Loss: -0.010265767170737187\nTRAIN: \t Epoch: 69 \t Loss: -0.010332274861866608\nTRAIN: \t Epoch: 69 \t Loss: -0.010328962878488442\nTRAIN: \t Epoch: 69 \t Loss: -0.010246793661887446\nTRAIN: \t Epoch: 69 \t Loss: -0.01013976411501828\nTRAIN: \t Epoch: 69 \t Loss: -0.010113429534249008\nTRAIN: \t Epoch: 69 \t Loss: -0.010185302390406529\nTRAIN: \t Epoch: 69 \t Loss: -0.010217990074808336\nVALD: \t Epoch: 69 \t Loss: 0.0009770325850695372\nVALD: \t Epoch: 69 \t Loss: 0.0008394093019887805\nVALD: \t Epoch: 69 \t Loss: -0.0008773178948710362\nVALD: \t Epoch: 69 \t Loss: 0.003569263790268451\nVALD: \t Epoch: 69 \t Loss: 0.0018538189586251974\nVALD: \t Epoch: 69 \t Loss: 0.0016962074533556446\n******************************\nEpoch: social-tag : 69\ntrain_loss -0.010217990074808336\nval_loss 0.0016962074533556446\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 70 \t Loss: -0.011340579017996788\nTRAIN: \t Epoch: 70 \t Loss: -0.010831546504050493\nTRAIN: \t Epoch: 70 \t Loss: -0.010121998687585195\nTRAIN: \t Epoch: 70 \t Loss: -0.01001955196261406\nTRAIN: \t Epoch: 70 \t Loss: -0.010289379954338073\nTRAIN: \t Epoch: 70 \t Loss: -0.010283714160323143\nTRAIN: \t Epoch: 70 \t Loss: -0.010317879462880748\nTRAIN: \t Epoch: 70 \t Loss: -0.010215631104074419\nTRAIN: \t Epoch: 70 \t Loss: -0.010270727280941274\nTRAIN: \t Epoch: 70 \t Loss: -0.010353745520114898\nTRAIN: \t Epoch: 70 \t Loss: -0.010428364557976072\nTRAIN: \t Epoch: 70 \t Loss: -0.010384603248288235\nTRAIN: \t Epoch: 70 \t Loss: -0.010250807023392273\nTRAIN: \t Epoch: 70 \t Loss: -0.010289645088570458\nTRAIN: \t Epoch: 70 \t Loss: -0.010386051051318646\nTRAIN: \t Epoch: 70 \t Loss: -0.010465909610502422\nTRAIN: \t Epoch: 70 \t Loss: -0.010444863589809221\nTRAIN: \t Epoch: 70 \t Loss: -0.01043818559911516\nTRAIN: \t Epoch: 70 \t Loss: -0.010399783079169299\nTRAIN: \t Epoch: 70 \t Loss: -0.010451373923569918\nTRAIN: \t Epoch: 70 \t Loss: -0.010532351831595102\nTRAIN: \t Epoch: 70 \t Loss: -0.010507675432859246\nVALD: \t Epoch: 70 \t Loss: -0.001627827645279467\nVALD: \t Epoch: 70 \t Loss: -0.0026885721017606556\nVALD: \t Epoch: 70 \t Loss: -0.004602573695592582\nVALD: \t Epoch: 70 \t Loss: 0.00394807624979876\nVALD: \t Epoch: 70 \t Loss: 0.002266387059353292\nVALD: \t Epoch: 70 \t Loss: 0.0021401122155966183\n******************************\nEpoch: social-tag : 70\ntrain_loss -0.010507675432859246\nval_loss 0.0021401122155966183\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 71 \t Loss: -0.010801256634294987\nTRAIN: \t Epoch: 71 \t Loss: -0.010926584247499704\nTRAIN: \t Epoch: 71 \t Loss: -0.01028127440561851\nTRAIN: \t Epoch: 71 \t Loss: -0.01040168129839003\nTRAIN: \t Epoch: 71 \t Loss: -0.010635872185230256\nTRAIN: \t Epoch: 71 \t Loss: -0.010631244163960218\nTRAIN: \t Epoch: 71 \t Loss: -0.010695373373372214\nTRAIN: \t Epoch: 71 \t Loss: -0.010643259272910655\nTRAIN: \t Epoch: 71 \t Loss: -0.010568489010135332\nTRAIN: \t Epoch: 71 \t Loss: -0.010575119033455848\nTRAIN: \t Epoch: 71 \t Loss: -0.01062867875126275\nTRAIN: \t Epoch: 71 \t Loss: -0.010760614881291986\nTRAIN: \t Epoch: 71 \t Loss: -0.010761261631089907\nTRAIN: \t Epoch: 71 \t Loss: -0.010556326846459083\nTRAIN: \t Epoch: 71 \t Loss: -0.010528717252115408\nTRAIN: \t Epoch: 71 \t Loss: -0.010598734836094081\nTRAIN: \t Epoch: 71 \t Loss: -0.010660589946543468\nTRAIN: \t Epoch: 71 \t Loss: -0.010717756425340971\nTRAIN: \t Epoch: 71 \t Loss: -0.010789120716876105\nTRAIN: \t Epoch: 71 \t Loss: -0.0108601663261652\nTRAIN: \t Epoch: 71 \t Loss: -0.010854889727419331\nTRAIN: \t Epoch: 71 \t Loss: -0.010812217157661808\nVALD: \t Epoch: 71 \t Loss: 0.0027180351316928864\nVALD: \t Epoch: 71 \t Loss: 0.0016045226948335767\nVALD: \t Epoch: 71 \t Loss: -0.001084309925014774\nVALD: \t Epoch: 71 \t Loss: 0.007748332165647298\nVALD: \t Epoch: 71 \t Loss: 0.005893505993299186\nVALD: \t Epoch: 71 \t Loss: 0.006171423083905018\n******************************\nEpoch: social-tag : 71\ntrain_loss -0.010812217157661808\nval_loss 0.006171423083905018\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 72 \t Loss: -0.011733787134289742\nTRAIN: \t Epoch: 72 \t Loss: -0.011274931952357292\nTRAIN: \t Epoch: 72 \t Loss: -0.011139820329844952\nTRAIN: \t Epoch: 72 \t Loss: -0.011358348885551095\nTRAIN: \t Epoch: 72 \t Loss: -0.011108070984482766\nTRAIN: \t Epoch: 72 \t Loss: -0.010716137010604143\nTRAIN: \t Epoch: 72 \t Loss: -0.010831612428384168\nTRAIN: \t Epoch: 72 \t Loss: -0.010926254442892969\nTRAIN: \t Epoch: 72 \t Loss: -0.01065474148425791\nTRAIN: \t Epoch: 72 \t Loss: -0.010207340773195028\nTRAIN: \t Epoch: 72 \t Loss: -0.010082006369802084\nTRAIN: \t Epoch: 72 \t Loss: -0.010152640597273907\nTRAIN: \t Epoch: 72 \t Loss: -0.010299009700807242\nTRAIN: \t Epoch: 72 \t Loss: -0.010382178505616528\nTRAIN: \t Epoch: 72 \t Loss: -0.010239201597869396\nTRAIN: \t Epoch: 72 \t Loss: -0.010141341597773135\nTRAIN: \t Epoch: 72 \t Loss: -0.010120115571600549\nTRAIN: \t Epoch: 72 \t Loss: -0.010146466239045063\nTRAIN: \t Epoch: 72 \t Loss: -0.010241905423371415\nTRAIN: \t Epoch: 72 \t Loss: -0.010345583874732256\nTRAIN: \t Epoch: 72 \t Loss: -0.01039197344687723\nTRAIN: \t Epoch: 72 \t Loss: -0.010351816199626272\nVALD: \t Epoch: 72 \t Loss: 0.0005948639591224492\nVALD: \t Epoch: 72 \t Loss: 0.00027304609466227703\nVALD: \t Epoch: 72 \t Loss: -0.0006753447548059436\nVALD: \t Epoch: 72 \t Loss: 0.00532606110755296\nVALD: \t Epoch: 72 \t Loss: 0.0034047860230202787\nVALD: \t Epoch: 72 \t Loss: 0.003233053765231461\n******************************\nEpoch: social-tag : 72\ntrain_loss -0.010351816199626272\nval_loss 0.003233053765231461\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 73 \t Loss: -0.011282766237854958\nTRAIN: \t Epoch: 73 \t Loss: -0.011374603025615215\nTRAIN: \t Epoch: 73 \t Loss: -0.011520935222506523\nTRAIN: \t Epoch: 73 \t Loss: -0.011537156300619245\nTRAIN: \t Epoch: 73 \t Loss: -0.01104227565228939\nTRAIN: \t Epoch: 73 \t Loss: -0.010739041647563377\nTRAIN: \t Epoch: 73 \t Loss: -0.010747951056276048\nTRAIN: \t Epoch: 73 \t Loss: -0.010801075375638902\nTRAIN: \t Epoch: 73 \t Loss: -0.010662082479231887\nTRAIN: \t Epoch: 73 \t Loss: -0.01053445041179657\nTRAIN: \t Epoch: 73 \t Loss: -0.010643243197013031\nTRAIN: \t Epoch: 73 \t Loss: -0.010724316040674845\nTRAIN: \t Epoch: 73 \t Loss: -0.0107269029921064\nTRAIN: \t Epoch: 73 \t Loss: -0.010628175855215107\nTRAIN: \t Epoch: 73 \t Loss: -0.01055141439040502\nTRAIN: \t Epoch: 73 \t Loss: -0.01053184800548479\nTRAIN: \t Epoch: 73 \t Loss: -0.01061785505974994\nTRAIN: \t Epoch: 73 \t Loss: -0.010592569234884448\nTRAIN: \t Epoch: 73 \t Loss: -0.010527178007913264\nTRAIN: \t Epoch: 73 \t Loss: -0.01050041913986206\nTRAIN: \t Epoch: 73 \t Loss: -0.010547498728902567\nTRAIN: \t Epoch: 73 \t Loss: -0.010545918586224157\nVALD: \t Epoch: 73 \t Loss: -0.004758872091770172\nVALD: \t Epoch: 73 \t Loss: -0.004941244143992662\nVALD: \t Epoch: 73 \t Loss: -0.005654760325948398\nVALD: \t Epoch: 73 \t Loss: -0.00257315079215914\nVALD: \t Epoch: 73 \t Loss: -0.0033347472548484803\nVALD: \t Epoch: 73 \t Loss: -0.0033499180480386272\n******************************\nEpoch: social-tag : 73\ntrain_loss -0.010545918586224157\nval_loss -0.0033499180480386272\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 74 \t Loss: -0.010861087590456009\nTRAIN: \t Epoch: 74 \t Loss: -0.011022802907973528\nTRAIN: \t Epoch: 74 \t Loss: -0.011136946578820547\nTRAIN: \t Epoch: 74 \t Loss: -0.010903912130743265\nTRAIN: \t Epoch: 74 \t Loss: -0.010628323070704937\nTRAIN: \t Epoch: 74 \t Loss: -0.010744916430364052\nTRAIN: \t Epoch: 74 \t Loss: -0.010878725908696651\nTRAIN: \t Epoch: 74 \t Loss: -0.011008695233613253\nTRAIN: \t Epoch: 74 \t Loss: -0.010839024558663368\nTRAIN: \t Epoch: 74 \t Loss: -0.010605665761977435\nTRAIN: \t Epoch: 74 \t Loss: -0.01065064509483901\nTRAIN: \t Epoch: 74 \t Loss: -0.010777332354336977\nTRAIN: \t Epoch: 74 \t Loss: -0.010825431762406459\nTRAIN: \t Epoch: 74 \t Loss: -0.010894442043666328\nTRAIN: \t Epoch: 74 \t Loss: -0.010927990886072318\nTRAIN: \t Epoch: 74 \t Loss: -0.010880048212129623\nTRAIN: \t Epoch: 74 \t Loss: -0.010738773435792503\nTRAIN: \t Epoch: 74 \t Loss: -0.010676448233425617\nTRAIN: \t Epoch: 74 \t Loss: -0.0107061297289635\nTRAIN: \t Epoch: 74 \t Loss: -0.010758863808587194\nTRAIN: \t Epoch: 74 \t Loss: -0.010762025469115801\nTRAIN: \t Epoch: 74 \t Loss: -0.010739689755054523\nVALD: \t Epoch: 74 \t Loss: 0.002619061153382063\nVALD: \t Epoch: 74 \t Loss: 0.0009402962168678641\nVALD: \t Epoch: 74 \t Loss: -0.0011713757024457057\nVALD: \t Epoch: 74 \t Loss: 0.0014603726449422538\nVALD: \t Epoch: 74 \t Loss: 0.00026403539814054964\nVALD: \t Epoch: 74 \t Loss: 0.00014228763228113002\n******************************\nEpoch: social-tag : 74\ntrain_loss -0.010739689755054523\nval_loss 0.00014228763228113002\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 75 \t Loss: -0.011343990452587605\nTRAIN: \t Epoch: 75 \t Loss: -0.011711646802723408\nTRAIN: \t Epoch: 75 \t Loss: -0.011055147275328636\nTRAIN: \t Epoch: 75 \t Loss: -0.010151739115826786\nTRAIN: \t Epoch: 75 \t Loss: -0.009971491154283285\nTRAIN: \t Epoch: 75 \t Loss: -0.010339219511176148\nTRAIN: \t Epoch: 75 \t Loss: -0.010340652362044369\nTRAIN: \t Epoch: 75 \t Loss: -0.010485943115781993\nTRAIN: \t Epoch: 75 \t Loss: -0.01073812036257651\nTRAIN: \t Epoch: 75 \t Loss: -0.0108875238802284\nTRAIN: \t Epoch: 75 \t Loss: -0.010891382878815586\nTRAIN: \t Epoch: 75 \t Loss: -0.01082654200339069\nTRAIN: \t Epoch: 75 \t Loss: -0.010742111812130762\nTRAIN: \t Epoch: 75 \t Loss: -0.010660030779295735\nTRAIN: \t Epoch: 75 \t Loss: -0.010687986295670271\nTRAIN: \t Epoch: 75 \t Loss: -0.010714706819271669\nTRAIN: \t Epoch: 75 \t Loss: -0.010730139390729806\nTRAIN: \t Epoch: 75 \t Loss: -0.010712121380493045\nTRAIN: \t Epoch: 75 \t Loss: -0.010652398666072833\nTRAIN: \t Epoch: 75 \t Loss: -0.010690014087595045\nTRAIN: \t Epoch: 75 \t Loss: -0.01074708281971869\nTRAIN: \t Epoch: 75 \t Loss: -0.010790424779244771\nVALD: \t Epoch: 75 \t Loss: -0.00620306096971035\nVALD: \t Epoch: 75 \t Loss: -0.005112814484164119\nVALD: \t Epoch: 75 \t Loss: -0.005669265985488892\nVALD: \t Epoch: 75 \t Loss: -0.001498676836490631\nVALD: \t Epoch: 75 \t Loss: -0.0023383799009025095\nVALD: \t Epoch: 75 \t Loss: -0.002282389532774687\n******************************\nEpoch: social-tag : 75\ntrain_loss -0.010790424779244771\nval_loss -0.002282389532774687\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 76 \t Loss: -0.011446150951087475\nTRAIN: \t Epoch: 76 \t Loss: -0.011594836600124836\nTRAIN: \t Epoch: 76 \t Loss: -0.011085831870635351\nTRAIN: \t Epoch: 76 \t Loss: -0.01030057854950428\nTRAIN: \t Epoch: 76 \t Loss: -0.010279335640370845\nTRAIN: \t Epoch: 76 \t Loss: -0.010460656757156054\nTRAIN: \t Epoch: 76 \t Loss: -0.010675015726259776\nTRAIN: \t Epoch: 76 \t Loss: -0.010849435231648386\nTRAIN: \t Epoch: 76 \t Loss: -0.01092231853140725\nTRAIN: \t Epoch: 76 \t Loss: -0.010957314912229777\nTRAIN: \t Epoch: 76 \t Loss: -0.010801800018684431\nTRAIN: \t Epoch: 76 \t Loss: -0.010779114672914147\nTRAIN: \t Epoch: 76 \t Loss: -0.010921640464892754\nTRAIN: \t Epoch: 76 \t Loss: -0.010907212338809456\nTRAIN: \t Epoch: 76 \t Loss: -0.010943968718250593\nTRAIN: \t Epoch: 76 \t Loss: -0.010944781708531082\nTRAIN: \t Epoch: 76 \t Loss: -0.010887951042283983\nTRAIN: \t Epoch: 76 \t Loss: -0.010862927935603593\nTRAIN: \t Epoch: 76 \t Loss: -0.010863974935522205\nTRAIN: \t Epoch: 76 \t Loss: -0.010861295834183693\nTRAIN: \t Epoch: 76 \t Loss: -0.010828739830425807\nTRAIN: \t Epoch: 76 \t Loss: -0.01079287850022102\nVALD: \t Epoch: 76 \t Loss: -0.003127611940726638\nVALD: \t Epoch: 76 \t Loss: -0.002931389259174466\nVALD: \t Epoch: 76 \t Loss: -0.004222842399030924\nVALD: \t Epoch: 76 \t Loss: 0.0006945283384993672\nVALD: \t Epoch: 76 \t Loss: -0.0008881584741175175\nVALD: \t Epoch: 76 \t Loss: -0.0009548787874254313\n******************************\nEpoch: social-tag : 76\ntrain_loss -0.01079287850022102\nval_loss -0.0009548787874254313\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 77 \t Loss: -0.011954602785408497\nTRAIN: \t Epoch: 77 \t Loss: -0.0121222585439682\nTRAIN: \t Epoch: 77 \t Loss: -0.011522141906122366\nTRAIN: \t Epoch: 77 \t Loss: -0.010691575473174453\nTRAIN: \t Epoch: 77 \t Loss: -0.010461471043527126\nTRAIN: \t Epoch: 77 \t Loss: -0.010728283785283566\nTRAIN: \t Epoch: 77 \t Loss: -0.010936020341302668\nTRAIN: \t Epoch: 77 \t Loss: -0.010873969993554056\nTRAIN: \t Epoch: 77 \t Loss: -0.010799137978918023\nTRAIN: \t Epoch: 77 \t Loss: -0.010828587412834167\nTRAIN: \t Epoch: 77 \t Loss: -0.01089848027649251\nTRAIN: \t Epoch: 77 \t Loss: -0.010841961950063705\nTRAIN: \t Epoch: 77 \t Loss: -0.010807092158267131\nTRAIN: \t Epoch: 77 \t Loss: -0.010877570603042841\nTRAIN: \t Epoch: 77 \t Loss: -0.010958668527503808\nTRAIN: \t Epoch: 77 \t Loss: -0.011004824831616133\nTRAIN: \t Epoch: 77 \t Loss: -0.010922021780382185\nTRAIN: \t Epoch: 77 \t Loss: -0.01085950480774045\nTRAIN: \t Epoch: 77 \t Loss: -0.010885772638415036\nTRAIN: \t Epoch: 77 \t Loss: -0.010922155808657408\nTRAIN: \t Epoch: 77 \t Loss: -0.010932471070970808\nTRAIN: \t Epoch: 77 \t Loss: -0.010922535831239108\nVALD: \t Epoch: 77 \t Loss: -0.004882961045950651\nVALD: \t Epoch: 77 \t Loss: -0.004528179531916976\nVALD: \t Epoch: 77 \t Loss: -0.0052152870533366995\nVALD: \t Epoch: 77 \t Loss: 0.0020570909837260842\nVALD: \t Epoch: 77 \t Loss: 0.0004646786488592625\nVALD: \t Epoch: 77 \t Loss: 0.00038854220148288845\n******************************\nEpoch: social-tag : 77\ntrain_loss -0.010922535831239108\nval_loss 0.00038854220148288845\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 78 \t Loss: -0.012201497331261635\nTRAIN: \t Epoch: 78 \t Loss: -0.012145298067480326\nTRAIN: \t Epoch: 78 \t Loss: -0.011425687621037165\nTRAIN: \t Epoch: 78 \t Loss: -0.010696202982217073\nTRAIN: \t Epoch: 78 \t Loss: -0.010550809465348721\nTRAIN: \t Epoch: 78 \t Loss: -0.010689195400724808\nTRAIN: \t Epoch: 78 \t Loss: -0.01093287207186222\nTRAIN: \t Epoch: 78 \t Loss: -0.011035236530005932\nTRAIN: \t Epoch: 78 \t Loss: -0.011085207470589213\nTRAIN: \t Epoch: 78 \t Loss: -0.010872680693864822\nTRAIN: \t Epoch: 78 \t Loss: -0.01063820728185502\nTRAIN: \t Epoch: 78 \t Loss: -0.010594178612033526\nTRAIN: \t Epoch: 78 \t Loss: -0.010799600886037717\nTRAIN: \t Epoch: 78 \t Loss: -0.010850078559347562\nTRAIN: \t Epoch: 78 \t Loss: -0.010890844215949377\nTRAIN: \t Epoch: 78 \t Loss: -0.01095210190396756\nTRAIN: \t Epoch: 78 \t Loss: -0.01087242244359325\nTRAIN: \t Epoch: 78 \t Loss: -0.010757440390686194\nTRAIN: \t Epoch: 78 \t Loss: -0.010731286026145282\nTRAIN: \t Epoch: 78 \t Loss: -0.010796817811205983\nTRAIN: \t Epoch: 78 \t Loss: -0.010875287510099866\nTRAIN: \t Epoch: 78 \t Loss: -0.010878415381672994\nVALD: \t Epoch: 78 \t Loss: 0.007833399809896946\nVALD: \t Epoch: 78 \t Loss: 0.00869743525981903\nVALD: \t Epoch: 78 \t Loss: 0.005154089032051464\nVALD: \t Epoch: 78 \t Loss: 0.0092967527161818\nVALD: \t Epoch: 78 \t Loss: 0.006676013232208788\nVALD: \t Epoch: 78 \t Loss: 0.006426173600960861\n******************************\nEpoch: social-tag : 78\ntrain_loss -0.010878415381672994\nval_loss 0.006426173600960861\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 79 \t Loss: -0.012201817706227303\nTRAIN: \t Epoch: 79 \t Loss: -0.012007176876068115\nTRAIN: \t Epoch: 79 \t Loss: -0.01214738842099905\nTRAIN: \t Epoch: 79 \t Loss: -0.011417907197028399\nTRAIN: \t Epoch: 79 \t Loss: -0.010402732994407415\nTRAIN: \t Epoch: 79 \t Loss: -0.010299749594802657\nTRAIN: \t Epoch: 79 \t Loss: -0.010506813613963979\nTRAIN: \t Epoch: 79 \t Loss: -0.01076591486344114\nTRAIN: \t Epoch: 79 \t Loss: -0.010880854756881794\nTRAIN: \t Epoch: 79 \t Loss: -0.010974411061033607\nTRAIN: \t Epoch: 79 \t Loss: -0.010904844215309078\nTRAIN: \t Epoch: 79 \t Loss: -0.010813570076910159\nTRAIN: \t Epoch: 79 \t Loss: -0.010733694661981784\nTRAIN: \t Epoch: 79 \t Loss: -0.010775995407519596\nTRAIN: \t Epoch: 79 \t Loss: -0.010768531231830518\nTRAIN: \t Epoch: 79 \t Loss: -0.01077012563473545\nTRAIN: \t Epoch: 79 \t Loss: -0.010842159143922961\nTRAIN: \t Epoch: 79 \t Loss: -0.010769304172653291\nTRAIN: \t Epoch: 79 \t Loss: -0.010718054358700388\nTRAIN: \t Epoch: 79 \t Loss: -0.010706364479847252\nTRAIN: \t Epoch: 79 \t Loss: -0.010713561149757533\nTRAIN: \t Epoch: 79 \t Loss: -0.01072139245603414\nVALD: \t Epoch: 79 \t Loss: -0.005249819718301296\nVALD: \t Epoch: 79 \t Loss: -0.003003580932272598\nVALD: \t Epoch: 79 \t Loss: -0.004339481063652784\nVALD: \t Epoch: 79 \t Loss: 0.002194288434111513\nVALD: \t Epoch: 79 \t Loss: 0.0005071500432677567\nVALD: \t Epoch: 79 \t Loss: 0.00041318000717596574\n******************************\nEpoch: social-tag : 79\ntrain_loss -0.01072139245603414\nval_loss 0.00041318000717596574\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 80 \t Loss: -0.011673168279230595\nTRAIN: \t Epoch: 80 \t Loss: -0.011611877009272575\nTRAIN: \t Epoch: 80 \t Loss: -0.011415150947868824\nTRAIN: \t Epoch: 80 \t Loss: -0.010997707955539227\nTRAIN: \t Epoch: 80 \t Loss: -0.011120582930743694\nTRAIN: \t Epoch: 80 \t Loss: -0.011252440667400757\nTRAIN: \t Epoch: 80 \t Loss: -0.011173895826297147\nTRAIN: \t Epoch: 80 \t Loss: -0.011111458647064865\nTRAIN: \t Epoch: 80 \t Loss: -0.011142467562523153\nTRAIN: \t Epoch: 80 \t Loss: -0.01116879666224122\nTRAIN: \t Epoch: 80 \t Loss: -0.011160340752791275\nTRAIN: \t Epoch: 80 \t Loss: -0.011102279415354133\nTRAIN: \t Epoch: 80 \t Loss: -0.011089590163185047\nTRAIN: \t Epoch: 80 \t Loss: -0.01117962332708495\nTRAIN: \t Epoch: 80 \t Loss: -0.01118675737331311\nTRAIN: \t Epoch: 80 \t Loss: -0.011098544986452907\nTRAIN: \t Epoch: 80 \t Loss: -0.011007006992312038\nTRAIN: \t Epoch: 80 \t Loss: -0.011045785465588173\nTRAIN: \t Epoch: 80 \t Loss: -0.011126598196202203\nTRAIN: \t Epoch: 80 \t Loss: -0.011121942522004246\nTRAIN: \t Epoch: 80 \t Loss: -0.01114322870437588\nTRAIN: \t Epoch: 80 \t Loss: -0.011142378563940847\nVALD: \t Epoch: 80 \t Loss: 0.003814543830230832\nVALD: \t Epoch: 80 \t Loss: 0.003962541348300874\nVALD: \t Epoch: 80 \t Loss: 0.0013913844401637714\nVALD: \t Epoch: 80 \t Loss: 0.013885143911466002\nVALD: \t Epoch: 80 \t Loss: 0.010787984705530107\nVALD: \t Epoch: 80 \t Loss: 0.010678725170366691\n******************************\nEpoch: social-tag : 80\ntrain_loss -0.011142378563940847\nval_loss 0.010678725170366691\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 81 \t Loss: -0.01138464268296957\nTRAIN: \t Epoch: 81 \t Loss: -0.011450630147010088\nTRAIN: \t Epoch: 81 \t Loss: -0.011298563331365585\nTRAIN: \t Epoch: 81 \t Loss: -0.010573289124295115\nTRAIN: \t Epoch: 81 \t Loss: -0.010176426731050014\nTRAIN: \t Epoch: 81 \t Loss: -0.010397692831854025\nTRAIN: \t Epoch: 81 \t Loss: -0.01064429312412228\nTRAIN: \t Epoch: 81 \t Loss: -0.010775446076877415\nTRAIN: \t Epoch: 81 \t Loss: -0.01089598559257057\nTRAIN: \t Epoch: 81 \t Loss: -0.010835217032581567\nTRAIN: \t Epoch: 81 \t Loss: -0.010758655484427105\nTRAIN: \t Epoch: 81 \t Loss: -0.010822603634248177\nTRAIN: \t Epoch: 81 \t Loss: -0.01085096475883172\nTRAIN: \t Epoch: 81 \t Loss: -0.010772316317473139\nTRAIN: \t Epoch: 81 \t Loss: -0.010804648076494535\nTRAIN: \t Epoch: 81 \t Loss: -0.010914307786151767\nTRAIN: \t Epoch: 81 \t Loss: -0.010871480602551909\nTRAIN: \t Epoch: 81 \t Loss: -0.010850317310541868\nTRAIN: \t Epoch: 81 \t Loss: -0.010878181281058412\nTRAIN: \t Epoch: 81 \t Loss: -0.01091322973370552\nTRAIN: \t Epoch: 81 \t Loss: -0.01095143479428121\nTRAIN: \t Epoch: 81 \t Loss: -0.010994076857232962\nVALD: \t Epoch: 81 \t Loss: 0.0005201578605920076\nVALD: \t Epoch: 81 \t Loss: 0.00016922910435823724\nVALD: \t Epoch: 81 \t Loss: -0.003185419137783659\nVALD: \t Epoch: 81 \t Loss: 0.00341499986461713\nVALD: \t Epoch: 81 \t Loss: 0.0020983169291866944\nVALD: \t Epoch: 81 \t Loss: 0.0023511075584048574\n******************************\nEpoch: social-tag : 81\ntrain_loss -0.010994076857232962\nval_loss 0.0023511075584048574\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 82 \t Loss: -0.011017837561666965\nTRAIN: \t Epoch: 82 \t Loss: -0.011123497504740953\nTRAIN: \t Epoch: 82 \t Loss: -0.010764575252930323\nTRAIN: \t Epoch: 82 \t Loss: -0.010664103785529733\nTRAIN: \t Epoch: 82 \t Loss: -0.011154215224087239\nTRAIN: \t Epoch: 82 \t Loss: -0.010996502358466387\nTRAIN: \t Epoch: 82 \t Loss: -0.010988735992993628\nTRAIN: \t Epoch: 82 \t Loss: -0.011058527859859169\nTRAIN: \t Epoch: 82 \t Loss: -0.011172023084428575\nTRAIN: \t Epoch: 82 \t Loss: -0.01117048216983676\nTRAIN: \t Epoch: 82 \t Loss: -0.011234675618735228\nTRAIN: \t Epoch: 82 \t Loss: -0.011349238455295563\nTRAIN: \t Epoch: 82 \t Loss: -0.011350188619242264\nTRAIN: \t Epoch: 82 \t Loss: -0.011200561254684414\nTRAIN: \t Epoch: 82 \t Loss: -0.011142238043248653\nTRAIN: \t Epoch: 82 \t Loss: -0.011173765233252198\nTRAIN: \t Epoch: 82 \t Loss: -0.011224232011419885\nTRAIN: \t Epoch: 82 \t Loss: -0.011285470643391212\nTRAIN: \t Epoch: 82 \t Loss: -0.011242369385926347\nTRAIN: \t Epoch: 82 \t Loss: -0.011136379139497877\nTRAIN: \t Epoch: 82 \t Loss: -0.011052143937420277\nTRAIN: \t Epoch: 82 \t Loss: -0.01105577834418689\nVALD: \t Epoch: 82 \t Loss: 0.011168090626597404\nVALD: \t Epoch: 82 \t Loss: 0.007381392410025001\nVALD: \t Epoch: 82 \t Loss: 0.0020560983878870807\nVALD: \t Epoch: 82 \t Loss: 0.013250139658339322\nVALD: \t Epoch: 82 \t Loss: 0.01022739598993212\nVALD: \t Epoch: 82 \t Loss: 0.010221417586911809\n******************************\nEpoch: social-tag : 82\ntrain_loss -0.01105577834418689\nval_loss 0.010221417586911809\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 83 \t Loss: -0.012297946959733963\nTRAIN: \t Epoch: 83 \t Loss: -0.011997256428003311\nTRAIN: \t Epoch: 83 \t Loss: -0.011009960435330868\nTRAIN: \t Epoch: 83 \t Loss: -0.010513462126255035\nTRAIN: \t Epoch: 83 \t Loss: -0.010675706528127193\nTRAIN: \t Epoch: 83 \t Loss: -0.011006386019289494\nTRAIN: \t Epoch: 83 \t Loss: -0.011171137381877218\nTRAIN: \t Epoch: 83 \t Loss: -0.011281664366833866\nTRAIN: \t Epoch: 83 \t Loss: -0.01114041182316012\nTRAIN: \t Epoch: 83 \t Loss: -0.01093461737036705\nTRAIN: \t Epoch: 83 \t Loss: -0.010997742583805864\nTRAIN: \t Epoch: 83 \t Loss: -0.011152599705383182\nTRAIN: \t Epoch: 83 \t Loss: -0.011133419851271005\nTRAIN: \t Epoch: 83 \t Loss: -0.011033929618341582\nTRAIN: \t Epoch: 83 \t Loss: -0.011036639908949535\nTRAIN: \t Epoch: 83 \t Loss: -0.011033936694730073\nTRAIN: \t Epoch: 83 \t Loss: -0.011076399070375106\nTRAIN: \t Epoch: 83 \t Loss: -0.01108569527665774\nTRAIN: \t Epoch: 83 \t Loss: -0.011028449492234933\nTRAIN: \t Epoch: 83 \t Loss: -0.011060724779963494\nTRAIN: \t Epoch: 83 \t Loss: -0.011065430540059294\nTRAIN: \t Epoch: 83 \t Loss: -0.011065708756232818\nVALD: \t Epoch: 83 \t Loss: 0.00488602090626955\nVALD: \t Epoch: 83 \t Loss: 0.006357609294354916\nVALD: \t Epoch: 83 \t Loss: 0.003456306178122759\nVALD: \t Epoch: 83 \t Loss: 0.005639526876620948\nVALD: \t Epoch: 83 \t Loss: 0.0035765222273766994\nVALD: \t Epoch: 83 \t Loss: 0.003352709678989468\n******************************\nEpoch: social-tag : 83\ntrain_loss -0.011065708756232818\nval_loss 0.003352709678989468\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 84 \t Loss: -0.011298238299787045\nTRAIN: \t Epoch: 84 \t Loss: -0.010874458588659763\nTRAIN: \t Epoch: 84 \t Loss: -0.010669545891384283\nTRAIN: \t Epoch: 84 \t Loss: -0.010434767929837108\nTRAIN: \t Epoch: 84 \t Loss: -0.01058144699782133\nTRAIN: \t Epoch: 84 \t Loss: -0.010907058293620745\nTRAIN: \t Epoch: 84 \t Loss: -0.011077427970511573\nTRAIN: \t Epoch: 84 \t Loss: -0.010915839695371687\nTRAIN: \t Epoch: 84 \t Loss: -0.010724929782251516\nTRAIN: \t Epoch: 84 \t Loss: -0.010625745914876462\nTRAIN: \t Epoch: 84 \t Loss: -0.01084935021671382\nTRAIN: \t Epoch: 84 \t Loss: -0.01106767434005936\nTRAIN: \t Epoch: 84 \t Loss: -0.011100380753095333\nTRAIN: \t Epoch: 84 \t Loss: -0.011232453864067793\nTRAIN: \t Epoch: 84 \t Loss: -0.011251551720003286\nTRAIN: \t Epoch: 84 \t Loss: -0.011273044277913868\nTRAIN: \t Epoch: 84 \t Loss: -0.011254082981716184\nTRAIN: \t Epoch: 84 \t Loss: -0.011179531044844124\nTRAIN: \t Epoch: 84 \t Loss: -0.01118023218096871\nTRAIN: \t Epoch: 84 \t Loss: -0.011235173186287284\nTRAIN: \t Epoch: 84 \t Loss: -0.011248772476045858\nTRAIN: \t Epoch: 84 \t Loss: -0.011243881314618369\nVALD: \t Epoch: 84 \t Loss: -0.0067938161082565784\nVALD: \t Epoch: 84 \t Loss: -0.005785658722743392\nVALD: \t Epoch: 84 \t Loss: -0.0068418763888378935\nVALD: \t Epoch: 84 \t Loss: -0.003377443761564791\nVALD: \t Epoch: 84 \t Loss: -0.003946057055145502\nVALD: \t Epoch: 84 \t Loss: -0.0037189427876111233\n******************************\nEpoch: social-tag : 84\ntrain_loss -0.011243881314618369\nval_loss -0.0037189427876111233\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 85 \t Loss: -0.011253542266786098\nTRAIN: \t Epoch: 85 \t Loss: -0.011098045855760574\nTRAIN: \t Epoch: 85 \t Loss: -0.010820334777235985\nTRAIN: \t Epoch: 85 \t Loss: -0.010989391012117267\nTRAIN: \t Epoch: 85 \t Loss: -0.011176450923085213\nTRAIN: \t Epoch: 85 \t Loss: -0.011128068435937166\nTRAIN: \t Epoch: 85 \t Loss: -0.01113769824483565\nTRAIN: \t Epoch: 85 \t Loss: -0.010946029215119779\nTRAIN: \t Epoch: 85 \t Loss: -0.010938699978093306\nTRAIN: \t Epoch: 85 \t Loss: -0.010953265707939863\nTRAIN: \t Epoch: 85 \t Loss: -0.011138142086565495\nTRAIN: \t Epoch: 85 \t Loss: -0.01128139952197671\nTRAIN: \t Epoch: 85 \t Loss: -0.01130183284672407\nTRAIN: \t Epoch: 85 \t Loss: -0.011135029872613294\nTRAIN: \t Epoch: 85 \t Loss: -0.011078565381467342\nTRAIN: \t Epoch: 85 \t Loss: -0.011094928428065032\nTRAIN: \t Epoch: 85 \t Loss: -0.01115631525788237\nTRAIN: \t Epoch: 85 \t Loss: -0.011132866195920441\nTRAIN: \t Epoch: 85 \t Loss: -0.010978010777187975\nTRAIN: \t Epoch: 85 \t Loss: -0.010889072157442569\nTRAIN: \t Epoch: 85 \t Loss: -0.010930836998990603\nTRAIN: \t Epoch: 85 \t Loss: -0.010970541219522966\nVALD: \t Epoch: 85 \t Loss: -0.0014718130696564913\nVALD: \t Epoch: 85 \t Loss: -0.0005385496042435989\nVALD: \t Epoch: 85 \t Loss: -0.0019665771687868983\nVALD: \t Epoch: 85 \t Loss: 0.0051273399803903885\nVALD: \t Epoch: 85 \t Loss: 0.0027144108957145364\nVALD: \t Epoch: 85 \t Loss: 0.0025311570219469794\n******************************\nEpoch: social-tag : 85\ntrain_loss -0.010970541219522966\nval_loss 0.0025311570219469794\n{'min_val_epoch': 55, 'min_val_loss': -0.005239669594800833}\n******************************\nTRAIN: \t Epoch: 86 \t Loss: -0.012820505537092686\nTRAIN: \t Epoch: 86 \t Loss: -0.01272075530141592\nTRAIN: \t Epoch: 86 \t Loss: -0.01257621186474959\nTRAIN: \t Epoch: 86 \t Loss: -0.012392511824145913\nTRAIN: \t Epoch: 86 \t Loss: -0.01215897835791111\nTRAIN: \t Epoch: 86 \t Loss: -0.012018475836763779\nTRAIN: \t Epoch: 86 \t Loss: -0.01179995560752494\nTRAIN: \t Epoch: 86 \t Loss: -0.011637051706202328\nTRAIN: \t Epoch: 86 \t Loss: -0.011617754275600115\nTRAIN: \t Epoch: 86 \t Loss: -0.011602847743779421\nTRAIN: \t Epoch: 86 \t Loss: -0.011579353954981674\nTRAIN: \t Epoch: 86 \t Loss: -0.011518121308957538\nTRAIN: \t Epoch: 86 \t Loss: -0.01150030900652592\nTRAIN: \t Epoch: 86 \t Loss: -0.011521440864141499\nTRAIN: \t Epoch: 86 \t Loss: -0.011505414980153244\nTRAIN: \t Epoch: 86 \t Loss: -0.011388358427211642\nTRAIN: \t Epoch: 86 \t Loss: -0.011374104877605158\nTRAIN: \t Epoch: 86 \t Loss: -0.011416818170497814\nTRAIN: \t Epoch: 86 \t Loss: -0.011477960371657422\nTRAIN: \t Epoch: 86 \t Loss: -0.011491820635274052\nTRAIN: \t Epoch: 86 \t Loss: -0.011353737275515283\nTRAIN: \t Epoch: 86 \t Loss: -0.011243958447523135\nVALD: \t Epoch: 86 \t Loss: -0.011079614982008934\nVALD: \t Epoch: 86 \t Loss: -0.009677549358457327\nVALD: \t Epoch: 86 \t Loss: -0.00960503425449133\nVALD: \t Epoch: 86 \t Loss: -0.0059198582312092185\nVALD: \t Epoch: 86 \t Loss: -0.006500768195837736\nVALD: \t Epoch: 86 \t Loss: -0.006450308103001479\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 70/70 [00:02<00:00, 25.86it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.777610299073503  FDE: 1.2162984800980141\n**************************************************\n******************************\nEpoch: social-tag : 86\ntrain_loss -0.011243958447523135\nval_loss -0.006450308103001479\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 87 \t Loss: -0.010101952590048313\nTRAIN: \t Epoch: 87 \t Loss: -0.011131733655929565\nTRAIN: \t Epoch: 87 \t Loss: -0.011520496880014738\nTRAIN: \t Epoch: 87 \t Loss: -0.01177149941213429\nTRAIN: \t Epoch: 87 \t Loss: -0.01153921354562044\nTRAIN: \t Epoch: 87 \t Loss: -0.011072036344558\nTRAIN: \t Epoch: 87 \t Loss: -0.010782030677156789\nTRAIN: \t Epoch: 87 \t Loss: -0.010871890466660261\nTRAIN: \t Epoch: 87 \t Loss: -0.011069314761294259\nTRAIN: \t Epoch: 87 \t Loss: -0.011284013465046882\nTRAIN: \t Epoch: 87 \t Loss: -0.011377565054730936\nTRAIN: \t Epoch: 87 \t Loss: -0.011343476440136632\nTRAIN: \t Epoch: 87 \t Loss: -0.011249811603472782\nTRAIN: \t Epoch: 87 \t Loss: -0.011251602854047502\nTRAIN: \t Epoch: 87 \t Loss: -0.011370266477266947\nTRAIN: \t Epoch: 87 \t Loss: -0.011430837330408394\nTRAIN: \t Epoch: 87 \t Loss: -0.01144448245930321\nTRAIN: \t Epoch: 87 \t Loss: -0.011430172042714225\nTRAIN: \t Epoch: 87 \t Loss: -0.011396254902999652\nTRAIN: \t Epoch: 87 \t Loss: -0.011379063688218593\nTRAIN: \t Epoch: 87 \t Loss: -0.011358887223260743\nTRAIN: \t Epoch: 87 \t Loss: -0.011333318715463635\nVALD: \t Epoch: 87 \t Loss: 0.039223428815603256\nVALD: \t Epoch: 87 \t Loss: 0.04340650886297226\nVALD: \t Epoch: 87 \t Loss: 0.040731014062960945\nVALD: \t Epoch: 87 \t Loss: 0.04028492700308561\nVALD: \t Epoch: 87 \t Loss: 0.039667819440364835\nVALD: \t Epoch: 87 \t Loss: 0.039174960734266226\n******************************\nEpoch: social-tag : 87\ntrain_loss -0.011333318715463635\nval_loss 0.039174960734266226\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 88 \t Loss: -0.011117924936115742\nTRAIN: \t Epoch: 88 \t Loss: -0.01194361224770546\nTRAIN: \t Epoch: 88 \t Loss: -0.011923773214221\nTRAIN: \t Epoch: 88 \t Loss: -0.011720251524820924\nTRAIN: \t Epoch: 88 \t Loss: -0.011415414698421954\nTRAIN: \t Epoch: 88 \t Loss: -0.01137815834954381\nTRAIN: \t Epoch: 88 \t Loss: -0.011460929842931884\nTRAIN: \t Epoch: 88 \t Loss: -0.011384911835193634\nTRAIN: \t Epoch: 88 \t Loss: -0.01137444811562697\nTRAIN: \t Epoch: 88 \t Loss: -0.011396098788827658\nTRAIN: \t Epoch: 88 \t Loss: -0.011399355801669035\nTRAIN: \t Epoch: 88 \t Loss: -0.011387906037271023\nTRAIN: \t Epoch: 88 \t Loss: -0.011381414647285756\nTRAIN: \t Epoch: 88 \t Loss: -0.0113447165515806\nTRAIN: \t Epoch: 88 \t Loss: -0.01132086869329214\nTRAIN: \t Epoch: 88 \t Loss: -0.011383396747987717\nTRAIN: \t Epoch: 88 \t Loss: -0.011399161125368932\nTRAIN: \t Epoch: 88 \t Loss: -0.011389376957797341\nTRAIN: \t Epoch: 88 \t Loss: -0.011381101402405062\nTRAIN: \t Epoch: 88 \t Loss: -0.011372547736391425\nTRAIN: \t Epoch: 88 \t Loss: -0.011290961521722022\nTRAIN: \t Epoch: 88 \t Loss: -0.011276048518168862\nVALD: \t Epoch: 88 \t Loss: -0.003199892584234476\nVALD: \t Epoch: 88 \t Loss: -0.002447016828227788\nVALD: \t Epoch: 88 \t Loss: -0.005152127899539967\nVALD: \t Epoch: 88 \t Loss: 0.0013795859122183174\nVALD: \t Epoch: 88 \t Loss: 0.00037391220685094594\nVALD: \t Epoch: 88 \t Loss: 0.000728719455726219\n******************************\nEpoch: social-tag : 88\ntrain_loss -0.011276048518168862\nval_loss 0.000728719455726219\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 89 \t Loss: -0.012121341191232204\nTRAIN: \t Epoch: 89 \t Loss: -0.012275408022105694\nTRAIN: \t Epoch: 89 \t Loss: -0.012191513553261757\nTRAIN: \t Epoch: 89 \t Loss: -0.012276055756956339\nTRAIN: \t Epoch: 89 \t Loss: -0.012031603418290616\nTRAIN: \t Epoch: 89 \t Loss: -0.011437224689871073\nTRAIN: \t Epoch: 89 \t Loss: -0.0113681202222194\nTRAIN: \t Epoch: 89 \t Loss: -0.011518039740622044\nTRAIN: \t Epoch: 89 \t Loss: -0.0116857441349162\nTRAIN: \t Epoch: 89 \t Loss: -0.011721274629235268\nTRAIN: \t Epoch: 89 \t Loss: -0.011780615303326736\nTRAIN: \t Epoch: 89 \t Loss: -0.011701472646867236\nTRAIN: \t Epoch: 89 \t Loss: -0.011613962097236743\nTRAIN: \t Epoch: 89 \t Loss: -0.01163229372884546\nTRAIN: \t Epoch: 89 \t Loss: -0.011627350747585297\nTRAIN: \t Epoch: 89 \t Loss: -0.011553302698303014\nTRAIN: \t Epoch: 89 \t Loss: -0.011542176334735225\nTRAIN: \t Epoch: 89 \t Loss: -0.011543260059422918\nTRAIN: \t Epoch: 89 \t Loss: -0.011572222176351045\nTRAIN: \t Epoch: 89 \t Loss: -0.01151758902706206\nTRAIN: \t Epoch: 89 \t Loss: -0.011478727816471032\nTRAIN: \t Epoch: 89 \t Loss: -0.011484679542281256\nVALD: \t Epoch: 89 \t Loss: -0.005217257887125015\nVALD: \t Epoch: 89 \t Loss: -0.0024100522423395887\nVALD: \t Epoch: 89 \t Loss: -0.003764145939688509\nVALD: \t Epoch: 89 \t Loss: 0.0038743687837268226\nVALD: \t Epoch: 89 \t Loss: 0.0016309001890476792\nVALD: \t Epoch: 89 \t Loss: 0.001514596322720701\n******************************\nEpoch: social-tag : 89\ntrain_loss -0.011484679542281256\nval_loss 0.001514596322720701\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 90 \t Loss: -0.012742998078465462\nTRAIN: \t Epoch: 90 \t Loss: -0.012855709064751863\nTRAIN: \t Epoch: 90 \t Loss: -0.01271257239083449\nTRAIN: \t Epoch: 90 \t Loss: -0.012465174309909344\nTRAIN: \t Epoch: 90 \t Loss: -0.012119483761489391\nTRAIN: \t Epoch: 90 \t Loss: -0.011834478316207727\nTRAIN: \t Epoch: 90 \t Loss: -0.011845628065722329\nTRAIN: \t Epoch: 90 \t Loss: -0.011856193421408534\nTRAIN: \t Epoch: 90 \t Loss: -0.011858554867406687\nTRAIN: \t Epoch: 90 \t Loss: -0.011953291017562151\nTRAIN: \t Epoch: 90 \t Loss: -0.01173850843174891\nTRAIN: \t Epoch: 90 \t Loss: -0.011639778114234408\nTRAIN: \t Epoch: 90 \t Loss: -0.011624521647508327\nTRAIN: \t Epoch: 90 \t Loss: -0.011688741921846355\nTRAIN: \t Epoch: 90 \t Loss: -0.011709053255617618\nTRAIN: \t Epoch: 90 \t Loss: -0.011697830981574953\nTRAIN: \t Epoch: 90 \t Loss: -0.011694368961102822\nTRAIN: \t Epoch: 90 \t Loss: -0.011666135552028814\nTRAIN: \t Epoch: 90 \t Loss: -0.01165271290626965\nTRAIN: \t Epoch: 90 \t Loss: -0.011656463658437134\nTRAIN: \t Epoch: 90 \t Loss: -0.011665835045278072\nTRAIN: \t Epoch: 90 \t Loss: -0.011669648613921188\nVALD: \t Epoch: 90 \t Loss: 0.01008828915655613\nVALD: \t Epoch: 90 \t Loss: 0.010982205159962177\nVALD: \t Epoch: 90 \t Loss: 0.005723405318955581\nVALD: \t Epoch: 90 \t Loss: 0.01859934418462217\nVALD: \t Epoch: 90 \t Loss: 0.015355448424816131\nVALD: \t Epoch: 90 \t Loss: 0.015461839780663\n******************************\nEpoch: social-tag : 90\ntrain_loss -0.011669648613921188\nval_loss 0.015461839780663\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 91 \t Loss: -0.012673628516495228\nTRAIN: \t Epoch: 91 \t Loss: -0.012041577603667974\nTRAIN: \t Epoch: 91 \t Loss: -0.011555488842229048\nTRAIN: \t Epoch: 91 \t Loss: -0.011279965518042445\nTRAIN: \t Epoch: 91 \t Loss: -0.011519743874669076\nTRAIN: \t Epoch: 91 \t Loss: -0.011576999754955372\nTRAIN: \t Epoch: 91 \t Loss: -0.011294251840029444\nTRAIN: \t Epoch: 91 \t Loss: -0.011015938012860715\nTRAIN: \t Epoch: 91 \t Loss: -0.011049790204399161\nTRAIN: \t Epoch: 91 \t Loss: -0.011238780152052642\nTRAIN: \t Epoch: 91 \t Loss: -0.011263858696276491\nTRAIN: \t Epoch: 91 \t Loss: -0.011264965869486332\nTRAIN: \t Epoch: 91 \t Loss: -0.011271437630057335\nTRAIN: \t Epoch: 91 \t Loss: -0.011226116280470575\nTRAIN: \t Epoch: 91 \t Loss: -0.011318114399909974\nTRAIN: \t Epoch: 91 \t Loss: -0.011290242953691632\nTRAIN: \t Epoch: 91 \t Loss: -0.011295842883341452\nTRAIN: \t Epoch: 91 \t Loss: -0.011291313399043348\nTRAIN: \t Epoch: 91 \t Loss: -0.011323942753829454\nTRAIN: \t Epoch: 91 \t Loss: -0.011277558701112866\nTRAIN: \t Epoch: 91 \t Loss: -0.011329806542822294\nTRAIN: \t Epoch: 91 \t Loss: -0.011393361031688094\nVALD: \t Epoch: 91 \t Loss: 0.000578508188482374\nVALD: \t Epoch: 91 \t Loss: 0.005148177704541013\nVALD: \t Epoch: 91 \t Loss: 0.002679558606663098\nVALD: \t Epoch: 91 \t Loss: 0.011725351781933568\nVALD: \t Epoch: 91 \t Loss: 0.008529464702587575\nVALD: \t Epoch: 91 \t Loss: 0.00825379208915613\n******************************\nEpoch: social-tag : 91\ntrain_loss -0.011393361031688094\nval_loss 0.00825379208915613\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 92 \t Loss: -0.012482348829507828\nTRAIN: \t Epoch: 92 \t Loss: -0.01176093053072691\nTRAIN: \t Epoch: 92 \t Loss: -0.011518022666374842\nTRAIN: \t Epoch: 92 \t Loss: -0.011575249722227454\nTRAIN: \t Epoch: 92 \t Loss: -0.011741745099425317\nTRAIN: \t Epoch: 92 \t Loss: -0.011673192027956247\nTRAIN: \t Epoch: 92 \t Loss: -0.011400902111615454\nTRAIN: \t Epoch: 92 \t Loss: -0.011272484785877168\nTRAIN: \t Epoch: 92 \t Loss: -0.011458735188676251\nTRAIN: \t Epoch: 92 \t Loss: -0.01153239831328392\nTRAIN: \t Epoch: 92 \t Loss: -0.01149820341643962\nTRAIN: \t Epoch: 92 \t Loss: -0.011481955414637923\nTRAIN: \t Epoch: 92 \t Loss: -0.011530569467980128\nTRAIN: \t Epoch: 92 \t Loss: -0.011521195394120045\nTRAIN: \t Epoch: 92 \t Loss: -0.011431613005697727\nTRAIN: \t Epoch: 92 \t Loss: -0.01141827245010063\nTRAIN: \t Epoch: 92 \t Loss: -0.011455024866496815\nTRAIN: \t Epoch: 92 \t Loss: -0.011455534232987298\nTRAIN: \t Epoch: 92 \t Loss: -0.01132984170200009\nTRAIN: \t Epoch: 92 \t Loss: -0.011306365439668297\nTRAIN: \t Epoch: 92 \t Loss: -0.011363563926092215\nTRAIN: \t Epoch: 92 \t Loss: -0.011377889335262497\nVALD: \t Epoch: 92 \t Loss: -0.0028648292645812035\nVALD: \t Epoch: 92 \t Loss: -0.0014526449776894879\nVALD: \t Epoch: 92 \t Loss: -0.004154104843716293\nVALD: \t Epoch: 92 \t Loss: 0.0013814493231620872\nVALD: \t Epoch: 92 \t Loss: 5.008516163798049e-05\nVALD: \t Epoch: 92 \t Loss: 0.00011664756686624253\n******************************\nEpoch: social-tag : 92\ntrain_loss -0.011377889335262497\nval_loss 0.00011664756686624253\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 93 \t Loss: -0.011358809657394886\nTRAIN: \t Epoch: 93 \t Loss: -0.011673228815197945\nTRAIN: \t Epoch: 93 \t Loss: -0.011567829797665278\nTRAIN: \t Epoch: 93 \t Loss: -0.0113409124314785\nTRAIN: \t Epoch: 93 \t Loss: -0.011348767764866352\nTRAIN: \t Epoch: 93 \t Loss: -0.011616006183127562\nTRAIN: \t Epoch: 93 \t Loss: -0.011774694946195399\nTRAIN: \t Epoch: 93 \t Loss: -0.011774579645134509\nTRAIN: \t Epoch: 93 \t Loss: -0.011881442844039865\nTRAIN: \t Epoch: 93 \t Loss: -0.011753060482442379\nTRAIN: \t Epoch: 93 \t Loss: -0.011655897410078482\nTRAIN: \t Epoch: 93 \t Loss: -0.011727320263162255\nTRAIN: \t Epoch: 93 \t Loss: -0.011813301163224073\nTRAIN: \t Epoch: 93 \t Loss: -0.01180616081027048\nTRAIN: \t Epoch: 93 \t Loss: -0.011876518838107586\nTRAIN: \t Epoch: 93 \t Loss: -0.011820135754533112\nTRAIN: \t Epoch: 93 \t Loss: -0.011741683325346778\nTRAIN: \t Epoch: 93 \t Loss: -0.011693793090267314\nTRAIN: \t Epoch: 93 \t Loss: -0.011737910491463385\nTRAIN: \t Epoch: 93 \t Loss: -0.01176050417125225\nTRAIN: \t Epoch: 93 \t Loss: -0.011753819616777557\nTRAIN: \t Epoch: 93 \t Loss: -0.011703457845177642\nVALD: \t Epoch: 93 \t Loss: -0.004191359039396048\nVALD: \t Epoch: 93 \t Loss: -0.0021894705496379174\nVALD: \t Epoch: 93 \t Loss: -0.0032102489543224997\nVALD: \t Epoch: 93 \t Loss: 0.00020315924120950513\nVALD: \t Epoch: 93 \t Loss: -0.001059663892374374\nVALD: \t Epoch: 93 \t Loss: -0.0011424801820381122\n******************************\nEpoch: social-tag : 93\ntrain_loss -0.011703457845177642\nval_loss -0.0011424801820381122\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 94 \t Loss: -0.01243204902857542\nTRAIN: \t Epoch: 94 \t Loss: -0.012671986129134893\nTRAIN: \t Epoch: 94 \t Loss: -0.01236805971711874\nTRAIN: \t Epoch: 94 \t Loss: -0.011451414320617914\nTRAIN: \t Epoch: 94 \t Loss: -0.01096977237612009\nTRAIN: \t Epoch: 94 \t Loss: -0.011095456313341856\nTRAIN: \t Epoch: 94 \t Loss: -0.011285179294645786\nTRAIN: \t Epoch: 94 \t Loss: -0.011428389814682305\nTRAIN: \t Epoch: 94 \t Loss: -0.011404498066339228\nTRAIN: \t Epoch: 94 \t Loss: -0.011247186362743378\nTRAIN: \t Epoch: 94 \t Loss: -0.011161545545540073\nTRAIN: \t Epoch: 94 \t Loss: -0.011277188857396444\nTRAIN: \t Epoch: 94 \t Loss: -0.011422526091337204\nTRAIN: \t Epoch: 94 \t Loss: -0.011496175812291247\nTRAIN: \t Epoch: 94 \t Loss: -0.011483734163145224\nTRAIN: \t Epoch: 94 \t Loss: -0.011432113475166261\nTRAIN: \t Epoch: 94 \t Loss: -0.011374632718370241\nTRAIN: \t Epoch: 94 \t Loss: -0.011435984168201685\nTRAIN: \t Epoch: 94 \t Loss: -0.011500217853800246\nTRAIN: \t Epoch: 94 \t Loss: -0.011557026347145439\nTRAIN: \t Epoch: 94 \t Loss: -0.011525687539861315\nTRAIN: \t Epoch: 94 \t Loss: -0.011527333225339276\nVALD: \t Epoch: 94 \t Loss: -0.005153536796569824\nVALD: \t Epoch: 94 \t Loss: -0.0031039846944622695\nVALD: \t Epoch: 94 \t Loss: -0.0035636258544400334\nVALD: \t Epoch: 94 \t Loss: 0.005443408532300964\nVALD: \t Epoch: 94 \t Loss: 0.003000582964159548\nVALD: \t Epoch: 94 \t Loss: 0.0028170790861953387\n******************************\nEpoch: social-tag : 94\ntrain_loss -0.011527333225339276\nval_loss 0.0028170790861953387\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 95 \t Loss: -0.013023192062973976\nTRAIN: \t Epoch: 95 \t Loss: -0.012959541287273169\nTRAIN: \t Epoch: 95 \t Loss: -0.012577982308963934\nTRAIN: \t Epoch: 95 \t Loss: -0.012107676826417446\nTRAIN: \t Epoch: 95 \t Loss: -0.011950899474322796\nTRAIN: \t Epoch: 95 \t Loss: -0.011828007642179728\nTRAIN: \t Epoch: 95 \t Loss: -0.011803043340998036\nTRAIN: \t Epoch: 95 \t Loss: -0.0118092946941033\nTRAIN: \t Epoch: 95 \t Loss: -0.01197281065914366\nTRAIN: \t Epoch: 95 \t Loss: -0.011780128162354232\nTRAIN: \t Epoch: 95 \t Loss: -0.011544234796681187\nTRAIN: \t Epoch: 95 \t Loss: -0.011520543290923039\nTRAIN: \t Epoch: 95 \t Loss: -0.011557385755273012\nTRAIN: \t Epoch: 95 \t Loss: -0.011641183136297124\nTRAIN: \t Epoch: 95 \t Loss: -0.011677516872684162\nTRAIN: \t Epoch: 95 \t Loss: -0.0116515351110138\nTRAIN: \t Epoch: 95 \t Loss: -0.011607062728965984\nTRAIN: \t Epoch: 95 \t Loss: -0.011416951970507702\nTRAIN: \t Epoch: 95 \t Loss: -0.011341989138408712\nTRAIN: \t Epoch: 95 \t Loss: -0.011406461289152504\nTRAIN: \t Epoch: 95 \t Loss: -0.011482641145232179\nTRAIN: \t Epoch: 95 \t Loss: -0.011542817078116123\nVALD: \t Epoch: 95 \t Loss: -0.009650209918618202\nVALD: \t Epoch: 95 \t Loss: -0.007431998150423169\nVALD: \t Epoch: 95 \t Loss: -0.008546998258680105\nVALD: \t Epoch: 95 \t Loss: -0.0051396749913692474\nVALD: \t Epoch: 95 \t Loss: -0.005731003731489182\nVALD: \t Epoch: 95 \t Loss: -0.00561842013037566\n******************************\nEpoch: social-tag : 95\ntrain_loss -0.011542817078116123\nval_loss -0.00561842013037566\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 96 \t Loss: -0.0114844786003232\nTRAIN: \t Epoch: 96 \t Loss: -0.01202335860580206\nTRAIN: \t Epoch: 96 \t Loss: -0.012354481344421705\nTRAIN: \t Epoch: 96 \t Loss: -0.012337576132267714\nTRAIN: \t Epoch: 96 \t Loss: -0.012339287623763084\nTRAIN: \t Epoch: 96 \t Loss: -0.012314781080931425\nTRAIN: \t Epoch: 96 \t Loss: -0.01204983557441405\nTRAIN: \t Epoch: 96 \t Loss: -0.011857122299261391\nTRAIN: \t Epoch: 96 \t Loss: -0.011961003972424401\nTRAIN: \t Epoch: 96 \t Loss: -0.011971509642899036\nTRAIN: \t Epoch: 96 \t Loss: -0.01178599081256173\nTRAIN: \t Epoch: 96 \t Loss: -0.011570290274297198\nTRAIN: \t Epoch: 96 \t Loss: -0.011652770022360178\nTRAIN: \t Epoch: 96 \t Loss: -0.011837248623903309\nTRAIN: \t Epoch: 96 \t Loss: -0.011896086484193802\nTRAIN: \t Epoch: 96 \t Loss: -0.011774385289754719\nTRAIN: \t Epoch: 96 \t Loss: -0.011634637985159369\nTRAIN: \t Epoch: 96 \t Loss: -0.011677930700696178\nTRAIN: \t Epoch: 96 \t Loss: -0.011764466468440858\nTRAIN: \t Epoch: 96 \t Loss: -0.011849094461649657\nTRAIN: \t Epoch: 96 \t Loss: -0.011862688343084994\nTRAIN: \t Epoch: 96 \t Loss: -0.011846714387890252\nVALD: \t Epoch: 96 \t Loss: -0.009225534275174141\nVALD: \t Epoch: 96 \t Loss: -0.007198066683486104\nVALD: \t Epoch: 96 \t Loss: -0.008414858176062504\nVALD: \t Epoch: 96 \t Loss: -0.004433085210621357\nVALD: \t Epoch: 96 \t Loss: -0.0050907106138765815\nVALD: \t Epoch: 96 \t Loss: -0.004954315747388385\n******************************\nEpoch: social-tag : 96\ntrain_loss -0.011846714387890252\nval_loss -0.004954315747388385\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 97 \t Loss: -0.013081877492368221\nTRAIN: \t Epoch: 97 \t Loss: -0.013059457298368216\nTRAIN: \t Epoch: 97 \t Loss: -0.011596820938090483\nTRAIN: \t Epoch: 97 \t Loss: -0.010758460965007544\nTRAIN: \t Epoch: 97 \t Loss: -0.010861670225858688\nTRAIN: \t Epoch: 97 \t Loss: -0.011238272922734419\nTRAIN: \t Epoch: 97 \t Loss: -0.011447064711579255\nTRAIN: \t Epoch: 97 \t Loss: -0.01162444509100169\nTRAIN: \t Epoch: 97 \t Loss: -0.011710323807266023\nTRAIN: \t Epoch: 97 \t Loss: -0.011649954970926047\nTRAIN: \t Epoch: 97 \t Loss: -0.011546983129598877\nTRAIN: \t Epoch: 97 \t Loss: -0.011608360645671686\nTRAIN: \t Epoch: 97 \t Loss: -0.01169620962956777\nTRAIN: \t Epoch: 97 \t Loss: -0.011746095027774572\nTRAIN: \t Epoch: 97 \t Loss: -0.011642548503975074\nTRAIN: \t Epoch: 97 \t Loss: -0.011511475720908493\nTRAIN: \t Epoch: 97 \t Loss: -0.011577327845289427\nTRAIN: \t Epoch: 97 \t Loss: -0.011635643223093616\nTRAIN: \t Epoch: 97 \t Loss: -0.011714096377162557\nTRAIN: \t Epoch: 97 \t Loss: -0.011751565942540764\nTRAIN: \t Epoch: 97 \t Loss: -0.011782359597938401\nTRAIN: \t Epoch: 97 \t Loss: -0.011819934416930166\nVALD: \t Epoch: 97 \t Loss: 0.0026646913029253483\nVALD: \t Epoch: 97 \t Loss: 0.004621111787855625\nVALD: \t Epoch: 97 \t Loss: -0.00047116416196028393\nVALD: \t Epoch: 97 \t Loss: 0.006442840676754713\nVALD: \t Epoch: 97 \t Loss: 0.006022301781922579\nVALD: \t Epoch: 97 \t Loss: 0.0066904163721835976\n******************************\nEpoch: social-tag : 97\ntrain_loss -0.011819934416930166\nval_loss 0.0066904163721835976\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 98 \t Loss: -0.012142911553382874\nTRAIN: \t Epoch: 98 \t Loss: -0.010753918439149857\nTRAIN: \t Epoch: 98 \t Loss: -0.010197779474159082\nTRAIN: \t Epoch: 98 \t Loss: -0.010535466251894832\nTRAIN: \t Epoch: 98 \t Loss: -0.010870106518268585\nTRAIN: \t Epoch: 98 \t Loss: -0.0111539950594306\nTRAIN: \t Epoch: 98 \t Loss: -0.011223097864006246\nTRAIN: \t Epoch: 98 \t Loss: -0.010908196796663105\nTRAIN: \t Epoch: 98 \t Loss: -0.010891379891998239\nTRAIN: \t Epoch: 98 \t Loss: -0.011057155579328537\nTRAIN: \t Epoch: 98 \t Loss: -0.011175110343505035\nTRAIN: \t Epoch: 98 \t Loss: -0.011303077607105175\nTRAIN: \t Epoch: 98 \t Loss: -0.011354596282427128\nTRAIN: \t Epoch: 98 \t Loss: -0.011240248967494284\nTRAIN: \t Epoch: 98 \t Loss: -0.011095897170404594\nTRAIN: \t Epoch: 98 \t Loss: -0.011146826029289514\nTRAIN: \t Epoch: 98 \t Loss: -0.011268536012400599\nTRAIN: \t Epoch: 98 \t Loss: -0.0113953762791223\nTRAIN: \t Epoch: 98 \t Loss: -0.01147771313002235\nTRAIN: \t Epoch: 98 \t Loss: -0.01158094429410994\nTRAIN: \t Epoch: 98 \t Loss: -0.011600900530105545\nTRAIN: \t Epoch: 98 \t Loss: -0.011556885272424995\nVALD: \t Epoch: 98 \t Loss: -0.004051842261105776\nVALD: \t Epoch: 98 \t Loss: -0.0013247033348307014\nVALD: \t Epoch: 98 \t Loss: -0.002813533181324601\nVALD: \t Epoch: 98 \t Loss: 0.002445833117235452\nVALD: \t Epoch: 98 \t Loss: 0.000749799469485879\nVALD: \t Epoch: 98 \t Loss: 0.0007059774587325978\n******************************\nEpoch: social-tag : 98\ntrain_loss -0.011556885272424995\nval_loss 0.0007059774587325978\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 99 \t Loss: -0.011679382994771004\nTRAIN: \t Epoch: 99 \t Loss: -0.012367394752800465\nTRAIN: \t Epoch: 99 \t Loss: -0.012478532580037912\nTRAIN: \t Epoch: 99 \t Loss: -0.012335908832028508\nTRAIN: \t Epoch: 99 \t Loss: -0.012218068726360798\nTRAIN: \t Epoch: 99 \t Loss: -0.012155226121346155\nTRAIN: \t Epoch: 99 \t Loss: -0.01223161191280399\nTRAIN: \t Epoch: 99 \t Loss: -0.012139920378103852\nTRAIN: \t Epoch: 99 \t Loss: -0.01204362739291456\nTRAIN: \t Epoch: 99 \t Loss: -0.012104812264442443\nTRAIN: \t Epoch: 99 \t Loss: -0.012201190164143389\nTRAIN: \t Epoch: 99 \t Loss: -0.012066986256589493\nTRAIN: \t Epoch: 99 \t Loss: -0.011893127400141496\nTRAIN: \t Epoch: 99 \t Loss: -0.011889118435127395\nTRAIN: \t Epoch: 99 \t Loss: -0.0118730328977108\nTRAIN: \t Epoch: 99 \t Loss: -0.011788087955210358\nTRAIN: \t Epoch: 99 \t Loss: -0.011716305442592678\nTRAIN: \t Epoch: 99 \t Loss: -0.011694797418183751\nTRAIN: \t Epoch: 99 \t Loss: -0.011723090649435395\nTRAIN: \t Epoch: 99 \t Loss: -0.011686166236177086\nTRAIN: \t Epoch: 99 \t Loss: -0.011612716618747939\nTRAIN: \t Epoch: 99 \t Loss: -0.011617547690975389\nVALD: \t Epoch: 99 \t Loss: -0.00705749960616231\nVALD: \t Epoch: 99 \t Loss: -0.004406280000694096\nVALD: \t Epoch: 99 \t Loss: -0.0056831207281599445\nVALD: \t Epoch: 99 \t Loss: -0.0010004423675127327\nVALD: \t Epoch: 99 \t Loss: -0.0017160814721137285\nVALD: \t Epoch: 99 \t Loss: -0.0016455898766942097\n******************************\nEpoch: social-tag : 99\ntrain_loss -0.011617547690975389\nval_loss -0.0016455898766942097\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 100 \t Loss: -0.013343028724193573\nTRAIN: \t Epoch: 100 \t Loss: -0.01282955752685666\nTRAIN: \t Epoch: 100 \t Loss: -0.012871756528814634\nTRAIN: \t Epoch: 100 \t Loss: -0.01296003395691514\nTRAIN: \t Epoch: 100 \t Loss: -0.012748206034302711\nTRAIN: \t Epoch: 100 \t Loss: -0.012315052561461926\nTRAIN: \t Epoch: 100 \t Loss: -0.011898603423365526\nTRAIN: \t Epoch: 100 \t Loss: -0.011856078403070569\nTRAIN: \t Epoch: 100 \t Loss: -0.012010272902746996\nTRAIN: \t Epoch: 100 \t Loss: -0.012009704113006591\nTRAIN: \t Epoch: 100 \t Loss: -0.011971543289043686\nTRAIN: \t Epoch: 100 \t Loss: -0.012101948726922274\nTRAIN: \t Epoch: 100 \t Loss: -0.011985757316534337\nTRAIN: \t Epoch: 100 \t Loss: -0.011743774371487754\nTRAIN: \t Epoch: 100 \t Loss: -0.011733236784736315\nTRAIN: \t Epoch: 100 \t Loss: -0.011824124259874225\nTRAIN: \t Epoch: 100 \t Loss: -0.011901639292345327\nTRAIN: \t Epoch: 100 \t Loss: -0.011884456241710318\nTRAIN: \t Epoch: 100 \t Loss: -0.011762209873842565\nTRAIN: \t Epoch: 100 \t Loss: -0.011717603867873549\nTRAIN: \t Epoch: 100 \t Loss: -0.011752946967525142\nTRAIN: \t Epoch: 100 \t Loss: -0.011788337406178993\nVALD: \t Epoch: 100 \t Loss: 0.004422985017299652\nVALD: \t Epoch: 100 \t Loss: 0.008509084582328796\nVALD: \t Epoch: 100 \t Loss: 0.00503777041255186\nVALD: \t Epoch: 100 \t Loss: 0.013203904585679993\nVALD: \t Epoch: 100 \t Loss: 0.010185564053244889\nVALD: \t Epoch: 100 \t Loss: 0.010262458184451768\n******************************\nEpoch: social-tag : 100\ntrain_loss -0.011788337406178993\nval_loss 0.010262458184451768\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 101 \t Loss: -0.013204104267060757\nTRAIN: \t Epoch: 101 \t Loss: -0.012544519733637571\nTRAIN: \t Epoch: 101 \t Loss: -0.012143299914896488\nTRAIN: \t Epoch: 101 \t Loss: -0.011716464534401894\nTRAIN: \t Epoch: 101 \t Loss: -0.011873315088450909\nTRAIN: \t Epoch: 101 \t Loss: -0.012071435805410147\nTRAIN: \t Epoch: 101 \t Loss: -0.012085402251354285\nTRAIN: \t Epoch: 101 \t Loss: -0.011987675563432276\nTRAIN: \t Epoch: 101 \t Loss: -0.011861522682011127\nTRAIN: \t Epoch: 101 \t Loss: -0.011976488772779703\nTRAIN: \t Epoch: 101 \t Loss: -0.012051012367010117\nTRAIN: \t Epoch: 101 \t Loss: -0.012032517542441687\nTRAIN: \t Epoch: 101 \t Loss: -0.011981406535666723\nTRAIN: \t Epoch: 101 \t Loss: -0.011974794923194818\nTRAIN: \t Epoch: 101 \t Loss: -0.012011501317222914\nTRAIN: \t Epoch: 101 \t Loss: -0.012011145649012178\nTRAIN: \t Epoch: 101 \t Loss: -0.011969467131968807\nTRAIN: \t Epoch: 101 \t Loss: -0.011897702597909503\nTRAIN: \t Epoch: 101 \t Loss: -0.011853015834563657\nTRAIN: \t Epoch: 101 \t Loss: -0.011912483442574739\nTRAIN: \t Epoch: 101 \t Loss: -0.011921675387947332\nTRAIN: \t Epoch: 101 \t Loss: -0.011933756645106756\nVALD: \t Epoch: 101 \t Loss: -0.004082836676388979\nVALD: \t Epoch: 101 \t Loss: -0.0023326234077103436\nVALD: \t Epoch: 101 \t Loss: -0.004233359242789447\nVALD: \t Epoch: 101 \t Loss: 0.0018893525411840528\nVALD: \t Epoch: 101 \t Loss: 7.693476509302854e-05\nVALD: \t Epoch: 101 \t Loss: 3.849229577815894e-06\n******************************\nEpoch: social-tag : 101\ntrain_loss -0.011933756645106756\nval_loss 3.849229577815894e-06\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 102 \t Loss: -0.012600786052644253\nTRAIN: \t Epoch: 102 \t Loss: -0.012663140427321196\nTRAIN: \t Epoch: 102 \t Loss: -0.01232402833799521\nTRAIN: \t Epoch: 102 \t Loss: -0.011921222088858485\nTRAIN: \t Epoch: 102 \t Loss: -0.011642704531550408\nTRAIN: \t Epoch: 102 \t Loss: -0.011627862074722847\nTRAIN: \t Epoch: 102 \t Loss: -0.011780283679919583\nTRAIN: \t Epoch: 102 \t Loss: -0.011816677171736956\nTRAIN: \t Epoch: 102 \t Loss: -0.011713364885913001\nTRAIN: \t Epoch: 102 \t Loss: -0.011676736734807491\nTRAIN: \t Epoch: 102 \t Loss: -0.01159575962546197\nTRAIN: \t Epoch: 102 \t Loss: -0.01154626882635057\nTRAIN: \t Epoch: 102 \t Loss: -0.011543596521593057\nTRAIN: \t Epoch: 102 \t Loss: -0.011587115204227822\nTRAIN: \t Epoch: 102 \t Loss: -0.011595060427983602\nTRAIN: \t Epoch: 102 \t Loss: -0.01175095560029149\nTRAIN: \t Epoch: 102 \t Loss: -0.011752716484753525\nTRAIN: \t Epoch: 102 \t Loss: -0.011631352826952934\nTRAIN: \t Epoch: 102 \t Loss: -0.01160317963283313\nTRAIN: \t Epoch: 102 \t Loss: -0.011694838246330618\nTRAIN: \t Epoch: 102 \t Loss: -0.011758969963661261\nTRAIN: \t Epoch: 102 \t Loss: -0.011841743424722065\nVALD: \t Epoch: 102 \t Loss: -0.0007345107733272016\nVALD: \t Epoch: 102 \t Loss: 0.003361797920661047\nVALD: \t Epoch: 102 \t Loss: 0.0005481271267247697\nVALD: \t Epoch: 102 \t Loss: 0.00554656820895616\nVALD: \t Epoch: 102 \t Loss: 0.0034240293432958426\nVALD: \t Epoch: 102 \t Loss: 0.003269881625292879\n******************************\nEpoch: social-tag : 102\ntrain_loss -0.011841743424722065\nval_loss 0.003269881625292879\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 103 \t Loss: -0.013054161332547665\nTRAIN: \t Epoch: 103 \t Loss: -0.012867206241935492\nTRAIN: \t Epoch: 103 \t Loss: -0.012011435814201832\nTRAIN: \t Epoch: 103 \t Loss: -0.01171354716643691\nTRAIN: \t Epoch: 103 \t Loss: -0.011836089752614498\nTRAIN: \t Epoch: 103 \t Loss: -0.011991679668426514\nTRAIN: \t Epoch: 103 \t Loss: -0.01204059539096696\nTRAIN: \t Epoch: 103 \t Loss: -0.011935808113776147\nTRAIN: \t Epoch: 103 \t Loss: -0.011681371885869238\nTRAIN: \t Epoch: 103 \t Loss: -0.011601513531059027\nTRAIN: \t Epoch: 103 \t Loss: -0.011636717329648409\nTRAIN: \t Epoch: 103 \t Loss: -0.011726016954829296\nTRAIN: \t Epoch: 103 \t Loss: -0.011805791766024552\nTRAIN: \t Epoch: 103 \t Loss: -0.011907502343612058\nTRAIN: \t Epoch: 103 \t Loss: -0.011793233826756478\nTRAIN: \t Epoch: 103 \t Loss: -0.011652537272311747\nTRAIN: \t Epoch: 103 \t Loss: -0.011693055193652125\nTRAIN: \t Epoch: 103 \t Loss: -0.011813951811442772\nTRAIN: \t Epoch: 103 \t Loss: -0.011860956437885761\nTRAIN: \t Epoch: 103 \t Loss: -0.011955014802515507\nTRAIN: \t Epoch: 103 \t Loss: -0.01189443642007453\nTRAIN: \t Epoch: 103 \t Loss: -0.01174349371791956\nVALD: \t Epoch: 103 \t Loss: -0.009741763584315777\nVALD: \t Epoch: 103 \t Loss: -0.007930746302008629\nVALD: \t Epoch: 103 \t Loss: -0.00897085169951121\nVALD: \t Epoch: 103 \t Loss: -0.005895490234252065\nVALD: \t Epoch: 103 \t Loss: -0.006488341046497226\nVALD: \t Epoch: 103 \t Loss: -0.006425072839765838\n******************************\nEpoch: social-tag : 103\ntrain_loss -0.01174349371791956\nval_loss -0.006425072839765838\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 104 \t Loss: -0.011982720345258713\nTRAIN: \t Epoch: 104 \t Loss: -0.01236811000853777\nTRAIN: \t Epoch: 104 \t Loss: -0.012690653403600058\nTRAIN: \t Epoch: 104 \t Loss: -0.01260682218708098\nTRAIN: \t Epoch: 104 \t Loss: -0.01249667927622795\nTRAIN: \t Epoch: 104 \t Loss: -0.012349743240823349\nTRAIN: \t Epoch: 104 \t Loss: -0.012194382159837655\nTRAIN: \t Epoch: 104 \t Loss: -0.012036291533149779\nTRAIN: \t Epoch: 104 \t Loss: -0.01205621835672193\nTRAIN: \t Epoch: 104 \t Loss: -0.012116501294076442\nTRAIN: \t Epoch: 104 \t Loss: -0.012207511135122993\nTRAIN: \t Epoch: 104 \t Loss: -0.012095231640463075\nTRAIN: \t Epoch: 104 \t Loss: -0.012005894158322077\nTRAIN: \t Epoch: 104 \t Loss: -0.01204281859099865\nTRAIN: \t Epoch: 104 \t Loss: -0.012158583539227644\nTRAIN: \t Epoch: 104 \t Loss: -0.012194665148854256\nTRAIN: \t Epoch: 104 \t Loss: -0.012039288166253007\nTRAIN: \t Epoch: 104 \t Loss: -0.011890524222205082\nTRAIN: \t Epoch: 104 \t Loss: -0.011894272395262593\nTRAIN: \t Epoch: 104 \t Loss: -0.011936095543205738\nTRAIN: \t Epoch: 104 \t Loss: -0.01199812295713595\nTRAIN: \t Epoch: 104 \t Loss: -0.012069043864895669\nVALD: \t Epoch: 104 \t Loss: -0.008695398457348347\nVALD: \t Epoch: 104 \t Loss: -0.0047524757392238826\nVALD: \t Epoch: 104 \t Loss: -0.007002242200542241\nVALD: \t Epoch: 104 \t Loss: 0.00011950430052820593\nVALD: \t Epoch: 104 \t Loss: -0.0011101839947514236\nVALD: \t Epoch: 104 \t Loss: -0.0010013121942227536\n******************************\nEpoch: social-tag : 104\ntrain_loss -0.012069043864895669\nval_loss -0.0010013121942227536\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 105 \t Loss: -0.013048362918198109\nTRAIN: \t Epoch: 105 \t Loss: -0.013240408152341843\nTRAIN: \t Epoch: 105 \t Loss: -0.012773622448245684\nTRAIN: \t Epoch: 105 \t Loss: -0.011524195899255574\nTRAIN: \t Epoch: 105 \t Loss: -0.010983114410191775\nTRAIN: \t Epoch: 105 \t Loss: -0.01124858926050365\nTRAIN: \t Epoch: 105 \t Loss: -0.011489642318338156\nTRAIN: \t Epoch: 105 \t Loss: -0.011565172404516488\nTRAIN: \t Epoch: 105 \t Loss: -0.011668798772411214\nTRAIN: \t Epoch: 105 \t Loss: -0.011802848102524877\nTRAIN: \t Epoch: 105 \t Loss: -0.011731950354508379\nTRAIN: \t Epoch: 105 \t Loss: -0.011591256712563336\nTRAIN: \t Epoch: 105 \t Loss: -0.011536086730372448\nTRAIN: \t Epoch: 105 \t Loss: -0.011661010162372674\nTRAIN: \t Epoch: 105 \t Loss: -0.011749788218488296\nTRAIN: \t Epoch: 105 \t Loss: -0.011736820888472721\nTRAIN: \t Epoch: 105 \t Loss: -0.01177609626970747\nTRAIN: \t Epoch: 105 \t Loss: -0.011737456053702367\nTRAIN: \t Epoch: 105 \t Loss: -0.011737658127554153\nTRAIN: \t Epoch: 105 \t Loss: -0.011729592201299965\nTRAIN: \t Epoch: 105 \t Loss: -0.011787270439700001\nTRAIN: \t Epoch: 105 \t Loss: -0.01178764190451247\nVALD: \t Epoch: 105 \t Loss: -0.002676114672794938\nVALD: \t Epoch: 105 \t Loss: -0.0007039349293336272\nVALD: \t Epoch: 105 \t Loss: -0.0032078411895781755\nVALD: \t Epoch: 105 \t Loss: 0.003048697079066187\nVALD: \t Epoch: 105 \t Loss: 0.0009006178472191096\nVALD: \t Epoch: 105 \t Loss: 0.0007792074346181118\n******************************\nEpoch: social-tag : 105\ntrain_loss -0.01178764190451247\nval_loss 0.0007792074346181118\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 106 \t Loss: -0.012921527028083801\nTRAIN: \t Epoch: 106 \t Loss: -0.012576649896800518\nTRAIN: \t Epoch: 106 \t Loss: -0.012427391484379768\nTRAIN: \t Epoch: 106 \t Loss: -0.011702961288392544\nTRAIN: \t Epoch: 106 \t Loss: -0.011676859110593796\nTRAIN: \t Epoch: 106 \t Loss: -0.011922433196256558\nTRAIN: \t Epoch: 106 \t Loss: -0.012035735350634371\nTRAIN: \t Epoch: 106 \t Loss: -0.012163118808530271\nTRAIN: \t Epoch: 106 \t Loss: -0.01228403030998177\nTRAIN: \t Epoch: 106 \t Loss: -0.012161205150187015\nTRAIN: \t Epoch: 106 \t Loss: -0.0119969556954774\nTRAIN: \t Epoch: 106 \t Loss: -0.01189679877522091\nTRAIN: \t Epoch: 106 \t Loss: -0.011983798673519721\nTRAIN: \t Epoch: 106 \t Loss: -0.012030179213200296\nTRAIN: \t Epoch: 106 \t Loss: -0.011955815243224303\nTRAIN: \t Epoch: 106 \t Loss: -0.011957437323872\nTRAIN: \t Epoch: 106 \t Loss: -0.011957101852578275\nTRAIN: \t Epoch: 106 \t Loss: -0.011971168789184757\nTRAIN: \t Epoch: 106 \t Loss: -0.011935774383968428\nTRAIN: \t Epoch: 106 \t Loss: -0.011947830952703953\nTRAIN: \t Epoch: 106 \t Loss: -0.011973872071220762\nTRAIN: \t Epoch: 106 \t Loss: -0.012013353394006698\nVALD: \t Epoch: 106 \t Loss: -0.0005113103543408215\nVALD: \t Epoch: 106 \t Loss: 0.002342907333513722\nVALD: \t Epoch: 106 \t Loss: -0.0006065788717630008\nVALD: \t Epoch: 106 \t Loss: 0.006163483645650558\nVALD: \t Epoch: 106 \t Loss: 0.004028869734611362\nVALD: \t Epoch: 106 \t Loss: 0.004058514467694543\n******************************\nEpoch: social-tag : 106\ntrain_loss -0.012013353394006698\nval_loss 0.004058514467694543\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 107 \t Loss: -0.012687602080404758\nTRAIN: \t Epoch: 107 \t Loss: -0.011921160388737917\nTRAIN: \t Epoch: 107 \t Loss: -0.011606005020439625\nTRAIN: \t Epoch: 107 \t Loss: -0.01183665543794632\nTRAIN: \t Epoch: 107 \t Loss: -0.011882334016263485\nTRAIN: \t Epoch: 107 \t Loss: -0.01198814498881499\nTRAIN: \t Epoch: 107 \t Loss: -0.011976837180554867\nTRAIN: \t Epoch: 107 \t Loss: -0.011863541440106928\nTRAIN: \t Epoch: 107 \t Loss: -0.011763873406582408\nTRAIN: \t Epoch: 107 \t Loss: -0.011765706911683082\nTRAIN: \t Epoch: 107 \t Loss: -0.011806264181028713\nTRAIN: \t Epoch: 107 \t Loss: -0.01190015890945991\nTRAIN: \t Epoch: 107 \t Loss: -0.011970039815283738\nTRAIN: \t Epoch: 107 \t Loss: -0.011964207581643547\nTRAIN: \t Epoch: 107 \t Loss: -0.01199325689425071\nTRAIN: \t Epoch: 107 \t Loss: -0.012046317046042532\nTRAIN: \t Epoch: 107 \t Loss: -0.012099176207009484\nTRAIN: \t Epoch: 107 \t Loss: -0.012018173622588316\nTRAIN: \t Epoch: 107 \t Loss: -0.01192268234138426\nTRAIN: \t Epoch: 107 \t Loss: -0.011928249476477503\nTRAIN: \t Epoch: 107 \t Loss: -0.012023345877726873\nTRAIN: \t Epoch: 107 \t Loss: -0.012088196453115029\nVALD: \t Epoch: 107 \t Loss: 0.00011294397700112313\nVALD: \t Epoch: 107 \t Loss: 0.0008632986209704541\nVALD: \t Epoch: 107 \t Loss: -0.003009047771532399\nVALD: \t Epoch: 107 \t Loss: 0.006760773871064885\nVALD: \t Epoch: 107 \t Loss: 0.004413177384412848\nVALD: \t Epoch: 107 \t Loss: 0.004544216741553761\n******************************\nEpoch: social-tag : 107\ntrain_loss -0.012088196453115029\nval_loss 0.004544216741553761\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 108 \t Loss: -0.013504073023796082\nTRAIN: \t Epoch: 108 \t Loss: -0.013614948373287916\nTRAIN: \t Epoch: 108 \t Loss: -0.013025801628828049\nTRAIN: \t Epoch: 108 \t Loss: -0.012139925267547369\nTRAIN: \t Epoch: 108 \t Loss: -0.011603136546909809\nTRAIN: \t Epoch: 108 \t Loss: -0.011898645044614872\nTRAIN: \t Epoch: 108 \t Loss: -0.012006915174424648\nTRAIN: \t Epoch: 108 \t Loss: -0.012216422357596457\nTRAIN: \t Epoch: 108 \t Loss: -0.012191329151391983\nTRAIN: \t Epoch: 108 \t Loss: -0.012130706664174795\nTRAIN: \t Epoch: 108 \t Loss: -0.012044981884008104\nTRAIN: \t Epoch: 108 \t Loss: -0.012016380360970894\nTRAIN: \t Epoch: 108 \t Loss: -0.01206201988344009\nTRAIN: \t Epoch: 108 \t Loss: -0.012063746939280204\nTRAIN: \t Epoch: 108 \t Loss: -0.011927532342573008\nTRAIN: \t Epoch: 108 \t Loss: -0.011883482104167342\nTRAIN: \t Epoch: 108 \t Loss: -0.011952884056988885\nTRAIN: \t Epoch: 108 \t Loss: -0.012045401614159346\nTRAIN: \t Epoch: 108 \t Loss: -0.012052278573575773\nTRAIN: \t Epoch: 108 \t Loss: -0.011965398164466023\nTRAIN: \t Epoch: 108 \t Loss: -0.011864967378122466\nTRAIN: \t Epoch: 108 \t Loss: -0.011895542812518629\nVALD: \t Epoch: 108 \t Loss: -0.006071907002478838\nVALD: \t Epoch: 108 \t Loss: -0.002057925215922296\nVALD: \t Epoch: 108 \t Loss: -0.0036185485466072955\nVALD: \t Epoch: 108 \t Loss: -0.001985355163924396\nVALD: \t Epoch: 108 \t Loss: -0.0028815707191824913\nVALD: \t Epoch: 108 \t Loss: -0.002837006931164951\n******************************\nEpoch: social-tag : 108\ntrain_loss -0.011895542812518629\nval_loss -0.002837006931164951\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 109 \t Loss: -0.013485974632203579\nTRAIN: \t Epoch: 109 \t Loss: -0.013537187594920397\nTRAIN: \t Epoch: 109 \t Loss: -0.013453926580647627\nTRAIN: \t Epoch: 109 \t Loss: -0.013302333187311888\nTRAIN: \t Epoch: 109 \t Loss: -0.01335310060530901\nTRAIN: \t Epoch: 109 \t Loss: -0.013133847465117773\nTRAIN: \t Epoch: 109 \t Loss: -0.012898590548762254\nTRAIN: \t Epoch: 109 \t Loss: -0.01287046424113214\nTRAIN: \t Epoch: 109 \t Loss: -0.013039911786715189\nTRAIN: \t Epoch: 109 \t Loss: -0.012966054771095515\nTRAIN: \t Epoch: 109 \t Loss: -0.012725468978963116\nTRAIN: \t Epoch: 109 \t Loss: -0.01254973995188872\nTRAIN: \t Epoch: 109 \t Loss: -0.012584251423294727\nTRAIN: \t Epoch: 109 \t Loss: -0.012628132543925728\nTRAIN: \t Epoch: 109 \t Loss: -0.012617871227363745\nTRAIN: \t Epoch: 109 \t Loss: -0.012668268289417028\nTRAIN: \t Epoch: 109 \t Loss: -0.01237338622484137\nTRAIN: \t Epoch: 109 \t Loss: -0.012086650227299996\nTRAIN: \t Epoch: 109 \t Loss: -0.012050603491891371\nTRAIN: \t Epoch: 109 \t Loss: -0.012079045944847166\nTRAIN: \t Epoch: 109 \t Loss: -0.012132946290962753\nTRAIN: \t Epoch: 109 \t Loss: -0.01216242379731293\nVALD: \t Epoch: 109 \t Loss: 0.003717482089996338\nVALD: \t Epoch: 109 \t Loss: 0.005185165908187628\nVALD: \t Epoch: 109 \t Loss: 0.0019443809675673644\nVALD: \t Epoch: 109 \t Loss: 0.010685241199098527\nVALD: \t Epoch: 109 \t Loss: 0.007797067565843463\nVALD: \t Epoch: 109 \t Loss: 0.007737755967360554\n******************************\nEpoch: social-tag : 109\ntrain_loss -0.01216242379731293\nval_loss 0.007737755967360554\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 110 \t Loss: -0.01235190499573946\nTRAIN: \t Epoch: 110 \t Loss: -0.013616336043924093\nTRAIN: \t Epoch: 110 \t Loss: -0.012744656763970852\nTRAIN: \t Epoch: 110 \t Loss: -0.01198628218844533\nTRAIN: \t Epoch: 110 \t Loss: -0.012050115130841732\nTRAIN: \t Epoch: 110 \t Loss: -0.012257769703865051\nTRAIN: \t Epoch: 110 \t Loss: -0.012287870049476624\nTRAIN: \t Epoch: 110 \t Loss: -0.012086662696674466\nTRAIN: \t Epoch: 110 \t Loss: -0.0121149273796214\nTRAIN: \t Epoch: 110 \t Loss: -0.01217976463958621\nTRAIN: \t Epoch: 110 \t Loss: -0.01218833097002723\nTRAIN: \t Epoch: 110 \t Loss: -0.012167656871800622\nTRAIN: \t Epoch: 110 \t Loss: -0.0120618837670638\nTRAIN: \t Epoch: 110 \t Loss: -0.01203086247135486\nTRAIN: \t Epoch: 110 \t Loss: -0.012161789213617642\nTRAIN: \t Epoch: 110 \t Loss: -0.012198555108625442\nTRAIN: \t Epoch: 110 \t Loss: -0.012110379031475852\nTRAIN: \t Epoch: 110 \t Loss: -0.012076375333385335\nTRAIN: \t Epoch: 110 \t Loss: -0.012077321592522295\nTRAIN: \t Epoch: 110 \t Loss: -0.012029090942814947\nTRAIN: \t Epoch: 110 \t Loss: -0.012001363117070426\nTRAIN: \t Epoch: 110 \t Loss: -0.012042747708154538\nVALD: \t Epoch: 110 \t Loss: -0.006206674035638571\nVALD: \t Epoch: 110 \t Loss: -0.003224009917175863\nVALD: \t Epoch: 110 \t Loss: -0.00481982671287066\nVALD: \t Epoch: 110 \t Loss: 0.0007369526247202884\nVALD: \t Epoch: 110 \t Loss: -0.0009036897827172652\nVALD: \t Epoch: 110 \t Loss: -0.0008041406247877714\n******************************\nEpoch: social-tag : 110\ntrain_loss -0.012042747708154538\nval_loss -0.0008041406247877714\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 111 \t Loss: -0.013209527358412743\nTRAIN: \t Epoch: 111 \t Loss: -0.01289995713159442\nTRAIN: \t Epoch: 111 \t Loss: -0.01298491812000672\nTRAIN: \t Epoch: 111 \t Loss: -0.013059840304777026\nTRAIN: \t Epoch: 111 \t Loss: -0.012725453823804855\nTRAIN: \t Epoch: 111 \t Loss: -0.012160971139868101\nTRAIN: \t Epoch: 111 \t Loss: -0.011892063011016165\nTRAIN: \t Epoch: 111 \t Loss: -0.012013152707368135\nTRAIN: \t Epoch: 111 \t Loss: -0.01218780575113164\nTRAIN: \t Epoch: 111 \t Loss: -0.012174920924007893\nTRAIN: \t Epoch: 111 \t Loss: -0.012155751359056343\nTRAIN: \t Epoch: 111 \t Loss: -0.012126240956907472\nTRAIN: \t Epoch: 111 \t Loss: -0.012171190017117904\nTRAIN: \t Epoch: 111 \t Loss: -0.012154775405568736\nTRAIN: \t Epoch: 111 \t Loss: -0.012157984636723995\nTRAIN: \t Epoch: 111 \t Loss: -0.012158278026618063\nTRAIN: \t Epoch: 111 \t Loss: -0.012187436880434261\nTRAIN: \t Epoch: 111 \t Loss: -0.012191067708449231\nTRAIN: \t Epoch: 111 \t Loss: -0.01221064492864044\nTRAIN: \t Epoch: 111 \t Loss: -0.012228004541248083\nTRAIN: \t Epoch: 111 \t Loss: -0.0122427200188949\nTRAIN: \t Epoch: 111 \t Loss: -0.012248486933203103\nVALD: \t Epoch: 111 \t Loss: -0.005606533493846655\nVALD: \t Epoch: 111 \t Loss: -0.003591796034015715\nVALD: \t Epoch: 111 \t Loss: -0.006027240073308349\nVALD: \t Epoch: 111 \t Loss: 0.0014522030833177269\nVALD: \t Epoch: 111 \t Loss: -2.159378491342068e-05\nVALD: \t Epoch: 111 \t Loss: 0.0002077551953720324\n******************************\nEpoch: social-tag : 111\ntrain_loss -0.012248486933203103\nval_loss 0.0002077551953720324\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 112 \t Loss: -0.013602412305772305\nTRAIN: \t Epoch: 112 \t Loss: -0.012768908869475126\nTRAIN: \t Epoch: 112 \t Loss: -0.012434175238013268\nTRAIN: \t Epoch: 112 \t Loss: -0.011940216412767768\nTRAIN: \t Epoch: 112 \t Loss: -0.012059752829372883\nTRAIN: \t Epoch: 112 \t Loss: -0.012194582261145115\nTRAIN: \t Epoch: 112 \t Loss: -0.012154489223446165\nTRAIN: \t Epoch: 112 \t Loss: -0.012010693782940507\nTRAIN: \t Epoch: 112 \t Loss: -0.012055784567362733\nTRAIN: \t Epoch: 112 \t Loss: -0.012080858368426562\nTRAIN: \t Epoch: 112 \t Loss: -0.012131909501146187\nTRAIN: \t Epoch: 112 \t Loss: -0.012178257185344895\nTRAIN: \t Epoch: 112 \t Loss: -0.012223462717464337\nTRAIN: \t Epoch: 112 \t Loss: -0.0122019755654037\nTRAIN: \t Epoch: 112 \t Loss: -0.012048668476442497\nTRAIN: \t Epoch: 112 \t Loss: -0.01202123996336013\nTRAIN: \t Epoch: 112 \t Loss: -0.012097790453802137\nTRAIN: \t Epoch: 112 \t Loss: -0.012159476367135843\nTRAIN: \t Epoch: 112 \t Loss: -0.012150846519752553\nTRAIN: \t Epoch: 112 \t Loss: -0.012099970737472177\nTRAIN: \t Epoch: 112 \t Loss: -0.012147576991646062\nTRAIN: \t Epoch: 112 \t Loss: -0.012173062209809063\nVALD: \t Epoch: 112 \t Loss: -0.010709145106375217\nVALD: \t Epoch: 112 \t Loss: -0.006739851902239025\nVALD: \t Epoch: 112 \t Loss: -0.008130838085586825\nVALD: \t Epoch: 112 \t Loss: -0.003010634274687618\nVALD: \t Epoch: 112 \t Loss: -0.00428117117844522\nVALD: \t Epoch: 112 \t Loss: -0.004247251638408864\n******************************\nEpoch: social-tag : 112\ntrain_loss -0.012173062209809063\nval_loss -0.004247251638408864\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 113 \t Loss: -0.012550587765872478\nTRAIN: \t Epoch: 113 \t Loss: -0.012545967474579811\nTRAIN: \t Epoch: 113 \t Loss: -0.012398951376477877\nTRAIN: \t Epoch: 113 \t Loss: -0.012283979216590524\nTRAIN: \t Epoch: 113 \t Loss: -0.012295262515544891\nTRAIN: \t Epoch: 113 \t Loss: -0.012402488694836697\nTRAIN: \t Epoch: 113 \t Loss: -0.012526366859674454\nTRAIN: \t Epoch: 113 \t Loss: -0.012305432232096791\nTRAIN: \t Epoch: 113 \t Loss: -0.01205728616979387\nTRAIN: \t Epoch: 113 \t Loss: -0.01204955531284213\nTRAIN: \t Epoch: 113 \t Loss: -0.01202964283187281\nTRAIN: \t Epoch: 113 \t Loss: -0.012030063119406501\nTRAIN: \t Epoch: 113 \t Loss: -0.012151444474091897\nTRAIN: \t Epoch: 113 \t Loss: -0.012134943223957504\nTRAIN: \t Epoch: 113 \t Loss: -0.012050808904071649\nTRAIN: \t Epoch: 113 \t Loss: -0.012053152662701905\nTRAIN: \t Epoch: 113 \t Loss: -0.012159632957156967\nTRAIN: \t Epoch: 113 \t Loss: -0.012191433252559768\nTRAIN: \t Epoch: 113 \t Loss: -0.012262447786174323\nTRAIN: \t Epoch: 113 \t Loss: -0.01233115759678185\nTRAIN: \t Epoch: 113 \t Loss: -0.012252177998778365\nTRAIN: \t Epoch: 113 \t Loss: -0.012135106806695141\nVALD: \t Epoch: 113 \t Loss: -0.003462489927187562\nVALD: \t Epoch: 113 \t Loss: 0.0009132159175351262\nVALD: \t Epoch: 113 \t Loss: -0.00036909958968559903\nVALD: \t Epoch: 113 \t Loss: 0.010587829863652587\nVALD: \t Epoch: 113 \t Loss: 0.007227101270109415\nVALD: \t Epoch: 113 \t Loss: 0.006961375824881323\n******************************\nEpoch: social-tag : 113\ntrain_loss -0.012135106806695141\nval_loss 0.006961375824881323\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 114 \t Loss: -0.011799884960055351\nTRAIN: \t Epoch: 114 \t Loss: -0.012180606834590435\nTRAIN: \t Epoch: 114 \t Loss: -0.012649894381562868\nTRAIN: \t Epoch: 114 \t Loss: -0.012851167703047395\nTRAIN: \t Epoch: 114 \t Loss: -0.012564966455101967\nTRAIN: \t Epoch: 114 \t Loss: -0.011881858265648285\nTRAIN: \t Epoch: 114 \t Loss: -0.011674931006772178\nTRAIN: \t Epoch: 114 \t Loss: -0.011923323734663427\nTRAIN: \t Epoch: 114 \t Loss: -0.012001564933194054\nTRAIN: \t Epoch: 114 \t Loss: -0.012097364291548729\nTRAIN: \t Epoch: 114 \t Loss: -0.01212747564369982\nTRAIN: \t Epoch: 114 \t Loss: -0.012024369866897663\nTRAIN: \t Epoch: 114 \t Loss: -0.011927180350399934\nTRAIN: \t Epoch: 114 \t Loss: -0.01198993922610368\nTRAIN: \t Epoch: 114 \t Loss: -0.011991721764206886\nTRAIN: \t Epoch: 114 \t Loss: -0.012092392717022449\nTRAIN: \t Epoch: 114 \t Loss: -0.012203792822273338\nTRAIN: \t Epoch: 114 \t Loss: -0.01219422116668688\nTRAIN: \t Epoch: 114 \t Loss: -0.012277109352381606\nTRAIN: \t Epoch: 114 \t Loss: -0.012245655385777354\nTRAIN: \t Epoch: 114 \t Loss: -0.01215776184662467\nTRAIN: \t Epoch: 114 \t Loss: -0.012136941195818424\nVALD: \t Epoch: 114 \t Loss: -0.011787673458456993\nVALD: \t Epoch: 114 \t Loss: -0.008578921435400844\nVALD: \t Epoch: 114 \t Loss: -0.009392728252957264\nVALD: \t Epoch: 114 \t Loss: -0.0012591093545779586\nVALD: \t Epoch: 114 \t Loss: -0.0027841058559715747\nVALD: \t Epoch: 114 \t Loss: -0.002807356213981455\n******************************\nEpoch: social-tag : 114\ntrain_loss -0.012136941195818424\nval_loss -0.002807356213981455\n{'min_val_epoch': 86, 'min_val_loss': -0.006450308103001479}\n******************************\nTRAIN: \t Epoch: 115 \t Loss: -0.0127338245511055\nTRAIN: \t Epoch: 115 \t Loss: -0.013114867731928825\nTRAIN: \t Epoch: 115 \t Loss: -0.012816059403121471\nTRAIN: \t Epoch: 115 \t Loss: -0.012931313598528504\nTRAIN: \t Epoch: 115 \t Loss: -0.013033406808972359\nTRAIN: \t Epoch: 115 \t Loss: -0.012818236369639635\nTRAIN: \t Epoch: 115 \t Loss: -0.012646361919386047\nTRAIN: \t Epoch: 115 \t Loss: -0.012499762582592666\nTRAIN: \t Epoch: 115 \t Loss: -0.01254381187674072\nTRAIN: \t Epoch: 115 \t Loss: -0.012601609621196986\nTRAIN: \t Epoch: 115 \t Loss: -0.01253314900465987\nTRAIN: \t Epoch: 115 \t Loss: -0.01241139144015809\nTRAIN: \t Epoch: 115 \t Loss: -0.012350169655222159\nTRAIN: \t Epoch: 115 \t Loss: -0.012379787263593503\nTRAIN: \t Epoch: 115 \t Loss: -0.01249331006159385\nTRAIN: \t Epoch: 115 \t Loss: -0.012553557578939945\nTRAIN: \t Epoch: 115 \t Loss: -0.012497375061845077\nTRAIN: \t Epoch: 115 \t Loss: -0.012420821417537\nTRAIN: \t Epoch: 115 \t Loss: -0.012445546294513502\nTRAIN: \t Epoch: 115 \t Loss: -0.012492961343377828\nTRAIN: \t Epoch: 115 \t Loss: -0.012491775987048944\nTRAIN: \t Epoch: 115 \t Loss: -0.012440763940931009\nVALD: \t Epoch: 115 \t Loss: -0.009748592972755432\nVALD: \t Epoch: 115 \t Loss: -0.007189185358583927\nVALD: \t Epoch: 115 \t Loss: -0.007577133364975452\nVALD: \t Epoch: 115 \t Loss: -0.007844389881938696\nVALD: \t Epoch: 115 \t Loss: -0.007829166576266288\nVALD: \t Epoch: 115 \t Loss: -0.007709600539370017\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 70/70 [00:02<00:00, 25.98it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.9093217921154171  FDE: 1.4357146606614044\n**************************************************\n******************************\nEpoch: social-tag : 115\ntrain_loss -0.012440763940931009\nval_loss -0.007709600539370017\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 116 \t Loss: -0.012574545107781887\nTRAIN: \t Epoch: 116 \t Loss: -0.012848238460719585\nTRAIN: \t Epoch: 116 \t Loss: -0.013043818684915701\nTRAIN: \t Epoch: 116 \t Loss: -0.012452329508960247\nTRAIN: \t Epoch: 116 \t Loss: -0.011914950050413608\nTRAIN: \t Epoch: 116 \t Loss: -0.012021400034427643\nTRAIN: \t Epoch: 116 \t Loss: -0.012176100430744035\nTRAIN: \t Epoch: 116 \t Loss: -0.012259699054993689\nTRAIN: \t Epoch: 116 \t Loss: -0.012262418452236388\nTRAIN: \t Epoch: 116 \t Loss: -0.012093075923621655\nTRAIN: \t Epoch: 116 \t Loss: -0.011916487477719784\nTRAIN: \t Epoch: 116 \t Loss: -0.011891651432961226\nTRAIN: \t Epoch: 116 \t Loss: -0.011934426326591235\nTRAIN: \t Epoch: 116 \t Loss: -0.01204954566700118\nTRAIN: \t Epoch: 116 \t Loss: -0.012101384003957112\nTRAIN: \t Epoch: 116 \t Loss: -0.012069092830643058\nTRAIN: \t Epoch: 116 \t Loss: -0.011991131228997427\nTRAIN: \t Epoch: 116 \t Loss: -0.011997043258614011\nTRAIN: \t Epoch: 116 \t Loss: -0.012095853049111994\nTRAIN: \t Epoch: 116 \t Loss: -0.012121228035539389\nTRAIN: \t Epoch: 116 \t Loss: -0.012124715372920036\nTRAIN: \t Epoch: 116 \t Loss: -0.012097143955675875\nVALD: \t Epoch: 116 \t Loss: -0.007600443437695503\nVALD: \t Epoch: 116 \t Loss: -0.004645587294362485\nVALD: \t Epoch: 116 \t Loss: -0.005963254331921537\nVALD: \t Epoch: 116 \t Loss: -0.0017592324293218553\nVALD: \t Epoch: 116 \t Loss: -0.0029325923416763542\nVALD: \t Epoch: 116 \t Loss: -0.0029932768055886934\n******************************\nEpoch: social-tag : 116\ntrain_loss -0.012097143955675875\nval_loss -0.0029932768055886934\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 117 \t Loss: -0.013437771238386631\nTRAIN: \t Epoch: 117 \t Loss: -0.013270020950585604\nTRAIN: \t Epoch: 117 \t Loss: -0.013470893415311972\nTRAIN: \t Epoch: 117 \t Loss: -0.013045142171904445\nTRAIN: \t Epoch: 117 \t Loss: -0.012654453702270984\nTRAIN: \t Epoch: 117 \t Loss: -0.012741359882056713\nTRAIN: \t Epoch: 117 \t Loss: -0.012833800565983568\nTRAIN: \t Epoch: 117 \t Loss: -0.01278927642852068\nTRAIN: \t Epoch: 117 \t Loss: -0.012688436028030183\nTRAIN: \t Epoch: 117 \t Loss: -0.012571656610816717\nTRAIN: \t Epoch: 117 \t Loss: -0.012593664567578922\nTRAIN: \t Epoch: 117 \t Loss: -0.01262515721221765\nTRAIN: \t Epoch: 117 \t Loss: -0.01259713713079691\nTRAIN: \t Epoch: 117 \t Loss: -0.01244316529482603\nTRAIN: \t Epoch: 117 \t Loss: -0.01241051834076643\nTRAIN: \t Epoch: 117 \t Loss: -0.012462741928175092\nTRAIN: \t Epoch: 117 \t Loss: -0.012532706701141946\nTRAIN: \t Epoch: 117 \t Loss: -0.01257025393553906\nTRAIN: \t Epoch: 117 \t Loss: -0.012425785394091355\nTRAIN: \t Epoch: 117 \t Loss: -0.012247108109295368\nTRAIN: \t Epoch: 117 \t Loss: -0.012227388631020273\nTRAIN: \t Epoch: 117 \t Loss: -0.012267924339595775\nVALD: \t Epoch: 117 \t Loss: -0.007550142239779234\nVALD: \t Epoch: 117 \t Loss: -0.004250074300216511\nVALD: \t Epoch: 117 \t Loss: -0.006340009800624102\nVALD: \t Epoch: 117 \t Loss: 0.004633120013750158\nVALD: \t Epoch: 117 \t Loss: 0.0023174328613094985\nVALD: \t Epoch: 117 \t Loss: 0.0023021782212185138\n******************************\nEpoch: social-tag : 117\ntrain_loss -0.012267924339595775\nval_loss 0.0023021782212185138\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 118 \t Loss: -0.012130474671721458\nTRAIN: \t Epoch: 118 \t Loss: -0.013450074009597301\nTRAIN: \t Epoch: 118 \t Loss: -0.013335842949648699\nTRAIN: \t Epoch: 118 \t Loss: -0.01327159907668829\nTRAIN: \t Epoch: 118 \t Loss: -0.013109963946044445\nTRAIN: \t Epoch: 118 \t Loss: -0.013004100726296505\nTRAIN: \t Epoch: 118 \t Loss: -0.012760584774826254\nTRAIN: \t Epoch: 118 \t Loss: -0.012538892915472388\nTRAIN: \t Epoch: 118 \t Loss: -0.012431360056830777\nTRAIN: \t Epoch: 118 \t Loss: -0.012566712684929371\nTRAIN: \t Epoch: 118 \t Loss: -0.0126318165355108\nTRAIN: \t Epoch: 118 \t Loss: -0.012627281481400132\nTRAIN: \t Epoch: 118 \t Loss: -0.012683741605052581\nTRAIN: \t Epoch: 118 \t Loss: -0.01253367162176541\nTRAIN: \t Epoch: 118 \t Loss: -0.012286769412457942\nTRAIN: \t Epoch: 118 \t Loss: -0.012256608519237489\nTRAIN: \t Epoch: 118 \t Loss: -0.012371484847629772\nTRAIN: \t Epoch: 118 \t Loss: -0.012403482002102666\nTRAIN: \t Epoch: 118 \t Loss: -0.012443569831942258\nTRAIN: \t Epoch: 118 \t Loss: -0.0124395202845335\nTRAIN: \t Epoch: 118 \t Loss: -0.012451928862858386\nTRAIN: \t Epoch: 118 \t Loss: -0.012402885649319824\nVALD: \t Epoch: 118 \t Loss: -0.010896957479417324\nVALD: \t Epoch: 118 \t Loss: -0.007267362321726978\nVALD: \t Epoch: 118 \t Loss: -0.008119361087058982\nVALD: \t Epoch: 118 \t Loss: -0.00215661822585389\nVALD: \t Epoch: 118 \t Loss: -0.003424991061910987\nVALD: \t Epoch: 118 \t Loss: -0.003423012714042808\n******************************\nEpoch: social-tag : 118\ntrain_loss -0.012402885649319824\nval_loss -0.003423012714042808\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 119 \t Loss: -0.013586273416876793\nTRAIN: \t Epoch: 119 \t Loss: -0.013652035966515541\nTRAIN: \t Epoch: 119 \t Loss: -0.013658194492260614\nTRAIN: \t Epoch: 119 \t Loss: -0.013236247235909104\nTRAIN: \t Epoch: 119 \t Loss: -0.01250801533460617\nTRAIN: \t Epoch: 119 \t Loss: -0.01247309846803546\nTRAIN: \t Epoch: 119 \t Loss: -0.012672408750014645\nTRAIN: \t Epoch: 119 \t Loss: -0.012769634369760752\nTRAIN: \t Epoch: 119 \t Loss: -0.012862927487326993\nTRAIN: \t Epoch: 119 \t Loss: -0.012862897012382746\nTRAIN: \t Epoch: 119 \t Loss: -0.012917099859226833\nTRAIN: \t Epoch: 119 \t Loss: -0.012734967361514768\nTRAIN: \t Epoch: 119 \t Loss: -0.012608000435508214\nTRAIN: \t Epoch: 119 \t Loss: -0.012622372779463018\nTRAIN: \t Epoch: 119 \t Loss: -0.01266560802857081\nTRAIN: \t Epoch: 119 \t Loss: -0.012653926387429237\nTRAIN: \t Epoch: 119 \t Loss: -0.01263324762968456\nTRAIN: \t Epoch: 119 \t Loss: -0.012582815666165616\nTRAIN: \t Epoch: 119 \t Loss: -0.012569527092732881\nTRAIN: \t Epoch: 119 \t Loss: -0.01258753496222198\nTRAIN: \t Epoch: 119 \t Loss: -0.012575421704068071\nTRAIN: \t Epoch: 119 \t Loss: -0.012541287334859907\nVALD: \t Epoch: 119 \t Loss: -0.0049631642177701\nVALD: \t Epoch: 119 \t Loss: -5.657179281115532e-05\nVALD: \t Epoch: 119 \t Loss: -0.0029244013130664825\nVALD: \t Epoch: 119 \t Loss: 0.003565196879208088\nVALD: \t Epoch: 119 \t Loss: 0.001445135474205017\nVALD: \t Epoch: 119 \t Loss: 0.0013812464930004244\n******************************\nEpoch: social-tag : 119\ntrain_loss -0.012541287334859907\nval_loss 0.0013812464930004244\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 120 \t Loss: -0.013257823884487152\nTRAIN: \t Epoch: 120 \t Loss: -0.013574689161032438\nTRAIN: \t Epoch: 120 \t Loss: -0.013796756664911905\nTRAIN: \t Epoch: 120 \t Loss: -0.013186486205086112\nTRAIN: \t Epoch: 120 \t Loss: -0.01255839318037033\nTRAIN: \t Epoch: 120 \t Loss: -0.012288574129343033\nTRAIN: \t Epoch: 120 \t Loss: -0.012452561408281326\nTRAIN: \t Epoch: 120 \t Loss: -0.012588405865244567\nTRAIN: \t Epoch: 120 \t Loss: -0.012407452385458682\nTRAIN: \t Epoch: 120 \t Loss: -0.012355734594166279\nTRAIN: \t Epoch: 120 \t Loss: -0.012320903976532545\nTRAIN: \t Epoch: 120 \t Loss: -0.012384621504073342\nTRAIN: \t Epoch: 120 \t Loss: -0.01241121577242246\nTRAIN: \t Epoch: 120 \t Loss: -0.012375588129673685\nTRAIN: \t Epoch: 120 \t Loss: -0.012312945661445458\nTRAIN: \t Epoch: 120 \t Loss: -0.012334179482422769\nTRAIN: \t Epoch: 120 \t Loss: -0.012448638677597046\nTRAIN: \t Epoch: 120 \t Loss: -0.01251626770115561\nTRAIN: \t Epoch: 120 \t Loss: -0.012456216163149006\nTRAIN: \t Epoch: 120 \t Loss: -0.012302452279254795\nTRAIN: \t Epoch: 120 \t Loss: -0.012297942790956725\nTRAIN: \t Epoch: 120 \t Loss: -0.012283892991084813\nVALD: \t Epoch: 120 \t Loss: -0.006779455579817295\nVALD: \t Epoch: 120 \t Loss: -0.0022902945056557655\nVALD: \t Epoch: 120 \t Loss: -0.0036261814335982003\nVALD: \t Epoch: 120 \t Loss: 0.0012968317605555058\nVALD: \t Epoch: 120 \t Loss: -0.0005386585369706153\nVALD: \t Epoch: 120 \t Loss: -0.0006534729040030277\n******************************\nEpoch: social-tag : 120\ntrain_loss -0.012283892991084813\nval_loss -0.0006534729040030277\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 121 \t Loss: -0.013909873552620411\nTRAIN: \t Epoch: 121 \t Loss: -0.013663878198713064\nTRAIN: \t Epoch: 121 \t Loss: -0.013552457404633364\nTRAIN: \t Epoch: 121 \t Loss: -0.013118659611791372\nTRAIN: \t Epoch: 121 \t Loss: -0.012655211240053177\nTRAIN: \t Epoch: 121 \t Loss: -0.012470448234428963\nTRAIN: \t Epoch: 121 \t Loss: -0.012641424047095435\nTRAIN: \t Epoch: 121 \t Loss: -0.012821348966099322\nTRAIN: \t Epoch: 121 \t Loss: -0.012875815129114522\nTRAIN: \t Epoch: 121 \t Loss: -0.012857338320463896\nTRAIN: \t Epoch: 121 \t Loss: -0.012887881560759111\nTRAIN: \t Epoch: 121 \t Loss: -0.012843168806284666\nTRAIN: \t Epoch: 121 \t Loss: -0.012774976185308052\nTRAIN: \t Epoch: 121 \t Loss: -0.012695204067443098\nTRAIN: \t Epoch: 121 \t Loss: -0.012703696824610233\nTRAIN: \t Epoch: 121 \t Loss: -0.01278311776695773\nTRAIN: \t Epoch: 121 \t Loss: -0.012772304060704568\nTRAIN: \t Epoch: 121 \t Loss: -0.012684136422144042\nTRAIN: \t Epoch: 121 \t Loss: -0.012654347394249942\nTRAIN: \t Epoch: 121 \t Loss: -0.012697407975792885\nTRAIN: \t Epoch: 121 \t Loss: -0.012704512326135523\nTRAIN: \t Epoch: 121 \t Loss: -0.012676153105936101\nVALD: \t Epoch: 121 \t Loss: -0.011480993591248989\nVALD: \t Epoch: 121 \t Loss: -0.008642027853056788\nVALD: \t Epoch: 121 \t Loss: -0.009247935842722654\nVALD: \t Epoch: 121 \t Loss: -0.004878594656474888\nVALD: \t Epoch: 121 \t Loss: -0.005699579697102308\nVALD: \t Epoch: 121 \t Loss: -0.005674527009779756\n******************************\nEpoch: social-tag : 121\ntrain_loss -0.012676153105936101\nval_loss -0.005674527009779756\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 122 \t Loss: -0.013213244266808033\nTRAIN: \t Epoch: 122 \t Loss: -0.013150832150131464\nTRAIN: \t Epoch: 122 \t Loss: -0.012666785779098669\nTRAIN: \t Epoch: 122 \t Loss: -0.012096955673769116\nTRAIN: \t Epoch: 122 \t Loss: -0.01215813886374235\nTRAIN: \t Epoch: 122 \t Loss: -0.01235898102944096\nTRAIN: \t Epoch: 122 \t Loss: -0.012429001608065196\nTRAIN: \t Epoch: 122 \t Loss: -0.012595111620612442\nTRAIN: \t Epoch: 122 \t Loss: -0.012621892926593622\nTRAIN: \t Epoch: 122 \t Loss: -0.012400162033736705\nTRAIN: \t Epoch: 122 \t Loss: -0.01233152240853418\nTRAIN: \t Epoch: 122 \t Loss: -0.01240464074847599\nTRAIN: \t Epoch: 122 \t Loss: -0.012463063718034672\nTRAIN: \t Epoch: 122 \t Loss: -0.012510280657027448\nTRAIN: \t Epoch: 122 \t Loss: -0.012378782282272975\nTRAIN: \t Epoch: 122 \t Loss: -0.012350974488072097\nTRAIN: \t Epoch: 122 \t Loss: -0.012371700037928188\nTRAIN: \t Epoch: 122 \t Loss: -0.012410048343655136\nTRAIN: \t Epoch: 122 \t Loss: -0.012387139134501157\nTRAIN: \t Epoch: 122 \t Loss: -0.012361181201413274\nTRAIN: \t Epoch: 122 \t Loss: -0.012362230613472917\nTRAIN: \t Epoch: 122 \t Loss: -0.012335790466363383\nVALD: \t Epoch: 122 \t Loss: -0.012124311178922653\nVALD: \t Epoch: 122 \t Loss: -0.009020742261782289\nVALD: \t Epoch: 122 \t Loss: -0.009474419833471378\nVALD: \t Epoch: 122 \t Loss: -0.007430662284605205\nVALD: \t Epoch: 122 \t Loss: -0.007816251832991838\nVALD: \t Epoch: 122 \t Loss: -0.007694854214787483\n******************************\nEpoch: social-tag : 122\ntrain_loss -0.012335790466363383\nval_loss -0.007694854214787483\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 123 \t Loss: -0.013481088913977146\nTRAIN: \t Epoch: 123 \t Loss: -0.013718167319893837\nTRAIN: \t Epoch: 123 \t Loss: -0.013617528602480888\nTRAIN: \t Epoch: 123 \t Loss: -0.013102481607347727\nTRAIN: \t Epoch: 123 \t Loss: -0.012485309317708016\nTRAIN: \t Epoch: 123 \t Loss: -0.01249914507692059\nTRAIN: \t Epoch: 123 \t Loss: -0.012673697301319667\nTRAIN: \t Epoch: 123 \t Loss: -0.012812439352273941\nTRAIN: \t Epoch: 123 \t Loss: -0.012834583409130573\nTRAIN: \t Epoch: 123 \t Loss: -0.012833088357001542\nTRAIN: \t Epoch: 123 \t Loss: -0.012606339021162554\nTRAIN: \t Epoch: 123 \t Loss: -0.012511700003718337\nTRAIN: \t Epoch: 123 \t Loss: -0.012533145097012702\nTRAIN: \t Epoch: 123 \t Loss: -0.012694830340998513\nTRAIN: \t Epoch: 123 \t Loss: -0.012804742219547431\nTRAIN: \t Epoch: 123 \t Loss: -0.012729589303489774\nTRAIN: \t Epoch: 123 \t Loss: -0.012635731795693146\nTRAIN: \t Epoch: 123 \t Loss: -0.012683902556697527\nTRAIN: \t Epoch: 123 \t Loss: -0.012695971582280962\nTRAIN: \t Epoch: 123 \t Loss: -0.012626259494572878\nTRAIN: \t Epoch: 123 \t Loss: -0.012566911411427316\nTRAIN: \t Epoch: 123 \t Loss: -0.012586701066104471\nVALD: \t Epoch: 123 \t Loss: -0.002856131875887513\nVALD: \t Epoch: 123 \t Loss: 0.003439290332607925\nVALD: \t Epoch: 123 \t Loss: 0.0009946555364876986\nVALD: \t Epoch: 123 \t Loss: 0.008159950317349285\nVALD: \t Epoch: 123 \t Loss: 0.00518528469838202\nVALD: \t Epoch: 123 \t Loss: 0.004935203486997069\n******************************\nEpoch: social-tag : 123\ntrain_loss -0.012586701066104471\nval_loss 0.004935203486997069\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 124 \t Loss: -0.01382449734956026\nTRAIN: \t Epoch: 124 \t Loss: -0.013591766357421875\nTRAIN: \t Epoch: 124 \t Loss: -0.01292053641130527\nTRAIN: \t Epoch: 124 \t Loss: -0.012057123705744743\nTRAIN: \t Epoch: 124 \t Loss: -0.012119322642683983\nTRAIN: \t Epoch: 124 \t Loss: -0.012417735842367014\nTRAIN: \t Epoch: 124 \t Loss: -0.012559813314250537\nTRAIN: \t Epoch: 124 \t Loss: -0.012629154603928328\nTRAIN: \t Epoch: 124 \t Loss: -0.012570797569221921\nTRAIN: \t Epoch: 124 \t Loss: -0.012732817698270083\nTRAIN: \t Epoch: 124 \t Loss: -0.012659360146657988\nTRAIN: \t Epoch: 124 \t Loss: -0.012419320565337936\nTRAIN: \t Epoch: 124 \t Loss: -0.012369887258570928\nTRAIN: \t Epoch: 124 \t Loss: -0.012447978502937726\nTRAIN: \t Epoch: 124 \t Loss: -0.012483104504644871\nTRAIN: \t Epoch: 124 \t Loss: -0.012504170590545982\nTRAIN: \t Epoch: 124 \t Loss: -0.012517960796899655\nTRAIN: \t Epoch: 124 \t Loss: -0.012513251612997718\nTRAIN: \t Epoch: 124 \t Loss: -0.012454600024380182\nTRAIN: \t Epoch: 124 \t Loss: -0.012445665197446942\nTRAIN: \t Epoch: 124 \t Loss: -0.012457673171801227\nTRAIN: \t Epoch: 124 \t Loss: -0.012417492917880978\nVALD: \t Epoch: 124 \t Loss: -0.0044863540679216385\nVALD: \t Epoch: 124 \t Loss: 0.0013772656675428152\nVALD: \t Epoch: 124 \t Loss: -0.0018054683071871598\nVALD: \t Epoch: 124 \t Loss: 0.0027771665481850505\nVALD: \t Epoch: 124 \t Loss: 0.0011068697087466717\nVALD: \t Epoch: 124 \t Loss: 0.0012156494864911745\n******************************\nEpoch: social-tag : 124\ntrain_loss -0.012417492917880978\nval_loss 0.0012156494864911745\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 125 \t Loss: -0.01320426445454359\nTRAIN: \t Epoch: 125 \t Loss: -0.012822895310819149\nTRAIN: \t Epoch: 125 \t Loss: -0.012883453629910946\nTRAIN: \t Epoch: 125 \t Loss: -0.012581987772136927\nTRAIN: \t Epoch: 125 \t Loss: -0.012331916205585002\nTRAIN: \t Epoch: 125 \t Loss: -0.012381310729930798\nTRAIN: \t Epoch: 125 \t Loss: -0.01272975972720555\nTRAIN: \t Epoch: 125 \t Loss: -0.012922302703373134\nTRAIN: \t Epoch: 125 \t Loss: -0.01295736814952559\nTRAIN: \t Epoch: 125 \t Loss: -0.012863784842193127\nTRAIN: \t Epoch: 125 \t Loss: -0.012802691537548195\nTRAIN: \t Epoch: 125 \t Loss: -0.012723973952233791\nTRAIN: \t Epoch: 125 \t Loss: -0.012703607288690714\nTRAIN: \t Epoch: 125 \t Loss: -0.012737093160727195\nTRAIN: \t Epoch: 125 \t Loss: -0.012783426294724147\nTRAIN: \t Epoch: 125 \t Loss: -0.01269794290419668\nTRAIN: \t Epoch: 125 \t Loss: -0.012644989909056355\nTRAIN: \t Epoch: 125 \t Loss: -0.012674762815650966\nTRAIN: \t Epoch: 125 \t Loss: -0.012664950648812871\nTRAIN: \t Epoch: 125 \t Loss: -0.012690951535478234\nTRAIN: \t Epoch: 125 \t Loss: -0.012728259588281313\nTRAIN: \t Epoch: 125 \t Loss: -0.012705752537109266\nVALD: \t Epoch: 125 \t Loss: -0.007619030773639679\nVALD: \t Epoch: 125 \t Loss: -0.0031644930131733418\nVALD: \t Epoch: 125 \t Loss: -0.003902252453068892\nVALD: \t Epoch: 125 \t Loss: 0.001864022808149457\nVALD: \t Epoch: 125 \t Loss: 6.00004568696022e-06\nVALD: \t Epoch: 125 \t Loss: -9.252480936772895e-05\n******************************\nEpoch: social-tag : 125\ntrain_loss -0.012705752537109266\nval_loss -9.252480936772895e-05\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 126 \t Loss: -0.014071065932512283\nTRAIN: \t Epoch: 126 \t Loss: -0.0139408097602427\nTRAIN: \t Epoch: 126 \t Loss: -0.013132973263661066\nTRAIN: \t Epoch: 126 \t Loss: -0.012467016000300646\nTRAIN: \t Epoch: 126 \t Loss: -0.012469509989023209\nTRAIN: \t Epoch: 126 \t Loss: -0.01260341020921866\nTRAIN: \t Epoch: 126 \t Loss: -0.01276735176465341\nTRAIN: \t Epoch: 126 \t Loss: -0.012820081668905914\nTRAIN: \t Epoch: 126 \t Loss: -0.012460765635801686\nTRAIN: \t Epoch: 126 \t Loss: -0.01237660152837634\nTRAIN: \t Epoch: 126 \t Loss: -0.012523056685247204\nTRAIN: \t Epoch: 126 \t Loss: -0.01259595031539599\nTRAIN: \t Epoch: 126 \t Loss: -0.012668378078020535\nTRAIN: \t Epoch: 126 \t Loss: -0.012765376695564814\nTRAIN: \t Epoch: 126 \t Loss: -0.012714653213818868\nTRAIN: \t Epoch: 126 \t Loss: -0.012691793730482459\nTRAIN: \t Epoch: 126 \t Loss: -0.012685011086218497\nTRAIN: \t Epoch: 126 \t Loss: -0.012723516155448224\nTRAIN: \t Epoch: 126 \t Loss: -0.012703343098492999\nTRAIN: \t Epoch: 126 \t Loss: -0.012585593247786164\nTRAIN: \t Epoch: 126 \t Loss: -0.012565064864854017\nTRAIN: \t Epoch: 126 \t Loss: -0.012627693566642501\nVALD: \t Epoch: 126 \t Loss: -0.009253452531993389\nVALD: \t Epoch: 126 \t Loss: -0.005654669366776943\nVALD: \t Epoch: 126 \t Loss: -0.006550362954537074\nVALD: \t Epoch: 126 \t Loss: -0.002421025652438402\nVALD: \t Epoch: 126 \t Loss: -0.0036163469776511192\nVALD: \t Epoch: 126 \t Loss: -0.003553622074876771\n******************************\nEpoch: social-tag : 126\ntrain_loss -0.012627693566642501\nval_loss -0.003553622074876771\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 127 \t Loss: -0.013784188777208328\nTRAIN: \t Epoch: 127 \t Loss: -0.014406857080757618\nTRAIN: \t Epoch: 127 \t Loss: -0.013980887830257416\nTRAIN: \t Epoch: 127 \t Loss: -0.0125474464148283\nTRAIN: \t Epoch: 127 \t Loss: -0.012062776647508145\nTRAIN: \t Epoch: 127 \t Loss: -0.01238554979984959\nTRAIN: \t Epoch: 127 \t Loss: -0.012551517624940191\nTRAIN: \t Epoch: 127 \t Loss: -0.012449923669919372\nTRAIN: \t Epoch: 127 \t Loss: -0.012225612894528441\nTRAIN: \t Epoch: 127 \t Loss: -0.01189673449844122\nTRAIN: \t Epoch: 127 \t Loss: -0.011891892941837961\nTRAIN: \t Epoch: 127 \t Loss: -0.011980932438746095\nTRAIN: \t Epoch: 127 \t Loss: -0.012152977860890903\nTRAIN: \t Epoch: 127 \t Loss: -0.01227314816787839\nTRAIN: \t Epoch: 127 \t Loss: -0.012324881181120873\nTRAIN: \t Epoch: 127 \t Loss: -0.012318645836785436\nTRAIN: \t Epoch: 127 \t Loss: -0.012311912580009769\nTRAIN: \t Epoch: 127 \t Loss: -0.012357145444386534\nTRAIN: \t Epoch: 127 \t Loss: -0.012386640239703027\nTRAIN: \t Epoch: 127 \t Loss: -0.012408403772860765\nTRAIN: \t Epoch: 127 \t Loss: -0.012401744457227843\nTRAIN: \t Epoch: 127 \t Loss: -0.012389491747268869\nVALD: \t Epoch: 127 \t Loss: -0.012557786889374256\nVALD: \t Epoch: 127 \t Loss: -0.009457953507080674\nVALD: \t Epoch: 127 \t Loss: -0.010620750952512026\nVALD: \t Epoch: 127 \t Loss: -0.007009409426245838\nVALD: \t Epoch: 127 \t Loss: -0.007591851847246289\nVALD: \t Epoch: 127 \t Loss: -0.007510727576234124\n******************************\nEpoch: social-tag : 127\ntrain_loss -0.012389491747268869\nval_loss -0.007510727576234124\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 128 \t Loss: -0.01418189238756895\nTRAIN: \t Epoch: 128 \t Loss: -0.013734139502048492\nTRAIN: \t Epoch: 128 \t Loss: -0.013662059791386127\nTRAIN: \t Epoch: 128 \t Loss: -0.01305987755768001\nTRAIN: \t Epoch: 128 \t Loss: -0.012500927411019802\nTRAIN: \t Epoch: 128 \t Loss: -0.012568020261824131\nTRAIN: \t Epoch: 128 \t Loss: -0.012800360364573342\nTRAIN: \t Epoch: 128 \t Loss: -0.013052540249191225\nTRAIN: \t Epoch: 128 \t Loss: -0.013037377244068516\nTRAIN: \t Epoch: 128 \t Loss: -0.012844846118241549\nTRAIN: \t Epoch: 128 \t Loss: -0.012726234780116514\nTRAIN: \t Epoch: 128 \t Loss: -0.012710498801122109\nTRAIN: \t Epoch: 128 \t Loss: -0.012783064793508787\nTRAIN: \t Epoch: 128 \t Loss: -0.012867882581693786\nTRAIN: \t Epoch: 128 \t Loss: -0.012814614363014698\nTRAIN: \t Epoch: 128 \t Loss: -0.012557254696730524\nTRAIN: \t Epoch: 128 \t Loss: -0.012453892432591495\nTRAIN: \t Epoch: 128 \t Loss: -0.012510142599542936\nTRAIN: \t Epoch: 128 \t Loss: -0.012616174099476714\nTRAIN: \t Epoch: 128 \t Loss: -0.012595628760755062\nTRAIN: \t Epoch: 128 \t Loss: -0.012653996192273639\nTRAIN: \t Epoch: 128 \t Loss: -0.01263742840696708\nVALD: \t Epoch: 128 \t Loss: -0.005842755548655987\nVALD: \t Epoch: 128 \t Loss: -0.0031336452229879797\nVALD: \t Epoch: 128 \t Loss: -0.0046664593974128366\nVALD: \t Epoch: 128 \t Loss: -0.0020389835990499705\nVALD: \t Epoch: 128 \t Loss: -0.003041522181592882\nVALD: \t Epoch: 128 \t Loss: -0.0030550037708246347\n******************************\nEpoch: social-tag : 128\ntrain_loss -0.01263742840696708\nval_loss -0.0030550037708246347\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 129 \t Loss: -0.013744831085205078\nTRAIN: \t Epoch: 129 \t Loss: -0.014099369756877422\nTRAIN: \t Epoch: 129 \t Loss: -0.013820453236500422\nTRAIN: \t Epoch: 129 \t Loss: -0.01332941697910428\nTRAIN: \t Epoch: 129 \t Loss: -0.012947243638336658\nTRAIN: \t Epoch: 129 \t Loss: -0.01279552203292648\nTRAIN: \t Epoch: 129 \t Loss: -0.012967732468886035\nTRAIN: \t Epoch: 129 \t Loss: -0.013081421260721982\nTRAIN: \t Epoch: 129 \t Loss: -0.012992604014774164\nTRAIN: \t Epoch: 129 \t Loss: -0.012847063317894935\nTRAIN: \t Epoch: 129 \t Loss: -0.01277761288325895\nTRAIN: \t Epoch: 129 \t Loss: -0.012856417102739215\nTRAIN: \t Epoch: 129 \t Loss: -0.012941161457162637\nTRAIN: \t Epoch: 129 \t Loss: -0.01304397186530488\nTRAIN: \t Epoch: 129 \t Loss: -0.013131144394477209\nTRAIN: \t Epoch: 129 \t Loss: -0.01293523539789021\nTRAIN: \t Epoch: 129 \t Loss: -0.012734120492549503\nTRAIN: \t Epoch: 129 \t Loss: -0.012715731954409016\nTRAIN: \t Epoch: 129 \t Loss: -0.012741492129862309\nTRAIN: \t Epoch: 129 \t Loss: -0.012805395806208254\nTRAIN: \t Epoch: 129 \t Loss: -0.012837047644314311\nTRAIN: \t Epoch: 129 \t Loss: -0.012858539937428555\nVALD: \t Epoch: 129 \t Loss: 0.005674993619322777\nVALD: \t Epoch: 129 \t Loss: 0.009689359460026026\nVALD: \t Epoch: 129 \t Loss: 0.004462665567795436\nVALD: \t Epoch: 129 \t Loss: 0.01906524319201708\nVALD: \t Epoch: 129 \t Loss: 0.01632465897127986\nVALD: \t Epoch: 129 \t Loss: 0.016565371824033333\n******************************\nEpoch: social-tag : 129\ntrain_loss -0.012858539937428555\nval_loss 0.016565371824033333\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 130 \t Loss: -0.012958502396941185\nTRAIN: \t Epoch: 130 \t Loss: -0.013269794639199972\nTRAIN: \t Epoch: 130 \t Loss: -0.013264232936004797\nTRAIN: \t Epoch: 130 \t Loss: -0.012708738911896944\nTRAIN: \t Epoch: 130 \t Loss: -0.011903024651110173\nTRAIN: \t Epoch: 130 \t Loss: -0.01203252918397387\nTRAIN: \t Epoch: 130 \t Loss: -0.012290785887411662\nTRAIN: \t Epoch: 130 \t Loss: -0.012341528316028416\nTRAIN: \t Epoch: 130 \t Loss: -0.012556885162161456\nTRAIN: \t Epoch: 130 \t Loss: -0.0126296479254961\nTRAIN: \t Epoch: 130 \t Loss: -0.012492760846560652\nTRAIN: \t Epoch: 130 \t Loss: -0.012369380875801047\nTRAIN: \t Epoch: 130 \t Loss: -0.012392711754028615\nTRAIN: \t Epoch: 130 \t Loss: -0.012528970438454832\nTRAIN: \t Epoch: 130 \t Loss: -0.012656487276156743\nTRAIN: \t Epoch: 130 \t Loss: -0.012774274218827486\nTRAIN: \t Epoch: 130 \t Loss: -0.012605985188308884\nTRAIN: \t Epoch: 130 \t Loss: -0.012496938825481467\nTRAIN: \t Epoch: 130 \t Loss: -0.012501911358221582\nTRAIN: \t Epoch: 130 \t Loss: -0.012564010731875897\nTRAIN: \t Epoch: 130 \t Loss: -0.01264475445662226\nTRAIN: \t Epoch: 130 \t Loss: -0.01263662223541972\nVALD: \t Epoch: 130 \t Loss: -0.012978334911167622\nVALD: \t Epoch: 130 \t Loss: -0.008535860804840922\nVALD: \t Epoch: 130 \t Loss: -0.009471563156694174\nVALD: \t Epoch: 130 \t Loss: -0.0016142494278028607\nVALD: \t Epoch: 130 \t Loss: -0.002975362632423639\nVALD: \t Epoch: 130 \t Loss: -0.002950198683097507\n******************************\nEpoch: social-tag : 130\ntrain_loss -0.01263662223541972\nval_loss -0.002950198683097507\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 131 \t Loss: -0.014065263792872429\nTRAIN: \t Epoch: 131 \t Loss: -0.013597961515188217\nTRAIN: \t Epoch: 131 \t Loss: -0.013651484002669653\nTRAIN: \t Epoch: 131 \t Loss: -0.013565014116466045\nTRAIN: \t Epoch: 131 \t Loss: -0.01345544271171093\nTRAIN: \t Epoch: 131 \t Loss: -0.013399905059486628\nTRAIN: \t Epoch: 131 \t Loss: -0.01325084428702082\nTRAIN: \t Epoch: 131 \t Loss: -0.013249300071038306\nTRAIN: \t Epoch: 131 \t Loss: -0.013313944761951765\nTRAIN: \t Epoch: 131 \t Loss: -0.013126458320766688\nTRAIN: \t Epoch: 131 \t Loss: -0.012822382914071733\nTRAIN: \t Epoch: 131 \t Loss: -0.012815350666642189\nTRAIN: \t Epoch: 131 \t Loss: -0.013004115352836939\nTRAIN: \t Epoch: 131 \t Loss: -0.013084867303924901\nTRAIN: \t Epoch: 131 \t Loss: -0.013132070191204548\nTRAIN: \t Epoch: 131 \t Loss: -0.013065235398244113\nTRAIN: \t Epoch: 131 \t Loss: -0.012994146577137359\nTRAIN: \t Epoch: 131 \t Loss: -0.012999270783944262\nTRAIN: \t Epoch: 131 \t Loss: -0.013019929738029054\nTRAIN: \t Epoch: 131 \t Loss: -0.012911605788394809\nTRAIN: \t Epoch: 131 \t Loss: -0.012806685153572332\nTRAIN: \t Epoch: 131 \t Loss: -0.012782868063428458\nVALD: \t Epoch: 131 \t Loss: -0.008703860454261303\nVALD: \t Epoch: 131 \t Loss: -0.006108084926381707\nVALD: \t Epoch: 131 \t Loss: -0.008486030778537193\nVALD: \t Epoch: 131 \t Loss: -0.0010853094281628728\nVALD: \t Epoch: 131 \t Loss: -0.002326978649944067\nVALD: \t Epoch: 131 \t Loss: -0.0022698063403367996\n******************************\nEpoch: social-tag : 131\ntrain_loss -0.012782868063428458\nval_loss -0.0022698063403367996\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 132 \t Loss: -0.014347321353852749\nTRAIN: \t Epoch: 132 \t Loss: -0.01440121466293931\nTRAIN: \t Epoch: 132 \t Loss: -0.013676702976226807\nTRAIN: \t Epoch: 132 \t Loss: -0.013526136055588722\nTRAIN: \t Epoch: 132 \t Loss: -0.01302971299737692\nTRAIN: \t Epoch: 132 \t Loss: -0.012779597658663988\nTRAIN: \t Epoch: 132 \t Loss: -0.012923699537558215\nTRAIN: \t Epoch: 132 \t Loss: -0.013067090534605086\nTRAIN: \t Epoch: 132 \t Loss: -0.013007204048335552\nTRAIN: \t Epoch: 132 \t Loss: -0.012767145689576865\nTRAIN: \t Epoch: 132 \t Loss: -0.012655580670318821\nTRAIN: \t Epoch: 132 \t Loss: -0.012731079788257679\nTRAIN: \t Epoch: 132 \t Loss: -0.012808803755503435\nTRAIN: \t Epoch: 132 \t Loss: -0.012901374604552984\nTRAIN: \t Epoch: 132 \t Loss: -0.012993078430493672\nTRAIN: \t Epoch: 132 \t Loss: -0.013022438797634095\nTRAIN: \t Epoch: 132 \t Loss: -0.012973584399065551\nTRAIN: \t Epoch: 132 \t Loss: -0.012855410886307558\nTRAIN: \t Epoch: 132 \t Loss: -0.012778840517919315\nTRAIN: \t Epoch: 132 \t Loss: -0.012767880829051137\nTRAIN: \t Epoch: 132 \t Loss: -0.012773408554494381\nTRAIN: \t Epoch: 132 \t Loss: -0.012796142670581757\nVALD: \t Epoch: 132 \t Loss: -0.009438888169825077\nVALD: \t Epoch: 132 \t Loss: -0.005099514703033492\nVALD: \t Epoch: 132 \t Loss: -0.0064449364435859025\nVALD: \t Epoch: 132 \t Loss: 0.0009706370910862461\nVALD: \t Epoch: 132 \t Loss: -0.0006375386263243854\nVALD: \t Epoch: 132 \t Loss: -0.0006346365507466324\n******************************\nEpoch: social-tag : 132\ntrain_loss -0.012796142670581757\nval_loss -0.0006346365507466324\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 133 \t Loss: -0.014322195202112198\nTRAIN: \t Epoch: 133 \t Loss: -0.014484020881354809\nTRAIN: \t Epoch: 133 \t Loss: -0.013921916795273622\nTRAIN: \t Epoch: 133 \t Loss: -0.013463194714859128\nTRAIN: \t Epoch: 133 \t Loss: -0.012937706336379052\nTRAIN: \t Epoch: 133 \t Loss: -0.012916799013813337\nTRAIN: \t Epoch: 133 \t Loss: -0.012984031146126134\nTRAIN: \t Epoch: 133 \t Loss: -0.012991220457479358\nTRAIN: \t Epoch: 133 \t Loss: -0.012953067508836588\nTRAIN: \t Epoch: 133 \t Loss: -0.013034375011920929\nTRAIN: \t Epoch: 133 \t Loss: -0.013151473873718218\nTRAIN: \t Epoch: 133 \t Loss: -0.013195201832180222\nTRAIN: \t Epoch: 133 \t Loss: -0.013143763734171024\nTRAIN: \t Epoch: 133 \t Loss: -0.012887942338628429\nTRAIN: \t Epoch: 133 \t Loss: -0.012821984415253004\nTRAIN: \t Epoch: 133 \t Loss: -0.012868696299847215\nTRAIN: \t Epoch: 133 \t Loss: -0.012892841799732517\nTRAIN: \t Epoch: 133 \t Loss: -0.012966409170379242\nTRAIN: \t Epoch: 133 \t Loss: -0.012999417554391058\nTRAIN: \t Epoch: 133 \t Loss: -0.012895108200609684\nTRAIN: \t Epoch: 133 \t Loss: -0.012799091592785857\nTRAIN: \t Epoch: 133 \t Loss: -0.012815646637277928\nVALD: \t Epoch: 133 \t Loss: -0.008617903105914593\nVALD: \t Epoch: 133 \t Loss: -0.006196251604706049\nVALD: \t Epoch: 133 \t Loss: -0.007412330557902654\nVALD: \t Epoch: 133 \t Loss: -0.0034060769248753786\nVALD: \t Epoch: 133 \t Loss: -0.004291448183357716\nVALD: \t Epoch: 133 \t Loss: -0.004226208348391634\n******************************\nEpoch: social-tag : 133\ntrain_loss -0.012815646637277928\nval_loss -0.004226208348391634\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 134 \t Loss: -0.015045124106109142\nTRAIN: \t Epoch: 134 \t Loss: -0.014617288019508123\nTRAIN: \t Epoch: 134 \t Loss: -0.014203209429979324\nTRAIN: \t Epoch: 134 \t Loss: -0.013779339613392949\nTRAIN: \t Epoch: 134 \t Loss: -0.013369457609951497\nTRAIN: \t Epoch: 134 \t Loss: -0.013212827034294605\nTRAIN: \t Epoch: 134 \t Loss: -0.013252861265625273\nTRAIN: \t Epoch: 134 \t Loss: -0.013268310576677322\nTRAIN: \t Epoch: 134 \t Loss: -0.013374889579912027\nTRAIN: \t Epoch: 134 \t Loss: -0.013264999072998763\nTRAIN: \t Epoch: 134 \t Loss: -0.013070406730879437\nTRAIN: \t Epoch: 134 \t Loss: -0.012917610661437115\nTRAIN: \t Epoch: 134 \t Loss: -0.01304816440320932\nTRAIN: \t Epoch: 134 \t Loss: -0.013151166694504874\nTRAIN: \t Epoch: 134 \t Loss: -0.013174663794537385\nTRAIN: \t Epoch: 134 \t Loss: -0.01316024991683662\nTRAIN: \t Epoch: 134 \t Loss: -0.013134542523938067\nTRAIN: \t Epoch: 134 \t Loss: -0.013172349271674951\nTRAIN: \t Epoch: 134 \t Loss: -0.013183289109484145\nTRAIN: \t Epoch: 134 \t Loss: -0.013146738754585385\nTRAIN: \t Epoch: 134 \t Loss: -0.01305557428193944\nTRAIN: \t Epoch: 134 \t Loss: -0.01304531812239806\nVALD: \t Epoch: 134 \t Loss: -0.012767596170306206\nVALD: \t Epoch: 134 \t Loss: -0.00964647508226335\nVALD: \t Epoch: 134 \t Loss: -0.010563267239679893\nVALD: \t Epoch: 134 \t Loss: -0.006383312516845763\nVALD: \t Epoch: 134 \t Loss: -0.007048731204122305\nVALD: \t Epoch: 134 \t Loss: -0.006932316878528306\n******************************\nEpoch: social-tag : 134\ntrain_loss -0.01304531812239806\nval_loss -0.006932316878528306\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 135 \t Loss: -0.014830409549176693\nTRAIN: \t Epoch: 135 \t Loss: -0.01498778024688363\nTRAIN: \t Epoch: 135 \t Loss: -0.014561561867594719\nTRAIN: \t Epoch: 135 \t Loss: -0.01437756884843111\nTRAIN: \t Epoch: 135 \t Loss: -0.013253162056207657\nTRAIN: \t Epoch: 135 \t Loss: -0.012470501940697432\nTRAIN: \t Epoch: 135 \t Loss: -0.012384641117283277\nTRAIN: \t Epoch: 135 \t Loss: -0.012467346619814634\nTRAIN: \t Epoch: 135 \t Loss: -0.012667605963846048\nTRAIN: \t Epoch: 135 \t Loss: -0.012782625760883093\nTRAIN: \t Epoch: 135 \t Loss: -0.012879452837461775\nTRAIN: \t Epoch: 135 \t Loss: -0.013019690988585353\nTRAIN: \t Epoch: 135 \t Loss: -0.01272566537730969\nTRAIN: \t Epoch: 135 \t Loss: -0.012469298472361905\nTRAIN: \t Epoch: 135 \t Loss: -0.01244933952887853\nTRAIN: \t Epoch: 135 \t Loss: -0.012512448127381504\nTRAIN: \t Epoch: 135 \t Loss: -0.012596443952882992\nTRAIN: \t Epoch: 135 \t Loss: -0.01271871974070867\nTRAIN: \t Epoch: 135 \t Loss: -0.012799522741452643\nTRAIN: \t Epoch: 135 \t Loss: -0.012839215341955423\nTRAIN: \t Epoch: 135 \t Loss: -0.012835364877468064\nTRAIN: \t Epoch: 135 \t Loss: -0.012825148794766603\nVALD: \t Epoch: 135 \t Loss: -0.011420488357543945\nVALD: \t Epoch: 135 \t Loss: -0.006579826935194433\nVALD: \t Epoch: 135 \t Loss: -0.007868281487996379\nVALD: \t Epoch: 135 \t Loss: -0.002628265123348683\nVALD: \t Epoch: 135 \t Loss: -0.003993903985247016\nVALD: \t Epoch: 135 \t Loss: -0.004013384483528859\n******************************\nEpoch: social-tag : 135\ntrain_loss -0.012825148794766603\nval_loss -0.004013384483528859\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 136 \t Loss: -0.013892633840441704\nTRAIN: \t Epoch: 136 \t Loss: -0.014285835903137922\nTRAIN: \t Epoch: 136 \t Loss: -0.013621463750799498\nTRAIN: \t Epoch: 136 \t Loss: -0.012870021862909198\nTRAIN: \t Epoch: 136 \t Loss: -0.01270798984915018\nTRAIN: \t Epoch: 136 \t Loss: -0.013101601662735144\nTRAIN: \t Epoch: 136 \t Loss: -0.013302298794899668\nTRAIN: \t Epoch: 136 \t Loss: -0.013109090272337198\nTRAIN: \t Epoch: 136 \t Loss: -0.012783779348764155\nTRAIN: \t Epoch: 136 \t Loss: -0.012574974354356528\nTRAIN: \t Epoch: 136 \t Loss: -0.012690150094303217\nTRAIN: \t Epoch: 136 \t Loss: -0.012783444796999296\nTRAIN: \t Epoch: 136 \t Loss: -0.012779996945307804\nTRAIN: \t Epoch: 136 \t Loss: -0.012789963545011622\nTRAIN: \t Epoch: 136 \t Loss: -0.012827569184203943\nTRAIN: \t Epoch: 136 \t Loss: -0.012843068107031286\nTRAIN: \t Epoch: 136 \t Loss: -0.01272678714902962\nTRAIN: \t Epoch: 136 \t Loss: -0.012656757918496927\nTRAIN: \t Epoch: 136 \t Loss: -0.012692308690594999\nTRAIN: \t Epoch: 136 \t Loss: -0.012826748378574847\nTRAIN: \t Epoch: 136 \t Loss: -0.012810180939379193\nTRAIN: \t Epoch: 136 \t Loss: -0.012831931097075156\nVALD: \t Epoch: 136 \t Loss: 0.0016962150111794472\nVALD: \t Epoch: 136 \t Loss: 0.007346981670707464\nVALD: \t Epoch: 136 \t Loss: 0.0032985258537034192\nVALD: \t Epoch: 136 \t Loss: 0.008783503784798086\nVALD: \t Epoch: 136 \t Loss: 0.007769086072221399\nVALD: \t Epoch: 136 \t Loss: 0.008224725452336398\n******************************\nEpoch: social-tag : 136\ntrain_loss -0.012831931097075156\nval_loss 0.008224725452336398\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 137 \t Loss: -0.014247642830014229\nTRAIN: \t Epoch: 137 \t Loss: -0.013924573082476854\nTRAIN: \t Epoch: 137 \t Loss: -0.013812245180209478\nTRAIN: \t Epoch: 137 \t Loss: -0.013475023210048676\nTRAIN: \t Epoch: 137 \t Loss: -0.01313696764409542\nTRAIN: \t Epoch: 137 \t Loss: -0.012964216837038597\nTRAIN: \t Epoch: 137 \t Loss: -0.013155832354511534\nTRAIN: \t Epoch: 137 \t Loss: -0.01325822260696441\nTRAIN: \t Epoch: 137 \t Loss: -0.01296178986214929\nTRAIN: \t Epoch: 137 \t Loss: -0.012601446174085141\nTRAIN: \t Epoch: 137 \t Loss: -0.012667757120322098\nTRAIN: \t Epoch: 137 \t Loss: -0.012698738059649864\nTRAIN: \t Epoch: 137 \t Loss: -0.012693599941065678\nTRAIN: \t Epoch: 137 \t Loss: -0.01272328503962074\nTRAIN: \t Epoch: 137 \t Loss: -0.012816752058764298\nTRAIN: \t Epoch: 137 \t Loss: -0.01292084704618901\nTRAIN: \t Epoch: 137 \t Loss: -0.013063886367222843\nTRAIN: \t Epoch: 137 \t Loss: -0.013014739172326194\nTRAIN: \t Epoch: 137 \t Loss: -0.012977989488526395\nTRAIN: \t Epoch: 137 \t Loss: -0.012958379555493593\nTRAIN: \t Epoch: 137 \t Loss: -0.01298565555009104\nTRAIN: \t Epoch: 137 \t Loss: -0.01298783720074693\nVALD: \t Epoch: 137 \t Loss: 0.0042388602159917355\nVALD: \t Epoch: 137 \t Loss: 0.005902420729398727\nVALD: \t Epoch: 137 \t Loss: 0.0009813814734419186\nVALD: \t Epoch: 137 \t Loss: 0.013261233223602176\nVALD: \t Epoch: 137 \t Loss: 0.01121125016361475\nVALD: \t Epoch: 137 \t Loss: 0.011678038763277459\n******************************\nEpoch: social-tag : 137\ntrain_loss -0.01298783720074693\nval_loss 0.011678038763277459\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 138 \t Loss: -0.013799474574625492\nTRAIN: \t Epoch: 138 \t Loss: -0.014230011496692896\nTRAIN: \t Epoch: 138 \t Loss: -0.013734056614339352\nTRAIN: \t Epoch: 138 \t Loss: -0.012839228147640824\nTRAIN: \t Epoch: 138 \t Loss: -0.01253928616642952\nTRAIN: \t Epoch: 138 \t Loss: -0.012849636375904083\nTRAIN: \t Epoch: 138 \t Loss: -0.012971398287585803\nTRAIN: \t Epoch: 138 \t Loss: -0.012892934260889888\nTRAIN: \t Epoch: 138 \t Loss: -0.0126036097192102\nTRAIN: \t Epoch: 138 \t Loss: -0.012515999656170607\nTRAIN: \t Epoch: 138 \t Loss: -0.012596129016442732\nTRAIN: \t Epoch: 138 \t Loss: -0.01263270953980585\nTRAIN: \t Epoch: 138 \t Loss: -0.01270286631412231\nTRAIN: \t Epoch: 138 \t Loss: -0.01277164595999888\nTRAIN: \t Epoch: 138 \t Loss: -0.012746135021249454\nTRAIN: \t Epoch: 138 \t Loss: -0.012765205407049507\nTRAIN: \t Epoch: 138 \t Loss: -0.01273765743655317\nTRAIN: \t Epoch: 138 \t Loss: -0.012674026087754302\nTRAIN: \t Epoch: 138 \t Loss: -0.012742701035581137\nTRAIN: \t Epoch: 138 \t Loss: -0.012804861832410096\nTRAIN: \t Epoch: 138 \t Loss: -0.012826997653714247\nTRAIN: \t Epoch: 138 \t Loss: -0.012737610203145436\nVALD: \t Epoch: 138 \t Loss: -0.010709062218666077\nVALD: \t Epoch: 138 \t Loss: -0.00841288035735488\nVALD: \t Epoch: 138 \t Loss: -0.008184363755087057\nVALD: \t Epoch: 138 \t Loss: -0.0060382327501429245\nVALD: \t Epoch: 138 \t Loss: -0.006379338388796896\nVALD: \t Epoch: 138 \t Loss: -0.00631313311556975\n******************************\nEpoch: social-tag : 138\ntrain_loss -0.012737610203145436\nval_loss -0.00631313311556975\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 139 \t Loss: -0.01042158529162407\nTRAIN: \t Epoch: 139 \t Loss: -0.011837795376777649\nTRAIN: \t Epoch: 139 \t Loss: -0.012564873012403647\nTRAIN: \t Epoch: 139 \t Loss: -0.012484486447647214\nTRAIN: \t Epoch: 139 \t Loss: -0.012103622779250145\nTRAIN: \t Epoch: 139 \t Loss: -0.01199449428046743\nTRAIN: \t Epoch: 139 \t Loss: -0.012290727347135544\nTRAIN: \t Epoch: 139 \t Loss: -0.012555700959637761\nTRAIN: \t Epoch: 139 \t Loss: -0.012702881772485044\nTRAIN: \t Epoch: 139 \t Loss: -0.012681305781006813\nTRAIN: \t Epoch: 139 \t Loss: -0.01273010391741991\nTRAIN: \t Epoch: 139 \t Loss: -0.012763121165335178\nTRAIN: \t Epoch: 139 \t Loss: -0.012758511763352614\nTRAIN: \t Epoch: 139 \t Loss: -0.01277969272008964\nTRAIN: \t Epoch: 139 \t Loss: -0.012741837836802005\nTRAIN: \t Epoch: 139 \t Loss: -0.01272285875165835\nTRAIN: \t Epoch: 139 \t Loss: -0.01275111734867096\nTRAIN: \t Epoch: 139 \t Loss: -0.01277971159045895\nTRAIN: \t Epoch: 139 \t Loss: -0.012789456222794558\nTRAIN: \t Epoch: 139 \t Loss: -0.012763920612633229\nTRAIN: \t Epoch: 139 \t Loss: -0.01275427368957372\nTRAIN: \t Epoch: 139 \t Loss: -0.012794812149376587\nVALD: \t Epoch: 139 \t Loss: -0.010844491422176361\nVALD: \t Epoch: 139 \t Loss: -0.006389463669620454\nVALD: \t Epoch: 139 \t Loss: -0.008318297176932296\nVALD: \t Epoch: 139 \t Loss: -0.00032520684180781245\nVALD: \t Epoch: 139 \t Loss: -0.0017393361311405897\nVALD: \t Epoch: 139 \t Loss: -0.0016979082783853466\n******************************\nEpoch: social-tag : 139\ntrain_loss -0.012794812149376587\nval_loss -0.0016979082783853466\n{'min_val_epoch': 115, 'min_val_loss': -0.007709600539370017}\n******************************\nTRAIN: \t Epoch: 140 \t Loss: -0.013285847380757332\nTRAIN: \t Epoch: 140 \t Loss: -0.013338346499949694\nTRAIN: \t Epoch: 140 \t Loss: -0.01306626262764136\nTRAIN: \t Epoch: 140 \t Loss: -0.012765235966071486\nTRAIN: \t Epoch: 140 \t Loss: -0.012637361697852612\nTRAIN: \t Epoch: 140 \t Loss: -0.012824705491463343\nTRAIN: \t Epoch: 140 \t Loss: -0.013015113904007844\nTRAIN: \t Epoch: 140 \t Loss: -0.013107291306369007\nTRAIN: \t Epoch: 140 \t Loss: -0.013021866583989726\nTRAIN: \t Epoch: 140 \t Loss: -0.012746016029268503\nTRAIN: \t Epoch: 140 \t Loss: -0.012774587608873844\nTRAIN: \t Epoch: 140 \t Loss: -0.012838693723703424\nTRAIN: \t Epoch: 140 \t Loss: -0.01296903035388543\nTRAIN: \t Epoch: 140 \t Loss: -0.012984447846455234\nTRAIN: \t Epoch: 140 \t Loss: -0.0128730242451032\nTRAIN: \t Epoch: 140 \t Loss: -0.012740059231873602\nTRAIN: \t Epoch: 140 \t Loss: -0.012740155462833011\nTRAIN: \t Epoch: 140 \t Loss: -0.01276854777501689\nTRAIN: \t Epoch: 140 \t Loss: -0.012797269823127672\nTRAIN: \t Epoch: 140 \t Loss: -0.012829189375042915\nTRAIN: \t Epoch: 140 \t Loss: -0.012825531796330497\nTRAIN: \t Epoch: 140 \t Loss: -0.012745812508533415\nVALD: \t Epoch: 140 \t Loss: -0.011523961089551449\nVALD: \t Epoch: 140 \t Loss: -0.00886109215207398\nVALD: \t Epoch: 140 \t Loss: -0.009411565183351437\nVALD: \t Epoch: 140 \t Loss: -0.009035781375132501\nVALD: \t Epoch: 140 \t Loss: -0.00908375857397914\nVALD: \t Epoch: 140 \t Loss: -0.008992135016755625\n**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 70/70 [00:02<00:00, 26.10it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.7251130762181001  FDE: 1.1926335161153436\n**************************************************\n******************************\nEpoch: social-tag : 140\ntrain_loss -0.012745812508533415\nval_loss -0.008992135016755625\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 141 \t Loss: -0.013307278044521809\nTRAIN: \t Epoch: 141 \t Loss: -0.013414918445050716\nTRAIN: \t Epoch: 141 \t Loss: -0.013692566193640232\nTRAIN: \t Epoch: 141 \t Loss: -0.013802569825202227\nTRAIN: \t Epoch: 141 \t Loss: -0.013780885562300681\nTRAIN: \t Epoch: 141 \t Loss: -0.013763686486830315\nTRAIN: \t Epoch: 141 \t Loss: -0.013616751613361495\nTRAIN: \t Epoch: 141 \t Loss: -0.013602201594039798\nTRAIN: \t Epoch: 141 \t Loss: -0.013527108045915762\nTRAIN: \t Epoch: 141 \t Loss: -0.01330742621794343\nTRAIN: \t Epoch: 141 \t Loss: -0.01327093131840229\nTRAIN: \t Epoch: 141 \t Loss: -0.01336667329693834\nTRAIN: \t Epoch: 141 \t Loss: -0.013344743217413243\nTRAIN: \t Epoch: 141 \t Loss: -0.013227943796664476\nTRAIN: \t Epoch: 141 \t Loss: -0.013193211269875367\nTRAIN: \t Epoch: 141 \t Loss: -0.013258968887384981\nTRAIN: \t Epoch: 141 \t Loss: -0.013314177172587198\nTRAIN: \t Epoch: 141 \t Loss: -0.013306403170443244\nTRAIN: \t Epoch: 141 \t Loss: -0.013209393051894088\nTRAIN: \t Epoch: 141 \t Loss: -0.013165885070338845\nTRAIN: \t Epoch: 141 \t Loss: -0.013175590495978082\nTRAIN: \t Epoch: 141 \t Loss: -0.013202575550045103\nVALD: \t Epoch: 141 \t Loss: -0.009733060374855995\nVALD: \t Epoch: 141 \t Loss: -0.005437230342067778\nVALD: \t Epoch: 141 \t Loss: -0.0076056070781002445\nVALD: \t Epoch: 141 \t Loss: 0.0006073044496588409\nVALD: \t Epoch: 141 \t Loss: -0.0009398675058037043\nVALD: \t Epoch: 141 \t Loss: -0.0009384761874874433\n******************************\nEpoch: social-tag : 141\ntrain_loss -0.013202575550045103\nval_loss -0.0009384761874874433\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 142 \t Loss: -0.014437047764658928\nTRAIN: \t Epoch: 142 \t Loss: -0.014139293693006039\nTRAIN: \t Epoch: 142 \t Loss: -0.013070909306406975\nTRAIN: \t Epoch: 142 \t Loss: -0.012437879806384444\nTRAIN: \t Epoch: 142 \t Loss: -0.012652826122939586\nTRAIN: \t Epoch: 142 \t Loss: -0.012941369165976843\nTRAIN: \t Epoch: 142 \t Loss: -0.013150691187807493\nTRAIN: \t Epoch: 142 \t Loss: -0.013131888350471854\nTRAIN: \t Epoch: 142 \t Loss: -0.013003134789566198\nTRAIN: \t Epoch: 142 \t Loss: -0.013029354438185693\nTRAIN: \t Epoch: 142 \t Loss: -0.012924745170907541\nTRAIN: \t Epoch: 142 \t Loss: -0.012741342730199298\nTRAIN: \t Epoch: 142 \t Loss: -0.012706916277798323\nTRAIN: \t Epoch: 142 \t Loss: -0.012746047295097793\nTRAIN: \t Epoch: 142 \t Loss: -0.012852066072324912\nTRAIN: \t Epoch: 142 \t Loss: -0.012938541418407112\nTRAIN: \t Epoch: 142 \t Loss: -0.012971433975240764\nTRAIN: \t Epoch: 142 \t Loss: -0.012888073714243041\nTRAIN: \t Epoch: 142 \t Loss: -0.01288231467141917\nTRAIN: \t Epoch: 142 \t Loss: -0.012906445330008865\nTRAIN: \t Epoch: 142 \t Loss: -0.012997031078806945\nTRAIN: \t Epoch: 142 \t Loss: -0.013015629273556721\nVALD: \t Epoch: 142 \t Loss: -0.010869817808270454\nVALD: \t Epoch: 142 \t Loss: -0.006757668568752706\nVALD: \t Epoch: 142 \t Loss: -0.007853004693364104\nVALD: \t Epoch: 142 \t Loss: -0.004906631598714739\nVALD: \t Epoch: 142 \t Loss: -0.005602068500593305\nVALD: \t Epoch: 142 \t Loss: -0.005521438417561126\n******************************\nEpoch: social-tag : 142\ntrain_loss -0.013015629273556721\nval_loss -0.005521438417561126\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 143 \t Loss: -0.013630903325974941\nTRAIN: \t Epoch: 143 \t Loss: -0.012893441133201122\nTRAIN: \t Epoch: 143 \t Loss: -0.01256107414762179\nTRAIN: \t Epoch: 143 \t Loss: -0.012596749002113938\nTRAIN: \t Epoch: 143 \t Loss: -0.0127924345433712\nTRAIN: \t Epoch: 143 \t Loss: -0.013018845114856958\nTRAIN: \t Epoch: 143 \t Loss: -0.013250206064965044\nTRAIN: \t Epoch: 143 \t Loss: -0.013287221314385533\nTRAIN: \t Epoch: 143 \t Loss: -0.013362751756277349\nTRAIN: \t Epoch: 143 \t Loss: -0.013283379282802344\nTRAIN: \t Epoch: 143 \t Loss: -0.013139711777594957\nTRAIN: \t Epoch: 143 \t Loss: -0.013054760483404001\nTRAIN: \t Epoch: 143 \t Loss: -0.013159303925931454\nTRAIN: \t Epoch: 143 \t Loss: -0.013201565481722355\nTRAIN: \t Epoch: 143 \t Loss: -0.013074062516291935\nTRAIN: \t Epoch: 143 \t Loss: -0.012851578590925783\nTRAIN: \t Epoch: 143 \t Loss: -0.012828660361907062\nTRAIN: \t Epoch: 143 \t Loss: -0.012872099462482665\nTRAIN: \t Epoch: 143 \t Loss: -0.012952245624833986\nTRAIN: \t Epoch: 143 \t Loss: -0.013026376767084003\nTRAIN: \t Epoch: 143 \t Loss: -0.01301444521439927\nTRAIN: \t Epoch: 143 \t Loss: -0.012960614807087814\nVALD: \t Epoch: 143 \t Loss: -0.011287585832178593\nVALD: \t Epoch: 143 \t Loss: -0.008098448161035776\nVALD: \t Epoch: 143 \t Loss: -0.008797879641254744\nVALD: \t Epoch: 143 \t Loss: -0.007311499095521867\nVALD: \t Epoch: 143 \t Loss: -0.0076663817279040815\nVALD: \t Epoch: 143 \t Loss: -0.007569426083655069\n******************************\nEpoch: social-tag : 143\ntrain_loss -0.012960614807087814\nval_loss -0.007569426083655069\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 144 \t Loss: -0.01346738450229168\nTRAIN: \t Epoch: 144 \t Loss: -0.013465189840644598\nTRAIN: \t Epoch: 144 \t Loss: -0.013264011591672897\nTRAIN: \t Epoch: 144 \t Loss: -0.013307224493473768\nTRAIN: \t Epoch: 144 \t Loss: -0.013276573643088341\nTRAIN: \t Epoch: 144 \t Loss: -0.013128847194214662\nTRAIN: \t Epoch: 144 \t Loss: -0.0131318848580122\nTRAIN: \t Epoch: 144 \t Loss: -0.012993252486921847\nTRAIN: \t Epoch: 144 \t Loss: -0.013026137629316913\nTRAIN: \t Epoch: 144 \t Loss: -0.013138866424560547\nTRAIN: \t Epoch: 144 \t Loss: -0.013094688596373255\nTRAIN: \t Epoch: 144 \t Loss: -0.013121897277111808\nTRAIN: \t Epoch: 144 \t Loss: -0.013082987199035974\nTRAIN: \t Epoch: 144 \t Loss: -0.013037869041519505\nTRAIN: \t Epoch: 144 \t Loss: -0.013106404369076093\nTRAIN: \t Epoch: 144 \t Loss: -0.013166561431717128\nTRAIN: \t Epoch: 144 \t Loss: -0.013101876165498705\nTRAIN: \t Epoch: 144 \t Loss: -0.013034491489330927\nTRAIN: \t Epoch: 144 \t Loss: -0.013049112142700898\nTRAIN: \t Epoch: 144 \t Loss: -0.013092223461717368\nTRAIN: \t Epoch: 144 \t Loss: -0.013106441036576317\nTRAIN: \t Epoch: 144 \t Loss: -0.013095967046234296\nVALD: \t Epoch: 144 \t Loss: 0.00398379797115922\nVALD: \t Epoch: 144 \t Loss: 0.009058703435584903\nVALD: \t Epoch: 144 \t Loss: 0.003806492934624354\nVALD: \t Epoch: 144 \t Loss: 0.00865348195657134\nVALD: \t Epoch: 144 \t Loss: 0.006973218056373298\nVALD: \t Epoch: 144 \t Loss: 0.007195453503818223\n******************************\nEpoch: social-tag : 144\ntrain_loss -0.013095967046234296\nval_loss 0.007195453503818223\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 145 \t Loss: -0.014467349275946617\nTRAIN: \t Epoch: 145 \t Loss: -0.013348611537367105\nTRAIN: \t Epoch: 145 \t Loss: -0.012680753444631895\nTRAIN: \t Epoch: 145 \t Loss: -0.012911104829981923\nTRAIN: \t Epoch: 145 \t Loss: -0.012899663299322128\nTRAIN: \t Epoch: 145 \t Loss: -0.012632875082393488\nTRAIN: \t Epoch: 145 \t Loss: -0.012294005070413862\nTRAIN: \t Epoch: 145 \t Loss: -0.012381890788674355\nTRAIN: \t Epoch: 145 \t Loss: -0.012646385778983435\nTRAIN: \t Epoch: 145 \t Loss: -0.012802226189523935\nTRAIN: \t Epoch: 145 \t Loss: -0.012815738943490114\nTRAIN: \t Epoch: 145 \t Loss: -0.0128208352252841\nTRAIN: \t Epoch: 145 \t Loss: -0.012906951185029287\nTRAIN: \t Epoch: 145 \t Loss: -0.012801031116396189\nTRAIN: \t Epoch: 145 \t Loss: -0.012709945254027844\nTRAIN: \t Epoch: 145 \t Loss: -0.01275355095276609\nTRAIN: \t Epoch: 145 \t Loss: -0.012821020558476448\nTRAIN: \t Epoch: 145 \t Loss: -0.012912998131165901\nTRAIN: \t Epoch: 145 \t Loss: -0.012993059954360911\nTRAIN: \t Epoch: 145 \t Loss: -0.012948027113452554\nTRAIN: \t Epoch: 145 \t Loss: -0.012948888531398205\nTRAIN: \t Epoch: 145 \t Loss: -0.01293225887647865\nVALD: \t Epoch: 145 \t Loss: -0.010823928751051426\nVALD: \t Epoch: 145 \t Loss: -0.00730693677905947\nVALD: \t Epoch: 145 \t Loss: -0.008437030405427018\nVALD: \t Epoch: 145 \t Loss: -0.005997084837872535\nVALD: \t Epoch: 145 \t Loss: -0.006693611526861787\nVALD: \t Epoch: 145 \t Loss: -0.0066566269054557335\n******************************\nEpoch: social-tag : 145\ntrain_loss -0.01293225887647865\nval_loss -0.0066566269054557335\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 146 \t Loss: -0.014687981456518173\nTRAIN: \t Epoch: 146 \t Loss: -0.01430541556328535\nTRAIN: \t Epoch: 146 \t Loss: -0.013588045413295427\nTRAIN: \t Epoch: 146 \t Loss: -0.013395115733146667\nTRAIN: \t Epoch: 146 \t Loss: -0.013489901088178158\nTRAIN: \t Epoch: 146 \t Loss: -0.013545508806904158\nTRAIN: \t Epoch: 146 \t Loss: -0.013244085679096835\nTRAIN: \t Epoch: 146 \t Loss: -0.013130463194102049\nTRAIN: \t Epoch: 146 \t Loss: -0.013211819032828013\nTRAIN: \t Epoch: 146 \t Loss: -0.013343646377325057\nTRAIN: \t Epoch: 146 \t Loss: -0.013305505060336807\nTRAIN: \t Epoch: 146 \t Loss: -0.013159851776435971\nTRAIN: \t Epoch: 146 \t Loss: -0.013124409633187147\nTRAIN: \t Epoch: 146 \t Loss: -0.013209502611841475\nTRAIN: \t Epoch: 146 \t Loss: -0.01318629023929437\nTRAIN: \t Epoch: 146 \t Loss: -0.0132403161842376\nTRAIN: \t Epoch: 146 \t Loss: -0.013173250974539448\nTRAIN: \t Epoch: 146 \t Loss: -0.013080449930081764\nTRAIN: \t Epoch: 146 \t Loss: -0.013015891621379476\nTRAIN: \t Epoch: 146 \t Loss: -0.013082088623195886\nTRAIN: \t Epoch: 146 \t Loss: -0.013128139832544895\nTRAIN: \t Epoch: 146 \t Loss: -0.013150945899730525\nVALD: \t Epoch: 146 \t Loss: -0.01301968190819025\nVALD: \t Epoch: 146 \t Loss: -0.009437203407287598\nVALD: \t Epoch: 146 \t Loss: -0.010595440243681272\nVALD: \t Epoch: 146 \t Loss: -0.004547284683212638\nVALD: \t Epoch: 146 \t Loss: -0.005367863550782203\nVALD: \t Epoch: 146 \t Loss: -0.005254943426134008\n******************************\nEpoch: social-tag : 146\ntrain_loss -0.013150945899730525\nval_loss -0.005254943426134008\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 147 \t Loss: -0.014769282191991806\nTRAIN: \t Epoch: 147 \t Loss: -0.01459254790097475\nTRAIN: \t Epoch: 147 \t Loss: -0.013812656824787458\nTRAIN: \t Epoch: 147 \t Loss: -0.013061575591564178\nTRAIN: \t Epoch: 147 \t Loss: -0.012761168740689754\nTRAIN: \t Epoch: 147 \t Loss: -0.012705145558963219\nTRAIN: \t Epoch: 147 \t Loss: -0.012701624871364661\nTRAIN: \t Epoch: 147 \t Loss: -0.01280002435669303\nTRAIN: \t Epoch: 147 \t Loss: -0.012879278200368086\nTRAIN: \t Epoch: 147 \t Loss: -0.012698755785822868\nTRAIN: \t Epoch: 147 \t Loss: -0.012710278007117186\nTRAIN: \t Epoch: 147 \t Loss: -0.012759905386095246\nTRAIN: \t Epoch: 147 \t Loss: -0.012825467193929048\nTRAIN: \t Epoch: 147 \t Loss: -0.01288459642923304\nTRAIN: \t Epoch: 147 \t Loss: -0.012787639411787193\nTRAIN: \t Epoch: 147 \t Loss: -0.012677222432103008\nTRAIN: \t Epoch: 147 \t Loss: -0.01274639044833534\nTRAIN: \t Epoch: 147 \t Loss: -0.012818830243001381\nTRAIN: \t Epoch: 147 \t Loss: -0.01288873574843532\nTRAIN: \t Epoch: 147 \t Loss: -0.01294245575554669\nTRAIN: \t Epoch: 147 \t Loss: -0.01292484040771212\nTRAIN: \t Epoch: 147 \t Loss: -0.0129511376471665\nVALD: \t Epoch: 147 \t Loss: -0.013171463273465633\nVALD: \t Epoch: 147 \t Loss: -0.010284107644110918\nVALD: \t Epoch: 147 \t Loss: -0.011119009926915169\nVALD: \t Epoch: 147 \t Loss: -0.004285421222448349\nVALD: \t Epoch: 147 \t Loss: -0.005159866996109486\nVALD: \t Epoch: 147 \t Loss: -0.0050694659013639795\n******************************\nEpoch: social-tag : 147\ntrain_loss -0.0129511376471665\nval_loss -0.0050694659013639795\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 148 \t Loss: -0.014154850505292416\nTRAIN: \t Epoch: 148 \t Loss: -0.014448889996856451\nTRAIN: \t Epoch: 148 \t Loss: -0.014371732560296854\nTRAIN: \t Epoch: 148 \t Loss: -0.01382965873926878\nTRAIN: \t Epoch: 148 \t Loss: -0.01301755327731371\nTRAIN: \t Epoch: 148 \t Loss: -0.013018815002093712\nTRAIN: \t Epoch: 148 \t Loss: -0.013344904141766685\nTRAIN: \t Epoch: 148 \t Loss: -0.013519655796699226\nTRAIN: \t Epoch: 148 \t Loss: -0.013587061833176348\nTRAIN: \t Epoch: 148 \t Loss: -0.013531217072159051\nTRAIN: \t Epoch: 148 \t Loss: -0.013395031744783575\nTRAIN: \t Epoch: 148 \t Loss: -0.013251052315657338\nTRAIN: \t Epoch: 148 \t Loss: -0.013262086118069978\nTRAIN: \t Epoch: 148 \t Loss: -0.013339831759887082\nTRAIN: \t Epoch: 148 \t Loss: -0.013339152125020822\nTRAIN: \t Epoch: 148 \t Loss: -0.01337835262529552\nTRAIN: \t Epoch: 148 \t Loss: -0.013403909609598271\nTRAIN: \t Epoch: 148 \t Loss: -0.013261945245580541\nTRAIN: \t Epoch: 148 \t Loss: -0.013160000555217266\nTRAIN: \t Epoch: 148 \t Loss: -0.013210684340447188\nTRAIN: \t Epoch: 148 \t Loss: -0.01325443848257973\nTRAIN: \t Epoch: 148 \t Loss: -0.013293574870809833\nVALD: \t Epoch: 148 \t Loss: -0.012394534423947334\nVALD: \t Epoch: 148 \t Loss: -0.009725943440571427\nVALD: \t Epoch: 148 \t Loss: -0.010534395929425955\nVALD: \t Epoch: 148 \t Loss: -0.0063736317679286\nVALD: \t Epoch: 148 \t Loss: -0.006433122511953116\nVALD: \t Epoch: 148 \t Loss: -0.006323017010634596\n******************************\nEpoch: social-tag : 148\ntrain_loss -0.013293574870809833\nval_loss -0.006323017010634596\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 149 \t Loss: -0.01425485871732235\nTRAIN: \t Epoch: 149 \t Loss: -0.013851606752723455\nTRAIN: \t Epoch: 149 \t Loss: -0.013713202439248562\nTRAIN: \t Epoch: 149 \t Loss: -0.013194199185818434\nTRAIN: \t Epoch: 149 \t Loss: -0.012665780819952488\nTRAIN: \t Epoch: 149 \t Loss: -0.012640967809905609\nTRAIN: \t Epoch: 149 \t Loss: -0.012872690734054362\nTRAIN: \t Epoch: 149 \t Loss: -0.012958405539393425\nTRAIN: \t Epoch: 149 \t Loss: -0.013059540858699216\nTRAIN: \t Epoch: 149 \t Loss: -0.01293244156986475\nTRAIN: \t Epoch: 149 \t Loss: -0.01286677381193096\nTRAIN: \t Epoch: 149 \t Loss: -0.012941715385143956\nTRAIN: \t Epoch: 149 \t Loss: -0.013020277811357608\nTRAIN: \t Epoch: 149 \t Loss: -0.013079971274627107\nTRAIN: \t Epoch: 149 \t Loss: -0.013104698558648428\nTRAIN: \t Epoch: 149 \t Loss: -0.013032878749072552\nTRAIN: \t Epoch: 149 \t Loss: -0.012976538663839592\nTRAIN: \t Epoch: 149 \t Loss: -0.012957503708700338\nTRAIN: \t Epoch: 149 \t Loss: -0.013045488170495159\nTRAIN: \t Epoch: 149 \t Loss: -0.013078017625957727\nTRAIN: \t Epoch: 149 \t Loss: -0.013056702645761626\nTRAIN: \t Epoch: 149 \t Loss: -0.013025278163341474\nVALD: \t Epoch: 149 \t Loss: -0.013622665777802467\nVALD: \t Epoch: 149 \t Loss: -0.011128697078675032\nVALD: \t Epoch: 149 \t Loss: -0.011541835342844328\nVALD: \t Epoch: 149 \t Loss: -0.007211767020635307\nVALD: \t Epoch: 149 \t Loss: -0.007709574047476053\nVALD: \t Epoch: 149 \t Loss: -0.007636646587740291\n******************************\nEpoch: social-tag : 149\ntrain_loss -0.013025278163341474\nval_loss -0.007636646587740291\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 150 \t Loss: -0.013808846473693848\nTRAIN: \t Epoch: 150 \t Loss: -0.013773381244391203\nTRAIN: \t Epoch: 150 \t Loss: -0.0138282539943854\nTRAIN: \t Epoch: 150 \t Loss: -0.013921911595389247\nTRAIN: \t Epoch: 150 \t Loss: -0.014145293831825256\nTRAIN: \t Epoch: 150 \t Loss: -0.014211240224540234\nTRAIN: \t Epoch: 150 \t Loss: -0.014335936334516321\nTRAIN: \t Epoch: 150 \t Loss: -0.014316811226308346\nTRAIN: \t Epoch: 150 \t Loss: -0.01432695353610648\nTRAIN: \t Epoch: 150 \t Loss: -0.014337624609470367\nTRAIN: \t Epoch: 150 \t Loss: -0.014397601427679712\nTRAIN: \t Epoch: 150 \t Loss: -0.014419860749815902\nTRAIN: \t Epoch: 150 \t Loss: -0.014432805160490366\nTRAIN: \t Epoch: 150 \t Loss: -0.014394085189061505\nTRAIN: \t Epoch: 150 \t Loss: -0.014419057468573252\nTRAIN: \t Epoch: 150 \t Loss: -0.01436771551379934\nTRAIN: \t Epoch: 150 \t Loss: -0.01441943842698546\nTRAIN: \t Epoch: 150 \t Loss: -0.01444096847747763\nTRAIN: \t Epoch: 150 \t Loss: -0.014494519680738449\nTRAIN: \t Epoch: 150 \t Loss: -0.014476459845900536\nTRAIN: \t Epoch: 150 \t Loss: -0.014526301418386754\nTRAIN: \t Epoch: 150 \t Loss: -0.014483157865020916\nVALD: \t Epoch: 150 \t Loss: -0.011647152714431286\nVALD: \t Epoch: 150 \t Loss: -0.006105472275521606\nVALD: \t Epoch: 150 \t Loss: -0.007664247687595586\nVALD: \t Epoch: 150 \t Loss: -0.0007018469914328307\nVALD: \t Epoch: 150 \t Loss: -0.002166270813904703\nVALD: \t Epoch: 150 \t Loss: -0.002171697507076191\n******************************\nEpoch: social-tag : 150\ntrain_loss -0.014483157865020916\nval_loss -0.002171697507076191\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 151 \t Loss: -0.014448912814259529\nTRAIN: \t Epoch: 151 \t Loss: -0.01506873033940792\nTRAIN: \t Epoch: 151 \t Loss: -0.015433172384897867\nTRAIN: \t Epoch: 151 \t Loss: -0.01541080977767706\nTRAIN: \t Epoch: 151 \t Loss: -0.0151791051030159\nTRAIN: \t Epoch: 151 \t Loss: -0.015158587756256262\nTRAIN: \t Epoch: 151 \t Loss: -0.015034987325114864\nTRAIN: \t Epoch: 151 \t Loss: -0.01505136617925018\nTRAIN: \t Epoch: 151 \t Loss: -0.015033881904350387\nTRAIN: \t Epoch: 151 \t Loss: -0.015015143807977438\nTRAIN: \t Epoch: 151 \t Loss: -0.015001749704507265\nTRAIN: \t Epoch: 151 \t Loss: -0.01497007238989075\nTRAIN: \t Epoch: 151 \t Loss: -0.014904066203878475\nTRAIN: \t Epoch: 151 \t Loss: -0.014906561068658317\nTRAIN: \t Epoch: 151 \t Loss: -0.014804113718370596\nTRAIN: \t Epoch: 151 \t Loss: -0.014819179836194962\nTRAIN: \t Epoch: 151 \t Loss: -0.01490281494882177\nTRAIN: \t Epoch: 151 \t Loss: -0.014934379814399613\nTRAIN: \t Epoch: 151 \t Loss: -0.014955403871442141\nTRAIN: \t Epoch: 151 \t Loss: -0.014939602604135871\nTRAIN: \t Epoch: 151 \t Loss: -0.014935932388263089\nTRAIN: \t Epoch: 151 \t Loss: -0.014936618616593804\nVALD: \t Epoch: 151 \t Loss: -0.009001843631267548\nVALD: \t Epoch: 151 \t Loss: -0.003112106234766543\nVALD: \t Epoch: 151 \t Loss: -0.0053014355556418495\nVALD: \t Epoch: 151 \t Loss: 0.0008040682296268642\nVALD: \t Epoch: 151 \t Loss: -0.0007479434367269278\nVALD: \t Epoch: 151 \t Loss: -0.0006844690323553302\n******************************\nEpoch: social-tag : 151\ntrain_loss -0.014936618616593804\nval_loss -0.0006844690323553302\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 152 \t Loss: -0.015171588398516178\nTRAIN: \t Epoch: 152 \t Loss: -0.0149428965523839\nTRAIN: \t Epoch: 152 \t Loss: -0.01519173321624597\nTRAIN: \t Epoch: 152 \t Loss: -0.015516826417297125\nTRAIN: \t Epoch: 152 \t Loss: -0.015467124432325364\nTRAIN: \t Epoch: 152 \t Loss: -0.015394650710125765\nTRAIN: \t Epoch: 152 \t Loss: -0.015305125154554844\nTRAIN: \t Epoch: 152 \t Loss: -0.015314326155930758\nTRAIN: \t Epoch: 152 \t Loss: -0.01536063870622052\nTRAIN: \t Epoch: 152 \t Loss: -0.015309457387775182\nTRAIN: \t Epoch: 152 \t Loss: -0.015283566391603514\nTRAIN: \t Epoch: 152 \t Loss: -0.01527169649489224\nTRAIN: \t Epoch: 152 \t Loss: -0.01525481754484085\nTRAIN: \t Epoch: 152 \t Loss: -0.01524808703522597\nTRAIN: \t Epoch: 152 \t Loss: -0.015200040799876054\nTRAIN: \t Epoch: 152 \t Loss: -0.01518439530627802\nTRAIN: \t Epoch: 152 \t Loss: -0.015186343615984215\nTRAIN: \t Epoch: 152 \t Loss: -0.015159396247731315\nTRAIN: \t Epoch: 152 \t Loss: -0.0151196279023823\nTRAIN: \t Epoch: 152 \t Loss: -0.01510128416121006\nTRAIN: \t Epoch: 152 \t Loss: -0.015025056588153044\nTRAIN: \t Epoch: 152 \t Loss: -0.015045591616330926\nVALD: \t Epoch: 152 \t Loss: -0.005356069188565016\nVALD: \t Epoch: 152 \t Loss: 0.0008036175277084112\nVALD: \t Epoch: 152 \t Loss: -0.0025279222366710505\nVALD: \t Epoch: 152 \t Loss: 0.00559717055875808\nVALD: \t Epoch: 152 \t Loss: 0.0036233236081898213\nVALD: \t Epoch: 152 \t Loss: 0.0037828773034341407\n******************************\nEpoch: social-tag : 152\ntrain_loss -0.015045591616330926\nval_loss 0.0037828773034341407\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 153 \t Loss: -0.01624540612101555\nTRAIN: \t Epoch: 153 \t Loss: -0.015559810679405928\nTRAIN: \t Epoch: 153 \t Loss: -0.015747041441500187\nTRAIN: \t Epoch: 153 \t Loss: -0.015396615955978632\nTRAIN: \t Epoch: 153 \t Loss: -0.015151003934442997\nTRAIN: \t Epoch: 153 \t Loss: -0.015138397924602032\nTRAIN: \t Epoch: 153 \t Loss: -0.0149666660332254\nTRAIN: \t Epoch: 153 \t Loss: -0.015007275273092091\nTRAIN: \t Epoch: 153 \t Loss: -0.015146413093639744\nTRAIN: \t Epoch: 153 \t Loss: -0.015211865585297347\nTRAIN: \t Epoch: 153 \t Loss: -0.015080833892253313\nTRAIN: \t Epoch: 153 \t Loss: -0.015090095965812603\nTRAIN: \t Epoch: 153 \t Loss: -0.015067033612957368\nTRAIN: \t Epoch: 153 \t Loss: -0.015009428773607527\nTRAIN: \t Epoch: 153 \t Loss: -0.015051866695284844\nTRAIN: \t Epoch: 153 \t Loss: -0.015073486603796482\nTRAIN: \t Epoch: 153 \t Loss: -0.015124782700749004\nTRAIN: \t Epoch: 153 \t Loss: -0.015123929724925093\nTRAIN: \t Epoch: 153 \t Loss: -0.015123225797555949\nTRAIN: \t Epoch: 153 \t Loss: -0.015139545733109116\nTRAIN: \t Epoch: 153 \t Loss: -0.015110523395595096\nTRAIN: \t Epoch: 153 \t Loss: -0.015122273494782098\nVALD: \t Epoch: 153 \t Loss: -0.009914438240230083\nVALD: \t Epoch: 153 \t Loss: -0.004833994375076145\nVALD: \t Epoch: 153 \t Loss: -0.006626106759843727\nVALD: \t Epoch: 153 \t Loss: 0.001914617168949917\nVALD: \t Epoch: 153 \t Loss: 0.0002529804827645421\nVALD: \t Epoch: 153 \t Loss: 0.0002286559295360789\n******************************\nEpoch: social-tag : 153\ntrain_loss -0.015122273494782098\nval_loss 0.0002286559295360789\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 154 \t Loss: -0.01584085263311863\nTRAIN: \t Epoch: 154 \t Loss: -0.01618150807917118\nTRAIN: \t Epoch: 154 \t Loss: -0.015792877413332462\nTRAIN: \t Epoch: 154 \t Loss: -0.015642265556380153\nTRAIN: \t Epoch: 154 \t Loss: -0.015563092567026615\nTRAIN: \t Epoch: 154 \t Loss: -0.015449477825313807\nTRAIN: \t Epoch: 154 \t Loss: -0.015221957782549518\nTRAIN: \t Epoch: 154 \t Loss: -0.015246701426804066\nTRAIN: \t Epoch: 154 \t Loss: -0.015310153779056337\nTRAIN: \t Epoch: 154 \t Loss: -0.01540139876306057\nTRAIN: \t Epoch: 154 \t Loss: -0.015391631898554888\nTRAIN: \t Epoch: 154 \t Loss: -0.01536064020668467\nTRAIN: \t Epoch: 154 \t Loss: -0.015383927867962765\nTRAIN: \t Epoch: 154 \t Loss: -0.015305672705705677\nTRAIN: \t Epoch: 154 \t Loss: -0.015234802725414436\nTRAIN: \t Epoch: 154 \t Loss: -0.015225634851958603\nTRAIN: \t Epoch: 154 \t Loss: -0.015271401635425933\nTRAIN: \t Epoch: 154 \t Loss: -0.015234452258381579\nTRAIN: \t Epoch: 154 \t Loss: -0.015214662263660054\nTRAIN: \t Epoch: 154 \t Loss: -0.015185642894357443\nTRAIN: \t Epoch: 154 \t Loss: -0.015179192647337914\nTRAIN: \t Epoch: 154 \t Loss: -0.01517361203472747\nVALD: \t Epoch: 154 \t Loss: -0.008743948303163052\nVALD: \t Epoch: 154 \t Loss: -0.0019815193954855204\nVALD: \t Epoch: 154 \t Loss: -0.0040948631552358465\nVALD: \t Epoch: 154 \t Loss: 0.005491338321007788\nVALD: \t Epoch: 154 \t Loss: 0.003301476314663887\nVALD: \t Epoch: 154 \t Loss: 0.0032492433081973684\n******************************\nEpoch: social-tag : 154\ntrain_loss -0.01517361203472747\nval_loss 0.0032492433081973684\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 155 \t Loss: -0.015476169064640999\nTRAIN: \t Epoch: 155 \t Loss: -0.01516234828159213\nTRAIN: \t Epoch: 155 \t Loss: -0.015377051196992397\nTRAIN: \t Epoch: 155 \t Loss: -0.01540849031880498\nTRAIN: \t Epoch: 155 \t Loss: -0.015325428731739522\nTRAIN: \t Epoch: 155 \t Loss: -0.015319922007620335\nTRAIN: \t Epoch: 155 \t Loss: -0.015181406933282102\nTRAIN: \t Epoch: 155 \t Loss: -0.015104192541912198\nTRAIN: \t Epoch: 155 \t Loss: -0.01517710317340162\nTRAIN: \t Epoch: 155 \t Loss: -0.015243599563837052\nTRAIN: \t Epoch: 155 \t Loss: -0.015294903550635685\nTRAIN: \t Epoch: 155 \t Loss: -0.015286360634490848\nTRAIN: \t Epoch: 155 \t Loss: -0.015271029721658964\nTRAIN: \t Epoch: 155 \t Loss: -0.015295011789671012\nTRAIN: \t Epoch: 155 \t Loss: -0.015289820668598016\nTRAIN: \t Epoch: 155 \t Loss: -0.015299526217859238\nTRAIN: \t Epoch: 155 \t Loss: -0.015280080740066135\nTRAIN: \t Epoch: 155 \t Loss: -0.015252460052983629\nTRAIN: \t Epoch: 155 \t Loss: -0.015273560181652246\nTRAIN: \t Epoch: 155 \t Loss: -0.015251732943579555\nTRAIN: \t Epoch: 155 \t Loss: -0.015228098967955225\nTRAIN: \t Epoch: 155 \t Loss: -0.015248008561947616\nVALD: \t Epoch: 155 \t Loss: -0.010191468521952629\nVALD: \t Epoch: 155 \t Loss: -0.0035743279149755836\nVALD: \t Epoch: 155 \t Loss: -0.005038427421823144\nVALD: \t Epoch: 155 \t Loss: 0.002376702439505607\nVALD: \t Epoch: 155 \t Loss: 0.0006673930678516626\nVALD: \t Epoch: 155 \t Loss: 0.0005793858488852328\n******************************\nEpoch: social-tag : 155\ntrain_loss -0.015248008561947616\nval_loss 0.0005793858488852328\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 156 \t Loss: -0.014615381136536598\nTRAIN: \t Epoch: 156 \t Loss: -0.014945433475077152\nTRAIN: \t Epoch: 156 \t Loss: -0.0153391199807326\nTRAIN: \t Epoch: 156 \t Loss: -0.014976384118199348\nTRAIN: \t Epoch: 156 \t Loss: -0.015080677159130574\nTRAIN: \t Epoch: 156 \t Loss: -0.015091125387698412\nTRAIN: \t Epoch: 156 \t Loss: -0.015133376765464033\nTRAIN: \t Epoch: 156 \t Loss: -0.015183606999926269\nTRAIN: \t Epoch: 156 \t Loss: -0.015134890770746602\nTRAIN: \t Epoch: 156 \t Loss: -0.01518129501491785\nTRAIN: \t Epoch: 156 \t Loss: -0.01514658789065751\nTRAIN: \t Epoch: 156 \t Loss: -0.015202813937018314\nTRAIN: \t Epoch: 156 \t Loss: -0.015284138803298656\nTRAIN: \t Epoch: 156 \t Loss: -0.015320786807153906\nTRAIN: \t Epoch: 156 \t Loss: -0.015268546963731448\nTRAIN: \t Epoch: 156 \t Loss: -0.015259292151313275\nTRAIN: \t Epoch: 156 \t Loss: -0.015181500190759407\nTRAIN: \t Epoch: 156 \t Loss: -0.015196839450962014\nTRAIN: \t Epoch: 156 \t Loss: -0.015224881176101534\nTRAIN: \t Epoch: 156 \t Loss: -0.015234523685649037\nTRAIN: \t Epoch: 156 \t Loss: -0.01528492494530621\nTRAIN: \t Epoch: 156 \t Loss: -0.015297682794677078\nVALD: \t Epoch: 156 \t Loss: -0.011727126315236092\nVALD: \t Epoch: 156 \t Loss: -0.0050491843721829355\nVALD: \t Epoch: 156 \t Loss: -0.006779067489939432\nVALD: \t Epoch: 156 \t Loss: 0.0025447958905715495\nVALD: \t Epoch: 156 \t Loss: 0.0008950442308560014\nVALD: \t Epoch: 156 \t Loss: 0.0008333187437418735\n******************************\nEpoch: social-tag : 156\ntrain_loss -0.015297682794677078\nval_loss 0.0008333187437418735\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 157 \t Loss: -0.01572251506149769\nTRAIN: \t Epoch: 157 \t Loss: -0.015156968496739864\nTRAIN: \t Epoch: 157 \t Loss: -0.01518818580855926\nTRAIN: \t Epoch: 157 \t Loss: -0.015414523193612695\nTRAIN: \t Epoch: 157 \t Loss: -0.015421432256698609\nTRAIN: \t Epoch: 157 \t Loss: -0.015485046741863092\nTRAIN: \t Epoch: 157 \t Loss: -0.015413294147167887\nTRAIN: \t Epoch: 157 \t Loss: -0.015527736395597458\nTRAIN: \t Epoch: 157 \t Loss: -0.015511502615279622\nTRAIN: \t Epoch: 157 \t Loss: -0.015478000789880753\nTRAIN: \t Epoch: 157 \t Loss: -0.015435135584663261\nTRAIN: \t Epoch: 157 \t Loss: -0.015441562980413437\nTRAIN: \t Epoch: 157 \t Loss: -0.015473050710100394\nTRAIN: \t Epoch: 157 \t Loss: -0.015455201800380434\nTRAIN: \t Epoch: 157 \t Loss: -0.015415501967072486\nTRAIN: \t Epoch: 157 \t Loss: -0.01541985134826973\nTRAIN: \t Epoch: 157 \t Loss: -0.015394423211760381\nTRAIN: \t Epoch: 157 \t Loss: -0.01537244011544519\nTRAIN: \t Epoch: 157 \t Loss: -0.015319069161226875\nTRAIN: \t Epoch: 157 \t Loss: -0.015329621406272054\nTRAIN: \t Epoch: 157 \t Loss: -0.015324519769776435\nTRAIN: \t Epoch: 157 \t Loss: -0.015338148120489755\nVALD: \t Epoch: 157 \t Loss: -0.006425781641155481\nVALD: \t Epoch: 157 \t Loss: 0.00019527599215507507\nVALD: \t Epoch: 157 \t Loss: -0.0025272260730465255\nVALD: \t Epoch: 157 \t Loss: 0.012820906704291701\nVALD: \t Epoch: 157 \t Loss: 0.009511236986145376\nVALD: \t Epoch: 157 \t Loss: 0.009306831505488266\n******************************\nEpoch: social-tag : 157\ntrain_loss -0.015338148120489755\nval_loss 0.009306831505488266\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 158 \t Loss: -0.015748441219329834\nTRAIN: \t Epoch: 158 \t Loss: -0.016074382700026035\nTRAIN: \t Epoch: 158 \t Loss: -0.015687011492749054\nTRAIN: \t Epoch: 158 \t Loss: -0.015449058497324586\nTRAIN: \t Epoch: 158 \t Loss: -0.01551902573555708\nTRAIN: \t Epoch: 158 \t Loss: -0.015515243945022425\nTRAIN: \t Epoch: 158 \t Loss: -0.015664348910961832\nTRAIN: \t Epoch: 158 \t Loss: -0.015724943485110998\nTRAIN: \t Epoch: 158 \t Loss: -0.015606088667280145\nTRAIN: \t Epoch: 158 \t Loss: -0.015568622201681138\nTRAIN: \t Epoch: 158 \t Loss: -0.015509812076660719\nTRAIN: \t Epoch: 158 \t Loss: -0.015477696045612296\nTRAIN: \t Epoch: 158 \t Loss: -0.015533040277659893\nTRAIN: \t Epoch: 158 \t Loss: -0.015530682701085295\nTRAIN: \t Epoch: 158 \t Loss: -0.015517481043934822\nTRAIN: \t Epoch: 158 \t Loss: -0.015442353731486946\nTRAIN: \t Epoch: 158 \t Loss: -0.015421711127547658\nTRAIN: \t Epoch: 158 \t Loss: -0.015442940923902724\nTRAIN: \t Epoch: 158 \t Loss: -0.015451391178526376\nTRAIN: \t Epoch: 158 \t Loss: -0.015421133209019899\nTRAIN: \t Epoch: 158 \t Loss: -0.01543338242031279\nTRAIN: \t Epoch: 158 \t Loss: -0.015400875791827053\nVALD: \t Epoch: 158 \t Loss: -0.004473666660487652\nVALD: \t Epoch: 158 \t Loss: 0.0036181751638650894\nVALD: \t Epoch: 158 \t Loss: 0.000888201097647349\nVALD: \t Epoch: 158 \t Loss: 0.014148182235658169\nVALD: \t Epoch: 158 \t Loss: 0.01060741562396288\nVALD: \t Epoch: 158 \t Loss: 0.010329478386450897\n******************************\nEpoch: social-tag : 158\ntrain_loss -0.015400875791827053\nval_loss 0.010329478386450897\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 159 \t Loss: -0.015570350922644138\nTRAIN: \t Epoch: 159 \t Loss: -0.015026054810732603\nTRAIN: \t Epoch: 159 \t Loss: -0.015119624634583792\nTRAIN: \t Epoch: 159 \t Loss: -0.015247588977217674\nTRAIN: \t Epoch: 159 \t Loss: -0.015437261387705802\nTRAIN: \t Epoch: 159 \t Loss: -0.015497003371516863\nTRAIN: \t Epoch: 159 \t Loss: -0.01541791350713798\nTRAIN: \t Epoch: 159 \t Loss: -0.015397622715681791\nTRAIN: \t Epoch: 159 \t Loss: -0.015432295699914297\nTRAIN: \t Epoch: 159 \t Loss: -0.01540765967220068\nTRAIN: \t Epoch: 159 \t Loss: -0.015485540879043665\nTRAIN: \t Epoch: 159 \t Loss: -0.015471234917640686\nTRAIN: \t Epoch: 159 \t Loss: -0.015496483215918908\nTRAIN: \t Epoch: 159 \t Loss: -0.015530968617115701\nTRAIN: \t Epoch: 159 \t Loss: -0.015457961149513722\nTRAIN: \t Epoch: 159 \t Loss: -0.015435097448062152\nTRAIN: \t Epoch: 159 \t Loss: -0.015404601745745716\nTRAIN: \t Epoch: 159 \t Loss: -0.015406583125392595\nTRAIN: \t Epoch: 159 \t Loss: -0.015412927164058936\nTRAIN: \t Epoch: 159 \t Loss: -0.015422149794176221\nTRAIN: \t Epoch: 159 \t Loss: -0.015376247864748751\nTRAIN: \t Epoch: 159 \t Loss: -0.015393660526515338\nVALD: \t Epoch: 159 \t Loss: -0.007426697760820389\nVALD: \t Epoch: 159 \t Loss: -0.0020503534469753504\nVALD: \t Epoch: 159 \t Loss: -0.004896810743957758\nVALD: \t Epoch: 159 \t Loss: 0.001698463805951178\nVALD: \t Epoch: 159 \t Loss: 0.00027527520433068275\nVALD: \t Epoch: 159 \t Loss: 0.00043690030773480736\n******************************\nEpoch: social-tag : 159\ntrain_loss -0.015393660526515338\nval_loss 0.00043690030773480736\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 160 \t Loss: -0.014807134866714478\nTRAIN: \t Epoch: 160 \t Loss: -0.01477406220510602\nTRAIN: \t Epoch: 160 \t Loss: -0.014894958895941576\nTRAIN: \t Epoch: 160 \t Loss: -0.014976159436628222\nTRAIN: \t Epoch: 160 \t Loss: -0.015103914961218833\nTRAIN: \t Epoch: 160 \t Loss: -0.015252085402607918\nTRAIN: \t Epoch: 160 \t Loss: -0.015252933172242982\nTRAIN: \t Epoch: 160 \t Loss: -0.015305870212614536\nTRAIN: \t Epoch: 160 \t Loss: -0.015355863918860754\nTRAIN: \t Epoch: 160 \t Loss: -0.015386044047772884\nTRAIN: \t Epoch: 160 \t Loss: -0.015401511199095032\nTRAIN: \t Epoch: 160 \t Loss: -0.015445053732643524\nTRAIN: \t Epoch: 160 \t Loss: -0.015466519703085605\nTRAIN: \t Epoch: 160 \t Loss: -0.015449955061610256\nTRAIN: \t Epoch: 160 \t Loss: -0.01542024885614713\nTRAIN: \t Epoch: 160 \t Loss: -0.015390502288937569\nTRAIN: \t Epoch: 160 \t Loss: -0.015319613246795009\nTRAIN: \t Epoch: 160 \t Loss: -0.0153382557651235\nTRAIN: \t Epoch: 160 \t Loss: -0.015407727688158812\nTRAIN: \t Epoch: 160 \t Loss: -0.015427292929962278\nTRAIN: \t Epoch: 160 \t Loss: -0.015446609551353114\nTRAIN: \t Epoch: 160 \t Loss: -0.015448501294134335\nVALD: \t Epoch: 160 \t Loss: 0.0008093002252280712\nVALD: \t Epoch: 160 \t Loss: 0.007846634136512876\nVALD: \t Epoch: 160 \t Loss: 0.005045572994276881\nVALD: \t Epoch: 160 \t Loss: 0.017531500256154686\nVALD: \t Epoch: 160 \t Loss: 0.013792117265984416\nVALD: \t Epoch: 160 \t Loss: 0.013525453887202523\n******************************\nEpoch: social-tag : 160\ntrain_loss -0.015448501294134335\nval_loss 0.013525453887202523\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 161 \t Loss: -0.015482993796467781\nTRAIN: \t Epoch: 161 \t Loss: -0.015440916176885366\nTRAIN: \t Epoch: 161 \t Loss: -0.015873113957544167\nTRAIN: \t Epoch: 161 \t Loss: -0.015810100128874183\nTRAIN: \t Epoch: 161 \t Loss: -0.015880596078932285\nTRAIN: \t Epoch: 161 \t Loss: -0.015715391530344885\nTRAIN: \t Epoch: 161 \t Loss: -0.01581416292382138\nTRAIN: \t Epoch: 161 \t Loss: -0.015678689116612077\nTRAIN: \t Epoch: 161 \t Loss: -0.015592452966504626\nTRAIN: \t Epoch: 161 \t Loss: -0.015584720484912395\nTRAIN: \t Epoch: 161 \t Loss: -0.01549628402360461\nTRAIN: \t Epoch: 161 \t Loss: -0.015418108009422818\nTRAIN: \t Epoch: 161 \t Loss: -0.015335654338391928\nTRAIN: \t Epoch: 161 \t Loss: -0.015330483870846885\nTRAIN: \t Epoch: 161 \t Loss: -0.01534955222159624\nTRAIN: \t Epoch: 161 \t Loss: -0.01541221042862162\nTRAIN: \t Epoch: 161 \t Loss: -0.015469936380053268\nTRAIN: \t Epoch: 161 \t Loss: -0.015442938957777288\nTRAIN: \t Epoch: 161 \t Loss: -0.015423572867324478\nTRAIN: \t Epoch: 161 \t Loss: -0.015438557509332896\nTRAIN: \t Epoch: 161 \t Loss: -0.015481984686283838\nTRAIN: \t Epoch: 161 \t Loss: -0.01550354606587326\nVALD: \t Epoch: 161 \t Loss: 0.00045870072790421546\nVALD: \t Epoch: 161 \t Loss: 0.007804556356859393\nVALD: \t Epoch: 161 \t Loss: 0.0031301202737571052\nVALD: \t Epoch: 161 \t Loss: 0.017351043214148376\nVALD: \t Epoch: 161 \t Loss: 0.013596926169702784\nVALD: \t Epoch: 161 \t Loss: 0.013552601641100465\n******************************\nEpoch: social-tag : 161\ntrain_loss -0.01550354606587326\nval_loss 0.013552601641100465\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 162 \t Loss: -0.016335126012563705\nTRAIN: \t Epoch: 162 \t Loss: -0.01653956901282072\nTRAIN: \t Epoch: 162 \t Loss: -0.016059461360176403\nTRAIN: \t Epoch: 162 \t Loss: -0.01580698019824922\nTRAIN: \t Epoch: 162 \t Loss: -0.015754847042262553\nTRAIN: \t Epoch: 162 \t Loss: -0.01570464375739296\nTRAIN: \t Epoch: 162 \t Loss: -0.015611342420535428\nTRAIN: \t Epoch: 162 \t Loss: -0.015535030630417168\nTRAIN: \t Epoch: 162 \t Loss: -0.015615318280955156\nTRAIN: \t Epoch: 162 \t Loss: -0.015630554128438234\nTRAIN: \t Epoch: 162 \t Loss: -0.015603942542590878\nTRAIN: \t Epoch: 162 \t Loss: -0.015635996824130416\nTRAIN: \t Epoch: 162 \t Loss: -0.015540469007996412\nTRAIN: \t Epoch: 162 \t Loss: -0.015505121555179358\nTRAIN: \t Epoch: 162 \t Loss: -0.015506413703163465\nTRAIN: \t Epoch: 162 \t Loss: -0.015479882305953652\nTRAIN: \t Epoch: 162 \t Loss: -0.015504366279963185\nTRAIN: \t Epoch: 162 \t Loss: -0.015452068247314956\nTRAIN: \t Epoch: 162 \t Loss: -0.015440261138505057\nTRAIN: \t Epoch: 162 \t Loss: -0.01544742388650775\nTRAIN: \t Epoch: 162 \t Loss: -0.015500276482531003\nTRAIN: \t Epoch: 162 \t Loss: -0.015511960058708807\nVALD: \t Epoch: 162 \t Loss: -0.009230883792042732\nVALD: \t Epoch: 162 \t Loss: -0.0046691967036167625\nVALD: \t Epoch: 162 \t Loss: -0.005956169084432379\nVALD: \t Epoch: 162 \t Loss: -0.001432884384485078\nVALD: \t Epoch: 162 \t Loss: -0.002527024889423046\nVALD: \t Epoch: 162 \t Loss: -0.002474345104543097\n******************************\nEpoch: social-tag : 162\ntrain_loss -0.015511960058708807\nval_loss -0.002474345104543097\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 163 \t Loss: -0.014809200540184975\nTRAIN: \t Epoch: 163 \t Loss: -0.0153519781306386\nTRAIN: \t Epoch: 163 \t Loss: -0.01579187127451102\nTRAIN: \t Epoch: 163 \t Loss: -0.015747708501294255\nTRAIN: \t Epoch: 163 \t Loss: -0.015788781084120275\nTRAIN: \t Epoch: 163 \t Loss: -0.015859544432411592\nTRAIN: \t Epoch: 163 \t Loss: -0.015885533099727973\nTRAIN: \t Epoch: 163 \t Loss: -0.015956675983034074\nTRAIN: \t Epoch: 163 \t Loss: -0.015866241107384365\nTRAIN: \t Epoch: 163 \t Loss: -0.01574695445597172\nTRAIN: \t Epoch: 163 \t Loss: -0.015795417299324818\nTRAIN: \t Epoch: 163 \t Loss: -0.01586018068095048\nTRAIN: \t Epoch: 163 \t Loss: -0.015754335273343783\nTRAIN: \t Epoch: 163 \t Loss: -0.015621417933808905\nTRAIN: \t Epoch: 163 \t Loss: -0.015600331500172615\nTRAIN: \t Epoch: 163 \t Loss: -0.015591413422953337\nTRAIN: \t Epoch: 163 \t Loss: -0.015521114856442985\nTRAIN: \t Epoch: 163 \t Loss: -0.01554139812166492\nTRAIN: \t Epoch: 163 \t Loss: -0.015595070282487493\nTRAIN: \t Epoch: 163 \t Loss: -0.015571904787793755\nTRAIN: \t Epoch: 163 \t Loss: -0.0155417536873193\nTRAIN: \t Epoch: 163 \t Loss: -0.015530097291970382\nVALD: \t Epoch: 163 \t Loss: -0.00824812427163124\nVALD: \t Epoch: 163 \t Loss: -0.0018674808088690042\nVALD: \t Epoch: 163 \t Loss: -0.004291612189263105\nVALD: \t Epoch: 163 \t Loss: 0.01028307096567005\nVALD: \t Epoch: 163 \t Loss: 0.007234774250537157\nVALD: \t Epoch: 163 \t Loss: 0.007075167977900216\n******************************\nEpoch: social-tag : 163\ntrain_loss -0.015530097291970382\nval_loss 0.007075167977900216\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 164 \t Loss: -0.015337543562054634\nTRAIN: \t Epoch: 164 \t Loss: -0.016173388808965683\nTRAIN: \t Epoch: 164 \t Loss: -0.016059243430693943\nTRAIN: \t Epoch: 164 \t Loss: -0.016259970609098673\nTRAIN: \t Epoch: 164 \t Loss: -0.016287648305296897\nTRAIN: \t Epoch: 164 \t Loss: -0.016184090015788872\nTRAIN: \t Epoch: 164 \t Loss: -0.016029896081558297\nTRAIN: \t Epoch: 164 \t Loss: -0.015802156296558678\nTRAIN: \t Epoch: 164 \t Loss: -0.01581232508437501\nTRAIN: \t Epoch: 164 \t Loss: -0.015670087095350026\nTRAIN: \t Epoch: 164 \t Loss: -0.015622976574708115\nTRAIN: \t Epoch: 164 \t Loss: -0.01566354596676926\nTRAIN: \t Epoch: 164 \t Loss: -0.015617101811445676\nTRAIN: \t Epoch: 164 \t Loss: -0.015643505645649775\nTRAIN: \t Epoch: 164 \t Loss: -0.015599222046633561\nTRAIN: \t Epoch: 164 \t Loss: -0.015591978037264198\nTRAIN: \t Epoch: 164 \t Loss: -0.015574156635386102\nTRAIN: \t Epoch: 164 \t Loss: -0.015594277065247297\nTRAIN: \t Epoch: 164 \t Loss: -0.015583666629697146\nTRAIN: \t Epoch: 164 \t Loss: -0.015564859425649048\nTRAIN: \t Epoch: 164 \t Loss: -0.015549991368537857\nTRAIN: \t Epoch: 164 \t Loss: -0.015553742122821363\nVALD: \t Epoch: 164 \t Loss: -0.003710189601406455\nVALD: \t Epoch: 164 \t Loss: 0.003404377610422671\nVALD: \t Epoch: 164 \t Loss: -0.0009507747211803993\nVALD: \t Epoch: 164 \t Loss: 0.013956010399851948\nVALD: \t Epoch: 164 \t Loss: 0.010739156510680914\nVALD: \t Epoch: 164 \t Loss: 0.010711319270459088\n******************************\nEpoch: social-tag : 164\ntrain_loss -0.015553742122821363\nval_loss 0.010711319270459088\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 165 \t Loss: -0.015709975734353065\nTRAIN: \t Epoch: 165 \t Loss: -0.015957387164235115\nTRAIN: \t Epoch: 165 \t Loss: -0.015822988003492355\nTRAIN: \t Epoch: 165 \t Loss: -0.015810422599315643\nTRAIN: \t Epoch: 165 \t Loss: -0.015764366649091244\nTRAIN: \t Epoch: 165 \t Loss: -0.015617230286200842\nTRAIN: \t Epoch: 165 \t Loss: -0.015748427116445134\nTRAIN: \t Epoch: 165 \t Loss: -0.01572684559505433\nTRAIN: \t Epoch: 165 \t Loss: -0.015797482286062505\nTRAIN: \t Epoch: 165 \t Loss: -0.01575527722015977\nTRAIN: \t Epoch: 165 \t Loss: -0.015711018815636635\nTRAIN: \t Epoch: 165 \t Loss: -0.015688476851209998\nTRAIN: \t Epoch: 165 \t Loss: -0.01576304600502436\nTRAIN: \t Epoch: 165 \t Loss: -0.015730076602527072\nTRAIN: \t Epoch: 165 \t Loss: -0.01577880730231603\nTRAIN: \t Epoch: 165 \t Loss: -0.015771627076901495\nTRAIN: \t Epoch: 165 \t Loss: -0.015676806626074454\nTRAIN: \t Epoch: 165 \t Loss: -0.015657659930487473\nTRAIN: \t Epoch: 165 \t Loss: -0.015660300262664493\nTRAIN: \t Epoch: 165 \t Loss: -0.015614234935492277\nTRAIN: \t Epoch: 165 \t Loss: -0.015553606895818597\nTRAIN: \t Epoch: 165 \t Loss: -0.015541231225594155\nVALD: \t Epoch: 165 \t Loss: 0.010260474868118763\nVALD: \t Epoch: 165 \t Loss: 0.017136421520262957\nVALD: \t Epoch: 165 \t Loss: 0.011696339293848723\nVALD: \t Epoch: 165 \t Loss: 0.026319522890844382\nVALD: \t Epoch: 165 \t Loss: 0.021788595255929976\nVALD: \t Epoch: 165 \t Loss: 0.021628961610523138\n******************************\nEpoch: social-tag : 165\ntrain_loss -0.015541231225594155\nval_loss 0.021628961610523138\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 166 \t Loss: -0.01644260250031948\nTRAIN: \t Epoch: 166 \t Loss: -0.016081697307527065\nTRAIN: \t Epoch: 166 \t Loss: -0.015675947070121765\nTRAIN: \t Epoch: 166 \t Loss: -0.015525273745879531\nTRAIN: \t Epoch: 166 \t Loss: -0.015511555410921574\nTRAIN: \t Epoch: 166 \t Loss: -0.015680228515217703\nTRAIN: \t Epoch: 166 \t Loss: -0.015635539378438677\nTRAIN: \t Epoch: 166 \t Loss: -0.015589887625537813\nTRAIN: \t Epoch: 166 \t Loss: -0.015560871404078271\nTRAIN: \t Epoch: 166 \t Loss: -0.015489875711500645\nTRAIN: \t Epoch: 166 \t Loss: -0.015484147535806353\nTRAIN: \t Epoch: 166 \t Loss: -0.015481986027831832\nTRAIN: \t Epoch: 166 \t Loss: -0.015510865487158298\nTRAIN: \t Epoch: 166 \t Loss: -0.015495061076113157\nTRAIN: \t Epoch: 166 \t Loss: -0.015491108906765779\nTRAIN: \t Epoch: 166 \t Loss: -0.015541466709692031\nTRAIN: \t Epoch: 166 \t Loss: -0.015590491814210135\nTRAIN: \t Epoch: 166 \t Loss: -0.015574925630870793\nTRAIN: \t Epoch: 166 \t Loss: -0.01557751691066905\nTRAIN: \t Epoch: 166 \t Loss: -0.015611957991495728\nTRAIN: \t Epoch: 166 \t Loss: -0.015556752770429566\nTRAIN: \t Epoch: 166 \t Loss: -0.015577781564243392\nVALD: \t Epoch: 166 \t Loss: -0.0010511926375329494\nVALD: \t Epoch: 166 \t Loss: 0.007169243646785617\nVALD: \t Epoch: 166 \t Loss: 0.004822993704389471\nVALD: \t Epoch: 166 \t Loss: 0.02070942106001894\nVALD: \t Epoch: 166 \t Loss: 0.016645652035367674\nVALD: \t Epoch: 166 \t Loss: 0.016140265625929743\n******************************\nEpoch: social-tag : 166\ntrain_loss -0.015577781564243392\nval_loss 0.016140265625929743\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 167 \t Loss: -0.016224002465605736\nTRAIN: \t Epoch: 167 \t Loss: -0.016363861970603466\nTRAIN: \t Epoch: 167 \t Loss: -0.016390796129902203\nTRAIN: \t Epoch: 167 \t Loss: -0.016234349925071\nTRAIN: \t Epoch: 167 \t Loss: -0.016003847122192383\nTRAIN: \t Epoch: 167 \t Loss: -0.015940426072726648\nTRAIN: \t Epoch: 167 \t Loss: -0.015898773313633034\nTRAIN: \t Epoch: 167 \t Loss: -0.015839343541301787\nTRAIN: \t Epoch: 167 \t Loss: -0.01572609165062507\nTRAIN: \t Epoch: 167 \t Loss: -0.0157656772993505\nTRAIN: \t Epoch: 167 \t Loss: -0.01564942596649582\nTRAIN: \t Epoch: 167 \t Loss: -0.015624055794129768\nTRAIN: \t Epoch: 167 \t Loss: -0.01557866011101466\nTRAIN: \t Epoch: 167 \t Loss: -0.015583150221833162\nTRAIN: \t Epoch: 167 \t Loss: -0.015503518717984359\nTRAIN: \t Epoch: 167 \t Loss: -0.0155690319952555\nTRAIN: \t Epoch: 167 \t Loss: -0.01561154003309853\nTRAIN: \t Epoch: 167 \t Loss: -0.01562676376973589\nTRAIN: \t Epoch: 167 \t Loss: -0.015579325490091977\nTRAIN: \t Epoch: 167 \t Loss: -0.01555532137863338\nTRAIN: \t Epoch: 167 \t Loss: -0.015591158620303585\nTRAIN: \t Epoch: 167 \t Loss: -0.015583791510207006\nVALD: \t Epoch: 167 \t Loss: -0.009380267933011055\nVALD: \t Epoch: 167 \t Loss: -0.002640198450535536\nVALD: \t Epoch: 167 \t Loss: -0.004830286838114262\nVALD: \t Epoch: 167 \t Loss: 0.006634644465520978\nVALD: \t Epoch: 167 \t Loss: 0.004244554601609707\nVALD: \t Epoch: 167 \t Loss: 0.004179628836837682\n******************************\nEpoch: social-tag : 167\ntrain_loss -0.015583791510207006\nval_loss 0.004179628836837682\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 168 \t Loss: -0.01587962545454502\nTRAIN: \t Epoch: 168 \t Loss: -0.015396765898913145\nTRAIN: \t Epoch: 168 \t Loss: -0.015351790003478527\nTRAIN: \t Epoch: 168 \t Loss: -0.01540116872638464\nTRAIN: \t Epoch: 168 \t Loss: -0.015245328471064568\nTRAIN: \t Epoch: 168 \t Loss: -0.01543323447306951\nTRAIN: \t Epoch: 168 \t Loss: -0.015343796461820602\nTRAIN: \t Epoch: 168 \t Loss: -0.015265411348082125\nTRAIN: \t Epoch: 168 \t Loss: -0.015400265550447835\nTRAIN: \t Epoch: 168 \t Loss: -0.015492445137351752\nTRAIN: \t Epoch: 168 \t Loss: -0.015592981553213163\nTRAIN: \t Epoch: 168 \t Loss: -0.015661529498174787\nTRAIN: \t Epoch: 168 \t Loss: -0.01562931718161473\nTRAIN: \t Epoch: 168 \t Loss: -0.015589612003948008\nTRAIN: \t Epoch: 168 \t Loss: -0.01558752916753292\nTRAIN: \t Epoch: 168 \t Loss: -0.01562567229848355\nTRAIN: \t Epoch: 168 \t Loss: -0.015614253907080959\nTRAIN: \t Epoch: 168 \t Loss: -0.015632926848613553\nTRAIN: \t Epoch: 168 \t Loss: -0.015621313089995007\nTRAIN: \t Epoch: 168 \t Loss: -0.01564271510578692\nTRAIN: \t Epoch: 168 \t Loss: -0.01561386723603521\nTRAIN: \t Epoch: 168 \t Loss: -0.01560930525165058\nVALD: \t Epoch: 168 \t Loss: -0.005343188066035509\nVALD: \t Epoch: 168 \t Loss: 0.00317021063528955\nVALD: \t Epoch: 168 \t Loss: 0.0010023120945940416\nVALD: \t Epoch: 168 \t Loss: 0.01333459309535101\nVALD: \t Epoch: 168 \t Loss: 0.009875229792669415\nVALD: \t Epoch: 168 \t Loss: 0.009493596089834517\n******************************\nEpoch: social-tag : 168\ntrain_loss -0.01560930525165058\nval_loss 0.009493596089834517\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 169 \t Loss: -0.014530451968312263\nTRAIN: \t Epoch: 169 \t Loss: -0.014912125188857317\nTRAIN: \t Epoch: 169 \t Loss: -0.015365560539066792\nTRAIN: \t Epoch: 169 \t Loss: -0.015613134717568755\nTRAIN: \t Epoch: 169 \t Loss: -0.015522093884646893\nTRAIN: \t Epoch: 169 \t Loss: -0.01551579808195432\nTRAIN: \t Epoch: 169 \t Loss: -0.015355196914502553\nTRAIN: \t Epoch: 169 \t Loss: -0.015373342204838991\nTRAIN: \t Epoch: 169 \t Loss: -0.015534562576148245\nTRAIN: \t Epoch: 169 \t Loss: -0.015592515654861926\nTRAIN: \t Epoch: 169 \t Loss: -0.01569875160401518\nTRAIN: \t Epoch: 169 \t Loss: -0.015707861476888258\nTRAIN: \t Epoch: 169 \t Loss: -0.015664873644709587\nTRAIN: \t Epoch: 169 \t Loss: -0.0157137235094394\nTRAIN: \t Epoch: 169 \t Loss: -0.015716163565715154\nTRAIN: \t Epoch: 169 \t Loss: -0.015723393647931516\nTRAIN: \t Epoch: 169 \t Loss: -0.01575021697756122\nTRAIN: \t Epoch: 169 \t Loss: -0.01571926221044527\nTRAIN: \t Epoch: 169 \t Loss: -0.015719913681478875\nTRAIN: \t Epoch: 169 \t Loss: -0.01567023159004748\nTRAIN: \t Epoch: 169 \t Loss: -0.015634613067266486\nTRAIN: \t Epoch: 169 \t Loss: -0.01563277163240169\nVALD: \t Epoch: 169 \t Loss: -0.008184881880879402\nVALD: \t Epoch: 169 \t Loss: -0.001869663130491972\nVALD: \t Epoch: 169 \t Loss: -0.0047967468077937765\nVALD: \t Epoch: 169 \t Loss: 0.006178384879603982\nVALD: \t Epoch: 169 \t Loss: 0.003891979064792395\nVALD: \t Epoch: 169 \t Loss: 0.003804173464463516\n******************************\nEpoch: social-tag : 169\ntrain_loss -0.01563277163240169\nval_loss 0.003804173464463516\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 170 \t Loss: -0.01556664053350687\nTRAIN: \t Epoch: 170 \t Loss: -0.015524753835052252\nTRAIN: \t Epoch: 170 \t Loss: -0.015527568757534027\nTRAIN: \t Epoch: 170 \t Loss: -0.0155208979267627\nTRAIN: \t Epoch: 170 \t Loss: -0.01581172812730074\nTRAIN: \t Epoch: 170 \t Loss: -0.01571344987799724\nTRAIN: \t Epoch: 170 \t Loss: -0.015612558328679629\nTRAIN: \t Epoch: 170 \t Loss: -0.015570943476632237\nTRAIN: \t Epoch: 170 \t Loss: -0.015621524097190963\nTRAIN: \t Epoch: 170 \t Loss: -0.015617511421442031\nTRAIN: \t Epoch: 170 \t Loss: -0.015610041689466347\nTRAIN: \t Epoch: 170 \t Loss: -0.01551568298600614\nTRAIN: \t Epoch: 170 \t Loss: -0.015492595374011077\nTRAIN: \t Epoch: 170 \t Loss: -0.01554505932810051\nTRAIN: \t Epoch: 170 \t Loss: -0.01554317759970824\nTRAIN: \t Epoch: 170 \t Loss: -0.015585338929668069\nTRAIN: \t Epoch: 170 \t Loss: -0.015568847932359752\nTRAIN: \t Epoch: 170 \t Loss: -0.015586436726152897\nTRAIN: \t Epoch: 170 \t Loss: -0.0155763146222422\nTRAIN: \t Epoch: 170 \t Loss: -0.01562469438649714\nTRAIN: \t Epoch: 170 \t Loss: -0.015635797975673563\nTRAIN: \t Epoch: 170 \t Loss: -0.015620942603749903\nVALD: \t Epoch: 170 \t Loss: -0.008956183679401875\nVALD: \t Epoch: 170 \t Loss: -0.0030404088320210576\nVALD: \t Epoch: 170 \t Loss: -0.004906970464314024\nVALD: \t Epoch: 170 \t Loss: 0.00500039424514398\nVALD: \t Epoch: 170 \t Loss: 0.002954058488830924\nVALD: \t Epoch: 170 \t Loss: 0.0028893198084199065\n******************************\nEpoch: social-tag : 170\ntrain_loss -0.015620942603749903\nval_loss 0.0028893198084199065\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 171 \t Loss: -0.015742413699626923\nTRAIN: \t Epoch: 171 \t Loss: -0.015976526774466038\nTRAIN: \t Epoch: 171 \t Loss: -0.015977954491972923\nTRAIN: \t Epoch: 171 \t Loss: -0.01585682574659586\nTRAIN: \t Epoch: 171 \t Loss: -0.015833280235528945\nTRAIN: \t Epoch: 171 \t Loss: -0.015788584637145203\nTRAIN: \t Epoch: 171 \t Loss: -0.01585265408669199\nTRAIN: \t Epoch: 171 \t Loss: -0.01589236198924482\nTRAIN: \t Epoch: 171 \t Loss: -0.015873005613684654\nTRAIN: \t Epoch: 171 \t Loss: -0.01576309688389301\nTRAIN: \t Epoch: 171 \t Loss: -0.015649562362920155\nTRAIN: \t Epoch: 171 \t Loss: -0.015746791536609333\nTRAIN: \t Epoch: 171 \t Loss: -0.015733052761508867\nTRAIN: \t Epoch: 171 \t Loss: -0.015630622128290788\nTRAIN: \t Epoch: 171 \t Loss: -0.01563707503179709\nTRAIN: \t Epoch: 171 \t Loss: -0.015532164892647415\nTRAIN: \t Epoch: 171 \t Loss: -0.015550815062049557\nTRAIN: \t Epoch: 171 \t Loss: -0.01561165958022078\nTRAIN: \t Epoch: 171 \t Loss: -0.015625779124859133\nTRAIN: \t Epoch: 171 \t Loss: -0.01562769799493253\nTRAIN: \t Epoch: 171 \t Loss: -0.0156477823232611\nTRAIN: \t Epoch: 171 \t Loss: -0.015646703024954942\nVALD: \t Epoch: 171 \t Loss: -0.0049983602948486805\nVALD: \t Epoch: 171 \t Loss: 0.0016609698068350554\nVALD: \t Epoch: 171 \t Loss: -4.13420299688975e-05\nVALD: \t Epoch: 171 \t Loss: 0.012807806953787804\nVALD: \t Epoch: 171 \t Loss: 0.009261000994592905\nVALD: \t Epoch: 171 \t Loss: 0.008914663033051924\n******************************\nEpoch: social-tag : 171\ntrain_loss -0.015646703024954942\nval_loss 0.008914663033051924\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 172 \t Loss: -0.015294194221496582\nTRAIN: \t Epoch: 172 \t Loss: -0.015289242845028639\nTRAIN: \t Epoch: 172 \t Loss: -0.015307950787246227\nTRAIN: \t Epoch: 172 \t Loss: -0.015322689665481448\nTRAIN: \t Epoch: 172 \t Loss: -0.015559718571603298\nTRAIN: \t Epoch: 172 \t Loss: -0.015754790510982275\nTRAIN: \t Epoch: 172 \t Loss: -0.015889977903238366\nTRAIN: \t Epoch: 172 \t Loss: -0.01590671797748655\nTRAIN: \t Epoch: 172 \t Loss: -0.01584617907388343\nTRAIN: \t Epoch: 172 \t Loss: -0.015732930134981872\nTRAIN: \t Epoch: 172 \t Loss: -0.01569529165598479\nTRAIN: \t Epoch: 172 \t Loss: -0.015699471812695265\nTRAIN: \t Epoch: 172 \t Loss: -0.015702196038686313\nTRAIN: \t Epoch: 172 \t Loss: -0.015728914445000037\nTRAIN: \t Epoch: 172 \t Loss: -0.015694731598099074\nTRAIN: \t Epoch: 172 \t Loss: -0.015722439042292535\nTRAIN: \t Epoch: 172 \t Loss: -0.01570747047662735\nTRAIN: \t Epoch: 172 \t Loss: -0.015718462152613535\nTRAIN: \t Epoch: 172 \t Loss: -0.015724572990285724\nTRAIN: \t Epoch: 172 \t Loss: -0.015695739910006522\nTRAIN: \t Epoch: 172 \t Loss: -0.01565310616223585\nTRAIN: \t Epoch: 172 \t Loss: -0.01567150369366795\nVALD: \t Epoch: 172 \t Loss: -0.005753153469413519\nVALD: \t Epoch: 172 \t Loss: 0.0008182392921298742\nVALD: \t Epoch: 172 \t Loss: -0.0020677951785425344\nVALD: \t Epoch: 172 \t Loss: 0.012072773999534547\nVALD: \t Epoch: 172 \t Loss: 0.008911781525239349\nVALD: \t Epoch: 172 \t Loss: 0.008830535276369615\n******************************\nEpoch: social-tag : 172\ntrain_loss -0.01567150369366795\nval_loss 0.008830535276369615\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 173 \t Loss: -0.015102801844477654\nTRAIN: \t Epoch: 173 \t Loss: -0.01556794997304678\nTRAIN: \t Epoch: 173 \t Loss: -0.015701751535137493\nTRAIN: \t Epoch: 173 \t Loss: -0.015798982232809067\nTRAIN: \t Epoch: 173 \t Loss: -0.015600292943418026\nTRAIN: \t Epoch: 173 \t Loss: -0.015491731464862823\nTRAIN: \t Epoch: 173 \t Loss: -0.015601038932800293\nTRAIN: \t Epoch: 173 \t Loss: -0.015488642849959433\nTRAIN: \t Epoch: 173 \t Loss: -0.015513530518445704\nTRAIN: \t Epoch: 173 \t Loss: -0.015406267158687115\nTRAIN: \t Epoch: 173 \t Loss: -0.015415311960334127\nTRAIN: \t Epoch: 173 \t Loss: -0.015466784049446384\nTRAIN: \t Epoch: 173 \t Loss: -0.01551586353721527\nTRAIN: \t Epoch: 173 \t Loss: -0.01553819069106664\nTRAIN: \t Epoch: 173 \t Loss: -0.01563791763037443\nTRAIN: \t Epoch: 173 \t Loss: -0.015711016312707216\nTRAIN: \t Epoch: 173 \t Loss: -0.015666115974240443\nTRAIN: \t Epoch: 173 \t Loss: -0.015633664093911648\nTRAIN: \t Epoch: 173 \t Loss: -0.01562381408324367\nTRAIN: \t Epoch: 173 \t Loss: -0.015635897032916547\nTRAIN: \t Epoch: 173 \t Loss: -0.015695569859374137\nTRAIN: \t Epoch: 173 \t Loss: -0.015662724025801652\nVALD: \t Epoch: 173 \t Loss: -0.0032457804773002863\nVALD: \t Epoch: 173 \t Loss: 0.00632309156935662\nVALD: \t Epoch: 173 \t Loss: 0.002792588978384932\nVALD: \t Epoch: 173 \t Loss: 0.01577286300016567\nVALD: \t Epoch: 173 \t Loss: 0.01234648295212537\nVALD: \t Epoch: 173 \t Loss: 0.012122653165098392\n******************************\nEpoch: social-tag : 173\ntrain_loss -0.015662724025801652\nval_loss 0.012122653165098392\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 174 \t Loss: -0.01601441763341427\nTRAIN: \t Epoch: 174 \t Loss: -0.015792477875947952\nTRAIN: \t Epoch: 174 \t Loss: -0.015748742346962292\nTRAIN: \t Epoch: 174 \t Loss: -0.015887838322669268\nTRAIN: \t Epoch: 174 \t Loss: -0.01575946044176817\nTRAIN: \t Epoch: 174 \t Loss: -0.015583933486292759\nTRAIN: \t Epoch: 174 \t Loss: -0.015476268050926072\nTRAIN: \t Epoch: 174 \t Loss: -0.01552542601712048\nTRAIN: \t Epoch: 174 \t Loss: -0.01562204977704419\nTRAIN: \t Epoch: 174 \t Loss: -0.015579537022858858\nTRAIN: \t Epoch: 174 \t Loss: -0.015487851744348352\nTRAIN: \t Epoch: 174 \t Loss: -0.015509608512123426\nTRAIN: \t Epoch: 174 \t Loss: -0.015593299642205238\nTRAIN: \t Epoch: 174 \t Loss: -0.015621881798974105\nTRAIN: \t Epoch: 174 \t Loss: -0.01562747620046139\nTRAIN: \t Epoch: 174 \t Loss: -0.015568175120279193\nTRAIN: \t Epoch: 174 \t Loss: -0.015624375904307646\nTRAIN: \t Epoch: 174 \t Loss: -0.01564274076372385\nTRAIN: \t Epoch: 174 \t Loss: -0.01564475982204864\nTRAIN: \t Epoch: 174 \t Loss: -0.015625459235161544\nTRAIN: \t Epoch: 174 \t Loss: -0.015598541968280361\nTRAIN: \t Epoch: 174 \t Loss: -0.015623712710889066\nVALD: \t Epoch: 174 \t Loss: 0.003271428868174553\nVALD: \t Epoch: 174 \t Loss: 0.009873584844172001\nVALD: \t Epoch: 174 \t Loss: 0.005211660172790289\nVALD: \t Epoch: 174 \t Loss: 0.01854892645496875\nVALD: \t Epoch: 174 \t Loss: 0.015050582867115737\nVALD: \t Epoch: 174 \t Loss: 0.015071620724417947\n******************************\nEpoch: social-tag : 174\ntrain_loss -0.015623712710889066\nval_loss 0.015071620724417947\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 175 \t Loss: -0.015796570107340813\nTRAIN: \t Epoch: 175 \t Loss: -0.01640180218964815\nTRAIN: \t Epoch: 175 \t Loss: -0.016278982783357304\nTRAIN: \t Epoch: 175 \t Loss: -0.01608842145651579\nTRAIN: \t Epoch: 175 \t Loss: -0.01591678261756897\nTRAIN: \t Epoch: 175 \t Loss: -0.015763478664060433\nTRAIN: \t Epoch: 175 \t Loss: -0.015796494537166188\nTRAIN: \t Epoch: 175 \t Loss: -0.015719168120995164\nTRAIN: \t Epoch: 175 \t Loss: -0.015801454997724958\nTRAIN: \t Epoch: 175 \t Loss: -0.015797363221645357\nTRAIN: \t Epoch: 175 \t Loss: -0.015852745961059223\nTRAIN: \t Epoch: 175 \t Loss: -0.015876976152261097\nTRAIN: \t Epoch: 175 \t Loss: -0.015823703474150255\nTRAIN: \t Epoch: 175 \t Loss: -0.015775217741195644\nTRAIN: \t Epoch: 175 \t Loss: -0.015799814152220884\nTRAIN: \t Epoch: 175 \t Loss: -0.01578357390826568\nTRAIN: \t Epoch: 175 \t Loss: -0.01577528681167785\nTRAIN: \t Epoch: 175 \t Loss: -0.015806459149138793\nTRAIN: \t Epoch: 175 \t Loss: -0.015710603935938133\nTRAIN: \t Epoch: 175 \t Loss: -0.015712639875710012\nTRAIN: \t Epoch: 175 \t Loss: -0.015707417346891902\nTRAIN: \t Epoch: 175 \t Loss: -0.015723589292762522\nVALD: \t Epoch: 175 \t Loss: -0.000656565825920552\nVALD: \t Epoch: 175 \t Loss: 0.0050926961994264275\nVALD: \t Epoch: 175 \t Loss: 0.002567989305437853\nVALD: \t Epoch: 175 \t Loss: 0.014290003236965276\nVALD: \t Epoch: 175 \t Loss: 0.010917217994574457\nVALD: \t Epoch: 175 \t Loss: 0.010615857555107636\n******************************\nEpoch: social-tag : 175\ntrain_loss -0.015723589292762522\nval_loss 0.010615857555107636\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 176 \t Loss: -0.016437912359833717\nTRAIN: \t Epoch: 176 \t Loss: -0.016214308328926563\nTRAIN: \t Epoch: 176 \t Loss: -0.015918490787347157\nTRAIN: \t Epoch: 176 \t Loss: -0.015490424586459994\nTRAIN: \t Epoch: 176 \t Loss: -0.015427171438932418\nTRAIN: \t Epoch: 176 \t Loss: -0.01539301018541058\nTRAIN: \t Epoch: 176 \t Loss: -0.015486940342400755\nTRAIN: \t Epoch: 176 \t Loss: -0.015451488201506436\nTRAIN: \t Epoch: 176 \t Loss: -0.015404369578593306\nTRAIN: \t Epoch: 176 \t Loss: -0.015388221573084592\nTRAIN: \t Epoch: 176 \t Loss: -0.01535529490898956\nTRAIN: \t Epoch: 176 \t Loss: -0.015439873561263084\nTRAIN: \t Epoch: 176 \t Loss: -0.015457356921755351\nTRAIN: \t Epoch: 176 \t Loss: -0.015480908432177134\nTRAIN: \t Epoch: 176 \t Loss: -0.015493853638569513\nTRAIN: \t Epoch: 176 \t Loss: -0.01550515799317509\nTRAIN: \t Epoch: 176 \t Loss: -0.015531639732858715\nTRAIN: \t Epoch: 176 \t Loss: -0.015565370002554523\nTRAIN: \t Epoch: 176 \t Loss: -0.015606029939494635\nTRAIN: \t Epoch: 176 \t Loss: -0.015665787644684313\nTRAIN: \t Epoch: 176 \t Loss: -0.01567499242013409\nTRAIN: \t Epoch: 176 \t Loss: -0.015691268251443038\nVALD: \t Epoch: 176 \t Loss: 0.006507729645818472\nVALD: \t Epoch: 176 \t Loss: 0.009352720575407147\nVALD: \t Epoch: 176 \t Loss: 0.0030300600143770375\nVALD: \t Epoch: 176 \t Loss: 0.017319415346719325\nVALD: \t Epoch: 176 \t Loss: 0.014892799407243728\nVALD: \t Epoch: 176 \t Loss: 0.015250319694027755\n******************************\nEpoch: social-tag : 176\ntrain_loss -0.015691268251443038\nval_loss 0.015250319694027755\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 177 \t Loss: -0.015250656753778458\nTRAIN: \t Epoch: 177 \t Loss: -0.015765479765832424\nTRAIN: \t Epoch: 177 \t Loss: -0.01577485352754593\nTRAIN: \t Epoch: 177 \t Loss: -0.015826001297682524\nTRAIN: \t Epoch: 177 \t Loss: -0.015748798847198486\nTRAIN: \t Epoch: 177 \t Loss: -0.015594389444837967\nTRAIN: \t Epoch: 177 \t Loss: -0.01548006319041763\nTRAIN: \t Epoch: 177 \t Loss: -0.015523553476668894\nTRAIN: \t Epoch: 177 \t Loss: -0.01552846148196194\nTRAIN: \t Epoch: 177 \t Loss: -0.015522937849164009\nTRAIN: \t Epoch: 177 \t Loss: -0.015527688644149086\nTRAIN: \t Epoch: 177 \t Loss: -0.015620192512869835\nTRAIN: \t Epoch: 177 \t Loss: -0.015693037412487544\nTRAIN: \t Epoch: 177 \t Loss: -0.015596627390810422\nTRAIN: \t Epoch: 177 \t Loss: -0.0156300637871027\nTRAIN: \t Epoch: 177 \t Loss: -0.015649269334971905\nTRAIN: \t Epoch: 177 \t Loss: -0.015629999230013174\nTRAIN: \t Epoch: 177 \t Loss: -0.015616931642095247\nTRAIN: \t Epoch: 177 \t Loss: -0.015639901847431536\nTRAIN: \t Epoch: 177 \t Loss: -0.01564328223466873\nTRAIN: \t Epoch: 177 \t Loss: -0.015699391563733418\nTRAIN: \t Epoch: 177 \t Loss: -0.015702545707058865\nVALD: \t Epoch: 177 \t Loss: -0.003907429054379463\nVALD: \t Epoch: 177 \t Loss: 0.002751654479652643\nVALD: \t Epoch: 177 \t Loss: 0.0009399042464792728\nVALD: \t Epoch: 177 \t Loss: 0.009991492028348148\nVALD: \t Epoch: 177 \t Loss: 0.007201487850397825\nVALD: \t Epoch: 177 \t Loss: 0.006981907579977291\n******************************\nEpoch: social-tag : 177\ntrain_loss -0.015702545707058865\nval_loss 0.006981907579977291\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 178 \t Loss: -0.015484418720006943\nTRAIN: \t Epoch: 178 \t Loss: -0.01554667204618454\nTRAIN: \t Epoch: 178 \t Loss: -0.016101067264874775\nTRAIN: \t Epoch: 178 \t Loss: -0.016166063956916332\nTRAIN: \t Epoch: 178 \t Loss: -0.016039987653493883\nTRAIN: \t Epoch: 178 \t Loss: -0.01598760640869538\nTRAIN: \t Epoch: 178 \t Loss: -0.015729332209697792\nTRAIN: \t Epoch: 178 \t Loss: -0.015657437965273857\nTRAIN: \t Epoch: 178 \t Loss: -0.015652181063261297\nTRAIN: \t Epoch: 178 \t Loss: -0.015686224680393934\nTRAIN: \t Epoch: 178 \t Loss: -0.015682638114826244\nTRAIN: \t Epoch: 178 \t Loss: -0.015637143903101485\nTRAIN: \t Epoch: 178 \t Loss: -0.015721334645954464\nTRAIN: \t Epoch: 178 \t Loss: -0.015779068493949517\nTRAIN: \t Epoch: 178 \t Loss: -0.015777942476173243\nTRAIN: \t Epoch: 178 \t Loss: -0.01575688144657761\nTRAIN: \t Epoch: 178 \t Loss: -0.01572996602558038\nTRAIN: \t Epoch: 178 \t Loss: -0.015746575004110735\nTRAIN: \t Epoch: 178 \t Loss: -0.0157719881420857\nTRAIN: \t Epoch: 178 \t Loss: -0.015705628832802177\nTRAIN: \t Epoch: 178 \t Loss: -0.01568710662069775\nTRAIN: \t Epoch: 178 \t Loss: -0.01568891877973957\nVALD: \t Epoch: 178 \t Loss: -0.004649768117815256\nVALD: \t Epoch: 178 \t Loss: 0.0003460356965661049\nVALD: \t Epoch: 178 \t Loss: -0.0018833366533120473\nVALD: \t Epoch: 178 \t Loss: 0.0038081719540059566\nVALD: \t Epoch: 178 \t Loss: 0.0018950372003018856\nVALD: \t Epoch: 178 \t Loss: 0.001751393683706269\n******************************\nEpoch: social-tag : 178\ntrain_loss -0.01568891877973957\nval_loss 0.001751393683706269\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 179 \t Loss: -0.014482240192592144\nTRAIN: \t Epoch: 179 \t Loss: -0.01506664277985692\nTRAIN: \t Epoch: 179 \t Loss: -0.015322179533541203\nTRAIN: \t Epoch: 179 \t Loss: -0.01537264115177095\nTRAIN: \t Epoch: 179 \t Loss: -0.01548751387745142\nTRAIN: \t Epoch: 179 \t Loss: -0.015583608765155077\nTRAIN: \t Epoch: 179 \t Loss: -0.015623629758400577\nTRAIN: \t Epoch: 179 \t Loss: -0.015519758220762014\nTRAIN: \t Epoch: 179 \t Loss: -0.015652065061860614\nTRAIN: \t Epoch: 179 \t Loss: -0.015771808475255965\nTRAIN: \t Epoch: 179 \t Loss: -0.015697846104475586\nTRAIN: \t Epoch: 179 \t Loss: -0.015699790868287284\nTRAIN: \t Epoch: 179 \t Loss: -0.015713586042133663\nTRAIN: \t Epoch: 179 \t Loss: -0.015686674043536186\nTRAIN: \t Epoch: 179 \t Loss: -0.01567072073618571\nTRAIN: \t Epoch: 179 \t Loss: -0.015712862252257764\nTRAIN: \t Epoch: 179 \t Loss: -0.015745036413564402\nTRAIN: \t Epoch: 179 \t Loss: -0.015779671259224415\nTRAIN: \t Epoch: 179 \t Loss: -0.015794998328936726\nTRAIN: \t Epoch: 179 \t Loss: -0.015806172974407673\nTRAIN: \t Epoch: 179 \t Loss: -0.015766314940438383\nTRAIN: \t Epoch: 179 \t Loss: -0.015714439113007413\nVALD: \t Epoch: 179 \t Loss: -0.005041274707764387\nVALD: \t Epoch: 179 \t Loss: 0.0016145629342645407\nVALD: \t Epoch: 179 \t Loss: -0.001577605027705431\nVALD: \t Epoch: 179 \t Loss: 0.011321403668262064\nVALD: \t Epoch: 179 \t Loss: 0.008610590593889356\nVALD: \t Epoch: 179 \t Loss: 0.008571200840400927\n******************************\nEpoch: social-tag : 179\ntrain_loss -0.015714439113007413\nval_loss 0.008571200840400927\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 180 \t Loss: -0.015677960589528084\nTRAIN: \t Epoch: 180 \t Loss: -0.015827521681785583\nTRAIN: \t Epoch: 180 \t Loss: -0.015755015114943188\nTRAIN: \t Epoch: 180 \t Loss: -0.0157723231241107\nTRAIN: \t Epoch: 180 \t Loss: -0.015668879821896554\nTRAIN: \t Epoch: 180 \t Loss: -0.01565770773837964\nTRAIN: \t Epoch: 180 \t Loss: -0.01549431708242212\nTRAIN: \t Epoch: 180 \t Loss: -0.015525396214798093\nTRAIN: \t Epoch: 180 \t Loss: -0.015649907911817234\nTRAIN: \t Epoch: 180 \t Loss: -0.015772480703890323\nTRAIN: \t Epoch: 180 \t Loss: -0.01575989614833485\nTRAIN: \t Epoch: 180 \t Loss: -0.015832356953372557\nTRAIN: \t Epoch: 180 \t Loss: -0.015768486863145463\nTRAIN: \t Epoch: 180 \t Loss: -0.01576506732297795\nTRAIN: \t Epoch: 180 \t Loss: -0.01576741226017475\nTRAIN: \t Epoch: 180 \t Loss: -0.015839464613236487\nTRAIN: \t Epoch: 180 \t Loss: -0.01579147999120109\nTRAIN: \t Epoch: 180 \t Loss: -0.0157704160341786\nTRAIN: \t Epoch: 180 \t Loss: -0.015759713359569247\nTRAIN: \t Epoch: 180 \t Loss: -0.01574209872633219\nTRAIN: \t Epoch: 180 \t Loss: -0.015783645478742465\nTRAIN: \t Epoch: 180 \t Loss: -0.015745081935793537\nVALD: \t Epoch: 180 \t Loss: -0.006523909047245979\nVALD: \t Epoch: 180 \t Loss: 0.0010360968299210072\nVALD: \t Epoch: 180 \t Loss: -0.00023314403370022774\nVALD: \t Epoch: 180 \t Loss: 0.010227380902506411\nVALD: \t Epoch: 180 \t Loss: 0.007533012330532074\nVALD: \t Epoch: 180 \t Loss: 0.007263833637151754\n******************************\nEpoch: social-tag : 180\ntrain_loss -0.015745081935793537\nval_loss 0.007263833637151754\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 181 \t Loss: -0.015467389486730099\nTRAIN: \t Epoch: 181 \t Loss: -0.01528955576941371\nTRAIN: \t Epoch: 181 \t Loss: -0.015853428281843662\nTRAIN: \t Epoch: 181 \t Loss: -0.015882368432357907\nTRAIN: \t Epoch: 181 \t Loss: -0.015770557522773742\nTRAIN: \t Epoch: 181 \t Loss: -0.015716882112125557\nTRAIN: \t Epoch: 181 \t Loss: -0.015851639743362154\nTRAIN: \t Epoch: 181 \t Loss: -0.015806508716195822\nTRAIN: \t Epoch: 181 \t Loss: -0.015780813578102324\nTRAIN: \t Epoch: 181 \t Loss: -0.015770701877772807\nTRAIN: \t Epoch: 181 \t Loss: -0.015681800855831665\nTRAIN: \t Epoch: 181 \t Loss: -0.015726877376437187\nTRAIN: \t Epoch: 181 \t Loss: -0.01570559199899435\nTRAIN: \t Epoch: 181 \t Loss: -0.01573370922622936\nTRAIN: \t Epoch: 181 \t Loss: -0.015801242428521316\nTRAIN: \t Epoch: 181 \t Loss: -0.01577002857811749\nTRAIN: \t Epoch: 181 \t Loss: -0.015762277933604577\nTRAIN: \t Epoch: 181 \t Loss: -0.015753021981153224\nTRAIN: \t Epoch: 181 \t Loss: -0.01570825866962734\nTRAIN: \t Epoch: 181 \t Loss: -0.015736019983887673\nTRAIN: \t Epoch: 181 \t Loss: -0.01579155320567744\nTRAIN: \t Epoch: 181 \t Loss: -0.01576731273473058\nVALD: \t Epoch: 181 \t Loss: 0.003436872037127614\nVALD: \t Epoch: 181 \t Loss: 0.009708781377412379\nVALD: \t Epoch: 181 \t Loss: 0.004462127961839239\nVALD: \t Epoch: 181 \t Loss: 0.010662906861398369\nVALD: \t Epoch: 181 \t Loss: 0.00940870218910277\nVALD: \t Epoch: 181 \t Loss: 0.009665858790729984\n******************************\nEpoch: social-tag : 181\ntrain_loss -0.01576731273473058\nval_loss 0.009665858790729984\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 182 \t Loss: -0.01513202115893364\nTRAIN: \t Epoch: 182 \t Loss: -0.015457444824278355\nTRAIN: \t Epoch: 182 \t Loss: -0.015331326052546501\nTRAIN: \t Epoch: 182 \t Loss: -0.015514102298766375\nTRAIN: \t Epoch: 182 \t Loss: -0.015946635231375694\nTRAIN: \t Epoch: 182 \t Loss: -0.015874409582465887\nTRAIN: \t Epoch: 182 \t Loss: -0.015761523773627623\nTRAIN: \t Epoch: 182 \t Loss: -0.015780951478518546\nTRAIN: \t Epoch: 182 \t Loss: -0.015830486288501158\nTRAIN: \t Epoch: 182 \t Loss: -0.015796224866062403\nTRAIN: \t Epoch: 182 \t Loss: -0.01581744739616459\nTRAIN: \t Epoch: 182 \t Loss: -0.015894650714471936\nTRAIN: \t Epoch: 182 \t Loss: -0.015849359763356354\nTRAIN: \t Epoch: 182 \t Loss: -0.01585443690419197\nTRAIN: \t Epoch: 182 \t Loss: -0.015849157174428304\nTRAIN: \t Epoch: 182 \t Loss: -0.015776842017658055\nTRAIN: \t Epoch: 182 \t Loss: -0.01580136882908204\nTRAIN: \t Epoch: 182 \t Loss: -0.01575715115500821\nTRAIN: \t Epoch: 182 \t Loss: -0.01572736406600789\nTRAIN: \t Epoch: 182 \t Loss: -0.01573817073367536\nTRAIN: \t Epoch: 182 \t Loss: -0.015752750005395638\nTRAIN: \t Epoch: 182 \t Loss: -0.015749680375282384\nVALD: \t Epoch: 182 \t Loss: -0.005811389070004225\nVALD: \t Epoch: 182 \t Loss: 0.0029025499243289232\nVALD: \t Epoch: 182 \t Loss: 0.000723767327144742\nVALD: \t Epoch: 182 \t Loss: 0.013434638676699251\nVALD: \t Epoch: 182 \t Loss: 0.010032715555280448\nVALD: \t Epoch: 182 \t Loss: 0.009711330968209288\n******************************\nEpoch: social-tag : 182\ntrain_loss -0.015749680375282384\nval_loss 0.009711330968209288\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 183 \t Loss: -0.015415091998875141\nTRAIN: \t Epoch: 183 \t Loss: -0.015157764311879873\nTRAIN: \t Epoch: 183 \t Loss: -0.015406140747169653\nTRAIN: \t Epoch: 183 \t Loss: -0.01535608060657978\nTRAIN: \t Epoch: 183 \t Loss: -0.015272209979593753\nTRAIN: \t Epoch: 183 \t Loss: -0.015229402420421442\nTRAIN: \t Epoch: 183 \t Loss: -0.01527559823755707\nTRAIN: \t Epoch: 183 \t Loss: -0.015411521890200675\nTRAIN: \t Epoch: 183 \t Loss: -0.015320469625294209\nTRAIN: \t Epoch: 183 \t Loss: -0.015368183236569166\nTRAIN: \t Epoch: 183 \t Loss: -0.015335234220732342\nTRAIN: \t Epoch: 183 \t Loss: -0.015382546776284775\nTRAIN: \t Epoch: 183 \t Loss: -0.015485219084299527\nTRAIN: \t Epoch: 183 \t Loss: -0.015537399000355176\nTRAIN: \t Epoch: 183 \t Loss: -0.015642611185709636\nTRAIN: \t Epoch: 183 \t Loss: -0.015714651672169566\nTRAIN: \t Epoch: 183 \t Loss: -0.015774240169455025\nTRAIN: \t Epoch: 183 \t Loss: -0.01574949759783016\nTRAIN: \t Epoch: 183 \t Loss: -0.01571489816629573\nTRAIN: \t Epoch: 183 \t Loss: -0.015742846531793475\nTRAIN: \t Epoch: 183 \t Loss: -0.015755854680069854\nTRAIN: \t Epoch: 183 \t Loss: -0.015744481951372843\nVALD: \t Epoch: 183 \t Loss: -0.007063954137265682\nVALD: \t Epoch: 183 \t Loss: 0.001779715996235609\nVALD: \t Epoch: 183 \t Loss: -0.0005874081204334894\nVALD: \t Epoch: 183 \t Loss: 0.017654356081038713\nVALD: \t Epoch: 183 \t Loss: 0.013522386038675904\nVALD: \t Epoch: 183 \t Loss: 0.01319964767969919\n******************************\nEpoch: social-tag : 183\ntrain_loss -0.015744481951372843\nval_loss 0.01319964767969919\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 184 \t Loss: -0.016148429363965988\nTRAIN: \t Epoch: 184 \t Loss: -0.016445109620690346\nTRAIN: \t Epoch: 184 \t Loss: -0.016062053230901558\nTRAIN: \t Epoch: 184 \t Loss: -0.016088402131572366\nTRAIN: \t Epoch: 184 \t Loss: -0.016044956631958485\nTRAIN: \t Epoch: 184 \t Loss: -0.015880738695462544\nTRAIN: \t Epoch: 184 \t Loss: -0.015829692594707012\nTRAIN: \t Epoch: 184 \t Loss: -0.015906021115370095\nTRAIN: \t Epoch: 184 \t Loss: -0.015964976097974513\nTRAIN: \t Epoch: 184 \t Loss: -0.015887300483882427\nTRAIN: \t Epoch: 184 \t Loss: -0.015804960392415524\nTRAIN: \t Epoch: 184 \t Loss: -0.015728692368914683\nTRAIN: \t Epoch: 184 \t Loss: -0.015700330671209555\nTRAIN: \t Epoch: 184 \t Loss: -0.0157107323674219\nTRAIN: \t Epoch: 184 \t Loss: -0.015661533052722613\nTRAIN: \t Epoch: 184 \t Loss: -0.01570148696191609\nTRAIN: \t Epoch: 184 \t Loss: -0.015771604964838308\nTRAIN: \t Epoch: 184 \t Loss: -0.01580753953506549\nTRAIN: \t Epoch: 184 \t Loss: -0.015715497447864005\nTRAIN: \t Epoch: 184 \t Loss: -0.015724403550848365\nTRAIN: \t Epoch: 184 \t Loss: -0.015694760096569855\nTRAIN: \t Epoch: 184 \t Loss: -0.015712251783059357\nVALD: \t Epoch: 184 \t Loss: -0.010733835399150848\nVALD: \t Epoch: 184 \t Loss: -0.0061735776835121214\nVALD: \t Epoch: 184 \t Loss: -0.006834857204618554\nVALD: \t Epoch: 184 \t Loss: 0.0014029908634256572\nVALD: \t Epoch: 184 \t Loss: 4.928407724946737e-05\nVALD: \t Epoch: 184 \t Loss: -1.5803076552622245e-06\n******************************\nEpoch: social-tag : 184\ntrain_loss -0.015712251783059357\nval_loss -1.5803076552622245e-06\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 185 \t Loss: -0.015227161347866058\nTRAIN: \t Epoch: 185 \t Loss: -0.015326989348977804\nTRAIN: \t Epoch: 185 \t Loss: -0.015660342760384083\nTRAIN: \t Epoch: 185 \t Loss: -0.016067239688709378\nTRAIN: \t Epoch: 185 \t Loss: -0.01579399239271879\nTRAIN: \t Epoch: 185 \t Loss: -0.01563805155456066\nTRAIN: \t Epoch: 185 \t Loss: -0.015760725364089012\nTRAIN: \t Epoch: 185 \t Loss: -0.01578286779113114\nTRAIN: \t Epoch: 185 \t Loss: -0.015815293623341456\nTRAIN: \t Epoch: 185 \t Loss: -0.015829049050807953\nTRAIN: \t Epoch: 185 \t Loss: -0.015841837803071194\nTRAIN: \t Epoch: 185 \t Loss: -0.015819582312057417\nTRAIN: \t Epoch: 185 \t Loss: -0.015781946981755588\nTRAIN: \t Epoch: 185 \t Loss: -0.015786301278110062\nTRAIN: \t Epoch: 185 \t Loss: -0.0157505518446366\nTRAIN: \t Epoch: 185 \t Loss: -0.015753324725665152\nTRAIN: \t Epoch: 185 \t Loss: -0.015688087517286047\nTRAIN: \t Epoch: 185 \t Loss: -0.015701325765500467\nTRAIN: \t Epoch: 185 \t Loss: -0.015721442521010574\nTRAIN: \t Epoch: 185 \t Loss: -0.015726636769250034\nTRAIN: \t Epoch: 185 \t Loss: -0.015761068046447776\nTRAIN: \t Epoch: 185 \t Loss: -0.0157844858922907\nVALD: \t Epoch: 185 \t Loss: -0.0033309145364910364\nVALD: \t Epoch: 185 \t Loss: 0.004270565579645336\nVALD: \t Epoch: 185 \t Loss: 0.001963075716048479\nVALD: \t Epoch: 185 \t Loss: 0.020252765505574644\nVALD: \t Epoch: 185 \t Loss: 0.01569214155897498\nVALD: \t Epoch: 185 \t Loss: 0.015314603845278421\n******************************\nEpoch: social-tag : 185\ntrain_loss -0.0157844858922907\nval_loss 0.015314603845278421\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 186 \t Loss: -0.015053299255669117\nTRAIN: \t Epoch: 186 \t Loss: -0.014758279081434011\nTRAIN: \t Epoch: 186 \t Loss: -0.014885293009380499\nTRAIN: \t Epoch: 186 \t Loss: -0.015226051909849048\nTRAIN: \t Epoch: 186 \t Loss: -0.015414356254041195\nTRAIN: \t Epoch: 186 \t Loss: -0.015395872139682373\nTRAIN: \t Epoch: 186 \t Loss: -0.015234224897410189\nTRAIN: \t Epoch: 186 \t Loss: -0.015288365888409317\nTRAIN: \t Epoch: 186 \t Loss: -0.015271355397999287\nTRAIN: \t Epoch: 186 \t Loss: -0.015314621943980455\nTRAIN: \t Epoch: 186 \t Loss: -0.015397722832858562\nTRAIN: \t Epoch: 186 \t Loss: -0.015538151453559598\nTRAIN: \t Epoch: 186 \t Loss: -0.015573589202876274\nTRAIN: \t Epoch: 186 \t Loss: -0.015666123013943434\nTRAIN: \t Epoch: 186 \t Loss: -0.01573728254685799\nTRAIN: \t Epoch: 186 \t Loss: -0.0158035583444871\nTRAIN: \t Epoch: 186 \t Loss: -0.015835792867138106\nTRAIN: \t Epoch: 186 \t Loss: -0.015773631477107603\nTRAIN: \t Epoch: 186 \t Loss: -0.015794384391292146\nTRAIN: \t Epoch: 186 \t Loss: -0.0158081182744354\nTRAIN: \t Epoch: 186 \t Loss: -0.01580344544102748\nTRAIN: \t Epoch: 186 \t Loss: -0.01582227087106482\nVALD: \t Epoch: 186 \t Loss: -0.010470205917954445\nVALD: \t Epoch: 186 \t Loss: -0.003427310613915324\nVALD: \t Epoch: 186 \t Loss: -0.004753917766114076\nVALD: \t Epoch: 186 \t Loss: 0.00974755990318954\nVALD: \t Epoch: 186 \t Loss: 0.006507188268005848\nVALD: \t Epoch: 186 \t Loss: 0.006192921237512069\n******************************\nEpoch: social-tag : 186\ntrain_loss -0.01582227087106482\nval_loss 0.006192921237512069\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 187 \t Loss: -0.014934179373085499\nTRAIN: \t Epoch: 187 \t Loss: -0.01562574552372098\nTRAIN: \t Epoch: 187 \t Loss: -0.01603530813008547\nTRAIN: \t Epoch: 187 \t Loss: -0.01623883075080812\nTRAIN: \t Epoch: 187 \t Loss: -0.016167161427438258\nTRAIN: \t Epoch: 187 \t Loss: -0.01607117724294464\nTRAIN: \t Epoch: 187 \t Loss: -0.016010599609996592\nTRAIN: \t Epoch: 187 \t Loss: -0.01605690794531256\nTRAIN: \t Epoch: 187 \t Loss: -0.016019214048153825\nTRAIN: \t Epoch: 187 \t Loss: -0.01595438253134489\nTRAIN: \t Epoch: 187 \t Loss: -0.015960050238804382\nTRAIN: \t Epoch: 187 \t Loss: -0.015923939334849518\nTRAIN: \t Epoch: 187 \t Loss: -0.01600311567577032\nTRAIN: \t Epoch: 187 \t Loss: -0.0159355590253004\nTRAIN: \t Epoch: 187 \t Loss: -0.015977300020555654\nTRAIN: \t Epoch: 187 \t Loss: -0.015929277637042105\nTRAIN: \t Epoch: 187 \t Loss: -0.015917123897987252\nTRAIN: \t Epoch: 187 \t Loss: -0.015864865388721228\nTRAIN: \t Epoch: 187 \t Loss: -0.015873333488247897\nTRAIN: \t Epoch: 187 \t Loss: -0.015872493060305713\nTRAIN: \t Epoch: 187 \t Loss: -0.01580883176731212\nTRAIN: \t Epoch: 187 \t Loss: -0.015813600439143566\nVALD: \t Epoch: 187 \t Loss: 0.0007492476142942905\nVALD: \t Epoch: 187 \t Loss: 0.009343699784949422\nVALD: \t Epoch: 187 \t Loss: 0.0047624434034029646\nVALD: \t Epoch: 187 \t Loss: 0.018833463080227375\nVALD: \t Epoch: 187 \t Loss: 0.015277177561074495\nVALD: \t Epoch: 187 \t Loss: 0.015167846350055752\n******************************\nEpoch: social-tag : 187\ntrain_loss -0.015813600439143566\nval_loss 0.015167846350055752\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 188 \t Loss: -0.015392865985631943\nTRAIN: \t Epoch: 188 \t Loss: -0.015108659397810698\nTRAIN: \t Epoch: 188 \t Loss: -0.015197285761435827\nTRAIN: \t Epoch: 188 \t Loss: -0.015632264316082\nTRAIN: \t Epoch: 188 \t Loss: -0.015805956348776816\nTRAIN: \t Epoch: 188 \t Loss: -0.01574820466339588\nTRAIN: \t Epoch: 188 \t Loss: -0.01589461016867842\nTRAIN: \t Epoch: 188 \t Loss: -0.015807254589162767\nTRAIN: \t Epoch: 188 \t Loss: -0.015840659228463966\nTRAIN: \t Epoch: 188 \t Loss: -0.01582313487306237\nTRAIN: \t Epoch: 188 \t Loss: -0.015809551643377\nTRAIN: \t Epoch: 188 \t Loss: -0.015768642770126462\nTRAIN: \t Epoch: 188 \t Loss: -0.01583633765291709\nTRAIN: \t Epoch: 188 \t Loss: -0.015874389226415327\nTRAIN: \t Epoch: 188 \t Loss: -0.015900301250318685\nTRAIN: \t Epoch: 188 \t Loss: -0.015895015269052237\nTRAIN: \t Epoch: 188 \t Loss: -0.015867785715004978\nTRAIN: \t Epoch: 188 \t Loss: -0.01582554013778766\nTRAIN: \t Epoch: 188 \t Loss: -0.015789357896306012\nTRAIN: \t Epoch: 188 \t Loss: -0.015776989376172425\nTRAIN: \t Epoch: 188 \t Loss: -0.01577897737955763\nTRAIN: \t Epoch: 188 \t Loss: -0.015808555681786803\nVALD: \t Epoch: 188 \t Loss: -0.008289419114589691\nVALD: \t Epoch: 188 \t Loss: -0.0030622906051576138\nVALD: \t Epoch: 188 \t Loss: -0.004732281900942326\nVALD: \t Epoch: 188 \t Loss: 0.009443690301850438\nVALD: \t Epoch: 188 \t Loss: 0.006492020469158888\nVALD: \t Epoch: 188 \t Loss: 0.006279658523359985\n******************************\nEpoch: social-tag : 188\ntrain_loss -0.015808555681786803\nval_loss 0.006279658523359985\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 189 \t Loss: -0.015868356451392174\nTRAIN: \t Epoch: 189 \t Loss: -0.016241487115621567\nTRAIN: \t Epoch: 189 \t Loss: -0.016109736015399296\nTRAIN: \t Epoch: 189 \t Loss: -0.016020416747778654\nTRAIN: \t Epoch: 189 \t Loss: -0.016052236780524254\nTRAIN: \t Epoch: 189 \t Loss: -0.016034981856743496\nTRAIN: \t Epoch: 189 \t Loss: -0.015942143542425975\nTRAIN: \t Epoch: 189 \t Loss: -0.015969271771609783\nTRAIN: \t Epoch: 189 \t Loss: -0.015919229222668543\nTRAIN: \t Epoch: 189 \t Loss: -0.015852297097444533\nTRAIN: \t Epoch: 189 \t Loss: -0.015726821412417023\nTRAIN: \t Epoch: 189 \t Loss: -0.01567856331045429\nTRAIN: \t Epoch: 189 \t Loss: -0.015713213871304806\nTRAIN: \t Epoch: 189 \t Loss: -0.015765039649392878\nTRAIN: \t Epoch: 189 \t Loss: -0.015867966040968896\nTRAIN: \t Epoch: 189 \t Loss: -0.01581074483692646\nTRAIN: \t Epoch: 189 \t Loss: -0.015758632189210725\nTRAIN: \t Epoch: 189 \t Loss: -0.0157769654567043\nTRAIN: \t Epoch: 189 \t Loss: -0.01569954516660226\nTRAIN: \t Epoch: 189 \t Loss: -0.015754276188090444\nTRAIN: \t Epoch: 189 \t Loss: -0.015781508093433722\nTRAIN: \t Epoch: 189 \t Loss: -0.01576421466293198\nVALD: \t Epoch: 189 \t Loss: -0.010146867483854294\nVALD: \t Epoch: 189 \t Loss: -0.004381610488053411\nVALD: \t Epoch: 189 \t Loss: -0.006272354745306075\nVALD: \t Epoch: 189 \t Loss: 0.005803381820442155\nVALD: \t Epoch: 189 \t Loss: 0.003693518531508744\nVALD: \t Epoch: 189 \t Loss: 0.0036926180124282836\n******************************\nEpoch: social-tag : 189\ntrain_loss -0.01576421466293198\nval_loss 0.0036926180124282836\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 190 \t Loss: -0.01587400585412979\nTRAIN: \t Epoch: 190 \t Loss: -0.016286468133330345\nTRAIN: \t Epoch: 190 \t Loss: -0.016066051398714382\nTRAIN: \t Epoch: 190 \t Loss: -0.016182422637939453\nTRAIN: \t Epoch: 190 \t Loss: -0.016187382116913795\nTRAIN: \t Epoch: 190 \t Loss: -0.01613159632931153\nTRAIN: \t Epoch: 190 \t Loss: -0.016062668125544275\nTRAIN: \t Epoch: 190 \t Loss: -0.01601879275403917\nTRAIN: \t Epoch: 190 \t Loss: -0.01599131855699751\nTRAIN: \t Epoch: 190 \t Loss: -0.016038713045418262\nTRAIN: \t Epoch: 190 \t Loss: -0.016126387667926876\nTRAIN: \t Epoch: 190 \t Loss: -0.016064968736221392\nTRAIN: \t Epoch: 190 \t Loss: -0.015981194061728623\nTRAIN: \t Epoch: 190 \t Loss: -0.015969664922782352\nTRAIN: \t Epoch: 190 \t Loss: -0.0159883338958025\nTRAIN: \t Epoch: 190 \t Loss: -0.015996164525859058\nTRAIN: \t Epoch: 190 \t Loss: -0.015910387587021377\nTRAIN: \t Epoch: 190 \t Loss: -0.01582634774968028\nTRAIN: \t Epoch: 190 \t Loss: -0.015802146425764812\nTRAIN: \t Epoch: 190 \t Loss: -0.01583032854832709\nTRAIN: \t Epoch: 190 \t Loss: -0.01579803427947419\nTRAIN: \t Epoch: 190 \t Loss: -0.015779868961450654\nVALD: \t Epoch: 190 \t Loss: -0.006323277018964291\nVALD: \t Epoch: 190 \t Loss: 0.003478212747722864\nVALD: \t Epoch: 190 \t Loss: 0.0008006424953540167\nVALD: \t Epoch: 190 \t Loss: 0.014369003009051085\nVALD: \t Epoch: 190 \t Loss: 0.010838278336450458\nVALD: \t Epoch: 190 \t Loss: 0.01051144530277022\n******************************\nEpoch: social-tag : 190\ntrain_loss -0.015779868961450654\nval_loss 0.01051144530277022\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 191 \t Loss: -0.015758361667394638\nTRAIN: \t Epoch: 191 \t Loss: -0.01615446712821722\nTRAIN: \t Epoch: 191 \t Loss: -0.01621277133623759\nTRAIN: \t Epoch: 191 \t Loss: -0.01607411028817296\nTRAIN: \t Epoch: 191 \t Loss: -0.016020486131310463\nTRAIN: \t Epoch: 191 \t Loss: -0.016027336319287617\nTRAIN: \t Epoch: 191 \t Loss: -0.016013359118785177\nTRAIN: \t Epoch: 191 \t Loss: -0.01585745089687407\nTRAIN: \t Epoch: 191 \t Loss: -0.015798463279174432\nTRAIN: \t Epoch: 191 \t Loss: -0.01579084312543273\nTRAIN: \t Epoch: 191 \t Loss: -0.01587432377379049\nTRAIN: \t Epoch: 191 \t Loss: -0.015646359184756875\nTRAIN: \t Epoch: 191 \t Loss: -0.015667502051935747\nTRAIN: \t Epoch: 191 \t Loss: -0.015720528750015155\nTRAIN: \t Epoch: 191 \t Loss: -0.015711529242495696\nTRAIN: \t Epoch: 191 \t Loss: -0.015826036978978664\nTRAIN: \t Epoch: 191 \t Loss: -0.015821162556462428\nTRAIN: \t Epoch: 191 \t Loss: -0.015850214960260525\nTRAIN: \t Epoch: 191 \t Loss: -0.01579642756596992\nTRAIN: \t Epoch: 191 \t Loss: -0.015797459986060858\nTRAIN: \t Epoch: 191 \t Loss: -0.0158405765181496\nTRAIN: \t Epoch: 191 \t Loss: -0.01583798629377218\nVALD: \t Epoch: 191 \t Loss: -0.007118797395378351\nVALD: \t Epoch: 191 \t Loss: -0.0008663281332701445\nVALD: \t Epoch: 191 \t Loss: -0.004114497608194749\nVALD: \t Epoch: 191 \t Loss: 0.01064567535649985\nVALD: \t Epoch: 191 \t Loss: 0.007937567867338657\nVALD: \t Epoch: 191 \t Loss: 0.00793926024978811\n******************************\nEpoch: social-tag : 191\ntrain_loss -0.01583798629377218\nval_loss 0.00793926024978811\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 192 \t Loss: -0.017486732453107834\nTRAIN: \t Epoch: 192 \t Loss: -0.01728506200015545\nTRAIN: \t Epoch: 192 \t Loss: -0.016728020273149014\nTRAIN: \t Epoch: 192 \t Loss: -0.016496992437168956\nTRAIN: \t Epoch: 192 \t Loss: -0.016589570976793765\nTRAIN: \t Epoch: 192 \t Loss: -0.016530753889431555\nTRAIN: \t Epoch: 192 \t Loss: -0.016354574422751154\nTRAIN: \t Epoch: 192 \t Loss: -0.016387956915423274\nTRAIN: \t Epoch: 192 \t Loss: -0.016276854711274307\nTRAIN: \t Epoch: 192 \t Loss: -0.016171711124479772\nTRAIN: \t Epoch: 192 \t Loss: -0.016075463203543968\nTRAIN: \t Epoch: 192 \t Loss: -0.016086509994541604\nTRAIN: \t Epoch: 192 \t Loss: -0.01601507794111967\nTRAIN: \t Epoch: 192 \t Loss: -0.0160057102330029\nTRAIN: \t Epoch: 192 \t Loss: -0.015941167312363783\nTRAIN: \t Epoch: 192 \t Loss: -0.01593619299819693\nTRAIN: \t Epoch: 192 \t Loss: -0.01595648383612142\nTRAIN: \t Epoch: 192 \t Loss: -0.015931256270656984\nTRAIN: \t Epoch: 192 \t Loss: -0.015896136627385492\nTRAIN: \t Epoch: 192 \t Loss: -0.01587917641736567\nTRAIN: \t Epoch: 192 \t Loss: -0.015846749900707176\nTRAIN: \t Epoch: 192 \t Loss: -0.01584339822430157\nVALD: \t Epoch: 192 \t Loss: -0.0033248651307076216\nVALD: \t Epoch: 192 \t Loss: 0.003791412920691073\nVALD: \t Epoch: 192 \t Loss: 0.0015797152494390805\nVALD: \t Epoch: 192 \t Loss: 0.016930495155975223\nVALD: \t Epoch: 192 \t Loss: 0.013013525772839785\nVALD: \t Epoch: 192 \t Loss: 0.012658738288463969\n******************************\nEpoch: social-tag : 192\ntrain_loss -0.01584339822430157\nval_loss 0.012658738288463969\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 193 \t Loss: -0.016492070630192757\nTRAIN: \t Epoch: 193 \t Loss: -0.016388196498155594\nTRAIN: \t Epoch: 193 \t Loss: -0.015902509602407616\nTRAIN: \t Epoch: 193 \t Loss: -0.015991852851584554\nTRAIN: \t Epoch: 193 \t Loss: -0.015913712047040462\nTRAIN: \t Epoch: 193 \t Loss: -0.01572520301366846\nTRAIN: \t Epoch: 193 \t Loss: -0.015746044660253183\nTRAIN: \t Epoch: 193 \t Loss: -0.015658920747227967\nTRAIN: \t Epoch: 193 \t Loss: -0.015777499208019838\nTRAIN: \t Epoch: 193 \t Loss: -0.01574410805478692\nTRAIN: \t Epoch: 193 \t Loss: -0.01575959329916672\nTRAIN: \t Epoch: 193 \t Loss: -0.015806794865056872\nTRAIN: \t Epoch: 193 \t Loss: -0.015797898961374394\nTRAIN: \t Epoch: 193 \t Loss: -0.015836631413549185\nTRAIN: \t Epoch: 193 \t Loss: -0.01584298803160588\nTRAIN: \t Epoch: 193 \t Loss: -0.015800046152435243\nTRAIN: \t Epoch: 193 \t Loss: -0.015746232122182846\nTRAIN: \t Epoch: 193 \t Loss: -0.01576769000126256\nTRAIN: \t Epoch: 193 \t Loss: -0.01581102658651377\nTRAIN: \t Epoch: 193 \t Loss: -0.015858832653611897\nTRAIN: \t Epoch: 193 \t Loss: -0.01582938831831728\nTRAIN: \t Epoch: 193 \t Loss: -0.015824264598277998\nVALD: \t Epoch: 193 \t Loss: 0.005315264221280813\nVALD: \t Epoch: 193 \t Loss: 0.01377751468680799\nVALD: \t Epoch: 193 \t Loss: 0.00755376120408376\nVALD: \t Epoch: 193 \t Loss: 0.028997518122196198\nVALD: \t Epoch: 193 \t Loss: 0.024271943233907224\nVALD: \t Epoch: 193 \t Loss: 0.02415760656197866\n******************************\nEpoch: social-tag : 193\ntrain_loss -0.015824264598277998\nval_loss 0.02415760656197866\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 194 \t Loss: -0.01553150825202465\nTRAIN: \t Epoch: 194 \t Loss: -0.015026554465293884\nTRAIN: \t Epoch: 194 \t Loss: -0.015436202908555666\nTRAIN: \t Epoch: 194 \t Loss: -0.015431892592459917\nTRAIN: \t Epoch: 194 \t Loss: -0.015590711683034896\nTRAIN: \t Epoch: 194 \t Loss: -0.015581043747564157\nTRAIN: \t Epoch: 194 \t Loss: -0.015585064222770078\nTRAIN: \t Epoch: 194 \t Loss: -0.015558608574792743\nTRAIN: \t Epoch: 194 \t Loss: -0.015677022230294015\nTRAIN: \t Epoch: 194 \t Loss: -0.01565138129517436\nTRAIN: \t Epoch: 194 \t Loss: -0.015744664875621147\nTRAIN: \t Epoch: 194 \t Loss: -0.01571423071436584\nTRAIN: \t Epoch: 194 \t Loss: -0.015754013465574153\nTRAIN: \t Epoch: 194 \t Loss: -0.015714167085077082\nTRAIN: \t Epoch: 194 \t Loss: -0.015707280238469443\nTRAIN: \t Epoch: 194 \t Loss: -0.01575681916438043\nTRAIN: \t Epoch: 194 \t Loss: -0.015799459946506163\nTRAIN: \t Epoch: 194 \t Loss: -0.01586978344453706\nTRAIN: \t Epoch: 194 \t Loss: -0.015886254216495314\nTRAIN: \t Epoch: 194 \t Loss: -0.01591409360989928\nTRAIN: \t Epoch: 194 \t Loss: -0.015893797301465555\nTRAIN: \t Epoch: 194 \t Loss: -0.015865606208677992\nVALD: \t Epoch: 194 \t Loss: -0.009958280250430107\nVALD: \t Epoch: 194 \t Loss: -0.004119956458453089\nVALD: \t Epoch: 194 \t Loss: -0.0046561202422405286\nVALD: \t Epoch: 194 \t Loss: 0.003757378290174529\nVALD: \t Epoch: 194 \t Loss: 0.0017389210639521479\nVALD: \t Epoch: 194 \t Loss: 0.0015919317745349624\n******************************\nEpoch: social-tag : 194\ntrain_loss -0.015865606208677992\nval_loss 0.0015919317745349624\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 195 \t Loss: -0.01559209544211626\nTRAIN: \t Epoch: 195 \t Loss: -0.015607075300067663\nTRAIN: \t Epoch: 195 \t Loss: -0.015843974115947883\nTRAIN: \t Epoch: 195 \t Loss: -0.01571751944720745\nTRAIN: \t Epoch: 195 \t Loss: -0.01570187471807003\nTRAIN: \t Epoch: 195 \t Loss: -0.01567042184372743\nTRAIN: \t Epoch: 195 \t Loss: -0.01585141196846962\nTRAIN: \t Epoch: 195 \t Loss: -0.016012859297916293\nTRAIN: \t Epoch: 195 \t Loss: -0.015839885195924178\nTRAIN: \t Epoch: 195 \t Loss: -0.015794756729155777\nTRAIN: \t Epoch: 195 \t Loss: -0.015798432498492977\nTRAIN: \t Epoch: 195 \t Loss: -0.015780477396522958\nTRAIN: \t Epoch: 195 \t Loss: -0.015824907316038243\nTRAIN: \t Epoch: 195 \t Loss: -0.015831157631639923\nTRAIN: \t Epoch: 195 \t Loss: -0.01590519305318594\nTRAIN: \t Epoch: 195 \t Loss: -0.015880462247878313\nTRAIN: \t Epoch: 195 \t Loss: -0.015910048147334772\nTRAIN: \t Epoch: 195 \t Loss: -0.015849477301041286\nTRAIN: \t Epoch: 195 \t Loss: -0.01586500878788923\nTRAIN: \t Epoch: 195 \t Loss: -0.0158715202473104\nTRAIN: \t Epoch: 195 \t Loss: -0.015857502285923277\nTRAIN: \t Epoch: 195 \t Loss: -0.01585739570631373\nVALD: \t Epoch: 195 \t Loss: -0.010698311030864716\nVALD: \t Epoch: 195 \t Loss: -0.006256641587242484\nVALD: \t Epoch: 195 \t Loss: -0.008243112048755089\nVALD: \t Epoch: 195 \t Loss: 0.002394665381871164\nVALD: \t Epoch: 195 \t Loss: 0.0007825505919754506\nVALD: \t Epoch: 195 \t Loss: 0.0007898833771998233\n******************************\nEpoch: social-tag : 195\ntrain_loss -0.01585739570631373\nval_loss 0.0007898833771998233\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 196 \t Loss: -0.01566498354077339\nTRAIN: \t Epoch: 196 \t Loss: -0.015872794203460217\nTRAIN: \t Epoch: 196 \t Loss: -0.016044431055585544\nTRAIN: \t Epoch: 196 \t Loss: -0.01618213253095746\nTRAIN: \t Epoch: 196 \t Loss: -0.01612708680331707\nTRAIN: \t Epoch: 196 \t Loss: -0.016152735489110153\nTRAIN: \t Epoch: 196 \t Loss: -0.016162542892353877\nTRAIN: \t Epoch: 196 \t Loss: -0.016223445301875472\nTRAIN: \t Epoch: 196 \t Loss: -0.016139415713648003\nTRAIN: \t Epoch: 196 \t Loss: -0.016135974135249854\nTRAIN: \t Epoch: 196 \t Loss: -0.01602141084996137\nTRAIN: \t Epoch: 196 \t Loss: -0.01595782379930218\nTRAIN: \t Epoch: 196 \t Loss: -0.01598269692980326\nTRAIN: \t Epoch: 196 \t Loss: -0.01598963527274983\nTRAIN: \t Epoch: 196 \t Loss: -0.01593700995047887\nTRAIN: \t Epoch: 196 \t Loss: -0.015937634161673486\nTRAIN: \t Epoch: 196 \t Loss: -0.015915246351676827\nTRAIN: \t Epoch: 196 \t Loss: -0.015898965971751347\nTRAIN: \t Epoch: 196 \t Loss: -0.015918197747516006\nTRAIN: \t Epoch: 196 \t Loss: -0.015875986777246\nTRAIN: \t Epoch: 196 \t Loss: -0.01583654424619107\nTRAIN: \t Epoch: 196 \t Loss: -0.015849450631886454\nVALD: \t Epoch: 196 \t Loss: -0.010375537909567356\nVALD: \t Epoch: 196 \t Loss: -0.004860287590418011\nVALD: \t Epoch: 196 \t Loss: -0.006971953165096541\nVALD: \t Epoch: 196 \t Loss: 0.004639119579223916\nVALD: \t Epoch: 196 \t Loss: 0.002764013293199241\nVALD: \t Epoch: 196 \t Loss: 0.0028427173016649303\n******************************\nEpoch: social-tag : 196\ntrain_loss -0.015849450631886454\nval_loss 0.0028427173016649303\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 197 \t Loss: -0.01604846492409706\nTRAIN: \t Epoch: 197 \t Loss: -0.015767482109367847\nTRAIN: \t Epoch: 197 \t Loss: -0.01562991738319397\nTRAIN: \t Epoch: 197 \t Loss: -0.015616119373589754\nTRAIN: \t Epoch: 197 \t Loss: -0.015593199245631694\nTRAIN: \t Epoch: 197 \t Loss: -0.015575190850843986\nTRAIN: \t Epoch: 197 \t Loss: -0.015488544745104653\nTRAIN: \t Epoch: 197 \t Loss: -0.015497060492634773\nTRAIN: \t Epoch: 197 \t Loss: -0.015544522139761183\nTRAIN: \t Epoch: 197 \t Loss: -0.015592003241181374\nTRAIN: \t Epoch: 197 \t Loss: -0.015721624039790848\nTRAIN: \t Epoch: 197 \t Loss: -0.01580573370059331\nTRAIN: \t Epoch: 197 \t Loss: -0.015738286579457614\nTRAIN: \t Epoch: 197 \t Loss: -0.01572153131876673\nTRAIN: \t Epoch: 197 \t Loss: -0.015747748191157977\nTRAIN: \t Epoch: 197 \t Loss: -0.015736413886770606\nTRAIN: \t Epoch: 197 \t Loss: -0.01580101569347522\nTRAIN: \t Epoch: 197 \t Loss: -0.015792086927427187\nTRAIN: \t Epoch: 197 \t Loss: -0.015826183126160975\nTRAIN: \t Epoch: 197 \t Loss: -0.01584442239254713\nTRAIN: \t Epoch: 197 \t Loss: -0.01581561587573517\nTRAIN: \t Epoch: 197 \t Loss: -0.015845364899352808\nVALD: \t Epoch: 197 \t Loss: -0.00141477445140481\nVALD: \t Epoch: 197 \t Loss: 0.0049251781310886145\nVALD: \t Epoch: 197 \t Loss: 0.0009916764684021473\nVALD: \t Epoch: 197 \t Loss: 0.0173096185317263\nVALD: \t Epoch: 197 \t Loss: 0.013838033605861711\nVALD: \t Epoch: 197 \t Loss: 0.013813972784290937\n******************************\nEpoch: social-tag : 197\ntrain_loss -0.015845364899352808\nval_loss 0.013813972784290937\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 198 \t Loss: -0.016046909615397453\nTRAIN: \t Epoch: 198 \t Loss: -0.016081112436950207\nTRAIN: \t Epoch: 198 \t Loss: -0.015557327307760715\nTRAIN: \t Epoch: 198 \t Loss: -0.015380185330286622\nTRAIN: \t Epoch: 198 \t Loss: -0.015488838963210582\nTRAIN: \t Epoch: 198 \t Loss: -0.015517863910645247\nTRAIN: \t Epoch: 198 \t Loss: -0.01557112711348704\nTRAIN: \t Epoch: 198 \t Loss: -0.015511633711867034\nTRAIN: \t Epoch: 198 \t Loss: -0.01560589557306634\nTRAIN: \t Epoch: 198 \t Loss: -0.015653187688440085\nTRAIN: \t Epoch: 198 \t Loss: -0.015722359022633595\nTRAIN: \t Epoch: 198 \t Loss: -0.01569058233872056\nTRAIN: \t Epoch: 198 \t Loss: -0.015714077565532465\nTRAIN: \t Epoch: 198 \t Loss: -0.015778277202376297\nTRAIN: \t Epoch: 198 \t Loss: -0.015756109543144704\nTRAIN: \t Epoch: 198 \t Loss: -0.015870274684857577\nTRAIN: \t Epoch: 198 \t Loss: -0.01591291284079061\nTRAIN: \t Epoch: 198 \t Loss: -0.015897988186528284\nTRAIN: \t Epoch: 198 \t Loss: -0.015897491701731558\nTRAIN: \t Epoch: 198 \t Loss: -0.015917849773541094\nTRAIN: \t Epoch: 198 \t Loss: -0.01591430546804553\nTRAIN: \t Epoch: 198 \t Loss: -0.015881166167062317\nVALD: \t Epoch: 198 \t Loss: -0.010528688319027424\nVALD: \t Epoch: 198 \t Loss: -0.00665642146486789\nVALD: \t Epoch: 198 \t Loss: -0.00736628604742388\nVALD: \t Epoch: 198 \t Loss: 0.0017945123254321516\nVALD: \t Epoch: 198 \t Loss: 0.00022534807212650775\nVALD: \t Epoch: 198 \t Loss: 0.00016873879075953455\n******************************\nEpoch: social-tag : 198\ntrain_loss -0.015881166167062317\nval_loss 0.00016873879075953455\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 199 \t Loss: -0.01587270386517048\nTRAIN: \t Epoch: 199 \t Loss: -0.016164853237569332\nTRAIN: \t Epoch: 199 \t Loss: -0.016025878489017487\nTRAIN: \t Epoch: 199 \t Loss: -0.015866280533373356\nTRAIN: \t Epoch: 199 \t Loss: -0.015825915336608886\nTRAIN: \t Epoch: 199 \t Loss: -0.01566243063037594\nTRAIN: \t Epoch: 199 \t Loss: -0.015580753928848676\nTRAIN: \t Epoch: 199 \t Loss: -0.015599195845425129\nTRAIN: \t Epoch: 199 \t Loss: -0.015700971086819965\nTRAIN: \t Epoch: 199 \t Loss: -0.015771109610795975\nTRAIN: \t Epoch: 199 \t Loss: -0.015781112523241478\nTRAIN: \t Epoch: 199 \t Loss: -0.015785878834625084\nTRAIN: \t Epoch: 199 \t Loss: -0.015738326841248915\nTRAIN: \t Epoch: 199 \t Loss: -0.015832327771931887\nTRAIN: \t Epoch: 199 \t Loss: -0.015810534233848254\nTRAIN: \t Epoch: 199 \t Loss: -0.015814174432307482\nTRAIN: \t Epoch: 199 \t Loss: -0.01581326050355154\nTRAIN: \t Epoch: 199 \t Loss: -0.01580424327403307\nTRAIN: \t Epoch: 199 \t Loss: -0.015795030582107995\nTRAIN: \t Epoch: 199 \t Loss: -0.01581127950921655\nTRAIN: \t Epoch: 199 \t Loss: -0.01584858074784279\nTRAIN: \t Epoch: 199 \t Loss: -0.015877999009521047\nVALD: \t Epoch: 199 \t Loss: -0.011488732881844044\nVALD: \t Epoch: 199 \t Loss: -0.00617145758587867\nVALD: \t Epoch: 199 \t Loss: -0.00784314121119678\nVALD: \t Epoch: 199 \t Loss: 0.00511615484720096\nVALD: \t Epoch: 199 \t Loss: 0.0028316123876720666\nVALD: \t Epoch: 199 \t Loss: 0.0026985712518746204\n******************************\nEpoch: social-tag : 199\ntrain_loss -0.015877999009521047\nval_loss 0.0026985712518746204\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 200 \t Loss: -0.01689889281988144\nTRAIN: \t Epoch: 200 \t Loss: -0.016204009298235178\nTRAIN: \t Epoch: 200 \t Loss: -0.016108238759140175\nTRAIN: \t Epoch: 200 \t Loss: -0.015824458794668317\nTRAIN: \t Epoch: 200 \t Loss: -0.016147881187498568\nTRAIN: \t Epoch: 200 \t Loss: -0.015994846510390442\nTRAIN: \t Epoch: 200 \t Loss: -0.016014741733670235\nTRAIN: \t Epoch: 200 \t Loss: -0.01601376850157976\nTRAIN: \t Epoch: 200 \t Loss: -0.016152583062648773\nTRAIN: \t Epoch: 200 \t Loss: -0.016037771850824355\nTRAIN: \t Epoch: 200 \t Loss: -0.01608007214963436\nTRAIN: \t Epoch: 200 \t Loss: -0.01603928526553015\nTRAIN: \t Epoch: 200 \t Loss: -0.016074271944279853\nTRAIN: \t Epoch: 200 \t Loss: -0.01600621168368629\nTRAIN: \t Epoch: 200 \t Loss: -0.01596818839510282\nTRAIN: \t Epoch: 200 \t Loss: -0.015953378519043326\nTRAIN: \t Epoch: 200 \t Loss: -0.015903758432935265\nTRAIN: \t Epoch: 200 \t Loss: -0.015864713583141565\nTRAIN: \t Epoch: 200 \t Loss: -0.015830151393617455\nTRAIN: \t Epoch: 200 \t Loss: -0.015852973470464347\nTRAIN: \t Epoch: 200 \t Loss: -0.015851770705055623\nTRAIN: \t Epoch: 200 \t Loss: -0.01584975655245824\nVALD: \t Epoch: 200 \t Loss: -0.010908936150372028\nVALD: \t Epoch: 200 \t Loss: -0.0064708751160651445\nVALD: \t Epoch: 200 \t Loss: -0.007773704671611388\nVALD: \t Epoch: 200 \t Loss: 0.0012749541783705354\nVALD: \t Epoch: 200 \t Loss: -0.00015301816165447235\nVALD: \t Epoch: 200 \t Loss: -0.0001945799131962386\n******************************\nEpoch: social-tag : 200\ntrain_loss -0.01584975655245824\nval_loss -0.0001945799131962386\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 201 \t Loss: -0.01661483757197857\nTRAIN: \t Epoch: 201 \t Loss: -0.01621101424098015\nTRAIN: \t Epoch: 201 \t Loss: -0.016557301705082256\nTRAIN: \t Epoch: 201 \t Loss: -0.016380523797124624\nTRAIN: \t Epoch: 201 \t Loss: -0.016305030882358552\nTRAIN: \t Epoch: 201 \t Loss: -0.016241048152248066\nTRAIN: \t Epoch: 201 \t Loss: -0.016181273385882378\nTRAIN: \t Epoch: 201 \t Loss: -0.016065256670117378\nTRAIN: \t Epoch: 201 \t Loss: -0.016072689038183954\nTRAIN: \t Epoch: 201 \t Loss: -0.016011577378958464\nTRAIN: \t Epoch: 201 \t Loss: -0.015923662788488648\nTRAIN: \t Epoch: 201 \t Loss: -0.015913539255658787\nTRAIN: \t Epoch: 201 \t Loss: -0.01591987907886505\nTRAIN: \t Epoch: 201 \t Loss: -0.01589781671230282\nTRAIN: \t Epoch: 201 \t Loss: -0.015929839263359707\nTRAIN: \t Epoch: 201 \t Loss: -0.015971453278325498\nTRAIN: \t Epoch: 201 \t Loss: -0.01593069896540221\nTRAIN: \t Epoch: 201 \t Loss: -0.0158771900460124\nTRAIN: \t Epoch: 201 \t Loss: -0.01586464145465901\nTRAIN: \t Epoch: 201 \t Loss: -0.015942867193371058\nTRAIN: \t Epoch: 201 \t Loss: -0.01592251206082957\nTRAIN: \t Epoch: 201 \t Loss: -0.015912120218122883\nVALD: \t Epoch: 201 \t Loss: -0.00722641684114933\nVALD: \t Epoch: 201 \t Loss: -0.0002492459025233984\nVALD: \t Epoch: 201 \t Loss: -0.0010551213442037504\nVALD: \t Epoch: 201 \t Loss: 0.009869754372630268\nVALD: \t Epoch: 201 \t Loss: 0.0068560413550585505\nVALD: \t Epoch: 201 \t Loss: 0.006557426358940023\n******************************\nEpoch: social-tag : 201\ntrain_loss -0.015912120218122883\nval_loss 0.006557426358940023\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 202 \t Loss: -0.015437174588441849\nTRAIN: \t Epoch: 202 \t Loss: -0.015810463577508926\nTRAIN: \t Epoch: 202 \t Loss: -0.016068632404009502\nTRAIN: \t Epoch: 202 \t Loss: -0.01591378077864647\nTRAIN: \t Epoch: 202 \t Loss: -0.0159171961247921\nTRAIN: \t Epoch: 202 \t Loss: -0.015624817460775375\nTRAIN: \t Epoch: 202 \t Loss: -0.01585335550563676\nTRAIN: \t Epoch: 202 \t Loss: -0.015781827736645937\nTRAIN: \t Epoch: 202 \t Loss: -0.0157466610479686\nTRAIN: \t Epoch: 202 \t Loss: -0.015706888865679502\nTRAIN: \t Epoch: 202 \t Loss: -0.01572401034222408\nTRAIN: \t Epoch: 202 \t Loss: -0.01575137092731893\nTRAIN: \t Epoch: 202 \t Loss: -0.015800151615761794\nTRAIN: \t Epoch: 202 \t Loss: -0.015827372204512358\nTRAIN: \t Epoch: 202 \t Loss: -0.01575994845479727\nTRAIN: \t Epoch: 202 \t Loss: -0.015845457033719867\nTRAIN: \t Epoch: 202 \t Loss: -0.01587342465405955\nTRAIN: \t Epoch: 202 \t Loss: -0.015927121301905975\nTRAIN: \t Epoch: 202 \t Loss: -0.0158845458767916\nTRAIN: \t Epoch: 202 \t Loss: -0.01583903660066426\nTRAIN: \t Epoch: 202 \t Loss: -0.015875924068192642\nTRAIN: \t Epoch: 202 \t Loss: -0.015890561314416744\nVALD: \t Epoch: 202 \t Loss: -0.008998172357678413\nVALD: \t Epoch: 202 \t Loss: -0.0017675033304840326\nVALD: \t Epoch: 202 \t Loss: -0.0036887115178008876\nVALD: \t Epoch: 202 \t Loss: 0.006676996243186295\nVALD: \t Epoch: 202 \t Loss: 0.004439417831599712\nVALD: \t Epoch: 202 \t Loss: 0.004253898397313826\n******************************\nEpoch: social-tag : 202\ntrain_loss -0.015890561314416744\nval_loss 0.004253898397313826\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 203 \t Loss: -0.01553819514811039\nTRAIN: \t Epoch: 203 \t Loss: -0.015350501984357834\nTRAIN: \t Epoch: 203 \t Loss: -0.015584194411834082\nTRAIN: \t Epoch: 203 \t Loss: -0.015698911156505346\nTRAIN: \t Epoch: 203 \t Loss: -0.015591495484113694\nTRAIN: \t Epoch: 203 \t Loss: -0.015732401981949806\nTRAIN: \t Epoch: 203 \t Loss: -0.015790122960294996\nTRAIN: \t Epoch: 203 \t Loss: -0.01570431306026876\nTRAIN: \t Epoch: 203 \t Loss: -0.015785900875926018\nTRAIN: \t Epoch: 203 \t Loss: -0.01581779830157757\nTRAIN: \t Epoch: 203 \t Loss: -0.015818197280168533\nTRAIN: \t Epoch: 203 \t Loss: -0.01589181972667575\nTRAIN: \t Epoch: 203 \t Loss: -0.015928242212304704\nTRAIN: \t Epoch: 203 \t Loss: -0.01592931603746755\nTRAIN: \t Epoch: 203 \t Loss: -0.0158433568974336\nTRAIN: \t Epoch: 203 \t Loss: -0.015855154138989747\nTRAIN: \t Epoch: 203 \t Loss: -0.01588244122617385\nTRAIN: \t Epoch: 203 \t Loss: -0.015832904519306287\nTRAIN: \t Epoch: 203 \t Loss: -0.015839045181086187\nTRAIN: \t Epoch: 203 \t Loss: -0.01586018092930317\nTRAIN: \t Epoch: 203 \t Loss: -0.015902203374675343\nTRAIN: \t Epoch: 203 \t Loss: -0.015910665051513343\nVALD: \t Epoch: 203 \t Loss: -0.01195379626005888\nVALD: \t Epoch: 203 \t Loss: -0.008740087738260627\nVALD: \t Epoch: 203 \t Loss: -0.010019194179524979\nVALD: \t Epoch: 203 \t Loss: -0.002937612938694656\nVALD: \t Epoch: 203 \t Loss: -0.003524638805538416\nVALD: \t Epoch: 203 \t Loss: -0.0034361242020333354\n******************************\nEpoch: social-tag : 203\ntrain_loss -0.015910665051513343\nval_loss -0.0034361242020333354\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 204 \t Loss: -0.016055021435022354\nTRAIN: \t Epoch: 204 \t Loss: -0.016044064424932003\nTRAIN: \t Epoch: 204 \t Loss: -0.01618689236541589\nTRAIN: \t Epoch: 204 \t Loss: -0.01604983163997531\nTRAIN: \t Epoch: 204 \t Loss: -0.016058238968253136\nTRAIN: \t Epoch: 204 \t Loss: -0.016139031077424686\nTRAIN: \t Epoch: 204 \t Loss: -0.016220676313553537\nTRAIN: \t Epoch: 204 \t Loss: -0.01621267944574356\nTRAIN: \t Epoch: 204 \t Loss: -0.016182733078797657\nTRAIN: \t Epoch: 204 \t Loss: -0.016122325602918864\nTRAIN: \t Epoch: 204 \t Loss: -0.01609727613289248\nTRAIN: \t Epoch: 204 \t Loss: -0.016093410008276503\nTRAIN: \t Epoch: 204 \t Loss: -0.0160606587305665\nTRAIN: \t Epoch: 204 \t Loss: -0.016043690099780048\nTRAIN: \t Epoch: 204 \t Loss: -0.016086805673937003\nTRAIN: \t Epoch: 204 \t Loss: -0.016051293758209795\nTRAIN: \t Epoch: 204 \t Loss: -0.01604030593572294\nTRAIN: \t Epoch: 204 \t Loss: -0.016025983159326844\nTRAIN: \t Epoch: 204 \t Loss: -0.016006502420886567\nTRAIN: \t Epoch: 204 \t Loss: -0.01596587630920112\nTRAIN: \t Epoch: 204 \t Loss: -0.015940113392259394\nTRAIN: \t Epoch: 204 \t Loss: -0.01592563645415931\nVALD: \t Epoch: 204 \t Loss: -0.0020865474361926317\nVALD: \t Epoch: 204 \t Loss: 0.0057774962624534965\nVALD: \t Epoch: 204 \t Loss: 0.003155746885264913\nVALD: \t Epoch: 204 \t Loss: 0.016572796797845513\nVALD: \t Epoch: 204 \t Loss: 0.012660785298794507\nVALD: \t Epoch: 204 \t Loss: 0.012234044983757264\n******************************\nEpoch: social-tag : 204\ntrain_loss -0.01592563645415931\nval_loss 0.012234044983757264\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 205 \t Loss: -0.016191095113754272\nTRAIN: \t Epoch: 205 \t Loss: -0.016873379237949848\nTRAIN: \t Epoch: 205 \t Loss: -0.01723432168364525\nTRAIN: \t Epoch: 205 \t Loss: -0.016641980735585093\nTRAIN: \t Epoch: 205 \t Loss: -0.016328117437660696\nTRAIN: \t Epoch: 205 \t Loss: -0.016221205548693735\nTRAIN: \t Epoch: 205 \t Loss: -0.01609593802796943\nTRAIN: \t Epoch: 205 \t Loss: -0.016058365697972476\nTRAIN: \t Epoch: 205 \t Loss: -0.015908139136930306\nTRAIN: \t Epoch: 205 \t Loss: -0.015869065746665002\nTRAIN: \t Epoch: 205 \t Loss: -0.015866352753205734\nTRAIN: \t Epoch: 205 \t Loss: -0.01589443829531471\nTRAIN: \t Epoch: 205 \t Loss: -0.015809443277808335\nTRAIN: \t Epoch: 205 \t Loss: -0.01583829295954534\nTRAIN: \t Epoch: 205 \t Loss: -0.01580425798892975\nTRAIN: \t Epoch: 205 \t Loss: -0.015835869940929115\nTRAIN: \t Epoch: 205 \t Loss: -0.015822383465574068\nTRAIN: \t Epoch: 205 \t Loss: -0.0158085277200573\nTRAIN: \t Epoch: 205 \t Loss: -0.015832801055359214\nTRAIN: \t Epoch: 205 \t Loss: -0.015832114825025202\nTRAIN: \t Epoch: 205 \t Loss: -0.01584966059419371\nTRAIN: \t Epoch: 205 \t Loss: -0.015900845844398923\nVALD: \t Epoch: 205 \t Loss: -0.004679706413298845\nVALD: \t Epoch: 205 \t Loss: -0.001081326394341886\nVALD: \t Epoch: 205 \t Loss: -0.0037870133140434823\nVALD: \t Epoch: 205 \t Loss: 0.00884301500627771\nVALD: \t Epoch: 205 \t Loss: 0.00649962187744677\nVALD: \t Epoch: 205 \t Loss: 0.006503716517578472\n******************************\nEpoch: social-tag : 205\ntrain_loss -0.015900845844398923\nval_loss 0.006503716517578472\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 206 \t Loss: -0.01525422278791666\nTRAIN: \t Epoch: 206 \t Loss: -0.015521255787461996\nTRAIN: \t Epoch: 206 \t Loss: -0.015744501414398353\nTRAIN: \t Epoch: 206 \t Loss: -0.015987701015546918\nTRAIN: \t Epoch: 206 \t Loss: -0.015803841501474382\nTRAIN: \t Epoch: 206 \t Loss: -0.01567366688201825\nTRAIN: \t Epoch: 206 \t Loss: -0.01568110952419894\nTRAIN: \t Epoch: 206 \t Loss: -0.015738940332084894\nTRAIN: \t Epoch: 206 \t Loss: -0.015741071353356045\nTRAIN: \t Epoch: 206 \t Loss: -0.01567533379420638\nTRAIN: \t Epoch: 206 \t Loss: -0.015739064917645672\nTRAIN: \t Epoch: 206 \t Loss: -0.015771686953182023\nTRAIN: \t Epoch: 206 \t Loss: -0.015886864051795922\nTRAIN: \t Epoch: 206 \t Loss: -0.015905464134578193\nTRAIN: \t Epoch: 206 \t Loss: -0.015869339182972907\nTRAIN: \t Epoch: 206 \t Loss: -0.015905698528513312\nTRAIN: \t Epoch: 206 \t Loss: -0.015888584832496503\nTRAIN: \t Epoch: 206 \t Loss: -0.015859253601067595\nTRAIN: \t Epoch: 206 \t Loss: -0.015902076700800342\nTRAIN: \t Epoch: 206 \t Loss: -0.015942913386970757\nTRAIN: \t Epoch: 206 \t Loss: -0.015924069498266493\nTRAIN: \t Epoch: 206 \t Loss: -0.015910284352259746\nVALD: \t Epoch: 206 \t Loss: -0.003259125864133239\nVALD: \t Epoch: 206 \t Loss: 0.004034254816360772\nVALD: \t Epoch: 206 \t Loss: 0.0015369455019632976\nVALD: \t Epoch: 206 \t Loss: 0.01918289251625538\nVALD: \t Epoch: 206 \t Loss: 0.015028260019607841\nVALD: \t Epoch: 206 \t Loss: 0.014627258490883943\n******************************\nEpoch: social-tag : 206\ntrain_loss -0.015910284352259746\nval_loss 0.014627258490883943\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 207 \t Loss: -0.015640895813703537\nTRAIN: \t Epoch: 207 \t Loss: -0.01593035366386175\nTRAIN: \t Epoch: 207 \t Loss: -0.015934372320771217\nTRAIN: \t Epoch: 207 \t Loss: -0.01586550334468484\nTRAIN: \t Epoch: 207 \t Loss: -0.015794957615435125\nTRAIN: \t Epoch: 207 \t Loss: -0.015851852328826983\nTRAIN: \t Epoch: 207 \t Loss: -0.01601811990674053\nTRAIN: \t Epoch: 207 \t Loss: -0.015994747751392424\nTRAIN: \t Epoch: 207 \t Loss: -0.01581565642522441\nTRAIN: \t Epoch: 207 \t Loss: -0.015823203697800638\nTRAIN: \t Epoch: 207 \t Loss: -0.01592326858504252\nTRAIN: \t Epoch: 207 \t Loss: -0.01592748969172438\nTRAIN: \t Epoch: 207 \t Loss: -0.01591124772452391\nTRAIN: \t Epoch: 207 \t Loss: -0.015899023174175193\nTRAIN: \t Epoch: 207 \t Loss: -0.015872911425928275\nTRAIN: \t Epoch: 207 \t Loss: -0.01587748498423025\nTRAIN: \t Epoch: 207 \t Loss: -0.015918820603367162\nTRAIN: \t Epoch: 207 \t Loss: -0.01584769542225533\nTRAIN: \t Epoch: 207 \t Loss: -0.015873642099138937\nTRAIN: \t Epoch: 207 \t Loss: -0.01587171838618815\nTRAIN: \t Epoch: 207 \t Loss: -0.01590017757068078\nTRAIN: \t Epoch: 207 \t Loss: -0.015947083682089348\nVALD: \t Epoch: 207 \t Loss: -0.009456108324229717\nVALD: \t Epoch: 207 \t Loss: -0.004139660682994872\nVALD: \t Epoch: 207 \t Loss: -0.005890236119739711\nVALD: \t Epoch: 207 \t Loss: 0.006670140981441364\nVALD: \t Epoch: 207 \t Loss: 0.004390011238865554\nVALD: \t Epoch: 207 \t Loss: 0.00431350821798498\n******************************\nEpoch: social-tag : 207\ntrain_loss -0.015947083682089348\nval_loss 0.00431350821798498\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 208 \t Loss: -0.01507228147238493\nTRAIN: \t Epoch: 208 \t Loss: -0.015327457338571548\nTRAIN: \t Epoch: 208 \t Loss: -0.01568925380706787\nTRAIN: \t Epoch: 208 \t Loss: -0.015722407028079033\nTRAIN: \t Epoch: 208 \t Loss: -0.01574081927537918\nTRAIN: \t Epoch: 208 \t Loss: -0.01570389171441396\nTRAIN: \t Epoch: 208 \t Loss: -0.01578458638063499\nTRAIN: \t Epoch: 208 \t Loss: -0.01581140304915607\nTRAIN: \t Epoch: 208 \t Loss: -0.01584241684112284\nTRAIN: \t Epoch: 208 \t Loss: -0.01580012049525976\nTRAIN: \t Epoch: 208 \t Loss: -0.01582049900157885\nTRAIN: \t Epoch: 208 \t Loss: -0.015781587998693187\nTRAIN: \t Epoch: 208 \t Loss: -0.015825323688869294\nTRAIN: \t Epoch: 208 \t Loss: -0.015910408925265074\nTRAIN: \t Epoch: 208 \t Loss: -0.01588382348418236\nTRAIN: \t Epoch: 208 \t Loss: -0.01584025123156607\nTRAIN: \t Epoch: 208 \t Loss: -0.015854945954154518\nTRAIN: \t Epoch: 208 \t Loss: -0.015839189601441223\nTRAIN: \t Epoch: 208 \t Loss: -0.015873896840371583\nTRAIN: \t Epoch: 208 \t Loss: -0.015854386147111654\nTRAIN: \t Epoch: 208 \t Loss: -0.015898699029570536\nTRAIN: \t Epoch: 208 \t Loss: -0.01592671824100844\nVALD: \t Epoch: 208 \t Loss: -0.00877312384545803\nVALD: \t Epoch: 208 \t Loss: -0.00378584919963032\nVALD: \t Epoch: 208 \t Loss: -0.00511626829393208\nVALD: \t Epoch: 208 \t Loss: 0.0045646350481547415\nVALD: \t Epoch: 208 \t Loss: 0.0025673559401184322\nVALD: \t Epoch: 208 \t Loss: 0.0024756680397937697\n******************************\nEpoch: social-tag : 208\ntrain_loss -0.01592671824100844\nval_loss 0.0024756680397937697\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 209 \t Loss: -0.016251713037490845\nTRAIN: \t Epoch: 209 \t Loss: -0.016393229365348816\nTRAIN: \t Epoch: 209 \t Loss: -0.01635963221391042\nTRAIN: \t Epoch: 209 \t Loss: -0.016358537133783102\nTRAIN: \t Epoch: 209 \t Loss: -0.016232550144195557\nTRAIN: \t Epoch: 209 \t Loss: -0.0161763917033871\nTRAIN: \t Epoch: 209 \t Loss: -0.016176737844944\nTRAIN: \t Epoch: 209 \t Loss: -0.015973361441865563\nTRAIN: \t Epoch: 209 \t Loss: -0.015964136976334784\nTRAIN: \t Epoch: 209 \t Loss: -0.015980770625174046\nTRAIN: \t Epoch: 209 \t Loss: -0.01591123945333741\nTRAIN: \t Epoch: 209 \t Loss: -0.015941820417841274\nTRAIN: \t Epoch: 209 \t Loss: -0.015943852611459218\nTRAIN: \t Epoch: 209 \t Loss: -0.01598994660058192\nTRAIN: \t Epoch: 209 \t Loss: -0.015997391566634177\nTRAIN: \t Epoch: 209 \t Loss: -0.015983429388143122\nTRAIN: \t Epoch: 209 \t Loss: -0.015914606971337515\nTRAIN: \t Epoch: 209 \t Loss: -0.015874110731399722\nTRAIN: \t Epoch: 209 \t Loss: -0.01589192288290513\nTRAIN: \t Epoch: 209 \t Loss: -0.01587211536243558\nTRAIN: \t Epoch: 209 \t Loss: -0.01591882020944641\nTRAIN: \t Epoch: 209 \t Loss: -0.015930673678002622\nVALD: \t Epoch: 209 \t Loss: -0.0013654781505465508\nVALD: \t Epoch: 209 \t Loss: 0.005033067427575588\nVALD: \t Epoch: 209 \t Loss: 0.0003296537324786186\nVALD: \t Epoch: 209 \t Loss: 0.01790244714356959\nVALD: \t Epoch: 209 \t Loss: 0.01439116700203158\nVALD: \t Epoch: 209 \t Loss: 0.014313049145945996\n******************************\nEpoch: social-tag : 209\ntrain_loss -0.015930673678002622\nval_loss 0.014313049145945996\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 210 \t Loss: -0.01631433330476284\nTRAIN: \t Epoch: 210 \t Loss: -0.017056001350283623\nTRAIN: \t Epoch: 210 \t Loss: -0.01671726567049821\nTRAIN: \t Epoch: 210 \t Loss: -0.016735094599425793\nTRAIN: \t Epoch: 210 \t Loss: -0.016245518624782563\nTRAIN: \t Epoch: 210 \t Loss: -0.016305224349101383\nTRAIN: \t Epoch: 210 \t Loss: -0.01625192670949868\nTRAIN: \t Epoch: 210 \t Loss: -0.016116460436023772\nTRAIN: \t Epoch: 210 \t Loss: -0.015978070493373606\nTRAIN: \t Epoch: 210 \t Loss: -0.015876627899706362\nTRAIN: \t Epoch: 210 \t Loss: -0.015935274010354824\nTRAIN: \t Epoch: 210 \t Loss: -0.016004628036171198\nTRAIN: \t Epoch: 210 \t Loss: -0.016042750472059615\nTRAIN: \t Epoch: 210 \t Loss: -0.016036892975015298\nTRAIN: \t Epoch: 210 \t Loss: -0.01599266311774651\nTRAIN: \t Epoch: 210 \t Loss: -0.015936481533572078\nTRAIN: \t Epoch: 210 \t Loss: -0.015904195497141164\nTRAIN: \t Epoch: 210 \t Loss: -0.01591890574329429\nTRAIN: \t Epoch: 210 \t Loss: -0.015924131987910522\nTRAIN: \t Epoch: 210 \t Loss: -0.01598007921129465\nTRAIN: \t Epoch: 210 \t Loss: -0.016010890138291177\nTRAIN: \t Epoch: 210 \t Loss: -0.015986453951796254\nVALD: \t Epoch: 210 \t Loss: -0.002435573609545827\nVALD: \t Epoch: 210 \t Loss: 0.006244076299481094\nVALD: \t Epoch: 210 \t Loss: 0.00422930747057156\nVALD: \t Epoch: 210 \t Loss: 0.014266007801779779\nVALD: \t Epoch: 210 \t Loss: 0.011159916981705464\nVALD: \t Epoch: 210 \t Loss: 0.010912394278090109\n******************************\nEpoch: social-tag : 210\ntrain_loss -0.015986453951796254\nval_loss 0.010912394278090109\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 211 \t Loss: -0.015284927561879158\nTRAIN: \t Epoch: 211 \t Loss: -0.015431053936481476\nTRAIN: \t Epoch: 211 \t Loss: -0.015590765823920568\nTRAIN: \t Epoch: 211 \t Loss: -0.015719876158982515\nTRAIN: \t Epoch: 211 \t Loss: -0.0157005051150918\nTRAIN: \t Epoch: 211 \t Loss: -0.015363057143986225\nTRAIN: \t Epoch: 211 \t Loss: -0.015358586529535907\nTRAIN: \t Epoch: 211 \t Loss: -0.015414166613481939\nTRAIN: \t Epoch: 211 \t Loss: -0.01551564834598038\nTRAIN: \t Epoch: 211 \t Loss: -0.015702000353485346\nTRAIN: \t Epoch: 211 \t Loss: -0.015770764428783546\nTRAIN: \t Epoch: 211 \t Loss: -0.015779345218713086\nTRAIN: \t Epoch: 211 \t Loss: -0.015781310530236133\nTRAIN: \t Epoch: 211 \t Loss: -0.015734566042998006\nTRAIN: \t Epoch: 211 \t Loss: -0.015772729677458606\nTRAIN: \t Epoch: 211 \t Loss: -0.015834913298022002\nTRAIN: \t Epoch: 211 \t Loss: -0.015867257764672533\nTRAIN: \t Epoch: 211 \t Loss: -0.015843008696619008\nTRAIN: \t Epoch: 211 \t Loss: -0.015828752056940606\nTRAIN: \t Epoch: 211 \t Loss: -0.01591825592331588\nTRAIN: \t Epoch: 211 \t Loss: -0.015929613218066237\nTRAIN: \t Epoch: 211 \t Loss: -0.015931312870936506\nVALD: \t Epoch: 211 \t Loss: -0.007076760288327932\nVALD: \t Epoch: 211 \t Loss: -0.0008476001676172018\nVALD: \t Epoch: 211 \t Loss: -0.0022574419466157756\nVALD: \t Epoch: 211 \t Loss: 0.010165691957809031\nVALD: \t Epoch: 211 \t Loss: 0.007170008774846792\nVALD: \t Epoch: 211 \t Loss: 0.006944146616186833\n******************************\nEpoch: social-tag : 211\ntrain_loss -0.015931312870936506\nval_loss 0.006944146616186833\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 212 \t Loss: -0.014959287829697132\nTRAIN: \t Epoch: 212 \t Loss: -0.015605459455400705\nTRAIN: \t Epoch: 212 \t Loss: -0.015800980540613335\nTRAIN: \t Epoch: 212 \t Loss: -0.015525071183219552\nTRAIN: \t Epoch: 212 \t Loss: -0.015519128181040287\nTRAIN: \t Epoch: 212 \t Loss: -0.015580045835425457\nTRAIN: \t Epoch: 212 \t Loss: -0.01559415100408452\nTRAIN: \t Epoch: 212 \t Loss: -0.015629728673957288\nTRAIN: \t Epoch: 212 \t Loss: -0.01570538855675194\nTRAIN: \t Epoch: 212 \t Loss: -0.015759508591145277\nTRAIN: \t Epoch: 212 \t Loss: -0.015709025277332825\nTRAIN: \t Epoch: 212 \t Loss: -0.015684238635003567\nTRAIN: \t Epoch: 212 \t Loss: -0.015670897367482003\nTRAIN: \t Epoch: 212 \t Loss: -0.015699557095233883\nTRAIN: \t Epoch: 212 \t Loss: -0.015832413422564664\nTRAIN: \t Epoch: 212 \t Loss: -0.01588179887039587\nTRAIN: \t Epoch: 212 \t Loss: -0.015925579266074824\nTRAIN: \t Epoch: 212 \t Loss: -0.01595927666251858\nTRAIN: \t Epoch: 212 \t Loss: -0.01593856175283068\nTRAIN: \t Epoch: 212 \t Loss: -0.01597795500420034\nTRAIN: \t Epoch: 212 \t Loss: -0.016011648900097326\nTRAIN: \t Epoch: 212 \t Loss: -0.01602866110296609\nVALD: \t Epoch: 212 \t Loss: 0.0026480783708393574\nVALD: \t Epoch: 212 \t Loss: 0.010711681330576539\nVALD: \t Epoch: 212 \t Loss: 0.007281102637837951\nVALD: \t Epoch: 212 \t Loss: 0.01605522196769016\nVALD: \t Epoch: 212 \t Loss: 0.012346844148123638\nVALD: \t Epoch: 212 \t Loss: 0.011927831647070971\n******************************\nEpoch: social-tag : 212\ntrain_loss -0.01602866110296609\nval_loss 0.011927831647070971\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 213 \t Loss: -0.016549361869692802\nTRAIN: \t Epoch: 213 \t Loss: -0.01615795958787203\nTRAIN: \t Epoch: 213 \t Loss: -0.01605557029445966\nTRAIN: \t Epoch: 213 \t Loss: -0.016166488640010357\nTRAIN: \t Epoch: 213 \t Loss: -0.016238579899072646\nTRAIN: \t Epoch: 213 \t Loss: -0.01623528264462948\nTRAIN: \t Epoch: 213 \t Loss: -0.016272621495383128\nTRAIN: \t Epoch: 213 \t Loss: -0.016209929483011365\nTRAIN: \t Epoch: 213 \t Loss: -0.016046698205173016\nTRAIN: \t Epoch: 213 \t Loss: -0.016062235552817584\nTRAIN: \t Epoch: 213 \t Loss: -0.016075825234028427\nTRAIN: \t Epoch: 213 \t Loss: -0.016021805815398693\nTRAIN: \t Epoch: 213 \t Loss: -0.016026102722837374\nTRAIN: \t Epoch: 213 \t Loss: -0.015998329168983867\nTRAIN: \t Epoch: 213 \t Loss: -0.016028192390998206\nTRAIN: \t Epoch: 213 \t Loss: -0.015998193237464875\nTRAIN: \t Epoch: 213 \t Loss: -0.016004369375022018\nTRAIN: \t Epoch: 213 \t Loss: -0.016007156990882423\nTRAIN: \t Epoch: 213 \t Loss: -0.0159779869412121\nTRAIN: \t Epoch: 213 \t Loss: -0.015955871297046544\nTRAIN: \t Epoch: 213 \t Loss: -0.01587179715612105\nTRAIN: \t Epoch: 213 \t Loss: -0.01590678730387662\nVALD: \t Epoch: 213 \t Loss: -0.007786132860928774\nVALD: \t Epoch: 213 \t Loss: -0.003508379857521504\nVALD: \t Epoch: 213 \t Loss: -0.005315676447935402\nVALD: \t Epoch: 213 \t Loss: 0.004311880940804258\nVALD: \t Epoch: 213 \t Loss: 0.002448817458935082\nVALD: \t Epoch: 213 \t Loss: 0.0024688439500151257\n******************************\nEpoch: social-tag : 213\ntrain_loss -0.01590678730387662\nval_loss 0.0024688439500151257\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 214 \t Loss: -0.015851082280278206\nTRAIN: \t Epoch: 214 \t Loss: -0.01630702894181013\nTRAIN: \t Epoch: 214 \t Loss: -0.016180287425716717\nTRAIN: \t Epoch: 214 \t Loss: -0.016444291453808546\nTRAIN: \t Epoch: 214 \t Loss: -0.01606097221374512\nTRAIN: \t Epoch: 214 \t Loss: -0.016176453170677025\nTRAIN: \t Epoch: 214 \t Loss: -0.016149468453867093\nTRAIN: \t Epoch: 214 \t Loss: -0.01602264167740941\nTRAIN: \t Epoch: 214 \t Loss: -0.016014684198631182\nTRAIN: \t Epoch: 214 \t Loss: -0.015993533283472063\nTRAIN: \t Epoch: 214 \t Loss: -0.015998211435296318\nTRAIN: \t Epoch: 214 \t Loss: -0.01597967588653167\nTRAIN: \t Epoch: 214 \t Loss: -0.015932893093961936\nTRAIN: \t Epoch: 214 \t Loss: -0.015979398041963577\nTRAIN: \t Epoch: 214 \t Loss: -0.01595237577954928\nTRAIN: \t Epoch: 214 \t Loss: -0.015933072078041732\nTRAIN: \t Epoch: 214 \t Loss: -0.01593635735266349\nTRAIN: \t Epoch: 214 \t Loss: -0.015990494853920408\nTRAIN: \t Epoch: 214 \t Loss: -0.015976179097043842\nTRAIN: \t Epoch: 214 \t Loss: -0.015997552685439585\nTRAIN: \t Epoch: 214 \t Loss: -0.015988405084326154\nTRAIN: \t Epoch: 214 \t Loss: -0.016009088478567784\nVALD: \t Epoch: 214 \t Loss: -0.0025591454468667507\nVALD: \t Epoch: 214 \t Loss: 0.0023939579259604216\nVALD: \t Epoch: 214 \t Loss: 0.0002038030264278253\nVALD: \t Epoch: 214 \t Loss: 0.00876425753813237\nVALD: \t Epoch: 214 \t Loss: 0.0063684825785458084\nVALD: \t Epoch: 214 \t Loss: 0.006141376568738258\n******************************\nEpoch: social-tag : 214\ntrain_loss -0.016009088478567784\nval_loss 0.006141376568738258\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 215 \t Loss: -0.015209432691335678\nTRAIN: \t Epoch: 215 \t Loss: -0.015449753031134605\nTRAIN: \t Epoch: 215 \t Loss: -0.015524302298823992\nTRAIN: \t Epoch: 215 \t Loss: -0.015924543142318726\nTRAIN: \t Epoch: 215 \t Loss: -0.015911923348903657\nTRAIN: \t Epoch: 215 \t Loss: -0.016130941919982433\nTRAIN: \t Epoch: 215 \t Loss: -0.016094576567411423\nTRAIN: \t Epoch: 215 \t Loss: -0.01621008920483291\nTRAIN: \t Epoch: 215 \t Loss: -0.016175526504715283\nTRAIN: \t Epoch: 215 \t Loss: -0.01622067838907242\nTRAIN: \t Epoch: 215 \t Loss: -0.016176359558647328\nTRAIN: \t Epoch: 215 \t Loss: -0.01620799272010724\nTRAIN: \t Epoch: 215 \t Loss: -0.01614466603272236\nTRAIN: \t Epoch: 215 \t Loss: -0.01616635113688452\nTRAIN: \t Epoch: 215 \t Loss: -0.016099996988972028\nTRAIN: \t Epoch: 215 \t Loss: -0.016084365197457373\nTRAIN: \t Epoch: 215 \t Loss: -0.016034863724866334\nTRAIN: \t Epoch: 215 \t Loss: -0.016070715617388487\nTRAIN: \t Epoch: 215 \t Loss: -0.016125440744585114\nTRAIN: \t Epoch: 215 \t Loss: -0.016057876124978065\nTRAIN: \t Epoch: 215 \t Loss: -0.016011263997781845\nTRAIN: \t Epoch: 215 \t Loss: -0.01597177297465463\nVALD: \t Epoch: 215 \t Loss: -0.012690656818449497\nVALD: \t Epoch: 215 \t Loss: -0.008610255317762494\nVALD: \t Epoch: 215 \t Loss: -0.009209475635240475\nVALD: \t Epoch: 215 \t Loss: -0.0021826516604050994\nVALD: \t Epoch: 215 \t Loss: -0.0032463259063661098\nVALD: \t Epoch: 215 \t Loss: -0.003268037668683312\n******************************\nEpoch: social-tag : 215\ntrain_loss -0.01597177297465463\nval_loss -0.003268037668683312\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 216 \t Loss: -0.01539478451013565\nTRAIN: \t Epoch: 216 \t Loss: -0.01604664884507656\nTRAIN: \t Epoch: 216 \t Loss: -0.016138253112634022\nTRAIN: \t Epoch: 216 \t Loss: -0.015701663913205266\nTRAIN: \t Epoch: 216 \t Loss: -0.015844314731657504\nTRAIN: \t Epoch: 216 \t Loss: -0.015965280899157126\nTRAIN: \t Epoch: 216 \t Loss: -0.015923648673508848\nTRAIN: \t Epoch: 216 \t Loss: -0.015966612729243934\nTRAIN: \t Epoch: 216 \t Loss: -0.016153849764830537\nTRAIN: \t Epoch: 216 \t Loss: -0.01607811786234379\nTRAIN: \t Epoch: 216 \t Loss: -0.016074949198148468\nTRAIN: \t Epoch: 216 \t Loss: -0.01587104060066243\nTRAIN: \t Epoch: 216 \t Loss: -0.015770634134801533\nTRAIN: \t Epoch: 216 \t Loss: -0.015819541245166744\nTRAIN: \t Epoch: 216 \t Loss: -0.01583324937770764\nTRAIN: \t Epoch: 216 \t Loss: -0.015882520645391196\nTRAIN: \t Epoch: 216 \t Loss: -0.01589923642356606\nTRAIN: \t Epoch: 216 \t Loss: -0.015983372771491606\nTRAIN: \t Epoch: 216 \t Loss: -0.015940307933641106\nTRAIN: \t Epoch: 216 \t Loss: -0.015992705570533873\nTRAIN: \t Epoch: 216 \t Loss: -0.015992840175472554\nTRAIN: \t Epoch: 216 \t Loss: -0.015973877350346617\nVALD: \t Epoch: 216 \t Loss: -0.005519547965377569\nVALD: \t Epoch: 216 \t Loss: 0.0025418379809707403\nVALD: \t Epoch: 216 \t Loss: -0.00122453598305583\nVALD: \t Epoch: 216 \t Loss: 0.01494109199848026\nVALD: \t Epoch: 216 \t Loss: 0.011424731835722924\nVALD: \t Epoch: 216 \t Loss: 0.011198617405060566\n******************************\nEpoch: social-tag : 216\ntrain_loss -0.015973877350346617\nval_loss 0.011198617405060566\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 217 \t Loss: -0.015318281017243862\nTRAIN: \t Epoch: 217 \t Loss: -0.015924972016364336\nTRAIN: \t Epoch: 217 \t Loss: -0.016152240646382172\nTRAIN: \t Epoch: 217 \t Loss: -0.01643910282291472\nTRAIN: \t Epoch: 217 \t Loss: -0.01642650570720434\nTRAIN: \t Epoch: 217 \t Loss: -0.016291155014187098\nTRAIN: \t Epoch: 217 \t Loss: -0.01626718243850129\nTRAIN: \t Epoch: 217 \t Loss: -0.016322681796737015\nTRAIN: \t Epoch: 217 \t Loss: -0.016361221981545288\nTRAIN: \t Epoch: 217 \t Loss: -0.01640458730980754\nTRAIN: \t Epoch: 217 \t Loss: -0.016428874382241207\nTRAIN: \t Epoch: 217 \t Loss: -0.016355390660464764\nTRAIN: \t Epoch: 217 \t Loss: -0.016354803139200576\nTRAIN: \t Epoch: 217 \t Loss: -0.016263559194547788\nTRAIN: \t Epoch: 217 \t Loss: -0.0161787253494064\nTRAIN: \t Epoch: 217 \t Loss: -0.016187904227990657\nTRAIN: \t Epoch: 217 \t Loss: -0.016172847929684556\nTRAIN: \t Epoch: 217 \t Loss: -0.016126716199020546\nTRAIN: \t Epoch: 217 \t Loss: -0.016093574121202294\nTRAIN: \t Epoch: 217 \t Loss: -0.016063430346548557\nTRAIN: \t Epoch: 217 \t Loss: -0.01608098227353323\nTRAIN: \t Epoch: 217 \t Loss: -0.01603009405427176\nVALD: \t Epoch: 217 \t Loss: -0.0017293584533035755\nVALD: \t Epoch: 217 \t Loss: 0.005919207585975528\nVALD: \t Epoch: 217 \t Loss: 0.003915361674444284\nVALD: \t Epoch: 217 \t Loss: 0.019534378954631393\nVALD: \t Epoch: 217 \t Loss: 0.01534325492248172\nVALD: \t Epoch: 217 \t Loss: 0.014964681978083469\n******************************\nEpoch: social-tag : 217\ntrain_loss -0.01603009405427176\nval_loss 0.014964681978083469\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 218 \t Loss: -0.016214486211538315\nTRAIN: \t Epoch: 218 \t Loss: -0.016168279573321342\nTRAIN: \t Epoch: 218 \t Loss: -0.016596830760439236\nTRAIN: \t Epoch: 218 \t Loss: -0.016613853629678488\nTRAIN: \t Epoch: 218 \t Loss: -0.016427073627710342\nTRAIN: \t Epoch: 218 \t Loss: -0.016328031818072002\nTRAIN: \t Epoch: 218 \t Loss: -0.016328374722174237\nTRAIN: \t Epoch: 218 \t Loss: -0.01623731409199536\nTRAIN: \t Epoch: 218 \t Loss: -0.016245446271366544\nTRAIN: \t Epoch: 218 \t Loss: -0.016201331838965417\nTRAIN: \t Epoch: 218 \t Loss: -0.01623939045450904\nTRAIN: \t Epoch: 218 \t Loss: -0.01610674095960955\nTRAIN: \t Epoch: 218 \t Loss: -0.0160896390533218\nTRAIN: \t Epoch: 218 \t Loss: -0.016102248736258064\nTRAIN: \t Epoch: 218 \t Loss: -0.016137749142944812\nTRAIN: \t Epoch: 218 \t Loss: -0.016099244123324752\nTRAIN: \t Epoch: 218 \t Loss: -0.016102247378405404\nTRAIN: \t Epoch: 218 \t Loss: -0.016154766807125673\nTRAIN: \t Epoch: 218 \t Loss: -0.016068724180130584\nTRAIN: \t Epoch: 218 \t Loss: -0.01607788070105016\nTRAIN: \t Epoch: 218 \t Loss: -0.01606454961888847\nTRAIN: \t Epoch: 218 \t Loss: -0.016035395635950717\nVALD: \t Epoch: 218 \t Loss: -0.005974693223834038\nVALD: \t Epoch: 218 \t Loss: 0.002136115450412035\nVALD: \t Epoch: 218 \t Loss: 0.0010810487437993288\nVALD: \t Epoch: 218 \t Loss: 0.019042044354137033\nVALD: \t Epoch: 218 \t Loss: 0.014954291004687547\nVALD: \t Epoch: 218 \t Loss: 0.01453628685664047\n******************************\nEpoch: social-tag : 218\ntrain_loss -0.016035395635950717\nval_loss 0.01453628685664047\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 219 \t Loss: -0.016077300533652306\nTRAIN: \t Epoch: 219 \t Loss: -0.01651371829211712\nTRAIN: \t Epoch: 219 \t Loss: -0.016543760895729065\nTRAIN: \t Epoch: 219 \t Loss: -0.01638196501880884\nTRAIN: \t Epoch: 219 \t Loss: -0.0162212997674942\nTRAIN: \t Epoch: 219 \t Loss: -0.0162008972838521\nTRAIN: \t Epoch: 219 \t Loss: -0.016310267416494235\nTRAIN: \t Epoch: 219 \t Loss: -0.016226247418671846\nTRAIN: \t Epoch: 219 \t Loss: -0.016243986164530117\nTRAIN: \t Epoch: 219 \t Loss: -0.016151659097522496\nTRAIN: \t Epoch: 219 \t Loss: -0.016135201112113216\nTRAIN: \t Epoch: 219 \t Loss: -0.01611367450095713\nTRAIN: \t Epoch: 219 \t Loss: -0.016076668308904536\nTRAIN: \t Epoch: 219 \t Loss: -0.015991524327546358\nTRAIN: \t Epoch: 219 \t Loss: -0.015910858971377215\nTRAIN: \t Epoch: 219 \t Loss: -0.015934186114463955\nTRAIN: \t Epoch: 219 \t Loss: -0.015957176411414847\nTRAIN: \t Epoch: 219 \t Loss: -0.01594965848036938\nTRAIN: \t Epoch: 219 \t Loss: -0.015960063834331538\nTRAIN: \t Epoch: 219 \t Loss: -0.015981789166107773\nTRAIN: \t Epoch: 219 \t Loss: -0.015929774514266422\nTRAIN: \t Epoch: 219 \t Loss: -0.015953769418452758\nVALD: \t Epoch: 219 \t Loss: -0.005703086499124765\nVALD: \t Epoch: 219 \t Loss: -0.0013594633201137185\nVALD: \t Epoch: 219 \t Loss: -0.003730722780649861\nVALD: \t Epoch: 219 \t Loss: 0.008298563945572823\nVALD: \t Epoch: 219 \t Loss: 0.006021677330136299\nVALD: \t Epoch: 219 \t Loss: 0.006000221114267002\n******************************\nEpoch: social-tag : 219\ntrain_loss -0.015953769418452758\nval_loss 0.006000221114267002\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 220 \t Loss: -0.01700524054467678\nTRAIN: \t Epoch: 220 \t Loss: -0.01649258378893137\nTRAIN: \t Epoch: 220 \t Loss: -0.01636083610355854\nTRAIN: \t Epoch: 220 \t Loss: -0.016299953684210777\nTRAIN: \t Epoch: 220 \t Loss: -0.016204971075057983\nTRAIN: \t Epoch: 220 \t Loss: -0.01625117628524701\nTRAIN: \t Epoch: 220 \t Loss: -0.01614005012171609\nTRAIN: \t Epoch: 220 \t Loss: -0.016140330117195845\nTRAIN: \t Epoch: 220 \t Loss: -0.016000492808719475\nTRAIN: \t Epoch: 220 \t Loss: -0.016098396945744754\nTRAIN: \t Epoch: 220 \t Loss: -0.01610035322267901\nTRAIN: \t Epoch: 220 \t Loss: -0.016054189919183653\nTRAIN: \t Epoch: 220 \t Loss: -0.015997269334128268\nTRAIN: \t Epoch: 220 \t Loss: -0.015992886453334774\nTRAIN: \t Epoch: 220 \t Loss: -0.0160064360126853\nTRAIN: \t Epoch: 220 \t Loss: -0.016024217067752033\nTRAIN: \t Epoch: 220 \t Loss: -0.01600010068539311\nTRAIN: \t Epoch: 220 \t Loss: -0.01604317408055067\nTRAIN: \t Epoch: 220 \t Loss: -0.016078179898230655\nTRAIN: \t Epoch: 220 \t Loss: -0.016052247071638704\nTRAIN: \t Epoch: 220 \t Loss: -0.015998057089746\nTRAIN: \t Epoch: 220 \t Loss: -0.016038233159474453\nVALD: \t Epoch: 220 \t Loss: -0.00889048632234335\nVALD: \t Epoch: 220 \t Loss: -0.003312239539809525\nVALD: \t Epoch: 220 \t Loss: -0.005027221593384941\nVALD: \t Epoch: 220 \t Loss: 0.00456866662716493\nVALD: \t Epoch: 220 \t Loss: 0.002505677239969373\nVALD: \t Epoch: 220 \t Loss: 0.0023588184493057657\n******************************\nEpoch: social-tag : 220\ntrain_loss -0.016038233159474453\nval_loss 0.0023588184493057657\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 221 \t Loss: -0.01698233373463154\nTRAIN: \t Epoch: 221 \t Loss: -0.01631760783493519\nTRAIN: \t Epoch: 221 \t Loss: -0.01624060794711113\nTRAIN: \t Epoch: 221 \t Loss: -0.015972512774169445\nTRAIN: \t Epoch: 221 \t Loss: -0.016029481962323188\nTRAIN: \t Epoch: 221 \t Loss: -0.015981345437467098\nTRAIN: \t Epoch: 221 \t Loss: -0.016084725303309306\nTRAIN: \t Epoch: 221 \t Loss: -0.015905332984402776\nTRAIN: \t Epoch: 221 \t Loss: -0.016070545547538333\nTRAIN: \t Epoch: 221 \t Loss: -0.01595133328810334\nTRAIN: \t Epoch: 221 \t Loss: -0.015914996154606342\nTRAIN: \t Epoch: 221 \t Loss: -0.01597606243255238\nTRAIN: \t Epoch: 221 \t Loss: -0.015977199046084516\nTRAIN: \t Epoch: 221 \t Loss: -0.016001662838139703\nTRAIN: \t Epoch: 221 \t Loss: -0.016041942499578\nTRAIN: \t Epoch: 221 \t Loss: -0.016102409863378853\nTRAIN: \t Epoch: 221 \t Loss: -0.016130529727567646\nTRAIN: \t Epoch: 221 \t Loss: -0.01612366068487366\nTRAIN: \t Epoch: 221 \t Loss: -0.0161044913785238\nTRAIN: \t Epoch: 221 \t Loss: -0.016081224801018833\nTRAIN: \t Epoch: 221 \t Loss: -0.01608394622980129\nTRAIN: \t Epoch: 221 \t Loss: -0.016056293602263693\nVALD: \t Epoch: 221 \t Loss: -0.008784990757703781\nVALD: \t Epoch: 221 \t Loss: -0.0033430138137191534\nVALD: \t Epoch: 221 \t Loss: -0.003971251814315717\nVALD: \t Epoch: 221 \t Loss: 0.004630069364793599\nVALD: \t Epoch: 221 \t Loss: 0.002444609813392162\nVALD: \t Epoch: 221 \t Loss: 0.0022928676370418434\n******************************\nEpoch: social-tag : 221\ntrain_loss -0.016056293602263693\nval_loss 0.0022928676370418434\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 222 \t Loss: -0.016270408406853676\nTRAIN: \t Epoch: 222 \t Loss: -0.016345126554369926\nTRAIN: \t Epoch: 222 \t Loss: -0.016143187259634335\nTRAIN: \t Epoch: 222 \t Loss: -0.016051086131483316\nTRAIN: \t Epoch: 222 \t Loss: -0.015924291871488094\nTRAIN: \t Epoch: 222 \t Loss: -0.015793646530558664\nTRAIN: \t Epoch: 222 \t Loss: -0.015750091655978134\nTRAIN: \t Epoch: 222 \t Loss: -0.015700069721788168\nTRAIN: \t Epoch: 222 \t Loss: -0.015730416195260152\nTRAIN: \t Epoch: 222 \t Loss: -0.015869879722595216\nTRAIN: \t Epoch: 222 \t Loss: -0.015860650180415672\nTRAIN: \t Epoch: 222 \t Loss: -0.015897714688132208\nTRAIN: \t Epoch: 222 \t Loss: -0.01587836344081622\nTRAIN: \t Epoch: 222 \t Loss: -0.015889517164656093\nTRAIN: \t Epoch: 222 \t Loss: -0.015926402062177658\nTRAIN: \t Epoch: 222 \t Loss: -0.0159768225857988\nTRAIN: \t Epoch: 222 \t Loss: -0.01599032979677705\nTRAIN: \t Epoch: 222 \t Loss: -0.016009818555580244\nTRAIN: \t Epoch: 222 \t Loss: -0.01605492428337273\nTRAIN: \t Epoch: 222 \t Loss: -0.016058136802166702\nTRAIN: \t Epoch: 222 \t Loss: -0.016021924714247387\nTRAIN: \t Epoch: 222 \t Loss: -0.016040419633341435\nVALD: \t Epoch: 222 \t Loss: -0.001504111452959478\nVALD: \t Epoch: 222 \t Loss: 0.005225763947237283\nVALD: \t Epoch: 222 \t Loss: 0.002717844482200841\nVALD: \t Epoch: 222 \t Loss: 0.0210275384306442\nVALD: \t Epoch: 222 \t Loss: 0.016564848041161893\nVALD: \t Epoch: 222 \t Loss: 0.016145436098855555\n******************************\nEpoch: social-tag : 222\ntrain_loss -0.016040419633341435\nval_loss 0.016145436098855555\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 223 \t Loss: -0.016371307894587517\nTRAIN: \t Epoch: 223 \t Loss: -0.01644506026059389\nTRAIN: \t Epoch: 223 \t Loss: -0.016231293479601543\nTRAIN: \t Epoch: 223 \t Loss: -0.016127643175423145\nTRAIN: \t Epoch: 223 \t Loss: -0.016078874841332434\nTRAIN: \t Epoch: 223 \t Loss: -0.015898482253154118\nTRAIN: \t Epoch: 223 \t Loss: -0.01600206962653569\nTRAIN: \t Epoch: 223 \t Loss: -0.01614506682381034\nTRAIN: \t Epoch: 223 \t Loss: -0.016096934883130923\nTRAIN: \t Epoch: 223 \t Loss: -0.016192619688808918\nTRAIN: \t Epoch: 223 \t Loss: -0.016177865761247547\nTRAIN: \t Epoch: 223 \t Loss: -0.01616902695968747\nTRAIN: \t Epoch: 223 \t Loss: -0.016109971544490412\nTRAIN: \t Epoch: 223 \t Loss: -0.016089864407799075\nTRAIN: \t Epoch: 223 \t Loss: -0.016163069444398086\nTRAIN: \t Epoch: 223 \t Loss: -0.01612575474428013\nTRAIN: \t Epoch: 223 \t Loss: -0.016120183281600475\nTRAIN: \t Epoch: 223 \t Loss: -0.01612274685046739\nTRAIN: \t Epoch: 223 \t Loss: -0.01613496837059134\nTRAIN: \t Epoch: 223 \t Loss: -0.016131664300337432\nTRAIN: \t Epoch: 223 \t Loss: -0.01605340718690838\nTRAIN: \t Epoch: 223 \t Loss: -0.016047245082136116\nVALD: \t Epoch: 223 \t Loss: -0.004407035186886787\nVALD: \t Epoch: 223 \t Loss: -0.0005828142166137695\nVALD: \t Epoch: 223 \t Loss: -0.0023647728376090527\nVALD: \t Epoch: 223 \t Loss: 0.009861341095529497\nVALD: \t Epoch: 223 \t Loss: 0.0073816018179059025\nVALD: \t Epoch: 223 \t Loss: 0.007320275722127972\n******************************\nEpoch: social-tag : 223\ntrain_loss -0.016047245082136116\nval_loss 0.007320275722127972\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 224 \t Loss: -0.016110006719827652\nTRAIN: \t Epoch: 224 \t Loss: -0.015873640775680542\nTRAIN: \t Epoch: 224 \t Loss: -0.015822588155666988\nTRAIN: \t Epoch: 224 \t Loss: -0.015981664415448904\nTRAIN: \t Epoch: 224 \t Loss: -0.01606876365840435\nTRAIN: \t Epoch: 224 \t Loss: -0.016089792363345623\nTRAIN: \t Epoch: 224 \t Loss: -0.016006386040576866\nTRAIN: \t Epoch: 224 \t Loss: -0.01601155020762235\nTRAIN: \t Epoch: 224 \t Loss: -0.016050630249083042\nTRAIN: \t Epoch: 224 \t Loss: -0.0159598795697093\nTRAIN: \t Epoch: 224 \t Loss: -0.01596062748946927\nTRAIN: \t Epoch: 224 \t Loss: -0.015949391294270754\nTRAIN: \t Epoch: 224 \t Loss: -0.01595077907236723\nTRAIN: \t Epoch: 224 \t Loss: -0.016001354902982712\nTRAIN: \t Epoch: 224 \t Loss: -0.01606141502658526\nTRAIN: \t Epoch: 224 \t Loss: -0.016068639582954347\nTRAIN: \t Epoch: 224 \t Loss: -0.016108145389486763\nTRAIN: \t Epoch: 224 \t Loss: -0.016099722331596747\nTRAIN: \t Epoch: 224 \t Loss: -0.016039194637223295\nTRAIN: \t Epoch: 224 \t Loss: -0.016088365484029053\nTRAIN: \t Epoch: 224 \t Loss: -0.01605921939370178\nTRAIN: \t Epoch: 224 \t Loss: -0.016030528772241124\nVALD: \t Epoch: 224 \t Loss: -0.005199562292546034\nVALD: \t Epoch: 224 \t Loss: -0.00037530530244112015\nVALD: \t Epoch: 224 \t Loss: -0.001900291070342064\nVALD: \t Epoch: 224 \t Loss: 0.012558524962514639\nVALD: \t Epoch: 224 \t Loss: 0.009534346917644144\nVALD: \t Epoch: 224 \t Loss: 0.009304186142981053\n******************************\nEpoch: social-tag : 224\ntrain_loss -0.016030528772241124\nval_loss 0.009304186142981053\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 225 \t Loss: -0.015628084540367126\nTRAIN: \t Epoch: 225 \t Loss: -0.015819011256098747\nTRAIN: \t Epoch: 225 \t Loss: -0.016206242764989536\nTRAIN: \t Epoch: 225 \t Loss: -0.01601439993828535\nTRAIN: \t Epoch: 225 \t Loss: -0.016061164438724518\nTRAIN: \t Epoch: 225 \t Loss: -0.01621257762114207\nTRAIN: \t Epoch: 225 \t Loss: -0.01602320865328823\nTRAIN: \t Epoch: 225 \t Loss: -0.015979043091647327\nTRAIN: \t Epoch: 225 \t Loss: -0.01614607260045078\nTRAIN: \t Epoch: 225 \t Loss: -0.016173113230615856\nTRAIN: \t Epoch: 225 \t Loss: -0.016190413300963966\nTRAIN: \t Epoch: 225 \t Loss: -0.01621138125968476\nTRAIN: \t Epoch: 225 \t Loss: -0.016230522368389826\nTRAIN: \t Epoch: 225 \t Loss: -0.016206903715751002\nTRAIN: \t Epoch: 225 \t Loss: -0.01618010432769855\nTRAIN: \t Epoch: 225 \t Loss: -0.016135622514411807\nTRAIN: \t Epoch: 225 \t Loss: -0.01610455278526334\nTRAIN: \t Epoch: 225 \t Loss: -0.01611308091216617\nTRAIN: \t Epoch: 225 \t Loss: -0.01612926568639906\nTRAIN: \t Epoch: 225 \t Loss: -0.01610487950965762\nTRAIN: \t Epoch: 225 \t Loss: -0.01614532531017349\nTRAIN: \t Epoch: 225 \t Loss: -0.016120658271830644\nVALD: \t Epoch: 225 \t Loss: -0.00017364879022352397\nVALD: \t Epoch: 225 \t Loss: 0.008190246313461103\nVALD: \t Epoch: 225 \t Loss: 0.005283190878496195\nVALD: \t Epoch: 225 \t Loss: 0.020631709376175422\nVALD: \t Epoch: 225 \t Loss: 0.0162057873851154\nVALD: \t Epoch: 225 \t Loss: 0.015711203735730976\n******************************\nEpoch: social-tag : 225\ntrain_loss -0.016120658271830644\nval_loss 0.015711203735730976\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 226 \t Loss: -0.015478117391467094\nTRAIN: \t Epoch: 226 \t Loss: -0.015621962957084179\nTRAIN: \t Epoch: 226 \t Loss: -0.015972906723618507\nTRAIN: \t Epoch: 226 \t Loss: -0.01633905665948987\nTRAIN: \t Epoch: 226 \t Loss: -0.016170631162822247\nTRAIN: \t Epoch: 226 \t Loss: -0.01590495292718212\nTRAIN: \t Epoch: 226 \t Loss: -0.01594198268971273\nTRAIN: \t Epoch: 226 \t Loss: -0.01601979520637542\nTRAIN: \t Epoch: 226 \t Loss: -0.015969182261162333\nTRAIN: \t Epoch: 226 \t Loss: -0.01593007128685713\nTRAIN: \t Epoch: 226 \t Loss: -0.01602269404313781\nTRAIN: \t Epoch: 226 \t Loss: -0.01608018608142932\nTRAIN: \t Epoch: 226 \t Loss: -0.016074136186104555\nTRAIN: \t Epoch: 226 \t Loss: -0.016054118584309305\nTRAIN: \t Epoch: 226 \t Loss: -0.01603067430357138\nTRAIN: \t Epoch: 226 \t Loss: -0.016078225220553577\nTRAIN: \t Epoch: 226 \t Loss: -0.016087054877596742\nTRAIN: \t Epoch: 226 \t Loss: -0.016048587548236053\nTRAIN: \t Epoch: 226 \t Loss: -0.01601512249755232\nTRAIN: \t Epoch: 226 \t Loss: -0.016020975727587938\nTRAIN: \t Epoch: 226 \t Loss: -0.016041160694190433\nTRAIN: \t Epoch: 226 \t Loss: -0.016063837303079437\nVALD: \t Epoch: 226 \t Loss: -0.01085006631910801\nVALD: \t Epoch: 226 \t Loss: -0.004543921444565058\nVALD: \t Epoch: 226 \t Loss: -0.0055999536998569965\nVALD: \t Epoch: 226 \t Loss: 0.006020564236678183\nVALD: \t Epoch: 226 \t Loss: 0.0037700253538787364\nVALD: \t Epoch: 226 \t Loss: 0.0035630489558433043\n******************************\nEpoch: social-tag : 226\ntrain_loss -0.016063837303079437\nval_loss 0.0035630489558433043\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 227 \t Loss: -0.017036868259310722\nTRAIN: \t Epoch: 227 \t Loss: -0.01661732792854309\nTRAIN: \t Epoch: 227 \t Loss: -0.016526035964488983\nTRAIN: \t Epoch: 227 \t Loss: -0.01650153798982501\nTRAIN: \t Epoch: 227 \t Loss: -0.016508206725120544\nTRAIN: \t Epoch: 227 \t Loss: -0.016514347555736702\nTRAIN: \t Epoch: 227 \t Loss: -0.01644598107252802\nTRAIN: \t Epoch: 227 \t Loss: -0.01641425723209977\nTRAIN: \t Epoch: 227 \t Loss: -0.01649509494503339\nTRAIN: \t Epoch: 227 \t Loss: -0.016437284462153912\nTRAIN: \t Epoch: 227 \t Loss: -0.016405100680210373\nTRAIN: \t Epoch: 227 \t Loss: -0.01634961925446987\nTRAIN: \t Epoch: 227 \t Loss: -0.016248207467679795\nTRAIN: \t Epoch: 227 \t Loss: -0.016171665463064398\nTRAIN: \t Epoch: 227 \t Loss: -0.016123975316683452\nTRAIN: \t Epoch: 227 \t Loss: -0.0161063966806978\nTRAIN: \t Epoch: 227 \t Loss: -0.016074230401393247\nTRAIN: \t Epoch: 227 \t Loss: -0.016043815554844007\nTRAIN: \t Epoch: 227 \t Loss: -0.016052254426636194\nTRAIN: \t Epoch: 227 \t Loss: -0.016055720020085575\nTRAIN: \t Epoch: 227 \t Loss: -0.016076616854185148\nTRAIN: \t Epoch: 227 \t Loss: -0.016056932324352985\nVALD: \t Epoch: 227 \t Loss: -0.005117988213896751\nVALD: \t Epoch: 227 \t Loss: 0.0027641644701361656\nVALD: \t Epoch: 227 \t Loss: 6.538520877559979e-05\nVALD: \t Epoch: 227 \t Loss: 0.011445811134763062\nVALD: \t Epoch: 227 \t Loss: 0.008668982330709696\nVALD: \t Epoch: 227 \t Loss: 0.008582577933416222\n******************************\nEpoch: social-tag : 227\ntrain_loss -0.016056932324352985\nval_loss 0.008582577933416222\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 228 \t Loss: -0.016512174159288406\nTRAIN: \t Epoch: 228 \t Loss: -0.01648374553769827\nTRAIN: \t Epoch: 228 \t Loss: -0.01637139357626438\nTRAIN: \t Epoch: 228 \t Loss: -0.016370197292417288\nTRAIN: \t Epoch: 228 \t Loss: -0.016037024557590485\nTRAIN: \t Epoch: 228 \t Loss: -0.016098008180658024\nTRAIN: \t Epoch: 228 \t Loss: -0.015909851395658085\nTRAIN: \t Epoch: 228 \t Loss: -0.01592867262661457\nTRAIN: \t Epoch: 228 \t Loss: -0.015928242976466816\nTRAIN: \t Epoch: 228 \t Loss: -0.016083209216594695\nTRAIN: \t Epoch: 228 \t Loss: -0.016034013507041065\nTRAIN: \t Epoch: 228 \t Loss: -0.01608401242022713\nTRAIN: \t Epoch: 228 \t Loss: -0.016104431106494024\nTRAIN: \t Epoch: 228 \t Loss: -0.016118664826665605\nTRAIN: \t Epoch: 228 \t Loss: -0.016035173336664835\nTRAIN: \t Epoch: 228 \t Loss: -0.016009178420063108\nTRAIN: \t Epoch: 228 \t Loss: -0.01608542906229987\nTRAIN: \t Epoch: 228 \t Loss: -0.016097095432794757\nTRAIN: \t Epoch: 228 \t Loss: -0.016063091737267218\nTRAIN: \t Epoch: 228 \t Loss: -0.016044960590079427\nTRAIN: \t Epoch: 228 \t Loss: -0.016043558583727906\nTRAIN: \t Epoch: 228 \t Loss: -0.016060873850029717\nVALD: \t Epoch: 228 \t Loss: -0.003146333387121558\nVALD: \t Epoch: 228 \t Loss: 0.0037368430057540536\nVALD: \t Epoch: 228 \t Loss: 0.0019020095933228731\nVALD: \t Epoch: 228 \t Loss: 0.016138217819388956\nVALD: \t Epoch: 228 \t Loss: 0.012543010688386858\nVALD: \t Epoch: 228 \t Loss: 0.012289691547101194\n******************************\nEpoch: social-tag : 228\ntrain_loss -0.016060873850029717\nval_loss 0.012289691547101194\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 229 \t Loss: -0.01589164137840271\nTRAIN: \t Epoch: 229 \t Loss: -0.015645498409867287\nTRAIN: \t Epoch: 229 \t Loss: -0.01568872667849064\nTRAIN: \t Epoch: 229 \t Loss: -0.015754330437630415\nTRAIN: \t Epoch: 229 \t Loss: -0.01588533855974674\nTRAIN: \t Epoch: 229 \t Loss: -0.01593185557673375\nTRAIN: \t Epoch: 229 \t Loss: -0.016111171937414577\nTRAIN: \t Epoch: 229 \t Loss: -0.016202163649722934\nTRAIN: \t Epoch: 229 \t Loss: -0.016249937729703054\nTRAIN: \t Epoch: 229 \t Loss: -0.016330177895724773\nTRAIN: \t Epoch: 229 \t Loss: -0.01637854918160222\nTRAIN: \t Epoch: 229 \t Loss: -0.0163339131201307\nTRAIN: \t Epoch: 229 \t Loss: -0.016199592142724074\nTRAIN: \t Epoch: 229 \t Loss: -0.01615482076470341\nTRAIN: \t Epoch: 229 \t Loss: -0.016189064582188925\nTRAIN: \t Epoch: 229 \t Loss: -0.0161557235987857\nTRAIN: \t Epoch: 229 \t Loss: -0.01613752280964571\nTRAIN: \t Epoch: 229 \t Loss: -0.016071224999096658\nTRAIN: \t Epoch: 229 \t Loss: -0.016090767654149157\nTRAIN: \t Epoch: 229 \t Loss: -0.016139138862490653\nTRAIN: \t Epoch: 229 \t Loss: -0.016107687371827308\nTRAIN: \t Epoch: 229 \t Loss: -0.016069235878744075\nVALD: \t Epoch: 229 \t Loss: -0.00914833601564169\nVALD: \t Epoch: 229 \t Loss: -0.003350181970745325\nVALD: \t Epoch: 229 \t Loss: -0.005249392551680406\nVALD: \t Epoch: 229 \t Loss: 0.00609179656021297\nVALD: \t Epoch: 229 \t Loss: 0.0037768244743347167\nVALD: \t Epoch: 229 \t Loss: 0.003611969648662842\n******************************\nEpoch: social-tag : 229\ntrain_loss -0.016069235878744075\nval_loss 0.003611969648662842\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 230 \t Loss: -0.017369398847222328\nTRAIN: \t Epoch: 230 \t Loss: -0.01703945267945528\nTRAIN: \t Epoch: 230 \t Loss: -0.01640545390546322\nTRAIN: \t Epoch: 230 \t Loss: -0.016262356657534838\nTRAIN: \t Epoch: 230 \t Loss: -0.016215989738702773\nTRAIN: \t Epoch: 230 \t Loss: -0.0160125734594961\nTRAIN: \t Epoch: 230 \t Loss: -0.01603527114327465\nTRAIN: \t Epoch: 230 \t Loss: -0.015959644922986627\nTRAIN: \t Epoch: 230 \t Loss: -0.015976545711358387\nTRAIN: \t Epoch: 230 \t Loss: -0.01591341309249401\nTRAIN: \t Epoch: 230 \t Loss: -0.015900255942886524\nTRAIN: \t Epoch: 230 \t Loss: -0.01597696092600624\nTRAIN: \t Epoch: 230 \t Loss: -0.015890577378181312\nTRAIN: \t Epoch: 230 \t Loss: -0.015961770101317337\nTRAIN: \t Epoch: 230 \t Loss: -0.015946026022235552\nTRAIN: \t Epoch: 230 \t Loss: -0.016002581105567515\nTRAIN: \t Epoch: 230 \t Loss: -0.015995862072004992\nTRAIN: \t Epoch: 230 \t Loss: -0.016003692626125283\nTRAIN: \t Epoch: 230 \t Loss: -0.015969238047929185\nTRAIN: \t Epoch: 230 \t Loss: -0.016015453590080143\nTRAIN: \t Epoch: 230 \t Loss: -0.01604213055578016\nTRAIN: \t Epoch: 230 \t Loss: -0.016059648840816915\nVALD: \t Epoch: 230 \t Loss: 0.0027437915559858084\nVALD: \t Epoch: 230 \t Loss: 0.012394386227242649\nVALD: \t Epoch: 230 \t Loss: 0.009426376549527049\nVALD: \t Epoch: 230 \t Loss: 0.025829651451203972\nVALD: \t Epoch: 230 \t Loss: 0.02083813603967428\nVALD: \t Epoch: 230 \t Loss: 0.020311704311858525\n******************************\nEpoch: social-tag : 230\ntrain_loss -0.016059648840816915\nval_loss 0.020311704311858525\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 231 \t Loss: -0.016054995357990265\nTRAIN: \t Epoch: 231 \t Loss: -0.016323795542120934\nTRAIN: \t Epoch: 231 \t Loss: -0.01613425835967064\nTRAIN: \t Epoch: 231 \t Loss: -0.016020264476537704\nTRAIN: \t Epoch: 231 \t Loss: -0.016248349845409394\nTRAIN: \t Epoch: 231 \t Loss: -0.016118171780059736\nTRAIN: \t Epoch: 231 \t Loss: -0.016169923623757704\nTRAIN: \t Epoch: 231 \t Loss: -0.016204384504817426\nTRAIN: \t Epoch: 231 \t Loss: -0.01620837559716569\nTRAIN: \t Epoch: 231 \t Loss: -0.016176981572061776\nTRAIN: \t Epoch: 231 \t Loss: -0.016098544424907726\nTRAIN: \t Epoch: 231 \t Loss: -0.016092159086838365\nTRAIN: \t Epoch: 231 \t Loss: -0.016052621560027965\nTRAIN: \t Epoch: 231 \t Loss: -0.016024712805769274\nTRAIN: \t Epoch: 231 \t Loss: -0.016015771962702274\nTRAIN: \t Epoch: 231 \t Loss: -0.01600846037035808\nTRAIN: \t Epoch: 231 \t Loss: -0.016024778706624228\nTRAIN: \t Epoch: 231 \t Loss: -0.016034573057873383\nTRAIN: \t Epoch: 231 \t Loss: -0.01604423627845551\nTRAIN: \t Epoch: 231 \t Loss: -0.016071237763389944\nTRAIN: \t Epoch: 231 \t Loss: -0.016092891804873943\nTRAIN: \t Epoch: 231 \t Loss: -0.0160952611714334\nVALD: \t Epoch: 231 \t Loss: -0.0047529456205666065\nVALD: \t Epoch: 231 \t Loss: 0.002177512040361762\nVALD: \t Epoch: 231 \t Loss: 0.0013389175777168323\nVALD: \t Epoch: 231 \t Loss: 0.017166809040645603\nVALD: \t Epoch: 231 \t Loss: 0.013416438928106799\nVALD: \t Epoch: 231 \t Loss: 0.01301782162803592\n******************************\nEpoch: social-tag : 231\ntrain_loss -0.0160952611714334\nval_loss 0.01301782162803592\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 232 \t Loss: -0.014649918302893639\nTRAIN: \t Epoch: 232 \t Loss: -0.015398850664496422\nTRAIN: \t Epoch: 232 \t Loss: -0.015807825451095898\nTRAIN: \t Epoch: 232 \t Loss: -0.016122216824442148\nTRAIN: \t Epoch: 232 \t Loss: -0.016197745129466055\nTRAIN: \t Epoch: 232 \t Loss: -0.016225681950648625\nTRAIN: \t Epoch: 232 \t Loss: -0.016174974186079844\nTRAIN: \t Epoch: 232 \t Loss: -0.016243140678852797\nTRAIN: \t Epoch: 232 \t Loss: -0.016393818996018834\nTRAIN: \t Epoch: 232 \t Loss: -0.016390800662338733\nTRAIN: \t Epoch: 232 \t Loss: -0.016252162696963005\nTRAIN: \t Epoch: 232 \t Loss: -0.016065037343651056\nTRAIN: \t Epoch: 232 \t Loss: -0.016082963404747155\nTRAIN: \t Epoch: 232 \t Loss: -0.016019189530717477\nTRAIN: \t Epoch: 232 \t Loss: -0.01598163805902004\nTRAIN: \t Epoch: 232 \t Loss: -0.015973719186149538\nTRAIN: \t Epoch: 232 \t Loss: -0.015971618649714133\nTRAIN: \t Epoch: 232 \t Loss: -0.016012228611442778\nTRAIN: \t Epoch: 232 \t Loss: -0.016035458171053937\nTRAIN: \t Epoch: 232 \t Loss: -0.016091035958379508\nTRAIN: \t Epoch: 232 \t Loss: -0.016091899236752874\nTRAIN: \t Epoch: 232 \t Loss: -0.0161148892698853\nVALD: \t Epoch: 232 \t Loss: -0.010347108356654644\nVALD: \t Epoch: 232 \t Loss: -0.006407988024875522\nVALD: \t Epoch: 232 \t Loss: -0.007845423960437378\nVALD: \t Epoch: 232 \t Loss: -1.8786522559821606e-05\nVALD: \t Epoch: 232 \t Loss: -0.0011565404012799263\nVALD: \t Epoch: 232 \t Loss: -0.001039699719033458\n******************************\nEpoch: social-tag : 232\ntrain_loss -0.0161148892698853\nval_loss -0.001039699719033458\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 233 \t Loss: -0.01650373637676239\nTRAIN: \t Epoch: 233 \t Loss: -0.01622533332556486\nTRAIN: \t Epoch: 233 \t Loss: -0.016463583956162136\nTRAIN: \t Epoch: 233 \t Loss: -0.016495705116540194\nTRAIN: \t Epoch: 233 \t Loss: -0.01625669002532959\nTRAIN: \t Epoch: 233 \t Loss: -0.01613600365817547\nTRAIN: \t Epoch: 233 \t Loss: -0.016160862520337105\nTRAIN: \t Epoch: 233 \t Loss: -0.016132030868902802\nTRAIN: \t Epoch: 233 \t Loss: -0.016082884329888556\nTRAIN: \t Epoch: 233 \t Loss: -0.01596725480630994\nTRAIN: \t Epoch: 233 \t Loss: -0.016021197238429027\nTRAIN: \t Epoch: 233 \t Loss: -0.016012551030144095\nTRAIN: \t Epoch: 233 \t Loss: -0.016011327003630307\nTRAIN: \t Epoch: 233 \t Loss: -0.016033768986484835\nTRAIN: \t Epoch: 233 \t Loss: -0.016068685116867223\nTRAIN: \t Epoch: 233 \t Loss: -0.016111179545987397\nTRAIN: \t Epoch: 233 \t Loss: -0.016133491662057006\nTRAIN: \t Epoch: 233 \t Loss: -0.016009578584796853\nTRAIN: \t Epoch: 233 \t Loss: -0.015998430255996555\nTRAIN: \t Epoch: 233 \t Loss: -0.016037472430616616\nTRAIN: \t Epoch: 233 \t Loss: -0.016030338991965567\nTRAIN: \t Epoch: 233 \t Loss: -0.016036646327595737\nVALD: \t Epoch: 233 \t Loss: -0.0026793440338224173\nVALD: \t Epoch: 233 \t Loss: 0.0064598406897857785\nVALD: \t Epoch: 233 \t Loss: 0.003124634580065807\nVALD: \t Epoch: 233 \t Loss: 0.022127971169538796\nVALD: \t Epoch: 233 \t Loss: 0.017514643189497293\nVALD: \t Epoch: 233 \t Loss: 0.017150118551922566\n******************************\nEpoch: social-tag : 233\ntrain_loss -0.016036646327595737\nval_loss 0.017150118551922566\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 234 \t Loss: -0.016228610649704933\nTRAIN: \t Epoch: 234 \t Loss: -0.015921607613563538\nTRAIN: \t Epoch: 234 \t Loss: -0.016494605069359142\nTRAIN: \t Epoch: 234 \t Loss: -0.016294379252940416\nTRAIN: \t Epoch: 234 \t Loss: -0.016428376361727715\nTRAIN: \t Epoch: 234 \t Loss: -0.01657946438839038\nTRAIN: \t Epoch: 234 \t Loss: -0.016658914940697805\nTRAIN: \t Epoch: 234 \t Loss: -0.01655338774435222\nTRAIN: \t Epoch: 234 \t Loss: -0.01657387034760581\nTRAIN: \t Epoch: 234 \t Loss: -0.01629269849509001\nTRAIN: \t Epoch: 234 \t Loss: -0.0162422816184434\nTRAIN: \t Epoch: 234 \t Loss: -0.0161634791487207\nTRAIN: \t Epoch: 234 \t Loss: -0.016119237559346054\nTRAIN: \t Epoch: 234 \t Loss: -0.01608886583042996\nTRAIN: \t Epoch: 234 \t Loss: -0.0161037415266037\nTRAIN: \t Epoch: 234 \t Loss: -0.016089801560156047\nTRAIN: \t Epoch: 234 \t Loss: -0.016080389566281262\nTRAIN: \t Epoch: 234 \t Loss: -0.0161299594781465\nTRAIN: \t Epoch: 234 \t Loss: -0.016102153170657784\nTRAIN: \t Epoch: 234 \t Loss: -0.016122985119000077\nTRAIN: \t Epoch: 234 \t Loss: -0.016125761016848542\nTRAIN: \t Epoch: 234 \t Loss: -0.016112643426795838\nVALD: \t Epoch: 234 \t Loss: -0.006527433171868324\nVALD: \t Epoch: 234 \t Loss: -0.0005055079236626625\nVALD: \t Epoch: 234 \t Loss: -0.002157056083281835\nVALD: \t Epoch: 234 \t Loss: 0.014613364823162556\nVALD: \t Epoch: 234 \t Loss: 0.0116546917532105\nVALD: \t Epoch: 234 \t Loss: 0.011457101408053528\n******************************\nEpoch: social-tag : 234\ntrain_loss -0.016112643426795838\nval_loss 0.011457101408053528\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 235 \t Loss: -0.01598089188337326\nTRAIN: \t Epoch: 235 \t Loss: -0.0162104619666934\nTRAIN: \t Epoch: 235 \t Loss: -0.016303276643157005\nTRAIN: \t Epoch: 235 \t Loss: -0.01590898516587913\nTRAIN: \t Epoch: 235 \t Loss: -0.015956889651715756\nTRAIN: \t Epoch: 235 \t Loss: -0.01606366488461693\nTRAIN: \t Epoch: 235 \t Loss: -0.016068699503583566\nTRAIN: \t Epoch: 235 \t Loss: -0.01608826976735145\nTRAIN: \t Epoch: 235 \t Loss: -0.016054506206678018\nTRAIN: \t Epoch: 235 \t Loss: -0.0160801294259727\nTRAIN: \t Epoch: 235 \t Loss: -0.01611530298197811\nTRAIN: \t Epoch: 235 \t Loss: -0.016116124655430514\nTRAIN: \t Epoch: 235 \t Loss: -0.016160051338374615\nTRAIN: \t Epoch: 235 \t Loss: -0.016192039674414054\nTRAIN: \t Epoch: 235 \t Loss: -0.016186878519753613\nTRAIN: \t Epoch: 235 \t Loss: -0.016184658685233444\nTRAIN: \t Epoch: 235 \t Loss: -0.01618232758825316\nTRAIN: \t Epoch: 235 \t Loss: -0.016190760613729555\nTRAIN: \t Epoch: 235 \t Loss: -0.016181116482537044\nTRAIN: \t Epoch: 235 \t Loss: -0.01617746432311833\nTRAIN: \t Epoch: 235 \t Loss: -0.01612938949394794\nTRAIN: \t Epoch: 235 \t Loss: -0.01611890668714924\nVALD: \t Epoch: 235 \t Loss: -0.002647670917212963\nVALD: \t Epoch: 235 \t Loss: 0.004952486604452133\nVALD: \t Epoch: 235 \t Loss: 0.0020044405634204545\nVALD: \t Epoch: 235 \t Loss: 0.011930647073313594\nVALD: \t Epoch: 235 \t Loss: 0.009692020376678556\nVALD: \t Epoch: 235 \t Loss: 0.009622287942153035\n******************************\nEpoch: social-tag : 235\ntrain_loss -0.01611890668714924\nval_loss 0.009622287942153035\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 236 \t Loss: -0.016196146607398987\nTRAIN: \t Epoch: 236 \t Loss: -0.01680881530046463\nTRAIN: \t Epoch: 236 \t Loss: -0.01642067606250445\nTRAIN: \t Epoch: 236 \t Loss: -0.01625879853963852\nTRAIN: \t Epoch: 236 \t Loss: -0.016083122044801713\nTRAIN: \t Epoch: 236 \t Loss: -0.015993224767347176\nTRAIN: \t Epoch: 236 \t Loss: -0.01594951722238745\nTRAIN: \t Epoch: 236 \t Loss: -0.016009872779250145\nTRAIN: \t Epoch: 236 \t Loss: -0.01603027019235823\nTRAIN: \t Epoch: 236 \t Loss: -0.016025604493916035\nTRAIN: \t Epoch: 236 \t Loss: -0.01606805385513739\nTRAIN: \t Epoch: 236 \t Loss: -0.016007433102155726\nTRAIN: \t Epoch: 236 \t Loss: -0.016020487922315415\nTRAIN: \t Epoch: 236 \t Loss: -0.0161062987920429\nTRAIN: \t Epoch: 236 \t Loss: -0.016153834325571857\nTRAIN: \t Epoch: 236 \t Loss: -0.01619794050930068\nTRAIN: \t Epoch: 236 \t Loss: -0.016173197230433718\nTRAIN: \t Epoch: 236 \t Loss: -0.016129427692956395\nTRAIN: \t Epoch: 236 \t Loss: -0.016148884339552177\nTRAIN: \t Epoch: 236 \t Loss: -0.01612263829447329\nTRAIN: \t Epoch: 236 \t Loss: -0.016062701254018714\nTRAIN: \t Epoch: 236 \t Loss: -0.016034000609036623\nVALD: \t Epoch: 236 \t Loss: -0.004927200265228748\nVALD: \t Epoch: 236 \t Loss: 0.0013964930549263954\nVALD: \t Epoch: 236 \t Loss: -0.00022483919747173786\nVALD: \t Epoch: 236 \t Loss: 0.01056086883181706\nVALD: \t Epoch: 236 \t Loss: 0.007901254901662469\nVALD: \t Epoch: 236 \t Loss: 0.007623541118069128\n******************************\nEpoch: social-tag : 236\ntrain_loss -0.016034000609036623\nval_loss 0.007623541118069128\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 237 \t Loss: -0.01713733747601509\nTRAIN: \t Epoch: 237 \t Loss: -0.016903906129300594\nTRAIN: \t Epoch: 237 \t Loss: -0.01719192663828532\nTRAIN: \t Epoch: 237 \t Loss: -0.01717227278277278\nTRAIN: \t Epoch: 237 \t Loss: -0.016869454830884933\nTRAIN: \t Epoch: 237 \t Loss: -0.016552879475057125\nTRAIN: \t Epoch: 237 \t Loss: -0.01636283951146262\nTRAIN: \t Epoch: 237 \t Loss: -0.01645409851334989\nTRAIN: \t Epoch: 237 \t Loss: -0.01625187819202741\nTRAIN: \t Epoch: 237 \t Loss: -0.016257922910153867\nTRAIN: \t Epoch: 237 \t Loss: -0.016125161370093174\nTRAIN: \t Epoch: 237 \t Loss: -0.01615389995276928\nTRAIN: \t Epoch: 237 \t Loss: -0.016239464139709107\nTRAIN: \t Epoch: 237 \t Loss: -0.01623940760535853\nTRAIN: \t Epoch: 237 \t Loss: -0.016227688267827035\nTRAIN: \t Epoch: 237 \t Loss: -0.01630107662640512\nTRAIN: \t Epoch: 237 \t Loss: -0.016363150166238054\nTRAIN: \t Epoch: 237 \t Loss: -0.016347932525806956\nTRAIN: \t Epoch: 237 \t Loss: -0.016274247661625083\nTRAIN: \t Epoch: 237 \t Loss: -0.01621148022823036\nTRAIN: \t Epoch: 237 \t Loss: -0.016181518900252524\nTRAIN: \t Epoch: 237 \t Loss: -0.016160646118424313\nVALD: \t Epoch: 237 \t Loss: -0.008385522291064262\nVALD: \t Epoch: 237 \t Loss: -0.004390467162011191\nVALD: \t Epoch: 237 \t Loss: -0.005523598888733734\nVALD: \t Epoch: 237 \t Loss: 0.007014525719569065\nVALD: \t Epoch: 237 \t Loss: 0.004416602395940572\nVALD: \t Epoch: 237 \t Loss: 0.004262375823137435\n******************************\nEpoch: social-tag : 237\ntrain_loss -0.016160646118424313\nval_loss 0.004262375823137435\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 238 \t Loss: -0.01574215106666088\nTRAIN: \t Epoch: 238 \t Loss: -0.01540575223043561\nTRAIN: \t Epoch: 238 \t Loss: -0.0158885782584548\nTRAIN: \t Epoch: 238 \t Loss: -0.01619817386381328\nTRAIN: \t Epoch: 238 \t Loss: -0.01624730359762907\nTRAIN: \t Epoch: 238 \t Loss: -0.016277375786254804\nTRAIN: \t Epoch: 238 \t Loss: -0.016340541653335094\nTRAIN: \t Epoch: 238 \t Loss: -0.01642350188922137\nTRAIN: \t Epoch: 238 \t Loss: -0.016369284130632877\nTRAIN: \t Epoch: 238 \t Loss: -0.0163304946385324\nTRAIN: \t Epoch: 238 \t Loss: -0.01629370984367349\nTRAIN: \t Epoch: 238 \t Loss: -0.016241270350292325\nTRAIN: \t Epoch: 238 \t Loss: -0.01626899358458244\nTRAIN: \t Epoch: 238 \t Loss: -0.016309863422065973\nTRAIN: \t Epoch: 238 \t Loss: -0.0162282503520449\nTRAIN: \t Epoch: 238 \t Loss: -0.016194912197533995\nTRAIN: \t Epoch: 238 \t Loss: -0.016116832109058604\nTRAIN: \t Epoch: 238 \t Loss: -0.016124460846185684\nTRAIN: \t Epoch: 238 \t Loss: -0.016060889698565006\nTRAIN: \t Epoch: 238 \t Loss: -0.016108636790886522\nTRAIN: \t Epoch: 238 \t Loss: -0.016115997829252764\nTRAIN: \t Epoch: 238 \t Loss: -0.01613246645490828\nVALD: \t Epoch: 238 \t Loss: 0.0007146773859858513\nVALD: \t Epoch: 238 \t Loss: 0.007185485679656267\nVALD: \t Epoch: 238 \t Loss: 0.003525809229662021\nVALD: \t Epoch: 238 \t Loss: 0.02154885104391724\nVALD: \t Epoch: 238 \t Loss: 0.01724141974154918\nVALD: \t Epoch: 238 \t Loss: 0.01693472578366654\n******************************\nEpoch: social-tag : 238\ntrain_loss -0.01613246645490828\nval_loss 0.01693472578366654\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 239 \t Loss: -0.015765275806188583\nTRAIN: \t Epoch: 239 \t Loss: -0.0158994160592556\nTRAIN: \t Epoch: 239 \t Loss: -0.016183796028296154\nTRAIN: \t Epoch: 239 \t Loss: -0.016359714325517416\nTRAIN: \t Epoch: 239 \t Loss: -0.016692164540290832\nTRAIN: \t Epoch: 239 \t Loss: -0.016622285979489487\nTRAIN: \t Epoch: 239 \t Loss: -0.01666423412305968\nTRAIN: \t Epoch: 239 \t Loss: -0.016620012698695064\nTRAIN: \t Epoch: 239 \t Loss: -0.01656280251012908\nTRAIN: \t Epoch: 239 \t Loss: -0.016452952940016984\nTRAIN: \t Epoch: 239 \t Loss: -0.016425673172555187\nTRAIN: \t Epoch: 239 \t Loss: -0.01635731174610555\nTRAIN: \t Epoch: 239 \t Loss: -0.016312700553009145\nTRAIN: \t Epoch: 239 \t Loss: -0.016310309126440967\nTRAIN: \t Epoch: 239 \t Loss: -0.01627044559766849\nTRAIN: \t Epoch: 239 \t Loss: -0.016249821346718818\nTRAIN: \t Epoch: 239 \t Loss: -0.01617679323124535\nTRAIN: \t Epoch: 239 \t Loss: -0.016225198852933116\nTRAIN: \t Epoch: 239 \t Loss: -0.016185730744741465\nTRAIN: \t Epoch: 239 \t Loss: -0.016169821145012975\nTRAIN: \t Epoch: 239 \t Loss: -0.01614176712575413\nTRAIN: \t Epoch: 239 \t Loss: -0.016148211455216742\nVALD: \t Epoch: 239 \t Loss: -0.006653084885329008\nVALD: \t Epoch: 239 \t Loss: 6.342306733131409e-06\nVALD: \t Epoch: 239 \t Loss: -0.002240134868770838\nVALD: \t Epoch: 239 \t Loss: 0.011608968605287373\nVALD: \t Epoch: 239 \t Loss: 0.008684600936248899\nVALD: \t Epoch: 239 \t Loss: 0.008513139402776053\n******************************\nEpoch: social-tag : 239\ntrain_loss -0.016148211455216742\nval_loss 0.008513139402776053\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 240 \t Loss: -0.016425451263785362\nTRAIN: \t Epoch: 240 \t Loss: -0.0163947232067585\nTRAIN: \t Epoch: 240 \t Loss: -0.01636853317419688\nTRAIN: \t Epoch: 240 \t Loss: -0.016259636264294386\nTRAIN: \t Epoch: 240 \t Loss: -0.016370295733213424\nTRAIN: \t Epoch: 240 \t Loss: -0.016196368572612602\nTRAIN: \t Epoch: 240 \t Loss: -0.01601089710103614\nTRAIN: \t Epoch: 240 \t Loss: -0.016197377466596663\nTRAIN: \t Epoch: 240 \t Loss: -0.016174226481881406\nTRAIN: \t Epoch: 240 \t Loss: -0.016138824913650752\nTRAIN: \t Epoch: 240 \t Loss: -0.01606111287732016\nTRAIN: \t Epoch: 240 \t Loss: -0.01600053006162246\nTRAIN: \t Epoch: 240 \t Loss: -0.01602010830090596\nTRAIN: \t Epoch: 240 \t Loss: -0.01603027286806277\nTRAIN: \t Epoch: 240 \t Loss: -0.0160454003761212\nTRAIN: \t Epoch: 240 \t Loss: -0.01601537549868226\nTRAIN: \t Epoch: 240 \t Loss: -0.016037392265656415\nTRAIN: \t Epoch: 240 \t Loss: -0.01608882492615117\nTRAIN: \t Epoch: 240 \t Loss: -0.01610247094772364\nTRAIN: \t Epoch: 240 \t Loss: -0.01609066603705287\nTRAIN: \t Epoch: 240 \t Loss: -0.016087213265044347\nTRAIN: \t Epoch: 240 \t Loss: -0.016103405027886052\nVALD: \t Epoch: 240 \t Loss: -0.007820052094757557\nVALD: \t Epoch: 240 \t Loss: 0.0001380695030093193\nVALD: \t Epoch: 240 \t Loss: -0.002214366259674231\nVALD: \t Epoch: 240 \t Loss: 0.01544550689868629\nVALD: \t Epoch: 240 \t Loss: 0.011754115717485547\nVALD: \t Epoch: 240 \t Loss: 0.011433266142778325\n******************************\nEpoch: social-tag : 240\ntrain_loss -0.016103405027886052\nval_loss 0.011433266142778325\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 241 \t Loss: -0.016173629090189934\nTRAIN: \t Epoch: 241 \t Loss: -0.015863097738474607\nTRAIN: \t Epoch: 241 \t Loss: -0.015400352887809277\nTRAIN: \t Epoch: 241 \t Loss: -0.015411931090056896\nTRAIN: \t Epoch: 241 \t Loss: -0.01562730222940445\nTRAIN: \t Epoch: 241 \t Loss: -0.015716463948289554\nTRAIN: \t Epoch: 241 \t Loss: -0.01589472379003252\nTRAIN: \t Epoch: 241 \t Loss: -0.015793291851878166\nTRAIN: \t Epoch: 241 \t Loss: -0.01588455732497904\nTRAIN: \t Epoch: 241 \t Loss: -0.015915935300290583\nTRAIN: \t Epoch: 241 \t Loss: -0.01605920730666681\nTRAIN: \t Epoch: 241 \t Loss: -0.016053710288057726\nTRAIN: \t Epoch: 241 \t Loss: -0.01611746159883646\nTRAIN: \t Epoch: 241 \t Loss: -0.01611290978533881\nTRAIN: \t Epoch: 241 \t Loss: -0.01617363914847374\nTRAIN: \t Epoch: 241 \t Loss: -0.016114409372676164\nTRAIN: \t Epoch: 241 \t Loss: -0.01608270061585833\nTRAIN: \t Epoch: 241 \t Loss: -0.016116143773413368\nTRAIN: \t Epoch: 241 \t Loss: -0.016139361762294645\nTRAIN: \t Epoch: 241 \t Loss: -0.016131264669820667\nTRAIN: \t Epoch: 241 \t Loss: -0.016139195389336065\nTRAIN: \t Epoch: 241 \t Loss: -0.01615027812053742\nVALD: \t Epoch: 241 \t Loss: -0.0004105039988644421\nVALD: \t Epoch: 241 \t Loss: 0.007949154212838039\nVALD: \t Epoch: 241 \t Loss: 0.004020827958205094\nVALD: \t Epoch: 241 \t Loss: 0.020269913933589123\nVALD: \t Epoch: 241 \t Loss: 0.016705034056212754\nVALD: \t Epoch: 241 \t Loss: 0.01659240879569993\n******************************\nEpoch: social-tag : 241\ntrain_loss -0.01615027812053742\nval_loss 0.01659240879569993\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 242 \t Loss: -0.01641201041638851\nTRAIN: \t Epoch: 242 \t Loss: -0.015801114961504936\nTRAIN: \t Epoch: 242 \t Loss: -0.016013380140066147\nTRAIN: \t Epoch: 242 \t Loss: -0.016378854867070913\nTRAIN: \t Epoch: 242 \t Loss: -0.016604897752404212\nTRAIN: \t Epoch: 242 \t Loss: -0.016475487500429153\nTRAIN: \t Epoch: 242 \t Loss: -0.016409501167280332\nTRAIN: \t Epoch: 242 \t Loss: -0.016446519875898957\nTRAIN: \t Epoch: 242 \t Loss: -0.01640705143411954\nTRAIN: \t Epoch: 242 \t Loss: -0.016530086286365986\nTRAIN: \t Epoch: 242 \t Loss: -0.016573793508789757\nTRAIN: \t Epoch: 242 \t Loss: -0.016585606926431257\nTRAIN: \t Epoch: 242 \t Loss: -0.01654707841002024\nTRAIN: \t Epoch: 242 \t Loss: -0.016475595400801728\nTRAIN: \t Epoch: 242 \t Loss: -0.016456857323646545\nTRAIN: \t Epoch: 242 \t Loss: -0.016373090038541704\nTRAIN: \t Epoch: 242 \t Loss: -0.016313990139785933\nTRAIN: \t Epoch: 242 \t Loss: -0.01629116406871213\nTRAIN: \t Epoch: 242 \t Loss: -0.016198239338241126\nTRAIN: \t Epoch: 242 \t Loss: -0.0161590360570699\nTRAIN: \t Epoch: 242 \t Loss: -0.016160198309946628\nTRAIN: \t Epoch: 242 \t Loss: -0.016140551412983043\nVALD: \t Epoch: 242 \t Loss: -0.00355738983489573\nVALD: \t Epoch: 242 \t Loss: 0.0047397654270753264\nVALD: \t Epoch: 242 \t Loss: 0.002323081096013387\nVALD: \t Epoch: 242 \t Loss: 0.02089182659983635\nVALD: \t Epoch: 242 \t Loss: 0.01667029896634631\nVALD: \t Epoch: 242 \t Loss: 0.016328715967635313\n******************************\nEpoch: social-tag : 242\ntrain_loss -0.016140551412983043\nval_loss 0.016328715967635313\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 243 \t Loss: -0.016790645197033882\nTRAIN: \t Epoch: 243 \t Loss: -0.01630034390836954\nTRAIN: \t Epoch: 243 \t Loss: -0.016523870329062145\nTRAIN: \t Epoch: 243 \t Loss: -0.016225098399445415\nTRAIN: \t Epoch: 243 \t Loss: -0.01638882402330637\nTRAIN: \t Epoch: 243 \t Loss: -0.016390940950562555\nTRAIN: \t Epoch: 243 \t Loss: -0.016298951181982244\nTRAIN: \t Epoch: 243 \t Loss: -0.0161675657145679\nTRAIN: \t Epoch: 243 \t Loss: -0.01623329520225525\nTRAIN: \t Epoch: 243 \t Loss: -0.01619496177881956\nTRAIN: \t Epoch: 243 \t Loss: -0.01621714149686423\nTRAIN: \t Epoch: 243 \t Loss: -0.0162294606367747\nTRAIN: \t Epoch: 243 \t Loss: -0.01622701694185917\nTRAIN: \t Epoch: 243 \t Loss: -0.016205066948064735\nTRAIN: \t Epoch: 243 \t Loss: -0.016147577265898386\nTRAIN: \t Epoch: 243 \t Loss: -0.016186284134164453\nTRAIN: \t Epoch: 243 \t Loss: -0.016203288536737945\nTRAIN: \t Epoch: 243 \t Loss: -0.016267450112435553\nTRAIN: \t Epoch: 243 \t Loss: -0.016234969426142543\nTRAIN: \t Epoch: 243 \t Loss: -0.016202972922474145\nTRAIN: \t Epoch: 243 \t Loss: -0.016168428984071528\nTRAIN: \t Epoch: 243 \t Loss: -0.016161460893586464\nVALD: \t Epoch: 243 \t Loss: -0.006079927086830139\nVALD: \t Epoch: 243 \t Loss: 0.0006650683935731649\nVALD: \t Epoch: 243 \t Loss: -0.0014853302078942459\nVALD: \t Epoch: 243 \t Loss: 0.01301048242021352\nVALD: \t Epoch: 243 \t Loss: 0.009780671680346131\nVALD: \t Epoch: 243 \t Loss: 0.00959987397672552\n******************************\nEpoch: social-tag : 243\ntrain_loss -0.016161460893586464\nval_loss 0.00959987397672552\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 244 \t Loss: -0.016470322385430336\nTRAIN: \t Epoch: 244 \t Loss: -0.016418863087892532\nTRAIN: \t Epoch: 244 \t Loss: -0.016480866819620132\nTRAIN: \t Epoch: 244 \t Loss: -0.016366988886147738\nTRAIN: \t Epoch: 244 \t Loss: -0.016514111310243607\nTRAIN: \t Epoch: 244 \t Loss: -0.016370114249487717\nTRAIN: \t Epoch: 244 \t Loss: -0.016074972094169686\nTRAIN: \t Epoch: 244 \t Loss: -0.016044091316871345\nTRAIN: \t Epoch: 244 \t Loss: -0.01598742242074675\nTRAIN: \t Epoch: 244 \t Loss: -0.016063686180859803\nTRAIN: \t Epoch: 244 \t Loss: -0.015863990072499622\nTRAIN: \t Epoch: 244 \t Loss: -0.015912439053257305\nTRAIN: \t Epoch: 244 \t Loss: -0.015943625941872597\nTRAIN: \t Epoch: 244 \t Loss: -0.015938098542392254\nTRAIN: \t Epoch: 244 \t Loss: -0.016006880750258762\nTRAIN: \t Epoch: 244 \t Loss: -0.016007843310944736\nTRAIN: \t Epoch: 244 \t Loss: -0.015991750447189108\nTRAIN: \t Epoch: 244 \t Loss: -0.01598071079287264\nTRAIN: \t Epoch: 244 \t Loss: -0.016010745183417673\nTRAIN: \t Epoch: 244 \t Loss: -0.016128834150731564\nTRAIN: \t Epoch: 244 \t Loss: -0.016136982788642246\nTRAIN: \t Epoch: 244 \t Loss: -0.01615553287458163\nVALD: \t Epoch: 244 \t Loss: -0.008504181168973446\nVALD: \t Epoch: 244 \t Loss: -0.003643151721917093\nVALD: \t Epoch: 244 \t Loss: -0.004066745517775416\nVALD: \t Epoch: 244 \t Loss: 0.007440698274876922\nVALD: \t Epoch: 244 \t Loss: 0.004762610094621778\nVALD: \t Epoch: 244 \t Loss: 0.004605900771407919\n******************************\nEpoch: social-tag : 244\ntrain_loss -0.01615553287458163\nval_loss 0.004605900771407919\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 245 \t Loss: -0.016736647114157677\nTRAIN: \t Epoch: 245 \t Loss: -0.016254710033535957\nTRAIN: \t Epoch: 245 \t Loss: -0.01596278293679158\nTRAIN: \t Epoch: 245 \t Loss: -0.016060279915109277\nTRAIN: \t Epoch: 245 \t Loss: -0.015903038159012794\nTRAIN: \t Epoch: 245 \t Loss: -0.016096155469616253\nTRAIN: \t Epoch: 245 \t Loss: -0.016022787695484503\nTRAIN: \t Epoch: 245 \t Loss: -0.01597286039032042\nTRAIN: \t Epoch: 245 \t Loss: -0.01609021404551135\nTRAIN: \t Epoch: 245 \t Loss: -0.016062676534056665\nTRAIN: \t Epoch: 245 \t Loss: -0.016013788906010715\nTRAIN: \t Epoch: 245 \t Loss: -0.01600012679894765\nTRAIN: \t Epoch: 245 \t Loss: -0.016035176097200468\nTRAIN: \t Epoch: 245 \t Loss: -0.01612652838230133\nTRAIN: \t Epoch: 245 \t Loss: -0.016121638442079225\nTRAIN: \t Epoch: 245 \t Loss: -0.016063328774180263\nTRAIN: \t Epoch: 245 \t Loss: -0.016083260285941994\nTRAIN: \t Epoch: 245 \t Loss: -0.01607787117568983\nTRAIN: \t Epoch: 245 \t Loss: -0.01607732754200697\nTRAIN: \t Epoch: 245 \t Loss: -0.016092695528641343\nTRAIN: \t Epoch: 245 \t Loss: -0.016093968236375423\nTRAIN: \t Epoch: 245 \t Loss: -0.016109411207092944\nVALD: \t Epoch: 245 \t Loss: 0.016875624656677246\nVALD: \t Epoch: 245 \t Loss: 0.022199684754014015\nVALD: \t Epoch: 245 \t Loss: 0.015443577508752545\nVALD: \t Epoch: 245 \t Loss: 0.028342196077574044\nVALD: \t Epoch: 245 \t Loss: 0.023834739765152334\nVALD: \t Epoch: 245 \t Loss: 0.02352585101669485\n******************************\nEpoch: social-tag : 245\ntrain_loss -0.016109411207092944\nval_loss 0.02352585101669485\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 246 \t Loss: -0.01575646363198757\nTRAIN: \t Epoch: 246 \t Loss: -0.015975316986441612\nTRAIN: \t Epoch: 246 \t Loss: -0.01640681487818559\nTRAIN: \t Epoch: 246 \t Loss: -0.016198124969378114\nTRAIN: \t Epoch: 246 \t Loss: -0.016160816140472888\nTRAIN: \t Epoch: 246 \t Loss: -0.016205077214787405\nTRAIN: \t Epoch: 246 \t Loss: -0.016155095105724677\nTRAIN: \t Epoch: 246 \t Loss: -0.01616430247668177\nTRAIN: \t Epoch: 246 \t Loss: -0.016035272946788207\nTRAIN: \t Epoch: 246 \t Loss: -0.01608417434617877\nTRAIN: \t Epoch: 246 \t Loss: -0.01604432248595086\nTRAIN: \t Epoch: 246 \t Loss: -0.01610773561211924\nTRAIN: \t Epoch: 246 \t Loss: -0.016035358134943705\nTRAIN: \t Epoch: 246 \t Loss: -0.01615635644910591\nTRAIN: \t Epoch: 246 \t Loss: -0.016125043543676536\nTRAIN: \t Epoch: 246 \t Loss: -0.016124168352689594\nTRAIN: \t Epoch: 246 \t Loss: -0.016115836853928426\nTRAIN: \t Epoch: 246 \t Loss: -0.01613812826366888\nTRAIN: \t Epoch: 246 \t Loss: -0.016148506565705725\nTRAIN: \t Epoch: 246 \t Loss: -0.016190536366775633\nTRAIN: \t Epoch: 246 \t Loss: -0.016183596414824326\nTRAIN: \t Epoch: 246 \t Loss: -0.016168436670217737\nVALD: \t Epoch: 246 \t Loss: -0.0070030223578214645\nVALD: \t Epoch: 246 \t Loss: -8.145719766616821e-05\nVALD: \t Epoch: 246 \t Loss: -0.0010698650342722733\nVALD: \t Epoch: 246 \t Loss: 0.009318291093222797\nVALD: \t Epoch: 246 \t Loss: 0.0065712624229490755\nVALD: \t Epoch: 246 \t Loss: 0.006272556971419941\n******************************\nEpoch: social-tag : 246\ntrain_loss -0.016168436670217737\nval_loss 0.006272556971419941\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 247 \t Loss: -0.015437886118888855\nTRAIN: \t Epoch: 247 \t Loss: -0.015533892437815666\nTRAIN: \t Epoch: 247 \t Loss: -0.01611499736706416\nTRAIN: \t Epoch: 247 \t Loss: -0.016299032606184483\nTRAIN: \t Epoch: 247 \t Loss: -0.01627747677266598\nTRAIN: \t Epoch: 247 \t Loss: -0.01602890606348713\nTRAIN: \t Epoch: 247 \t Loss: -0.01594492793083191\nTRAIN: \t Epoch: 247 \t Loss: -0.015907492488622665\nTRAIN: \t Epoch: 247 \t Loss: -0.015948877566390567\nTRAIN: \t Epoch: 247 \t Loss: -0.016030969470739363\nTRAIN: \t Epoch: 247 \t Loss: -0.016084964641115883\nTRAIN: \t Epoch: 247 \t Loss: -0.016162275491903227\nTRAIN: \t Epoch: 247 \t Loss: -0.016189811751246452\nTRAIN: \t Epoch: 247 \t Loss: -0.016262229665049484\nTRAIN: \t Epoch: 247 \t Loss: -0.01615590254465739\nTRAIN: \t Epoch: 247 \t Loss: -0.016119318723212928\nTRAIN: \t Epoch: 247 \t Loss: -0.016133585999555448\nTRAIN: \t Epoch: 247 \t Loss: -0.01610941279472576\nTRAIN: \t Epoch: 247 \t Loss: -0.01608818646912512\nTRAIN: \t Epoch: 247 \t Loss: -0.016186152352020144\nTRAIN: \t Epoch: 247 \t Loss: -0.016186232589894815\nTRAIN: \t Epoch: 247 \t Loss: -0.016186993854050147\nVALD: \t Epoch: 247 \t Loss: 0.0010742248268797994\nVALD: \t Epoch: 247 \t Loss: 0.007829490175936371\nVALD: \t Epoch: 247 \t Loss: 0.004761708822722237\nVALD: \t Epoch: 247 \t Loss: 0.023321947490330786\nVALD: \t Epoch: 247 \t Loss: 0.018691446699085644\nVALD: \t Epoch: 247 \t Loss: 0.018257327792658047\n******************************\nEpoch: social-tag : 247\ntrain_loss -0.016186993854050147\nval_loss 0.018257327792658047\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 248 \t Loss: -0.016038186848163605\nTRAIN: \t Epoch: 248 \t Loss: -0.0160305667668581\nTRAIN: \t Epoch: 248 \t Loss: -0.016313950220743816\nTRAIN: \t Epoch: 248 \t Loss: -0.01645187009125948\nTRAIN: \t Epoch: 248 \t Loss: -0.016439184546470642\nTRAIN: \t Epoch: 248 \t Loss: -0.016509103278319042\nTRAIN: \t Epoch: 248 \t Loss: -0.016289488412439823\nTRAIN: \t Epoch: 248 \t Loss: -0.016079704742878675\nTRAIN: \t Epoch: 248 \t Loss: -0.01611098874774244\nTRAIN: \t Epoch: 248 \t Loss: -0.016116855666041374\nTRAIN: \t Epoch: 248 \t Loss: -0.01620835048908537\nTRAIN: \t Epoch: 248 \t Loss: -0.016207822908957798\nTRAIN: \t Epoch: 248 \t Loss: -0.01629280886397912\nTRAIN: \t Epoch: 248 \t Loss: -0.016332559686686312\nTRAIN: \t Epoch: 248 \t Loss: -0.016303310667475066\nTRAIN: \t Epoch: 248 \t Loss: -0.016203022853005677\nTRAIN: \t Epoch: 248 \t Loss: -0.016188383376335398\nTRAIN: \t Epoch: 248 \t Loss: -0.016166919169740543\nTRAIN: \t Epoch: 248 \t Loss: -0.01619096605205222\nTRAIN: \t Epoch: 248 \t Loss: -0.016174711706116795\nTRAIN: \t Epoch: 248 \t Loss: -0.016189410439914183\nTRAIN: \t Epoch: 248 \t Loss: -0.01620618697770836\nVALD: \t Epoch: 248 \t Loss: -0.0018784970743581653\nVALD: \t Epoch: 248 \t Loss: 0.0029255252447910607\nVALD: \t Epoch: 248 \t Loss: 0.001216903212480247\nVALD: \t Epoch: 248 \t Loss: 0.01959518707008101\nVALD: \t Epoch: 248 \t Loss: 0.015585141110932454\nVALD: \t Epoch: 248 \t Loss: 0.015367174346112844\n******************************\nEpoch: social-tag : 248\ntrain_loss -0.01620618697770836\nval_loss 0.015367174346112844\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\nTRAIN: \t Epoch: 249 \t Loss: -0.015329650603234768\nTRAIN: \t Epoch: 249 \t Loss: -0.01590872136875987\nTRAIN: \t Epoch: 249 \t Loss: -0.016342524128655594\nTRAIN: \t Epoch: 249 \t Loss: -0.016181864077225327\nTRAIN: \t Epoch: 249 \t Loss: -0.016153969801962376\nTRAIN: \t Epoch: 249 \t Loss: -0.016247169269869726\nTRAIN: \t Epoch: 249 \t Loss: -0.01637280373168843\nTRAIN: \t Epoch: 249 \t Loss: -0.016297357506118715\nTRAIN: \t Epoch: 249 \t Loss: -0.016415916693707306\nTRAIN: \t Epoch: 249 \t Loss: -0.016466901544481514\nTRAIN: \t Epoch: 249 \t Loss: -0.016388355161656033\nTRAIN: \t Epoch: 249 \t Loss: -0.01636829786002636\nTRAIN: \t Epoch: 249 \t Loss: -0.016417687042401388\nTRAIN: \t Epoch: 249 \t Loss: -0.016351889885429825\nTRAIN: \t Epoch: 249 \t Loss: -0.01630104867120584\nTRAIN: \t Epoch: 249 \t Loss: -0.016278924187645316\nTRAIN: \t Epoch: 249 \t Loss: -0.016289626083829823\nTRAIN: \t Epoch: 249 \t Loss: -0.01625198544934392\nTRAIN: \t Epoch: 249 \t Loss: -0.01626087318321592\nTRAIN: \t Epoch: 249 \t Loss: -0.016274209832772613\nTRAIN: \t Epoch: 249 \t Loss: -0.016229846470412753\nTRAIN: \t Epoch: 249 \t Loss: -0.01619713306427002\nVALD: \t Epoch: 249 \t Loss: 0.002312992699444294\nVALD: \t Epoch: 249 \t Loss: 0.012255531270056963\nVALD: \t Epoch: 249 \t Loss: 0.0087195944506675\nVALD: \t Epoch: 249 \t Loss: 0.024243886524345726\nVALD: \t Epoch: 249 \t Loss: 0.019630100997164845\nVALD: \t Epoch: 249 \t Loss: 0.019060560107005364\n******************************\nEpoch: social-tag : 249\ntrain_loss -0.01619713306427002\nval_loss 0.019060560107005364\n{'min_val_epoch': 140, 'min_val_loss': -0.008992135016755625}\n******************************\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = 2 + 2"
      ],
      "metadata": {
        "id": "8RcVcb-JfLwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testowanie\n",
        "\n",
        "Jak działa poniższy kod?\n",
        "\n",
        "**Funkcja testowania:** Zdefiniowana funkcja test(KSTEPS=20) przeprowadza walidację modelu. Używa ona załadowanego modelu do przewidywania trajektorii i obliczania metryk ADE (Average Displacement Error) i FDE (Final Displacement Error) dla wielu próbek trajektorii.\n",
        "\n",
        "**Przygotowanie danych:** Dane testowe są ładowane za pomocą TrajectoryDataset i DataLoader. Dane te są następnie przekazywane do modelu w partii.\n",
        "\n",
        "**Przewidywanie trajektorii:** Model wykonuje przewidywanie trajektorii (V_pred) na podstawie obserwowanych trajektorii (V_obs). Przewidywania są reprezentowane jako rozkłady wielowymiarowe z parametrami określonymi przez model.\n",
        "\n",
        "**Próbkowanie i obliczanie metryk:** Dla każdej próbki trajektorii, skrypt próbkuje KSTEPS przewidywanych trajektorii z modelu, a następnie oblicza ADE i FDE dla każdej próbki. Wyniki są zapisywane w słowniku raw_data_dict.\n",
        "\n",
        "**Obliczanie średnich metryk:** Po przetestowaniu wszystkich partii danych, skrypt oblicza średni ADE i FDE dla wszystkich obiektów.\n",
        "\n",
        "**Iteracja po eksperymentach:** Skrypt iteruje przez różne zapisane modele (znajdujące się w katalogu ./checkpoint/), ładuje każdy z nich i przeprowadza testy, zbierając wyniki ADE i FDE.\n",
        "Wyświetlanie wyników: Na koniec skrypt wyświetla średni ADE i FDE dla wszystkich przetestowanych modeli.\n"
      ],
      "metadata": {
        "id": "g6uuPbh_fLwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import pickle\n",
        "import argparse\n",
        "import glob\n",
        "import torch.distributions.multivariate_normal as torchdist\n",
        "import copy\n",
        "\n",
        "def test(model,loader_test, KSTEPS=20):\n",
        "    model.eval()\n",
        "    ade_bigls = []\n",
        "    fde_bigls = []\n",
        "    raw_data_dict = {}\n",
        "    step =0\n",
        "    for batch in loader_test:\n",
        "        step+=1\n",
        "        #Get data\n",
        "        batch = [tensor for tensor in batch]\n",
        "        obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, non_linear_ped,\\\n",
        "         loss_mask,V_obs,A_obs,A_dir_obs,V_tr,A_tr,A_dir_tr = batch\n",
        "\n",
        "\n",
        "        num_of_objs = obs_traj_rel.shape[1]\n",
        "\n",
        "        #Forward\n",
        "        #V_obs = batch,seq,node,feat\n",
        "        #V_obs_tmp = batch,feat,seq,node\n",
        "        V_obs_tmp =V_obs.permute(0,3,1,2)\n",
        "\n",
        "        V_pred,_,_ = model(V_obs_tmp,A_obs.squeeze(),A_dir_obs.squeeze())\n",
        "        # print(V_pred.shape)\n",
        "        # torch.Size([1, 5, 12, 2])\n",
        "        # torch.Size([12, 2, 5])\n",
        "        V_pred = V_pred.permute(0,2,3,1)\n",
        "        # torch.Size([1, 12, 2, 5])>>seq,node,feat\n",
        "        # V_pred= torch.rand_like(V_tr).cuda()\n",
        "\n",
        "\n",
        "        V_tr = V_tr.squeeze()\n",
        "        A_tr = A_tr.squeeze()\n",
        "        A_dir_tr = A_dir_tr.squeeze()\n",
        "        V_pred = V_pred.squeeze()\n",
        "        num_of_objs = obs_traj_rel.shape[1]\n",
        "        V_pred,V_tr =  V_pred[:,:num_of_objs,:],V_tr[:,:num_of_objs,:]\n",
        "        #print(V_pred.shape)\n",
        "\n",
        "        #For now I have my bi-variate parameters\n",
        "        #normx =  V_pred[:,:,0:1]\n",
        "        #normy =  V_pred[:,:,1:2]\n",
        "        sx = torch.exp(V_pred[:,:,2]) #sx\n",
        "        sy = torch.exp(V_pred[:,:,3]) #sy\n",
        "        corr = torch.tanh(V_pred[:,:,4]) #corr\n",
        "\n",
        "        cov = torch.zeros(V_pred.shape[0],V_pred.shape[1],2,2)\n",
        "        cov[:,:,0,0]= sx*sx\n",
        "        cov[:,:,0,1]= corr*sx*sy\n",
        "        cov[:,:,1,0]= corr*sx*sy\n",
        "        cov[:,:,1,1]= sy*sy\n",
        "        mean = V_pred[:,:,0:2]\n",
        "\n",
        "        mvnormal = torchdist.MultivariateNormal(mean,cov)\n",
        "\n",
        "\n",
        "        ### Rel to abs\n",
        "        ##obs_traj.shape = torch.Size([1, 6, 2, 8]) Batch, Ped ID, x|y, Seq Len\n",
        "\n",
        "        #Now sample 20 samples\n",
        "        ade_ls = {}\n",
        "        fde_ls = {}\n",
        "        V_x = seq_to_nodes(obs_traj.data.cpu().numpy().copy())\n",
        "        V_x_rel_to_abs = nodes_rel_to_nodes_abs(V_obs.data.cpu().numpy().squeeze().copy(),\n",
        "                                                 V_x[0,:,:].copy())\n",
        "\n",
        "        V_y = seq_to_nodes(pred_traj_gt.data.cpu().numpy().copy())\n",
        "        V_y_rel_to_abs = nodes_rel_to_nodes_abs(V_tr.data.cpu().numpy().squeeze().copy(),\n",
        "                                                 V_x[-1,:,:].copy())\n",
        "\n",
        "        raw_data_dict[step] = {}\n",
        "        raw_data_dict[step]['obs'] = copy.deepcopy(V_x_rel_to_abs)\n",
        "        raw_data_dict[step]['trgt'] = copy.deepcopy(V_y_rel_to_abs)\n",
        "        raw_data_dict[step]['pred'] = []\n",
        "\n",
        "        for n in range(num_of_objs):\n",
        "            ade_ls[n]=[]\n",
        "            fde_ls[n]=[]\n",
        "\n",
        "        for k in range(KSTEPS):\n",
        "\n",
        "            V_pred = mvnormal.sample()\n",
        "\n",
        "\n",
        "\n",
        "            #V_pred = seq_to_nodes(pred_traj_gt.data.numpy().copy())\n",
        "            V_pred_rel_to_abs = nodes_rel_to_nodes_abs(V_pred.data.cpu().numpy().squeeze().copy(),\n",
        "                                                     V_x[-1,:,:].copy())\n",
        "            raw_data_dict[step]['pred'].append(copy.deepcopy(V_pred_rel_to_abs))\n",
        "\n",
        "           # print(V_pred_rel_to_abs.shape) #(12, 3, 2) = seq, ped, location\n",
        "            for n in range(num_of_objs):\n",
        "                pred = []\n",
        "                target = []\n",
        "                obsrvs = []\n",
        "                number_of = []\n",
        "                pred.append(V_pred_rel_to_abs[:,n:n+1,:])\n",
        "                target.append(V_y_rel_to_abs[:,n:n+1,:])\n",
        "                obsrvs.append(V_x_rel_to_abs[:,n:n+1,:])\n",
        "                number_of.append(1)\n",
        "\n",
        "                ade_ls[n].append(ade(pred,target,number_of))\n",
        "                fde_ls[n].append(fde(pred,target,number_of))\n",
        "\n",
        "        for n in range(num_of_objs):\n",
        "            ade_bigls.append(min(ade_ls[n]))\n",
        "            fde_bigls.append(min(fde_ls[n]))\n",
        "\n",
        "    ade_ = sum(ade_bigls)/len(ade_bigls)\n",
        "    fde_ = sum(fde_bigls)/len(fde_bigls)\n",
        "    return ade_,fde_,raw_data_dict\n",
        "\n",
        "#check how our new best saved model performs on test dataset\n",
        "######################################################################################################\n",
        "paths = ['/kaggle/working/checkpoint/social-tag']\n",
        "KSTEPS=20\n",
        "\n",
        "print(\"*\"*50)\n",
        "print('Number of samples:',KSTEPS)\n",
        "print(\"*\"*50)\n",
        "\n",
        "for feta in range(len(paths)):\n",
        "    ade_ls = []\n",
        "    fde_ls = []\n",
        "    path = paths[feta]\n",
        "    exps = glob.glob(path)\n",
        "    print('Model being tested are:',exps)\n",
        "\n",
        "    for exp_path in exps:\n",
        "        print(\"*\"*50)\n",
        "        print(\"Evaluating model:\",exp_path)\n",
        "\n",
        "        model_path = exp_path+'/val_best.pth'\n",
        "        args_path = exp_path+'/args.pkl'\n",
        "        with open(args_path,'rb') as f:\n",
        "            args = pickle.load(f)\n",
        "\n",
        "        #Data prep\n",
        "        obs_seq_len = args.obs_seq_len\n",
        "        pred_seq_len = args.pred_seq_len\n",
        "        data_set = '/kaggle/input/dataset/datasets/'+args.dataset+'/'\n",
        "\n",
        "        dset_test = TrajectoryDataset(\n",
        "                data_set+'test/',\n",
        "                obs_len=obs_seq_len,\n",
        "                pred_len=pred_seq_len,\n",
        "                skip=1,norm_lap_matr=True)\n",
        "\n",
        "        loader_test = DataLoader(\n",
        "                dset_test,\n",
        "                batch_size=1,#This is irrelative to the args batch size parameter\n",
        "                shuffle =False,\n",
        "                num_workers=1)\n",
        "\n",
        "\n",
        "\n",
        "        #Defining the model\n",
        "        modelka = social_stgcnn(n_stgcnn =args.n_stgcnn,n_txpcnn=args.n_txpcnn,\n",
        "        output_feat=args.output_size,seq_len=args.obs_seq_len,\n",
        "        kernel_size=args.kernel_size,pred_seq_len=args.pred_seq_len)\n",
        "        modelka.load_state_dict(torch.load(model_path))\n",
        "\n",
        "\n",
        "        ade_ =999999\n",
        "        fde_ =999999\n",
        "        print(\"Testing ....\")\n",
        "        ad,fd,raw_data_dic_= test(modelka,loader_test)\n",
        "        ade_= min(ade_,ad)\n",
        "        fde_ =min(fde_,fd)\n",
        "        ade_ls.append(ade_)\n",
        "        fde_ls.append(fde_)\n",
        "        print(\"ADE:\",ade_,\" FDE:\",fde_)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(\"*\"*50)\n",
        "############################################################################################################################"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-03T22:58:44.588532Z",
          "iopub.execute_input": "2024-06-03T22:58:44.588940Z",
          "iopub.status.idle": "2024-06-03T22:58:49.882018Z",
          "shell.execute_reply.started": "2024-06-03T22:58:44.588906Z",
          "shell.execute_reply": "2024-06-03T22:58:49.880813Z"
        },
        "trusted": true,
        "id": "1crxj_DYfLwH",
        "outputId": "b2e83ef5-f8b5-4c66-f687-fef1731a7dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "**************************************************\nNumber of samples: 20\n**************************************************\nModel being tested are: ['/kaggle/working/checkpoint/social-tag']\n**************************************************\nEvaluating model: /kaggle/working/checkpoint/social-tag\nProcessing Data .....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 70/70 [00:02<00:00, 25.95it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Testing ....\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ADE: 0.7154673519729199  FDE: 1.1690865252039901\n**************************************************\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}